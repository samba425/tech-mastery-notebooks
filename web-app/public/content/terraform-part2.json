{"id":"terraform-part2","title":"ðŸ—ï¸ Terraform Mastery Part 2","content":"# ðŸš€ Terraform Complete Mastery Guide - Part 2\n**Production Ready, Best Practices & Real-World Projects**\n\n> **Continuation from Part 1** - Advanced production concepts, testing, CI/CD, and complete projects  \n> *Days 12-22 + 5 Real-World Projects*\n\n---\n\n## ðŸ“š Table of Contents - Part 2\n\n### **PART 4: PRODUCTION READY**\n- [Day 12: Workspaces](#day-12-workspaces)\n- [Day 13: Remote State](#day-13-remote-state)\n- [Day 14: State Locking](#day-14-state-locking)\n- [Day 15: Import & Migration](#day-15-import--migration)\n- [Day 16: Provisioners](#day-16-provisioners)\n- [Day 17: Lifecycle & Conditions](#day-17-lifecycle--conditions)\n- [Day 18: Terraform Cloud](#day-18-terraform-cloud)\n\n### **PART 5: BEST PRACTICES**\n- [Day 19: Project Structure](#day-19-project-structure)\n- [Day 20: Security Best Practices](#day-20-security-best-practices)\n- [Day 21: Testing Strategies](#day-21-testing-strategies)\n- [Day 22: CI/CD Integration](#day-22-cicd-integration)\n\n### **PART 6: REAL-WORLD PROJECTS**\n- [Project 1: Simple Web Server](#project-1-simple-web-server)\n- [Project 2: VPC & Networking](#project-2-vpc--networking)\n- [Project 3: Multi-Tier Application](#project-3-multi-tier-application)\n- [Project 4: EKS Kubernetes Cluster](#project-4-eks-kubernetes-cluster)\n- [Project 5: Multi-Cloud Deployment](#project-5-multi-cloud-deployment)\n\n### **BONUS CONTENT**\n- [Advanced State Management](#advanced-state-management)\n- [Troubleshooting Guide](#troubleshooting-guide)\n- [Performance Optimization](#performance-optimization)\n- [Certification Prep](#certification-preparation)\n- [Cheat Sheets](#cheat-sheets)\n\n---\n\n## Day 12: Workspaces - Managing Multiple Environments\n\n### ðŸŽ¯ Learning Objectives\n- Understand Terraform workspaces\n- Manage multiple environments\n- Use workspace-specific configurations\n- Know when to use workspaces vs modules\n\n---\n\n### ðŸŒ What are Workspaces?\n\n**Workspace** = Named containers for Terraform state\n\nThink of workspaces like **Git branches** for your infrastructure:\n- Each workspace has its **own state file**\n- Same code, different state\n- Useful for dev/staging/prod environments\n\n```plaintext\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     Terraform Configuration         â”‚\nâ”‚         (same code)                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚                 â”‚\n   â”Œâ”€â”€â–¼â”€â”€â”          â”Œâ”€â”€â–¼â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”\n   â”‚ dev â”‚          â”‚stageâ”‚          â”‚ prod â”‚\n   â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”˜\n   State 1          State 2          State 3\n```\n\n---\n\n### ðŸ“‹ Workspace Commands\n\n```bash\n# List workspaces\nterraform workspace list\n# Output:\n# * default\n#   dev\n#   staging\n#   prod\n\n# Show current workspace\nterraform workspace show\n# default\n\n# Create new workspace\nterraform workspace new dev\n# Created and switched to workspace \"dev\"\n\n# Switch workspace\nterraform workspace select staging\n# Switched to workspace \"staging\"\n\n# Delete workspace\nterraform workspace delete dev\n# Deleted workspace \"dev\"\n```\n\n---\n\n### ðŸŽ¨ Basic Workspace Usage\n\n```hcl\n# File: main.tf\n\nterraform {\n  required_version = \">= 1.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n\n# Use workspace name in resources\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = var.instance_type\n  \n  tags = {\n    Name        = \"web-server-${terraform.workspace}\"\n    Environment = terraform.workspace\n  }\n}\n\n# Workspace-specific configuration\nlocals {\n  environment = terraform.workspace\n  \n  # Different settings per workspace\n  instance_count = {\n    default = 1\n    dev     = 1\n    staging = 2\n    prod    = 5\n  }\n  \n  instance_type = {\n    default = \"t2.micro\"\n    dev     = \"t2.micro\"\n    staging = \"t2.small\"\n    prod    = \"t3.large\"\n  }\n}\n\nresource \"aws_instance\" \"app\" {\n  count = local.instance_count[local.environment]\n  \n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = local.instance_type[local.environment]\n  \n  tags = {\n    Name = \"${terraform.workspace}-app-${count.index + 1}\"\n  }\n}\n\n# File: variables.tf\n\nvariable \"region\" {\n  type    = string\n  default = \"us-east-1\"\n}\n\nvariable \"instance_type\" {\n  type        = string\n  description = \"EC2 instance type\"\n  default     = \"t2.micro\"\n}\n\n# File: outputs.tf\n\noutput \"workspace\" {\n  description = \"Current workspace\"\n  value       = terraform.workspace\n}\n\noutput \"instance_ids\" {\n  description = \"Instance IDs in this workspace\"\n  value       = aws_instance.app[*].id\n}\n```\n\n**Usage:**\n\n```bash\n# Create dev environment\nterraform workspace new dev\nterraform apply\n\n# Create staging environment\nterraform workspace new staging\nterraform apply\n\n# Create production environment\nterraform workspace new prod\nterraform apply\n\n# Switch between environments\nterraform workspace select dev\nterraform plan\n\nterraform workspace select prod\nterraform plan\n```\n\n---\n\n### ðŸŽ¯ Advanced Workspace Patterns\n\n#### **1. Environment-Specific Variables**\n\n```hcl\n# File: variables.tf\n\nvariable \"environment_config\" {\n  type = map(object({\n    instance_count    = number\n    instance_type     = string\n    volume_size       = number\n    enable_monitoring = bool\n    multi_az          = bool\n  }))\n  \n  default = {\n    dev = {\n      instance_count    = 1\n      instance_type     = \"t2.micro\"\n      volume_size       = 20\n      enable_monitoring = false\n      multi_az          = false\n    }\n    staging = {\n      instance_count    = 2\n      instance_type     = \"t2.small\"\n      volume_size       = 30\n      enable_monitoring = false\n      multi_az          = false\n    }\n    prod = {\n      instance_count    = 5\n      instance_type     = \"t3.large\"\n      volume_size       = 50\n      enable_monitoring = true\n      multi_az          = true\n    }\n  }\n}\n\n# File: main.tf\n\nlocals {\n  environment = terraform.workspace\n  config      = var.environment_config[local.environment]\n}\n\nresource \"aws_instance\" \"app\" {\n  count = local.config.instance_count\n  \n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = local.config.instance_type\n  monitoring    = local.config.enable_monitoring\n  \n  root_block_device {\n    volume_size = local.config.volume_size\n    volume_type = \"gp3\"\n  }\n  \n  tags = {\n    Name        = \"${local.environment}-app-${count.index + 1}\"\n    Environment = local.environment\n  }\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"${local.environment}-database\"\n  engine         = \"mysql\"\n  instance_class = local.environment == \"prod\" ? \"db.r5.large\" : \"db.t3.micro\"\n  multi_az       = local.config.multi_az\n  \n  allocated_storage = local.config.volume_size\n  \n  backup_retention_period = local.environment == \"prod\" ? 30 : 7\n  \n  tags = {\n    Environment = local.environment\n  }\n}\n```\n\n---\n\n#### **2. Workspace-Specific Backend Configuration**\n\n```hcl\n# File: backend.tf\n\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"infrastructure/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n    \n    # Workspace states are stored at:\n    # s3://my-terraform-state/infrastructure/env:/dev/terraform.tfstate\n    # s3://my-terraform-state/infrastructure/env:/staging/terraform.tfstate\n    # s3://my-terraform-state/infrastructure/env:/prod/terraform.tfstate\n  }\n}\n```\n\n---\n\n#### **3. Complete Multi-Environment Setup**\n\n```hcl\n# ==================================================\n# File: variables.tf\n# ==================================================\n\nvariable \"project_name\" {\n  type    = string\n  default = \"myapp\"\n}\n\nvariable \"region\" {\n  type    = string\n  default = \"us-east-1\"\n}\n\nvariable \"vpc_config\" {\n  type = map(object({\n    cidr_block          = string\n    availability_zones  = list(string)\n    public_subnet_cidrs = list(string)\n  }))\n  \n  default = {\n    dev = {\n      cidr_block          = \"10.0.0.0/16\"\n      availability_zones  = [\"us-east-1a\"]\n      public_subnet_cidrs = [\"10.0.1.0/24\"]\n    }\n    staging = {\n      cidr_block          = \"10.1.0.0/16\"\n      availability_zones  = [\"us-east-1a\", \"us-east-1b\"]\n      public_subnet_cidrs = [\"10.1.1.0/24\", \"10.1.2.0/24\"]\n    }\n    prod = {\n      cidr_block          = \"10.2.0.0/16\"\n      availability_zones  = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n      public_subnet_cidrs = [\"10.2.1.0/24\", \"10.2.2.0/24\", \"10.2.3.0/24\"]\n    }\n  }\n}\n\n# ==================================================\n# File: main.tf\n# ==================================================\n\nlocals {\n  environment = terraform.workspace == \"default\" ? \"dev\" : terraform.workspace\n  vpc_config  = var.vpc_config[local.environment]\n  \n  common_tags = {\n    Project     = var.project_name\n    Environment = local.environment\n    Workspace   = terraform.workspace\n    ManagedBy   = \"Terraform\"\n  }\n}\n\n# VPC\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = local.vpc_config.cidr_block\n  enable_dns_hostnames = true\n  \n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-${local.environment}-vpc\"\n    }\n  )\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  \n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-${local.environment}-igw\"\n    }\n  )\n}\n\n# Public Subnets\nresource \"aws_subnet\" \"public\" {\n  count = length(local.vpc_config.public_subnet_cidrs)\n  \n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = local.vpc_config.public_subnet_cidrs[count.index]\n  availability_zone       = local.vpc_config.availability_zones[count.index]\n  map_public_ip_on_launch = true\n  \n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-${local.environment}-public-${count.index + 1}\"\n    }\n  )\n}\n\n# Route Table\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  \n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n  \n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"${var.project_name}-${local.environment}-public-rt\"\n    }\n  )\n}\n\n# Route Table Associations\nresource \"aws_route_table_association\" \"public\" {\n  count = length(aws_subnet.public)\n  \n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = aws_route_table.public.id\n}\n\n# ==================================================\n# File: outputs.tf\n# ==================================================\n\noutput \"workspace\" {\n  description = \"Current Terraform workspace\"\n  value       = terraform.workspace\n}\n\noutput \"environment\" {\n  description = \"Environment name\"\n  value       = local.environment\n}\n\noutput \"vpc_id\" {\n  description = \"VPC ID\"\n  value       = aws_vpc.main.id\n}\n\noutput \"vpc_cidr\" {\n  description = \"VPC CIDR block\"\n  value       = aws_vpc.main.cidr_block\n}\n\noutput \"subnet_ids\" {\n  description = \"Subnet IDs\"\n  value       = aws_subnet.public[*].id\n}\n\noutput \"resource_names\" {\n  description = \"Names of created resources\"\n  value = {\n    vpc     = aws_vpc.main.tags[\"Name\"]\n    subnets = aws_subnet.public[*].tags[\"Name\"]\n  }\n}\n```\n\n**Deploy to multiple environments:**\n\n```bash\n# Development\nterraform workspace new dev\nterraform apply -auto-approve\n\n# Staging\nterraform workspace new staging\nterraform apply -auto-approve\n\n# Production\nterraform workspace new prod\nterraform apply -auto-approve\n\n# View resources per environment\nterraform workspace select dev\nterraform output\n\nterraform workspace select staging\nterraform output\n\nterraform workspace select prod\nterraform output\n\n# Destroy specific environment\nterraform workspace select dev\nterraform destroy\n\n# List all workspaces\nterraform workspace list\n#   default\n# * dev\n#   staging\n#   prod\n```\n\n---\n\n### âš–ï¸ Workspaces vs Separate Directories\n\n#### **When to Use Workspaces:**\n\nâœ… **Use workspaces when:**\n- Same infrastructure, different environments\n- Small to medium projects\n- Environments have similar configurations\n- Quick environment switching needed\n- Testing changes before production\n\n#### **When to Use Separate Directories:**\n\nâœ… **Use separate directories when:**\n- Environments have different architectures\n- Different teams manage different environments\n- Need different backend configurations\n- Large, complex projects\n- Strict separation required (compliance)\n\n**Example structure with separate directories:**\n\n```\nterraform/\nâ”œâ”€â”€ environments/\nâ”‚   â”œâ”€â”€ dev/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â”œâ”€â”€ terraform.tfvars\nâ”‚   â”‚   â””â”€â”€ backend.tf\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â”œâ”€â”€ terraform.tfvars\nâ”‚   â”‚   â””â”€â”€ backend.tf\nâ”‚   â””â”€â”€ prod/\nâ”‚       â”œâ”€â”€ main.tf\nâ”‚       â”œâ”€â”€ variables.tf\nâ”‚       â”œâ”€â”€ terraform.tfvars\nâ”‚       â””â”€â”€ backend.tf\nâ””â”€â”€ modules/\n    â”œâ”€â”€ vpc/\n    â”œâ”€â”€ compute/\n    â””â”€â”€ database/\n```\n\n---\n\n### ðŸŽ“ Workspace Best Practices\n\n#### **1. Always Check Current Workspace**\n\n```bash\n# Before applying changes\necho \"Current workspace: $(terraform workspace show)\"\nterraform workspace show\n\n# Add to scripts\n#!/bin/bash\nWORKSPACE=$(terraform workspace show)\necho \"You are in workspace: $WORKSPACE\"\nread -p \"Continue? (y/n) \" -n 1 -r\necho\nif [[ ! $REPLY =~ ^[Yy]$ ]]; then\n    exit 1\nfi\nterraform apply\n```\n\n#### **2. Use Consistent Naming**\n\n```hcl\nlocals {\n  environment = terraform.workspace\n  \n  # Consistent resource naming\n  name_prefix = \"${var.project_name}-${local.environment}\"\n  \n  resource_names = {\n    vpc        = \"${local.name_prefix}-vpc\"\n    subnet     = \"${local.name_prefix}-subnet\"\n    instance   = \"${local.name_prefix}-instance\"\n    lb         = \"${local.name_prefix}-lb\"\n    db         = \"${local.name_prefix}-db\"\n  }\n}\n```\n\n#### **3. Protect Production Workspace**\n\n```hcl\n# Add preconditions\nresource \"aws_instance\" \"critical\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t3.large\"\n  \n  lifecycle {\n    precondition {\n      condition     = terraform.workspace == \"prod\"\n      error_message = \"This resource can only be created in production workspace.\"\n    }\n  }\n}\n\n# Or prevent certain actions\nlocals {\n  allowed_to_delete_db = terraform.workspace != \"prod\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier = \"${terraform.workspace}-db\"\n  \n  skip_final_snapshot = local.allowed_to_delete_db\n  \n  lifecycle {\n    prevent_destroy = terraform.workspace == \"prod\"\n  }\n}\n```\n\n#### **4. Document Workspace Usage**\n\n```markdown\n# README.md\n\n## Workspace Management\n\n### Available Workspaces\n- `dev` - Development environment\n- `staging` - Staging environment\n- `prod` - Production environment\n\n### Switching Workspaces\n\n```bash\n# List workspaces\nterraform workspace list\n\n# Switch to dev\nterraform workspace select dev\n\n# Switch to prod\nterraform workspace select prod\n```\n\n### Deploying to Environments\n\n```bash\n# Deploy to dev\nterraform workspace select dev\nterraform apply\n\n# Deploy to staging\nterraform workspace select staging\nterraform apply -var-file=\"staging.tfvars\"\n\n# Deploy to prod (requires approval)\nterraform workspace select prod\nterraform plan -out=prod.tfplan\n# Review plan\nterraform apply prod.tfplan\n```\n\n### State File Locations\n- Dev: `s3://bucket/project/env:/dev/terraform.tfstate`\n- Staging: `s3://bucket/project/env:/staging/terraform.tfstate`\n- Prod: `s3://bucket/project/env:/prod/terraform.tfstate`\n```\n\n---\n\n### âœ… Day 12 Checklist\n\n- [x] Understand Terraform workspaces\n- [x] Workspace commands (list, new, select, delete)\n- [x] Using workspace name in configurations\n- [x] Environment-specific variables\n- [x] Complete multi-environment setup\n- [x] Workspaces vs separate directories\n- [x] Best practices\n\n**Next:** Remote State - Collaboration and state management!\n\n---\n\n## Day 13: Remote State - Collaboration & State Management\n\n### ðŸŽ¯ Learning Objectives\n- Understand remote state backends\n- Configure S3 backend\n- Set up Azure Blob Storage\n- Use Terraform Cloud\n- Implement state locking\n- Enable team collaboration\n\n---\n\n### ðŸ—„ï¸ What is Remote State?\n\nBy default, Terraform stores state **locally** in `terraform.tfstate`.\n\n**Problems with local state:**\n- âŒ Not shared with team\n- âŒ No locking (conflicts)\n- âŒ No backup/versioning\n- âŒ Contains sensitive data in plain text\n- âŒ Single point of failure\n\n**Remote state backend** stores state in a shared location:\n- âœ… Team collaboration\n- âœ… State locking\n- âœ… Encryption\n- âœ… Versioning/backup\n- âœ… Centralized management\n\n---\n\n### ðŸ“¦ Backend Types\n\n| Backend | Use Case | Features |\n|---------|----------|----------|\n| **S3** | AWS environments | Encryption, versioning, locking (with DynamoDB) |\n| **Azure Blob** | Azure environments | Encryption, locking |\n| **GCS** | GCP environments | Encryption, versioning |\n| **Terraform Cloud** | Any environment | Full featured, UI, runs, policies |\n| **HTTP** | Custom solutions | Flexible, requires implementation |\n| **Consul** | HashiCorp stack | High availability, locking |\n| **etcd** | Kubernetes environments | Distributed, consistent |\n\n---\n\n### â˜ï¸ AWS S3 Backend (Most Common)\n\n#### **Prerequisites**\n\n```bash\n# 1. Create S3 bucket\naws s3 mb s3://my-terraform-state --region us-east-1\n\n# 2. Enable versioning\naws s3api put-bucket-versioning \\\n  --bucket my-terraform-state \\\n  --versioning-configuration Status=Enabled\n\n# 3. Enable encryption\naws s3api put-bucket-encryption \\\n  --bucket my-terraform-state \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n\n# 4. Block public access\naws s3api put-public-access-block \\\n  --bucket my-terraform-state \\\n  --public-access-block-configuration \\\n  \"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\"\n\n# 5. Create DynamoDB table for locking\naws dynamodb create-table \\\n  --table-name terraform-state-lock \\\n  --attribute-definitions AttributeName=LockID,AttributeType=S \\\n  --key-schema AttributeName=LockID,KeyType=HASH \\\n  --billing-mode PAY_PER_REQUEST \\\n  --region us-east-1\n```\n\n#### **Terraform Configuration with S3 Backend**\n\n```hcl\n# ==================================================\n# File: backend.tf\n# ==================================================\n\nterraform {\n  required_version = \">= 1.0\"\n  \n  # S3 backend configuration\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"           # S3 bucket name\n    key            = \"project/terraform.tfstate\"    # State file path\n    region         = \"us-east-1\"                    # AWS region\n    encrypt        = true                           # Enable encryption\n    dynamodb_table = \"terraform-state-lock\"         # DynamoDB table for locking\n    \n    # Optional: Use specific profile\n    profile = \"terraform\"\n    \n    # Optional: KMS encryption\n    # kms_key_id = \"arn:aws:kms:us-east-1:ACCOUNT_ID:key/KEY_ID\"\n  }\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n```\n\n**Initialize backend:**\n\n```bash\n# Initialize with backend configuration\nterraform init\n\n# Output:\n# Initializing the backend...\n# Successfully configured the backend \"s3\"!\n# ...\n```\n\n---\n\n#### **Migrate from Local to Remote State**\n\n```bash\n# 1. Start with local state (terraform.tfstate exists locally)\n\n# 2. Add backend configuration to backend.tf (as shown above)\n\n# 3. Initialize backend\nterraform init\n\n# Terraform will detect the change and ask:\n# Do you want to copy existing state to the new backend?\n# Enter a value: yes\n\n# State is now in S3!\n\n# 4. Remove local state file (after confirming remote state works)\nrm terraform.tfstate\nrm terraform.tfstate.backup\n```\n\n---\n\n#### **Complete S3 Backend Setup with Terraform**\n\n```hcl\n# ==================================================\n# File: backend-setup/main.tf\n# ==================================================\n# Run this FIRST to create the backend infrastructure\n\nterraform {\n  required_version = \">= 1.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n\nvariable \"region\" {\n  default = \"us-east-1\"\n}\n\nvariable \"state_bucket_name\" {\n  description = \"Name of S3 bucket for Terraform state\"\n  type        = string\n}\n\nvariable \"dynamodb_table_name\" {\n  description = \"Name of DynamoDB table for state locking\"\n  type        = string\n  default     = \"terraform-state-lock\"\n}\n\n# S3 Bucket for state\nresource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = var.state_bucket_name\n  \n  tags = {\n    Name        = \"Terraform State Bucket\"\n    Purpose     = \"Terraform State Storage\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n\n# Enable versioning\nresource \"aws_s3_bucket_versioning\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\n# Enable encryption\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\n# Block public access\nresource \"aws_s3_bucket_public_access_block\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n\n# Bucket lifecycle policy\nresource \"aws_s3_bucket_lifecycle_configuration\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  rule {\n    id     = \"delete-old-versions\"\n    status = \"Enabled\"\n    \n    noncurrent_version_expiration {\n      noncurrent_days = 90\n    }\n  }\n}\n\n# DynamoDB table for state locking\nresource \"aws_dynamodb_table\" \"terraform_lock\" {\n  name           = var.dynamodb_table_name\n  billing_mode   = \"PAY_PER_REQUEST\"\n  hash_key       = \"LockID\"\n  \n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n  \n  tags = {\n    Name      = \"Terraform State Lock Table\"\n    Purpose   = \"State Locking\"\n    ManagedBy = \"Terraform\"\n  }\n}\n\n# Outputs\noutput \"s3_bucket_name\" {\n  description = \"Name of S3 bucket for state\"\n  value       = aws_s3_bucket.terraform_state.id\n}\n\noutput \"s3_bucket_arn\" {\n  description = \"ARN of S3 bucket\"\n  value       = aws_s3_bucket.terraform_state.arn\n}\n\noutput \"dynamodb_table_name\" {\n  description = \"Name of DynamoDB table\"\n  value       = aws_dynamodb_table.terraform_lock.name\n}\n\noutput \"backend_config\" {\n  description = \"Backend configuration for other projects\"\n  value = <<-EOT\n    terraform {\n      backend \"s3\" {\n        bucket         = \"${aws_s3_bucket.terraform_state.id}\"\n        key            = \"PROJECT_NAME/terraform.tfstate\"\n        region         = \"${var.region}\"\n        encrypt        = true\n        dynamodb_table = \"${aws_dynamodb_table.terraform_lock.name}\"\n      }\n    }\n  EOT\n}\n```\n\n**Deploy backend infrastructure:**\n\n```bash\n# Go to backend-setup directory\ncd backend-setup\n\n# Create terraform.tfvars\ncat > terraform.tfvars <<EOF\nregion             = \"us-east-1\"\nstate_bucket_name  = \"my-company-terraform-state\"\ndynamodb_table_name = \"terraform-state-lock\"\nEOF\n\n# Deploy\nterraform init\nterraform apply\n\n# Copy the backend configuration\nterraform output backend_config\n```\n\n---\n\n### ðŸ” State Locking\n\nState locking **prevents** concurrent modifications:\n\n```plaintext\nDeveloper A starts: terraform apply\n  â”œâ”€ Acquires lock on DynamoDB\n  â”œâ”€ Makes changes\n  â””â”€ Releases lock\n\nDeveloper B tries: terraform apply (while A is working)\n  â””â”€ âŒ Error: state is locked!\n```\n\n**Force unlock (use carefully!):**\n\n```bash\n# If Terraform crashes and lock isn't released\nterraform force-unlock LOCK_ID\n\n# Get lock ID from error message:\n# Error: Error acquiring the state lock\n# Lock Info:\n#   ID:        abc123def456\n#   ...\n```\n\n---\n\n### ðŸŒ Azure Blob Storage Backend\n\n#### **Prerequisites**\n\n```bash\n# 1. Create storage account\naz storage account create \\\n  --name mystorageaccount \\\n  --resource-group myresourcegroup \\\n  --location eastus \\\n  --sku Standard_LRS \\\n  --encryption-services blob\n\n# 2. Create container\naz storage container create \\\n  --name terraform-state \\\n  --account-name mystorageaccount\n```\n\n#### **Backend Configuration**\n\n```hcl\n# File: backend.tf\n\nterraform {\n  backend \"azurerm\" {\n    resource_group_name  = \"myresourcegroup\"\n    storage_account_name = \"mystorageaccount\"\n    container_name       = \"terraform-state\"\n    key                  = \"project.terraform.tfstate\"\n    \n    # Authentication options:\n    \n    # Option 1: Service Principal\n    # client_id       = \"00000000-0000-0000-0000-000000000000\"\n    # client_secret   = \"secret\"\n    # tenant_id       = \"00000000-0000-0000-0000-000000000000\"\n    # subscription_id = \"00000000-0000-0000-0000-000000000000\"\n    \n    # Option 2: Managed Identity\n    # use_msi = true\n    \n    # Option 3: Azure CLI (default)\n    # Automatically uses Azure CLI credentials\n  }\n}\n```\n\n---\n\n### â˜ï¸ Google Cloud Storage Backend\n\n```hcl\n# File: backend.tf\n\nterraform {\n  backend \"gcs\" {\n    bucket  = \"my-terraform-state\"\n    prefix  = \"terraform/state\"\n    \n    # Optional: encryption key\n    # encryption_key = \"base64-encoded-key\"\n    \n    # Authentication via GOOGLE_APPLICATION_CREDENTIALS env var\n    # or Application Default Credentials\n  }\n}\n```\n\n**Create GCS bucket:**\n\n```bash\n# Create bucket\ngsutil mb -p PROJECT_ID -l us-east1 gs://my-terraform-state\n\n# Enable versioning\ngsutil versioning set on gs://my-terraform-state\n\n# Set lifecycle policy to delete old versions\ncat > lifecycle.json <<EOF\n{\n  \"lifecycle\": {\n    \"rule\": [\n      {\n        \"action\": {\"type\": \"Delete\"},\n        \"condition\": {\n          \"numNewerVersions\": 3,\n          \"isLive\": false\n        }\n      }\n    ]\n  }\n}\nEOF\n\ngsutil lifecycle set lifecycle.json gs://my-terraform-state\n```\n\n---\n\n### ðŸ¢ Terraform Cloud Backend\n\n**Benefits:**\n- Free for small teams (up to 5 users)\n- Built-in state storage & locking\n- Web UI for viewing state\n- Terraform runs in cloud\n- Cost estimation\n- Policy as code (Sentinel)\n- Team management\n\n#### **Setup**\n\n```hcl\n# File: backend.tf\n\nterraform {\n  cloud {\n    organization = \"my-company\"\n    \n    workspaces {\n      name = \"my-app-production\"\n      \n      # Or use tags for multiple workspaces\n      # tags = [\"app:myapp\", \"env:prod\"]\n    }\n  }\n}\n```\n\n**Login and Initialize:**\n\n```bash\n# Login to Terraform Cloud\nterraform login\n\n# Initialize\nterraform init\n\n# Run (executes remotely)\nterraform apply\n```\n\n---\n\n### ðŸ”„ Working with Remote State\n\n#### **Viewing Remote State**\n\n```bash\n# Show state\nterraform show\n\n# List resources\nterraform state list\n\n# Show specific resource\nterraform state show aws_instance.web\n\n# Pull state to local file\nterraform state pull > terraform.tfstate.json\n\n# View in JSON\ncat terraform.tfstate.json | jq\n```\n\n#### **Refreshing Remote State**\n\n```bash\n# Refresh state from real infrastructure\nterraform refresh\n\n# Or\nterraform apply -refresh-only\n```\n\n#### **Backing Up Remote State**\n\n```bash\n# Pull and save state regularly\nterraform state pull > backups/terraform-$(date +%Y%m%d-%H%M%S).tfstate\n\n# Automated backup script\n#!/bin/bash\nBACKUP_DIR=\"state-backups\"\nmkdir -p $BACKUP_DIR\nDATE=$(date +%Y%m%d-%H%M%S)\nterraform state pull > \"$BACKUP_DIR/terraform-$DATE.tfstate\"\necho \"State backed up to $BACKUP_DIR/terraform-$DATE.tfstate\"\n```\n\n---\n\n### ðŸŽ“ Complete Multi-Environment Example with Remote State\n\n```\nproject/\nâ”œâ”€â”€ backend-setup/           # Set up backend infrastructure once\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ”œâ”€â”€ environments/\nâ”‚   â”œâ”€â”€ dev/\nâ”‚   â”‚   â”œâ”€â”€ backend.tf      # Points to s3://bucket/dev/terraform.tfstate\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â””â”€â”€ terraform.tfvars\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ backend.tf      # Points to s3://bucket/staging/terraform.tfstate\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â””â”€â”€ terraform.tfvars\nâ”‚   â””â”€â”€ prod/\nâ”‚       â”œâ”€â”€ backend.tf      # Points to s3://bucket/prod/terraform.tfstate\nâ”‚       â”œâ”€â”€ main.tf\nâ”‚       â””â”€â”€ terraform.tfvars\nâ””â”€â”€ modules/\n    â””â”€â”€ vpc/\n```\n\n**Environment-specific backend:**\n\n```hcl\n# environments/dev/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n\n# environments/staging/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"staging/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n\n# environments/prod/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n```\n\n---\n\n### âš ï¸ Remote State Best Practices\n\n#### **1. Never Commit State Files**\n\n```gitignore\n# .gitignore\n\n# Local state files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\n\n# Variable files that may contain secrets\n*.tfvars\n*.tfvars.json\n\n# CLI configuration files\n.terraformrc\nterraform.rc\n```\n\n#### **2. Enable Encryption**\n\n```hcl\n# S3 with KMS encryption\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"project/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    kms_key_id     = \"arn:aws:kms:us-east-1:ACCOUNT:key/KEY_ID\"\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n```\n\n#### **3. Implement Access Controls**\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-terraform-state\",\n        \"arn:aws:s3:::my-terraform-state/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:DeleteItem\"\n      ],\n      \"Resource\": \"arn:aws:dynamodb:*:*:table/terraform-state-lock\"\n    }\n  ]\n}\n```\n\n#### **4. Enable Versioning**\n\nAlways enable versioning on S3:\n- Recover from accidental deletions\n- Rollback to previous state\n- Audit trail\n\n#### **5. Regular Backups**\n\n```bash\n# Automated backup script\n#!/bin/bash\nset -e\n\nBUCKET=\"my-terraform-state\"\nBACKUP_BUCKET=\"my-terraform-state-backups\"\nDATE=$(date +%Y/%m/%d)\n\n# Sync state bucket to backup bucket\naws s3 sync \\\n  s3://$BUCKET \\\n  s3://$BACKUP_BUCKET/$DATE/ \\\n  --storage-class GLACIER\n  \necho \"Backup completed: s3://$BACKUP_BUCKET/$DATE/\"\n```\n\n---\n\n### âœ… Day 13 Checklist\n\n- [x] Understand remote state backends\n- [x] Configure S3 backend with DynamoDB locking\n- [x] Set up Azure Blob Storage backend\n- [x] Use Google Cloud Storage backend\n- [x] Configure Terraform Cloud\n- [x] Migrate from local to remote state\n- [x] State locking mechanisms\n- [x] Remote state best practices\n- [x] Access controls and encryption\n\n**Next:** State management operations and importing existing infrastructure!\n\n---\n\n## Day 14-15: Advanced State Management & Import\n\n### ðŸŽ¯ Learning Objectives\n- Perform state operations\n- Import existing infrastructure\n- Move resources between states\n- Handle state drift\n- Recover from state issues\n\n---\n\n### ðŸ”§ Terraform State Commands\n\n#### **List Resources**\n\n```bash\n# List all resources in state\nterraform state list\n\n# Output:\n# aws_vpc.main\n# aws_subnet.public[0]\n# aws_subnet.public[1]\n# aws_instance.web[0]\n# aws_instance.web[1]\n\n# Filter by resource type\nterraform state list | grep aws_instance\n```\n\n#### **Show Resource Details**\n\n```bash\n# Show specific resource\nterraform state show aws_instance.web[0]\n\n# Output:\n# resource \"aws_instance\" \"web\" {\n#     ami                          = \"ami-12345\"\n#     instance_type                = \"t2.micro\"\n#     id                           = \"i-1234567890\"\n#     public_ip                    = \"54.123.45.67\"\n#     ...\n# }\n```\n\n#### **Move Resources**\n\n```bash\n# Rename resource\nterraform state mv aws_instance.old_name aws_instance.new_name\n\n# Move to module\nterraform state mv aws_instance.web module.compute.aws_instance.web\n\n# Move from module\nterraform state mv module.compute.aws_instance.web aws_instance.web\n\n# Move between different counts/for_each\nterraform state mv 'aws_instance.web[0]' 'aws_instance.web[\"primary\"]'\n```\n\n#### **Remove from State**\n\n```bash\n# Remove resource from state (doesn't delete real resource)\nterraform state rm aws_instance.web[0]\n\n# Remove module\nterraform state rm module.vpc\n\n# Remove all resources matching pattern\nterraform state list | grep 'aws_instance\\.temp' | xargs -n1 terraform state rm\n```\n\n#### **Replace Provider**\n\n```bash\n# If you change provider configuration\nterraform state replace-provider \\\n  registry.terraform.io/hashicorp/aws \\\n  registry.terraform.io/hashicorp/aws\n```\n\n---\n\n### ðŸ“¥ Importing Existing Infrastructure\n\n**Import** = Bring existing resources under Terraform management\n\n#### **Basic Import**\n\n```hcl\n# 1. Write the resource configuration\nresource \"aws_instance\" \"imported_server\" {\n  ami           = \"ami-12345\"  # Must match actual\n  instance_type = \"t2.micro\"    # Must match actual\n  \n  # Add all required arguments\n}\n```\n\n```bash\n# 2. Import the resource\nterraform import aws_instance.imported_server i-1234567890abcdef0\n\n# 3. Verify\nterraform plan  # Should show no changes if config matches reality\n```\n\n---\n\n#### **Import Examples**\n\n**Import EC2 Instance:**\n\n```hcl\nresource \"aws_instance\" \"existing\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t2.micro\"\n  \n  tags = {\n    Name = \"Existing Server\"\n  }\n}\n```\n\n```bash\nterraform import aws_instance.existing i-0123456789abcdef0\n```\n\n**Import VPC:**\n\n```hcl\nresource \"aws_vpc\" \"existing\" {\n  cidr_block = \"10.0.0.0/16\"\n  \n  tags = {\n    Name = \"Existing VPC\"\n  }\n}\n```\n\n```bash\nterraform import aws_vpc.existing vpc-0123456789abcdef0\n```\n\n**Import S3 Bucket:**\n\n```hcl\nresource \"aws_s3_bucket\" \"existing\" {\n  bucket = \"my-existing-bucket\"\n}\n```\n\n```bash\nterraform import aws_s3_bucket.existing my-existing-bucket\n```\n\n**Import RDS Instance:**\n\n```hcl\nresource \"aws_db_instance\" \"existing\" {\n  identifier     = \"production-db\"\n  engine         = \"mysql\"\n  engine_version = \"8.0.33\"\n  instance_class = \"db.t3.micro\"\n  \n  # ... other configuration\n}\n```\n\n```bash\nterraform import aws_db_instance.existing production-db\n```\n\n---\n\n#### **Bulk Import Script**\n\n```bash\n#!/bin/bash\n# import-infrastructure.sh\n\nset -e\n\necho \"Importing VPC...\"\nterraform import aws_vpc.main vpc-abc123\n\necho \"Importing Subnets...\"\nterraform import 'aws_subnet.public[0]' subnet-111111\nterraform import 'aws_subnet.public[1]' subnet-222222\nterraform import 'aws_subnet.private[0]' subnet-333333\n\necho \"Importing Security Groups...\"\nterraform import aws_security_group.web sg-web123\nterraform import aws_security_group.db sg-db456\n\necho \"Importing Instances...\"\nterraform import 'aws_instance.web[0]' i-instance1\nterraform import 'aws_instance.web[1]' i-instance2\n\necho \"Import complete!\"\nterraform plan\n```\n\n---\n\n#### **Generate Configuration from Import (Terraform 1.5+)**\n\n```hcl\n# Use import blocks (Terraform 1.5+)\nimport {\n  to = aws_instance.example\n  id = \"i-1234567890abcdef0\"\n}\n\n# Terraform will generate the configuration!\n```\n\n```bash\n# Generate configuration\nterraform plan -generate-config-out=generated.tf\n\n# Review generated.tf\ncat generated.tf\n\n# Apply\nterraform apply\n```\n\n---\n\n### ðŸ”„ Moving Resources Between States\n\n#### **Scenario: Split Monolithic State**\n\n**Initial setup:**\n\n```\nproject/\nâ””â”€â”€ main.tf  (contains VPC + compute + database)\n```\n\n**Goal:**\n\n```\nproject/\nâ”œâ”€â”€ networking/\nâ”‚   â””â”€â”€ main.tf  (VPC resources)\nâ”œâ”€â”€ compute/\nâ”‚   â””â”€â”€ main.tf  (EC2 instances)\nâ””â”€â”€ database/\n    â””â”€â”€ main.tf  (RDS instances)\n```\n\n**Process:**\n\n```bash\n# 1. Export current state\ncd project\nterraform state pull > full-state.json\n\n# 2. Create new directories with code split\nmkdir networking compute database\n# ... move code to appropriate files ...\n\n# 3. Initialize new states\ncd networking\nterraform init\n\ncd ../compute\nterraform init\n\ncd ../database\nterraform init\n\n# 4. Move resources using terraform state mv with -state and -state-out\n\n# Move VPC to networking\ncd ../project\nterraform state mv \\\n  -state=terraform.tfstate \\\n  -state-out=networking/terraform.tfstate \\\n  aws_vpc.main\n\nterraform state mv \\\n  -state=terraform.tfstate \\\n  -state-out=networking/terraform.tfstate \\\n  'aws_subnet.public[0]'\n\n# Move instances to compute\nterraform state mv \\\n  -state=terraform.tfstate \\\n  -state-out=compute/terraform.tfstate \\\n  'aws_instance.web[0]'\n\n# Move database to database/\nterraform state mv \\\n  -state=terraform.tfstate \\\n  -state-out=database/terraform.tfstate \\\n  aws_db_instance.main\n\n# 5. Verify each directory\ncd networking\nterraform plan  # Should show no changes\n\ncd ../compute\nterraform plan  # Should show no changes\n\ncd ../database\nterraform plan  # Should show no changes\n```\n\n---\n\n### ðŸ” Detecting and Handling State Drift\n\n**Drift** = Real infrastructure differs from Terraform state\n\n#### **Detect Drift**\n\n```bash\n# Refresh state and show changes\nterraform plan -refresh-only\n\n# Or\nterraform apply -refresh-only\n\n# Output will show drift:\n# Note: Objects have changed outside of Terraform\n# \n# Terraform detected the following changes made outside of Terraform:\n#\n#   # aws_instance.web[0] has changed\n#   ~ resource \"aws_instance\" \"web\" {\n#       ~ instance_type = \"t2.micro\" -> \"t2.small\"\n#       # ...\n#     }\n```\n\n#### **Fix Drift**\n\n**Option 1: Update Terraform to match reality**\n\n```hcl\n# Update your Terraform code to match real infrastructure\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.small\"  # Changed to match reality\n}\n```\n\n**Option 2: Fix infrastructure to match Terraform**\n\n```bash\n# Apply to restore desired state\nterraform apply\n\n# This will change instance_type back to t2.micro\n```\n\n**Option 3: Accept drift (use lifecycle ignore)**\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    # Ignore manual changes to instance_type\n    ignore_changes = [instance_type]\n  }\n}\n```\n\n---\n\n### ðŸš¨ State Disasters & Recovery\n\n#### **Lost State File**\n\n**Prevention:**\n- Always use remote state with versioning\n- Regular backups\n- Never commit state to git\n\n**Recovery:**\n\n```bash\n# Option 1: Restore from S3 versioning\naws s3api list-object-versions \\\n  --bucket my-terraform-state \\\n  --prefix project/terraform.tfstate\n\n# Download specific version\naws s3api get-object \\\n  --bucket my-terraform-state \\\n  --key project/terraform.tfstate \\\n  --version-id VERSION_ID \\\n  terraform.tfstate\n\n# Option 2: Recreate by importing all resources\n# Write resource configurations\n# Import each resource one by one\nterraform import aws_vpc.main vpc-123456\nterraform import aws_subnet.public[0] subnet-111111\n# ...\n```\n\n#### **Corrupted State**\n\n```bash\n# Restore from backup\nterraform state pull > corrupted-state.json\n# Fix the JSON manually or restore from backup\n\n# Push fixed state\nterraform state push fixed-state.json\n\n# Or restore S3 version\naws s3api copy-object \\\n  --bucket my-terraform-state \\\n  --copy-source my-terraform-state/project/terraform.tfstate?versionId=GOOD_VERSION \\\n  --key project/terraform.tfstate\n```\n\n#### **Locked State (Orphaned Lock)**\n\n```bash\n# Check error for Lock ID\n# Error: Error acquiring the state lock\n# Lock Info:\n#   ID:        abc123-def456-ghi789\n#   Path:      ...\n#   Operation: OperationTypeApply\n#   Who:       user@hostname\n#   Created:   2025-12-19 10:30:00\n\n# Force unlock\nterraform force-unlock abc123-def456-ghi789\n\n# Or remove lock from DynamoDB\naws dynamodb delete-item \\\n  --table-name terraform-state-lock \\\n  --key '{\"LockID\": {\"S\": \"my-bucket/project/terraform.tfstate\"}}'\n```\n\n---\n\n### ðŸŽ“ Complete Import Workflow Example\n\n**Scenario:** Import existing AWS infrastructure into Terraform\n\n```bash\n# ==================================================\n# Step 1: Discover existing resources\n# ==================================================\n\n# List VPCs\naws ec2 describe-vpcs --output table\n\n# List subnets\naws ec2 describe-subnets --output table\n\n# List instances\naws ec2 describe-instances --output table\n\n# List security groups\naws ec2 describe-security-groups --output table\n\n# ==================================================\n# Step 2: Create Terraform configuration\n# ==================================================\n```\n\n```hcl\n# File: main.tf\n\nterraform {\n  required_version = \">= 1.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n  \n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"imported/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\n# VPC (to be imported)\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n  \n  tags = {\n    Name = \"production-vpc\"\n  }\n}\n\n# Subnets (to be imported)\nresource \"aws_subnet\" \"public_1\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.1.0/24\"\n  availability_zone       = \"us-east-1a\"\n  map_public_ip_on_launch = true\n  \n  tags = {\n    Name = \"public-subnet-1\"\n  }\n}\n\nresource \"aws_subnet\" \"public_2\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.2.0/24\"\n  availability_zone       = \"us-east-1b\"\n  map_public_ip_on_launch = true\n  \n  tags = {\n    Name = \"public-subnet-2\"\n  }\n}\n\n# Security Group (to be imported)\nresource \"aws_security_group\" \"web\" {\n  name        = \"web-sg\"\n  description = \"Security group for web servers\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"web-sg\"\n  }\n}\n\n# Instances (to be imported)\nresource \"aws_instance\" \"web_1\" {\n  ami                    = \"ami-0c55b159cbfafe1f0\"\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public_1.id\n  vpc_security_group_ids = [aws_security_group.web.id]\n  \n  tags = {\n    Name = \"web-server-1\"\n  }\n}\n\nresource \"aws_instance\" \"web_2\" {\n  ami                    = \"ami-0c55b159cbfafe1f0\"\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public_2.id\n  vpc_security_group_ids = [aws_security_group.web.id]\n  \n  tags = {\n    Name = \"web-server-2\"\n  }\n}\n```\n\n```bash\n# ==================================================\n# Step 3: Initialize\n# ==================================================\n\nterraform init\n\n# ==================================================\n# Step 4: Import resources\n# ==================================================\n\n# Create import script\ncat > import.sh <<'EOF'\n#!/bin/bash\nset -e\n\necho \"Importing VPC...\"\nterraform import aws_vpc.main vpc-0a1b2c3d4e5f6g7h8\n\necho \"Importing Subnets...\"\nterraform import aws_subnet.public_1 subnet-111111111111\nterraform import aws_subnet.public_2 subnet-222222222222\n\necho \"Importing Security Group...\"\nterraform import aws_security_group.web sg-web1234567890\n\necho \"Importing Instances...\"\nterraform import aws_instance.web_1 i-0123456789abcdef0\nterraform import aws_instance.web_2 i-0fedcba9876543210\n\necho \"Import complete!\"\nEOF\n\nchmod +x import.sh\n./import.sh\n\n# ==================================================\n# Step 5: Verify and adjust configuration\n# ==================================================\n\nterraform plan\n\n# If plan shows changes, update your Terraform code to match reality\n# Iterate until plan shows no changes\n\n# ==================================================\n# Step 6: Document and commit\n# ==================================================\n\ncat > README.md <<EOF\n# Imported Infrastructure\n\nThis Terraform configuration manages the following imported resources:\n- VPC: vpc-0a1b2c3d4e5f6g7h8\n- Subnets: subnet-111111111111, subnet-222222222222\n- Security Group: sg-web1234567890\n- Instances: i-0123456789abcdef0, i-0fedcba9876543210\n\n## Import Date\n$(date)\n\n## Import Process\nResources were imported from existing AWS infrastructure.\nSee import.sh for the import commands used.\nEOF\n\ngit add .\ngit commit -m \"Import existing infrastructure into Terraform\"\n```\n\n---\n\n### âœ… Day 14-15 Checklist\n\n- [x] State commands (list, show, mv, rm)\n- [x] Importing existing resources\n- [x] Bulk import strategies\n- [x] Moving resources between states\n- [x] Detecting state drift\n- [x] Handling drift (update, fix, ignore)\n- [x] State disaster recovery\n- [x] Force unlocking\n- [x] Complete import workflow\n\n**Next:** Provisioners and post-deployment configuration!\n\n---\n\n## Day 16: Provisioners - Post-Deployment Configuration\n\n### ðŸŽ¯ Learning Objectives\n- Understand provisioners (when to use and avoid)\n- Use local-exec provisioner\n- Use remote-exec provisioner\n- Use file provisioner\n- Handle provisioner failures\n- Know alternatives to provisioners\n\n---\n\n### âš ï¸ Provisioners - Use Sparingly!\n\n**Provisioners** = Run scripts during resource creation/destruction\n\n**âŒ Terraform recommends AVOIDING provisioners when possible:**\n- Break declarative model\n- Create hidden dependencies\n- Harder to maintain\n- Error-prone\n\n**âœ… Better alternatives:**\n- Cloud-init / user_data\n- Configuration management (Ansible, Chef, Puppet)\n- Pre-baked AMIs (Packer)\n- Container images\n\n**When provisioners are acceptable:**\n- Bootstrapping infrastructure\n- Integration with external systems\n- No better alternative exists\n\n---\n\n### ðŸ–¥ï¸ Local-Exec Provisioner\n\nRuns commands **on the machine running Terraform**\n\n```hcl\n# Basic local-exec\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command = \"echo ${self.public_ip} > instance_ip.txt\"\n  }\n}\n```\n\n#### **Use Cases**\n\n```hcl\n# 1. Update DNS after resource creation\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command = <<-EOT\n      aws route53 change-resource-record-sets \\\n        --hosted-zone-id ${var.zone_id} \\\n        --change-batch file://dns-change.json\n    EOT\n  }\n}\n\n# 2. Trigger external API\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command = \"curl -X POST https://api.example.com/notify -d '{\\\"ip\\\":\\\"${self.public_ip}\\\"}'\"\n  }\n}\n\n# 3. Run Ansible playbook\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command = \"ansible-playbook -i '${self.public_ip},' playbook.yml\"\n  }\n}\n\n# 4. Save outputs to file\nresource \"aws_eks_cluster\" \"main\" {\n  name     = \"my-cluster\"\n  role_arn = aws_iam_role.eks.arn\n  \n  vpc_config {\n    subnet_ids = aws_subnet.private[*].id\n  }\n  \n  provisioner \"local-exec\" {\n    command = \"aws eks update-kubeconfig --name ${self.name} --region ${var.region}\"\n  }\n}\n\n# 5. Run scripts with environment variables\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command = \"./notify-slack.sh\"\n    \n    environment = {\n      INSTANCE_ID = self.id\n      PUBLIC_IP   = self.public_ip\n      ENVIRONMENT = var.environment\n    }\n  }\n}\n\n# 6. Working directory\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"local-exec\" {\n    command     = \"./deploy.sh ${self.public_ip}\"\n    working_dir = \"${path.module}/scripts\"\n  }\n}\n```\n\n---\n\n### ðŸŒ Remote-Exec Provisioner\n\nRuns commands **on the remote resource** (via SSH or WinRM)\n\n```hcl\n# Basic remote-exec\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  key_name      = aws_key_pair.deployer.key_name\n  \n  # Connection block required\n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.public_ip\n  }\n  \n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo yum update -y\",\n      \"sudo yum install -y httpd\",\n      \"sudo systemctl start httpd\",\n      \"sudo systemctl enable httpd\",\n    ]\n  }\n}\n```\n\n#### **Connection Types**\n\n```hcl\n# SSH Connection (Linux)\nresource \"aws_instance\" \"linux\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  key_name      = \"my-key\"\n  \n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.public_ip\n    port        = 22\n    timeout     = \"5m\"\n  }\n  \n  provisioner \"remote-exec\" {\n    inline = [\"echo 'Connected!'\"]\n  }\n}\n\n# WinRM Connection (Windows)\nresource \"aws_instance\" \"windows\" {\n  ami           = \"ami-windows\"\n  instance_type = \"t2.micro\"\n  \n  connection {\n    type     = \"winrm\"\n    user     = \"Administrator\"\n    password = var.admin_password\n    host     = self.public_ip\n    port     = 5985\n    https    = false\n    insecure = true\n    timeout  = \"10m\"\n  }\n  \n  provisioner \"remote-exec\" {\n    inline = [\n      \"powershell.exe -Command \\\"Write-Host 'Connected!'\\\"\",\n    ]\n  }\n}\n\n# SSH with Bastion Host\nresource \"aws_instance\" \"private\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  subnet_id     = aws_subnet.private.id\n  \n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.private_ip\n    \n    # Bastion configuration\n    bastion_host        = aws_instance.bastion.public_ip\n    bastion_user        = \"ec2-user\"\n    bastion_private_key = file(\"~/.ssh/id_rsa\")\n  }\n  \n  provisioner \"remote-exec\" {\n    inline = [\"echo 'Connected via bastion!'\"]\n  }\n}\n```\n\n#### **Remote-Exec Modes**\n\n```hcl\n# 1. Inline commands\nprovisioner \"remote-exec\" {\n  inline = [\n    \"sudo apt-get update\",\n    \"sudo apt-get install -y nginx\",\n    \"sudo systemctl start nginx\",\n  ]\n}\n\n# 2. Single script\nprovisioner \"remote-exec\" {\n  script = \"scripts/install.sh\"\n}\n\n# 3. Multiple scripts\nprovisioner \"remote-exec\" {\n  scripts = [\n    \"scripts/install-deps.sh\",\n    \"scripts/configure.sh\",\n    \"scripts/start-services.sh\",\n  ]\n}\n```\n\n#### **Complete Web Server Example**\n\n```hcl\n# File: main.tf\n\nresource \"aws_key_pair\" \"deployer\" {\n  key_name   = \"deployer-key\"\n  public_key = file(\"~/.ssh/id_rsa.pub\")\n}\n\nresource \"aws_security_group\" \"web\" {\n  name        = \"web-sg\"\n  description = \"Allow HTTP and SSH\"\n  \n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = \"t2.micro\"\n  key_name               = aws_key_pair.deployer.key_name\n  vpc_security_group_ids = [aws_security_group.web.id]\n  \n  tags = {\n    Name = \"web-server\"\n  }\n  \n  # Connection for remote-exec\n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.public_ip\n  }\n  \n  # Install and configure web server\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo yum update -y\",\n      \"sudo yum install -y httpd\",\n      \"sudo systemctl start httpd\",\n      \"sudo systemctl enable httpd\",\n      \"echo '<h1>Hello from Terraform!</h1>' | sudo tee /var/www/html/index.html\",\n    ]\n  }\n  \n  # Save IP locally\n  provisioner \"local-exec\" {\n    command = \"echo ${self.public_ip} > web_server_ip.txt\"\n  }\n}\n\noutput \"web_url\" {\n  value = \"http://${aws_instance.web.public_ip}\"\n}\n```\n\n---\n\n### ðŸ“ File Provisioner\n\nCopies files from **local machine** to **remote resource**\n\n```hcl\n# Basic file copy\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.public_ip\n  }\n  \n  # Copy single file\n  provisioner \"file\" {\n    source      = \"scripts/app.conf\"\n    destination = \"/tmp/app.conf\"\n  }\n  \n  # Copy directory\n  provisioner \"file\" {\n    source      = \"scripts/\"\n    destination = \"/tmp/scripts\"\n  }\n  \n  # Copy with content\n  provisioner \"file\" {\n    content     = templatefile(\"config.tpl\", { ip = self.private_ip })\n    destination = \"/tmp/config.ini\"\n  }\n}\n```\n\n#### **Complete Application Deployment**\n\n```hcl\n# File: main.tf\n\nresource \"aws_instance\" \"app\" {\n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = \"t2.micro\"\n  key_name               = aws_key_pair.deployer.key_name\n  vpc_security_group_ids = [aws_security_group.app.id]\n  \n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n    host        = self.public_ip\n  }\n  \n  # Upload application files\n  provisioner \"file\" {\n    source      = \"app/\"\n    destination = \"/tmp/app\"\n  }\n  \n  # Upload configuration\n  provisioner \"file\" {\n    content = templatefile(\"${path.module}/templates/app.conf.tpl\", {\n      db_host     = aws_db_instance.main.endpoint\n      db_name     = var.db_name\n      db_user     = var.db_user\n      environment = var.environment\n    })\n    destination = \"/tmp/app.conf\"\n  }\n  \n  # Upload service file\n  provisioner \"file\" {\n    source      = \"systemd/app.service\"\n    destination = \"/tmp/app.service\"\n  }\n  \n  # Install and start application\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo yum install -y python3 python3-pip\",\n      \"sudo pip3 install -r /tmp/app/requirements.txt\",\n      \"sudo mv /tmp/app /opt/app\",\n      \"sudo mv /tmp/app.conf /opt/app/config.ini\",\n      \"sudo mv /tmp/app.service /etc/systemd/system/app.service\",\n      \"sudo systemctl daemon-reload\",\n      \"sudo systemctl start app\",\n      \"sudo systemctl enable app\",\n    ]\n  }\n}\n```\n\n---\n\n### ðŸ”„ Provisioner Types: Creation vs Destruction\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  # Runs when resource is CREATED (default)\n  provisioner \"local-exec\" {\n    command = \"echo 'Instance created: ${self.id}' >> log.txt\"\n  }\n  \n  # Runs when resource is DESTROYED\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"echo 'Instance destroyed' >> log.txt\"\n  }\n  \n  # Can use self only in creation-time provisioners\n  provisioner \"local-exec\" {\n    command = \"echo 'IP: ${self.public_ip}' >> log.txt\"\n  }\n  \n  # Destroy provisioner - cannot use self\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"echo 'Cleaning up' >> log.txt\"\n    # Note: self attributes not available here\n  }\n}\n```\n\n#### **Cleanup Example**\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  # Register with monitoring on creation\n  provisioner \"local-exec\" {\n    command = \"curl -X POST https://monitor.example.com/register -d '{\\\"instance\\\":\\\"${self.id}\\\"}'\"\n  }\n  \n  # Deregister from monitoring on destruction\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"curl -X DELETE https://monitor.example.com/deregister/INSTANCE_ID\"\n    # Note: In real scenario, you'd need to store instance ID separately\n  }\n}\n```\n\n---\n\n### âš ï¸ Provisioner Failure Handling\n\n```hcl\n# Default: Fail the resource if provisioner fails\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"remote-exec\" {\n    inline = [\"some-command-that-might-fail\"]\n  }\n}\n\n# Continue on failure\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  provisioner \"remote-exec\" {\n    inline     = [\"some-command-that-might-fail\"]\n    on_failure = continue  # Resource still created even if this fails\n  }\n}\n\n# Multiple provisioners with different failure handling\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  # Critical: Must succeed\n  provisioner \"remote-exec\" {\n    inline     = [\"sudo yum install -y httpd\"]\n    on_failure = fail\n  }\n  \n  # Optional: Can fail\n  provisioner \"remote-exec\" {\n    inline     = [\"sudo yum install -y optional-package\"]\n    on_failure = continue\n  }\n  \n  # Critical: Must succeed\n  provisioner \"remote-exec\" {\n    inline     = [\"sudo systemctl start httpd\"]\n    on_failure = fail\n  }\n}\n```\n\n---\n\n### ðŸŽ¯ Better Alternatives to Provisioners\n\n#### **1. User Data (Best for AWS)**\n\n```hcl\n# Instead of provisioners, use user_data\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  user_data = <<-EOF\n              #!/bin/bash\n              yum update -y\n              yum install -y httpd\n              systemctl start httpd\n              systemctl enable httpd\n              echo '<h1>Hello from User Data!</h1>' > /var/www/html/index.html\n              EOF\n  \n  user_data_replace_on_change = true\n}\n\n# Or use templatefile\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  user_data = templatefile(\"${path.module}/user_data.sh\", {\n    db_endpoint = aws_db_instance.main.endpoint\n    app_version = var.app_version\n  })\n}\n```\n\n#### **2. Pre-Baked AMIs with Packer**\n\n```hcl\n# File: packer-template.pkr.hcl\n\npacker {\n  required_plugins {\n    amazon = {\n      source  = \"github.com/hashicorp/amazon\"\n      version = \"~> 1.0\"\n    }\n  }\n}\n\nsource \"amazon-ebs\" \"web\" {\n  ami_name      = \"web-server-{{timestamp}}\"\n  instance_type = \"t2.micro\"\n  region        = \"us-east-1\"\n  source_ami_filter {\n    filters = {\n      name                = \"amzn2-ami-hvm-*-x86_64-gp2\"\n      root-device-type    = \"ebs\"\n      virtualization-type = \"hvm\"\n    }\n    most_recent = true\n    owners      = [\"amazon\"]\n  }\n  ssh_username = \"ec2-user\"\n}\n\nbuild {\n  sources = [\"source.amazon-ebs.web\"]\n  \n  provisioner \"shell\" {\n    inline = [\n      \"sudo yum update -y\",\n      \"sudo yum install -y httpd php php-mysql\",\n      \"sudo systemctl enable httpd\",\n    ]\n  }\n  \n  provisioner \"file\" {\n    source      = \"app/\"\n    destination = \"/tmp/app\"\n  }\n  \n  provisioner \"shell\" {\n    inline = [\n      \"sudo mv /tmp/app/* /var/www/html/\",\n      \"sudo chown -R apache:apache /var/www/html\",\n    ]\n  }\n}\n```\n\n```hcl\n# File: terraform/main.tf\n\n# Use the pre-baked AMI\ndata \"aws_ami\" \"web\" {\n  most_recent = true\n  owners      = [\"self\"]\n  \n  filter {\n    name   = \"name\"\n    values = [\"web-server-*\"]\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.web.id\n  instance_type = \"t2.micro\"\n  \n  # No provisioners needed! Application already installed.\n}\n```\n\n#### **3. Configuration Management Tools**\n\n```hcl\n# Trigger Ansible after instance creation\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  key_name      = aws_key_pair.deployer.key_name\n  \n  tags = {\n    Name = \"web-server\"\n  }\n  \n  provisioner \"local-exec\" {\n    command = <<-EOT\n      sleep 30  # Wait for instance to be ready\n      ansible-playbook \\\n        -i '${self.public_ip},' \\\n        -u ec2-user \\\n        --private-key ~/.ssh/id_rsa \\\n        playbook.yml\n    EOT\n  }\n}\n```\n\n#### **4. Cloud-Init**\n\n```hcl\n# cloud-init config\ndata \"cloudinit_config\" \"web\" {\n  gzip          = true\n  base64_encode = true\n  \n  part {\n    content_type = \"text/cloud-config\"\n    content = yamlencode({\n      package_update = true\n      packages = [\n        \"httpd\",\n        \"php\",\n        \"php-mysql\"\n      ]\n      runcmd = [\n        [\"systemctl\", \"start\", \"httpd\"],\n        [\"systemctl\", \"enable\", \"httpd\"],\n      ]\n      write_files = [\n        {\n          path    = \"/var/www/html/index.php\"\n          content = file(\"${path.module}/app/index.php\")\n          owner   = \"apache:apache\"\n        }\n      ]\n    })\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  user_data     = data.cloudinit_config.web.rendered\n}\n```\n\n---\n\n### âœ… Day 16 Checklist\n\n- [x] Understand provisioners and their limitations\n- [x] local-exec provisioner\n- [x] remote-exec provisioner\n- [x] file provisioner\n- [x] Creation vs destruction provisioners\n- [x] Failure handling\n- [x] Better alternatives (user_data, Packer, Ansible, cloud-init)\n\n**Next:** Lifecycle rules and advanced resource management!\n\n---\n\n## Day 17: Lifecycle Rules & Advanced Resource Management\n\n### ðŸŽ¯ Learning Objectives\n- Use lifecycle meta-arguments\n- Prevent resource destruction\n- Create before destroy pattern\n- Ignore changes\n- Replace triggered by changes\n- Handle dependencies correctly\n\n---\n\n### ðŸ”„ Lifecycle Meta-Argument\n\nControl how Terraform creates, updates, and destroys resources.\n\n```hcl\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    create_before_destroy = true\n    prevent_destroy       = false\n    ignore_changes        = []\n    replace_triggered_by  = []\n    precondition {\n      # conditions\n    }\n    postcondition {\n      # conditions\n    }\n  }\n}\n```\n\n---\n\n### ðŸ›¡ï¸ Prevent Destroy\n\nPrevent accidental deletion of critical resources.\n\n```hcl\n# Protect production database\nresource \"aws_db_instance\" \"production\" {\n  identifier     = \"prod-db\"\n  engine         = \"mysql\"\n  instance_class = \"db.t3.micro\"\n  \n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\n# Trying to destroy will fail:\n# Error: Instance cannot be destroyed\n# Resource aws_db_instance.production has lifecycle.prevent_destroy set\n```\n\n#### **Conditional prevent_destroy**\n\n```hcl\nvariable \"environment\" {\n  type = string\n}\n\nlocals {\n  is_production = var.environment == \"prod\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"${var.environment}-db\"\n  engine         = \"mysql\"\n  instance_class = \"db.t3.micro\"\n  \n  lifecycle {\n    prevent_destroy = local.is_production\n  }\n}\n```\n\n#### **S3 Bucket Protection**\n\n```hcl\nresource \"aws_s3_bucket\" \"important_data\" {\n  bucket = \"my-important-data-bucket\"\n  \n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nresource \"aws_s3_bucket_versioning\" \"important_data\" {\n  bucket = aws_s3_bucket.important_data.id\n  \n  versioning_configuration {\n    status = \"Enabled\"\n  }\n  \n  lifecycle {\n    prevent_destroy = true\n  }\n}\n```\n\n---\n\n### ðŸ”„ Create Before Destroy\n\nCreate replacement before destroying original (minimizes downtime).\n\n```hcl\n# Default behavior (destroy then create)\n# Results in downtime!\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  # Change AMI:\n  # 1. Destroy old instance\n  # 2. Create new instance\n  # âš ï¸ Downtime!\n}\n\n# Create before destroy (zero downtime)\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n  \n  # Change AMI:\n  # 1. Create new instance\n  # 2. Destroy old instance\n  # âœ… No downtime!\n}\n```\n\n#### **Launch Configuration Example**\n\n```hcl\n# Launch configurations are immutable\n# Any change requires replacement\nresource \"aws_launch_configuration\" \"web\" {\n  name_prefix   = \"web-\"\n  image_id      = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_autoscaling_group\" \"web\" {\n  launch_configuration = aws_launch_configuration.web.id\n  min_size             = 2\n  max_size             = 10\n  desired_capacity     = 2\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n}\n```\n\n#### **Security Group with Create Before Destroy**\n\n```hcl\nresource \"aws_security_group\" \"app\" {\n  name        = \"app-sg-${random_id.sg.hex}\"\n  description = \"Security group for app\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"random_id\" \"sg\" {\n  byte_length = 4\n  \n  keepers = {\n    # Force new security group when rules change\n    ingress_rules = jsonencode([\"80\", \"443\"])\n  }\n}\n```\n\n---\n\n### ðŸ™ˆ Ignore Changes\n\nIgnore changes to specific attributes (useful for external modifications).\n\n```hcl\n# Ignore all changes to tags\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  tags = {\n    Name = \"web-server\"\n  }\n  \n  lifecycle {\n    ignore_changes = [tags]\n  }\n  \n  # Manual tag changes won't be reverted\n}\n\n# Ignore specific tag\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  tags = {\n    Name        = \"web-server\"\n    Environment = \"prod\"\n  }\n  \n  lifecycle {\n    ignore_changes = [tags[\"Environment\"]]\n  }\n}\n\n# Ignore AMI changes (useful for auto-patching)\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    ignore_changes = [ami]\n  }\n  \n  # AMI can be updated manually without Terraform reverting\n}\n\n# Ignore multiple attributes\nresource \"aws_autoscaling_group\" \"web\" {\n  launch_configuration = aws_launch_configuration.web.id\n  min_size             = 2\n  max_size             = 10\n  desired_capacity     = 5\n  \n  lifecycle {\n    ignore_changes = [\n      desired_capacity,  # Allow auto-scaling to change this\n      target_group_arns, # Allow external changes\n    ]\n  }\n}\n\n# Ignore all changes (dangerous!)\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    ignore_changes = all\n  }\n  \n  # Terraform will never update this resource\n  # Only useful for imported resources you don't want to manage\n}\n```\n\n#### **Real-World Example: Auto-Scaling**\n\n```hcl\nresource \"aws_autoscaling_group\" \"web\" {\n  name                 = \"web-asg\"\n  launch_configuration = aws_launch_configuration.web.id\n  vpc_zone_identifier  = aws_subnet.private[*].id\n  \n  min_size         = var.min_size\n  max_size         = var.max_size\n  desired_capacity = var.desired_size\n  \n  health_check_type         = \"ELB\"\n  health_check_grace_period = 300\n  \n  target_group_arns = [aws_lb_target_group.web.arn]\n  \n  lifecycle {\n    create_before_destroy = true\n    \n    # Allow ASG to scale without Terraform reverting\n    ignore_changes = [\n      desired_capacity,  # Auto-scaling changes this\n      target_group_arns, # May be updated by deployment tools\n    ]\n  }\n  \n  tag {\n    key                 = \"Name\"\n    value               = \"web-server\"\n    propagate_at_launch = true\n  }\n}\n```\n\n---\n\n### ðŸ”„ Replace Triggered By\n\nForce replacement when specific resources change.\n\n```hcl\n# Force instance replacement when certificate changes\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    replace_triggered_by = [\n      aws_acm_certificate.web.id\n    ]\n  }\n}\n\nresource \"aws_acm_certificate\" \"web\" {\n  domain_name       = \"example.com\"\n  validation_method = \"DNS\"\n}\n\n# Replace all instances when launch template changes\nresource \"aws_launch_template\" \"web\" {\n  name_prefix   = \"web-\"\n  image_id      = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n}\n\nresource \"aws_instance\" \"web\" {\n  count = 3\n  \n  launch_template {\n    id      = aws_launch_template.web.id\n    version = \"$Latest\"\n  }\n  \n  lifecycle {\n    replace_triggered_by = [\n      aws_launch_template.web.id\n    ]\n  }\n}\n\n# Replace based on null_resource trigger\nresource \"null_resource\" \"app_version\" {\n  triggers = {\n    version = var.app_version\n  }\n}\n\nresource \"aws_instance\" \"app\" {\n  ami           = \"ami-12345\"\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    replace_triggered_by = [\n      null_resource.app_version\n    ]\n  }\n}\n```\n\n---\n\n### âœ… Preconditions and Postconditions\n\nValidate assumptions before/after resource operations.\n\n```hcl\n# Precondition: Check before creating resource\nresource \"aws_instance\" \"web\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  \n  lifecycle {\n    precondition {\n      condition     = data.aws_ami.selected.architecture == \"x86_64\"\n      error_message = \"AMI must be x86_64 architecture.\"\n    }\n    \n    precondition {\n      condition     = contains([\"t2.micro\", \"t2.small\", \"t2.medium\"], var.instance_type)\n      error_message = \"Instance type must be t2.micro, t2.small, or t2.medium.\"\n    }\n  }\n}\n\n# Postcondition: Verify after creating resource\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  lifecycle {\n    postcondition {\n      condition     = self.public_ip != \"\"\n      error_message = \"Instance must have a public IP address.\"\n    }\n    \n    postcondition {\n      condition     = self.instance_state == \"running\"\n      error_message = \"Instance must be in running state.\"\n    }\n  }\n}\n\n# Data source with postcondition\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical\n  \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n  }\n  \n  lifecycle {\n    postcondition {\n      condition     = self.architecture == \"x86_64\"\n      error_message = \"AMI must be x86_64 architecture.\"\n    }\n    \n    postcondition {\n      condition     = self.root_device_type == \"ebs\"\n      error_message = \"AMI must use EBS root device.\"\n    }\n  }\n}\n```\n\n#### **Complex Validation Example**\n\n```hcl\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"instance_type\" {\n  type = string\n}\n\nvariable \"multi_az\" {\n  type = bool\n}\n\nlocals {\n  production_instance_types = [\"t3.large\", \"t3.xlarge\", \"m5.large\", \"m5.xlarge\"]\n  valid_environments        = [\"dev\", \"staging\", \"prod\"]\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"${var.environment}-database\"\n  engine         = \"mysql\"\n  instance_class = var.instance_type\n  multi_az       = var.multi_az\n  \n  lifecycle {\n    # Validate environment\n    precondition {\n      condition     = contains(local.valid_environments, var.environment)\n      error_message = \"Environment must be one of: ${join(\", \", local.valid_environments)}.\"\n    }\n    \n    # Production requirements\n    precondition {\n      condition = (\n        var.environment != \"prod\" ||\n        (contains(local.production_instance_types, var.instance_type) && var.multi_az)\n      )\n      error_message = \"Production databases must use ${join(\", \", local.production_instance_types)} and multi_az=true.\"\n    }\n    \n    # Verify after creation\n    postcondition {\n      condition     = self.status == \"available\"\n      error_message = \"Database must be in available state after creation.\"\n    }\n    \n    postcondition {\n      condition = (\n        var.environment != \"prod\" ||\n        self.backup_retention_period >= 7\n      )\n      error_message = \"Production databases must have backup retention >= 7 days.\"\n    }\n  }\n}\n```\n\n---\n\n### ðŸŽ“ Complete Lifecycle Example\n\n```hcl\n# ==================================================\n# File: main.tf\n# ==================================================\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"app_version\" {\n  type = string\n}\n\nlocals {\n  is_production = var.environment == \"prod\"\n}\n\n# S3 bucket for application assets (protected)\nresource \"aws_s3_bucket\" \"assets\" {\n  bucket = \"${var.environment}-app-assets\"\n  \n  lifecycle {\n    prevent_destroy = local.is_production\n  }\n}\n\n# Application version tracker\nresource \"null_resource\" \"app_version_tracker\" {\n  triggers = {\n    version = var.app_version\n  }\n}\n\n# Launch template (immutable - use create_before_destroy)\nresource \"aws_launch_template\" \"app\" {\n  name_prefix   = \"${var.environment}-app-\"\n  image_id      = data.aws_ami.app.id\n  instance_type = var.environment == \"prod\" ? \"t3.large\" : \"t2.micro\"\n  \n  user_data = base64encode(templatefile(\"${path.module}/user_data.sh\", {\n    app_version = var.app_version\n    environment = var.environment\n  }))\n  \n  lifecycle {\n    create_before_destroy = true\n    \n    precondition {\n      condition     = data.aws_ami.app.architecture == \"x86_64\"\n      error_message = \"AMI must be x86_64 architecture.\"\n    }\n  }\n}\n\n# Auto Scaling Group\nresource \"aws_autoscaling_group\" \"app\" {\n  name                = \"${var.environment}-app-asg\"\n  vpc_zone_identifier = aws_subnet.private[*].id\n  target_group_arns   = [aws_lb_target_group.app.arn]\n  \n  min_size         = local.is_production ? 3 : 1\n  max_size         = local.is_production ? 10 : 2\n  desired_capacity = local.is_production ? 5 : 1\n  \n  launch_template {\n    id      = aws_launch_template.app.id\n    version = \"$Latest\"\n  }\n  \n  health_check_type         = \"ELB\"\n  health_check_grace_period = 300\n  \n  lifecycle {\n    create_before_destroy = true\n    \n    # Allow auto-scaling to manage capacity\n    ignore_changes = [\n      desired_capacity,\n    ]\n    \n    # Force replacement when app version changes\n    replace_triggered_by = [\n      null_resource.app_version_tracker\n    ]\n  }\n  \n  tag {\n    key                 = \"Name\"\n    value               = \"${var.environment}-app-server\"\n    propagate_at_launch = true\n  }\n  \n  tag {\n    key                 = \"Environment\"\n    value               = var.environment\n    propagate_at_launch = true\n  }\n  \n  tag {\n    key                 = \"AppVersion\"\n    value               = var.app_version\n    propagate_at_launch = true\n  }\n}\n\n# Database (protected in production, ignore minor version upgrades)\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"${var.environment}-database\"\n  engine         = \"mysql\"\n  engine_version = \"8.0\"\n  instance_class = local.is_production ? \"db.r5.large\" : \"db.t3.micro\"\n  \n  allocated_storage     = local.is_production ? 100 : 20\n  max_allocated_storage = local.is_production ? 1000 : 100\n  \n  multi_az               = local.is_production\n  backup_retention_period = local.is_production ? 30 : 7\n  \n  lifecycle {\n    prevent_destroy = local.is_production\n    \n    # Allow AWS to update minor versions\n    ignore_changes = [\n      engine_version,  # Allow patch updates\n    ]\n    \n    # Validate production requirements\n    precondition {\n      condition = (\n        !local.is_production ||\n        (self.multi_az && self.backup_retention_period >= 7)\n      )\n      error_message = \"Production database must have multi_az=true and backup_retention >= 7 days.\"\n    }\n    \n    # Verify after creation\n    postcondition {\n      condition     = self.status == \"available\"\n      error_message = \"Database must be available after creation.\"\n    }\n  }\n}\n\n# Security group (allow external updates to rules)\nresource \"aws_security_group\" \"app\" {\n  name        = \"${var.environment}-app-sg\"\n  description = \"Security group for app servers\"\n  vpc_id      = aws_vpc.main.id\n  \n  lifecycle {\n    create_before_destroy = true\n    \n    # Allow external security tools to manage rules\n    ignore_changes = [\n      ingress,\n      egress,\n    ]\n  }\n}\n\n# ==================================================\n# Outputs\n# ==================================================\n\noutput \"asg_name\" {\n  description = \"Auto Scaling Group name\"\n  value       = aws_autoscaling_group.app.name\n}\n\noutput \"database_endpoint\" {\n  description = \"Database endpoint\"\n  value       = aws_db_instance.main.endpoint\n  sensitive   = true\n}\n```\n\n**Usage:**\n\n```bash\n# Development\nterraform apply -var=\"environment=dev\" -var=\"app_version=1.0.0\"\n\n# Production (database protected)\nterraform apply -var=\"environment=prod\" -var=\"app_version=1.0.0\"\n\n# Update app version (triggers ASG replacement)\nterraform apply -var=\"environment=prod\" -var=\"app_version=1.1.0\"\n\n# Try to destroy production database (fails)\nterraform destroy -target=aws_db_instance.main\n# Error: Instance cannot be destroyed\n```\n\n---\n\n### âœ… Day 17 Checklist\n\n- [x] Lifecycle meta-argument\n- [x] prevent_destroy\n- [x] create_before_destroy\n- [x] ignore_changes\n- [x] replace_triggered_by\n- [x] precondition and postcondition\n- [x] Real-world lifecycle patterns\n- [x] Complete production example\n\n**Next:** Terraform Cloud and Enterprise features!\n\n---\n\n## Day 18: Terraform Cloud & Collaboration\n\n### ðŸŽ¯ Learning Objectives\n- Understand Terraform Cloud benefits\n- Set up Terraform Cloud workspaces\n- Configure VCS integration\n- Use remote runs\n- Implement team collaboration\n- Use Sentinel policies (Enterprise)\n\n---\n\n### â˜ï¸ What is Terraform Cloud?\n\n**Terraform Cloud** = HashiCorp's managed service for Terraform\n\n**Benefits:**\n- âœ… Remote state storage (encrypted)\n- âœ… Remote execution (runs in cloud)\n- âœ… Collaboration (teams, RBAC)\n- âœ… VCS integration (GitHub, GitLab, Bitbucket)\n- âœ… Cost estimation\n- âœ… Policy as code (Sentinel)\n- âœ… Private module registry\n- âœ… Audit logs\n- âœ… UI for viewing state and runs\n\n**Pricing:**\n- **Free**: Up to 5 users, remote state, basic features\n- **Team**: $20/user/month, team management, SSO\n- **Business**: Custom pricing, Sentinel policies, audit logs\n\n**Terraform Cloud vs Enterprise:**\n- **Cloud**: SaaS, managed by HashiCorp\n- **Enterprise**: Self-hosted, more control\n\n---\n\n### ðŸš€ Getting Started with Terraform Cloud\n\n#### **1. Create Account**\n\n```bash\n# Visit https://app.terraform.io/signup\n\n# Or use CLI\nterraform login\n\n# Opens browser for authentication\n# Saves token to ~/.terraform.d/credentials.tfrc.json\n```\n\n#### **2. Create Organization**\n\n```bash\n# Via Web UI:\n# 1. Go to https://app.terraform.io\n# 2. Click \"Create an organization\"\n# 3. Enter organization name (e.g., \"my-company\")\n```\n\n#### **3. Configure Terraform**\n\n```hcl\n# File: main.tf\n\nterraform {\n  cloud {\n    organization = \"my-company\"\n    \n    workspaces {\n      name = \"my-app-production\"\n    }\n  }\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n```\n\n#### **4. Initialize and Run**\n\n```bash\n# Initialize (connects to Terraform Cloud)\nterraform init\n\n# Plan (runs remotely)\nterraform plan\n\n# Apply (runs remotely)\nterraform apply\n\n# View in UI\n# https://app.terraform.io/app/my-company/workspaces/my-app-production/runs\n```\n\n---\n\n### ðŸ¢ Workspaces in Terraform Cloud\n\n**Workspace** = Container for Terraform configuration, state, and variables\n\n```plaintext\nOrganization: my-company\nâ”œâ”€â”€ Workspace: frontend-prod\nâ”‚   â”œâ”€â”€ State\nâ”‚   â”œâ”€â”€ Variables\nâ”‚   â””â”€â”€ Runs\nâ”œâ”€â”€ Workspace: frontend-staging\nâ”‚   â”œâ”€â”€ State\nâ”‚   â”œâ”€â”€ Variables\nâ”‚   â””â”€â”€ Runs\nâ””â”€â”€ Workspace: backend-prod\n    â”œâ”€â”€ State\n    â”œâ”€â”€ Variables\n    â””â”€â”€ Runs\n```\n\n#### **Create Workspace via Terraform**\n\n```hcl\nterraform {\n  cloud {\n    organization = \"my-company\"\n    \n    workspaces {\n      # Single workspace\n      name = \"my-app-production\"\n      \n      # Or use tags for multiple workspaces\n      # tags = [\"app:myapp\", \"env:prod\"]\n    }\n  }\n}\n```\n\n#### **Workspace Variables**\n\nSet via UI or API:\n\n```bash\n# Via UI:\n# 1. Go to workspace\n# 2. Click \"Variables\"\n# 3. Add variables\n\n# Variable types:\n# - Terraform Variables (var.xxx)\n# - Environment Variables (AWS_ACCESS_KEY_ID, etc.)\n\n# Mark as sensitive to hide values\n```\n\n**Example:**\n\n```\nTerraform Variables:\n- region = \"us-east-1\"\n- instance_type = \"t3.large\"\n- environment = \"production\"\n\nEnvironment Variables:\n- AWS_ACCESS_KEY_ID = \"AKIAxxxx\"  (sensitive)\n- AWS_SECRET_ACCESS_KEY = \"xxxxx\" (sensitive)\n```\n\n---\n\n### ðŸ”— VCS Integration (GitHub, GitLab, Bitbucket)\n\nConnect workspace to Git repository for automatic runs.\n\n#### **Setup VCS Integration**\n\n```bash\n# 1. In Terraform Cloud UI:\n# Settings â†’ VCS Providers â†’ Add VCS Provider\n# Choose: GitHub, GitLab, Bitbucket, etc.\n\n# 2. Authorize Terraform Cloud\n\n# 3. Create workspace with VCS connection:\n# - Select repository\n# - Choose branch (e.g., main)\n# - Set working directory (if code is in subdirectory)\n```\n\n#### **Workflow**\n\n```plaintext\nDeveloper Workflow:\n1. Developer pushes code to GitHub\n2. Terraform Cloud detects push\n3. Automatically triggers terraform plan\n4. Shows plan in GitHub PR as comment\n5. Team reviews plan\n6. Merge PR â†’ triggers terraform apply\n```\n\n#### **Configuration**\n\n```hcl\n# .terraform.cloud.json (in repository root)\n{\n  \"organization\": \"my-company\",\n  \"workspaces\": {\n    \"name\": \"my-app-production\"\n  }\n}\n```\n\n**Working Directory:**\n\n```\nrepo/\nâ”œâ”€â”€ frontend/\nâ”‚   â””â”€â”€ main.tf  â† Workspace \"frontend\" uses this directory\nâ”œâ”€â”€ backend/\nâ”‚   â””â”€â”€ main.tf  â† Workspace \"backend\" uses this directory\nâ””â”€â”€ infrastructure/\n    â””â”€â”€ main.tf  â† Workspace \"infrastructure\" uses this directory\n```\n\n**Trigger Patterns:**\n\n```bash\n# Workspace settings:\n# VCS â†’ Trigger Patterns\n\n# Run only when specific paths change:\n# - Always trigger: false\n# - Patterns:\n#   - /frontend/**\n#   - /shared/**\n\n# Example: frontend workspace only runs when frontend/ or shared/ changes\n```\n\n---\n\n### ðŸŽ¯ Run Triggers\n\nAutomatically trigger workspace runs based on other workspaces.\n\n```plaintext\nExample:\n1. \"networking\" workspace creates VPC\n2. \"database\" workspace depends on VPC\n3. When \"networking\" completes â†’ auto-trigger \"database\"\n```\n\n**Setup:**\n\n```bash\n# In dependent workspace (e.g., \"database\"):\n# Settings â†’ Run Triggers â†’ Source Workspaces\n# Add: \"networking\"\n\n# Now when \"networking\" apply completes â†’ \"database\" runs automatically\n```\n\n---\n\n### ðŸ‘¥ Team Collaboration\n\n#### **Teams and Permissions**\n\n```bash\n# Organization Settings â†’ Teams\n\n# Create teams:\n# - Developers (read/write)\n# - Operators (admin)\n# - Auditors (read-only)\n\n# Workspace permissions:\n# - Read: View state, runs\n# - Plan: Trigger plans\n# - Write: Approve applies\n# - Admin: Manage workspace settings\n```\n\n**Example Structure:**\n\n```\nOrganization: my-company\nâ”œâ”€â”€ Team: developers\nâ”‚   â”œâ”€â”€ Workspace: dev-* (write)\nâ”‚   â””â”€â”€ Workspace: prod-* (plan)\nâ”œâ”€â”€ Team: operators\nâ”‚   â””â”€â”€ Workspace: * (admin)\nâ””â”€â”€ Team: managers\n    â””â”€â”€ Workspace: * (read)\n```\n\n#### **Approval Workflow**\n\n```hcl\n# Workspace settings:\n# Settings â†’ General â†’ Apply Method: Manual\n\n# Workflow:\n# 1. Developer pushes code\n# 2. Terraform plan runs automatically\n# 3. Plan requires approval\n# 4. Operator reviews and approves\n# 5. Terraform apply runs\n```\n\n---\n\n### ðŸ’° Cost Estimation\n\nTerraform Cloud shows estimated AWS costs before applying.\n\n```plaintext\nPlan output:\n\nResources: 12 to add, 5 to change, 0 to destroy.\n\nCost Estimation:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n+ aws_instance.web\n  +$8.76 per month\n\n+ aws_db_instance.main\n  +$50.00 per month\n\n+ aws_lb.main\n  +$20.00 per month\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nTotal Monthly Cost: $78.76\nCost Delta: +$25.00 from current\n```\n\n---\n\n### ðŸ›¡ï¸ Sentinel Policies (Enterprise/Business)\n\n**Sentinel** = Policy as code framework\n\n**Use cases:**\n- Enforce tagging standards\n- Require encryption\n- Limit instance types\n- Enforce naming conventions\n- Check for security groups\n- Validate CIDR ranges\n\n#### **Sentinel Policy Example**\n\n```hcl\n# File: require-tags.sentinel\n\nimport \"tfplan/v2\" as tfplan\n\n# Get all AWS instances\nall_instances = filter tfplan.resource_changes as _, rc {\n    rc.type is \"aws_instance\" and\n    rc.mode is \"managed\" and\n    (rc.change.actions contains \"create\" or rc.change.actions contains \"update\")\n}\n\n# Rule: All instances must have required tags\nrequired_tags = [\"Name\", \"Environment\", \"Owner\"]\n\ninstance_has_required_tags = rule {\n    all all_instances as _, instance {\n        all required_tags as tag {\n            instance.change.after.tags contains tag\n        }\n    }\n}\n\n# Main rule\nmain = rule {\n    instance_has_required_tags\n}\n```\n\n```hcl\n# File: enforce-encryption.sentinel\n\nimport \"tfplan/v2\" as tfplan\n\n# Get all S3 buckets\nall_buckets = filter tfplan.resource_changes as _, rc {\n    rc.type is \"aws_s3_bucket\" and\n    rc.mode is \"managed\"\n}\n\n# Get all bucket encryption configurations\nall_bucket_encryption = filter tfplan.resource_changes as _, rc {\n    rc.type is \"aws_s3_bucket_server_side_encryption_configuration\" and\n    rc.mode is \"managed\"\n}\n\n# Rule: All S3 buckets must have encryption\nmain = rule {\n    length(all_buckets) == length(all_bucket_encryption)\n}\n```\n\n```hcl\n# File: restrict-instance-types.sentinel\n\nimport \"tfplan/v2\" as tfplan\n\n# Allowed instance types\nallowed_types = [\"t3.micro\", \"t3.small\", \"t3.medium\"]\n\n# Get all instances\nall_instances = filter tfplan.resource_changes as _, rc {\n    rc.type is \"aws_instance\" and\n    rc.mode is \"managed\" and\n    (rc.change.actions contains \"create\" or rc.change.actions contains \"update\")\n}\n\n# Rule: Instance types must be in allowed list\ninstance_type_allowed = rule {\n    all all_instances as _, instance {\n        instance.change.after.instance_type in allowed_types\n    }\n}\n\nmain = rule {\n    instance_type_allowed\n}\n```\n\n#### **Policy Sets**\n\n```hcl\n# sentinel.hcl\n\npolicy \"require-tags\" {\n    enforcement_level = \"hard-mandatory\"  # Must pass\n}\n\npolicy \"enforce-encryption\" {\n    enforcement_level = \"hard-mandatory\"  # Must pass\n}\n\npolicy \"restrict-instance-types\" {\n    enforcement_level = \"soft-mandatory\"  # Can override\n}\n\npolicy \"check-naming-convention\" {\n    enforcement_level = \"advisory\"  # Warning only\n}\n```\n\n**Enforcement Levels:**\n- `advisory` - Warning only, doesn't block\n- `soft-mandatory` - Must pass, but can be overridden\n- `hard-mandatory` - Must pass, cannot be overridden\n\n---\n\n### ðŸ“¦ Private Module Registry\n\nShare modules within your organization.\n\n#### **Publish Module**\n\n```bash\n# 1. Create GitHub repository:\n# terraform-aws-vpc (naming convention important!)\n\n# 2. Add tag for version:\ngit tag v1.0.0\ngit push origin v1.0.0\n\n# 3. In Terraform Cloud UI:\n# Registry â†’ Modules â†’ Publish â†’ Connect to VCS\n# Select repository: terraform-aws-vpc\n\n# Module is now available as:\n# my-company/vpc/aws\n```\n\n#### **Use Private Module**\n\n```hcl\nterraform {\n  cloud {\n    organization = \"my-company\"\n    workspaces {\n      name = \"my-app\"\n    }\n  }\n}\n\nmodule \"vpc\" {\n  source  = \"app.terraform.io/my-company/vpc/aws\"\n  version = \"1.0.0\"\n  \n  project_name = \"myapp\"\n  environment  = \"prod\"\n  vpc_cidr     = \"10.0.0.0/16\"\n}\n```\n\n---\n\n### ðŸŽ“ Complete Terraform Cloud Setup Example\n\n```hcl\n# ==================================================\n# File: main.tf\n# ==================================================\n\nterraform {\n  required_version = \">= 1.0\"\n  \n  # Terraform Cloud configuration\n  cloud {\n    organization = \"my-company\"\n    \n    workspaces {\n      name = \"production-infrastructure\"\n    }\n  }\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\n# AWS provider (credentials from environment variables)\nprovider \"aws\" {\n  region = var.region\n}\n\n# ==================================================\n# Variables (set in Terraform Cloud UI)\n# ==================================================\n\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t3.micro\"\n}\n\n# ==================================================\n# Resources\n# ==================================================\n\nmodule \"vpc\" {\n  source  = \"app.terraform.io/my-company/vpc/aws\"\n  version = \"1.0.0\"\n  \n  project_name = \"myapp\"\n  environment  = var.environment\n  \n  vpc_cidr             = \"10.0.0.0/16\"\n  availability_zones   = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  public_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  private_subnet_cidrs = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n}\n\nresource \"aws_instance\" \"web\" {\n  count = 3\n  \n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = var.instance_type\n  subnet_id              = module.vpc.public_subnet_ids[count.index]\n  vpc_security_group_ids = [aws_security_group.web.id]\n  \n  tags = {\n    Name        = \"web-${count.index + 1}\"\n    Environment = var.environment\n    Owner       = \"platform-team\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n\n# ==================================================\n# Outputs\n# ==================================================\n\noutput \"vpc_id\" {\n  description = \"VPC ID\"\n  value       = module.vpc.vpc_id\n}\n\noutput \"instance_ips\" {\n  description = \"Instance public IPs\"\n  value       = aws_instance.web[*].public_ip\n}\n```\n\n**Terraform Cloud Configuration:**\n\n```bash\n# Workspace: production-infrastructure\n# Settings:\n\n# General:\n# - Execution Mode: Remote\n# - Apply Method: Manual (requires approval)\n# - Terraform Version: 1.7.0\n\n# VCS:\n# - Repository: github.com/mycompany/terraform-infrastructure\n# - Branch: main\n# - Working Directory: /production\n\n# Variables (Terraform):\n# - region = \"us-east-1\"\n# - environment = \"production\"\n# - instance_type = \"t3.large\"\n\n# Variables (Environment - Sensitive):\n# - AWS_ACCESS_KEY_ID = \"AKIAxxxx\"\n# - AWS_SECRET_ACCESS_KEY = \"xxxxx\"\n\n# Run Triggers:\n# - Source Workspaces: [\"networking-production\"]\n\n# Notifications:\n# - Slack: #infrastructure-alerts\n# - Email: platform-team@company.com\n\n# Team Access:\n# - developers: Plan\n# - operators: Write\n# - managers: Read\n```\n\n**Workflow:**\n\n```bash\n# 1. Developer creates branch\ngit checkout -b feature/add-cache\n\n# 2. Makes changes\n# ... edit main.tf ...\n\n# 3. Commits and pushes\ngit add .\ngit commit -m \"Add ElastiCache cluster\"\ngit push origin feature/add-cache\n\n# 4. Creates Pull Request\n# GitHub â†’ Create PR\n\n# 5. Terraform Cloud automatically:\n#    - Detects PR\n#    - Runs terraform plan\n#    - Posts plan as PR comment\n\n# 6. Team reviews plan in PR comment\n\n# 7. PR is approved and merged\n\n# 8. Terraform Cloud automatically:\n#    - Detects merge to main\n#    - Runs terraform plan\n#    - Waits for approval (Manual apply)\n\n# 9. Operator reviews plan in Terraform Cloud UI\n\n# 10. Operator clicks \"Confirm & Apply\"\n\n# 11. Terraform apply runs remotely\n\n# 12. Notifications sent to Slack/Email\n\n# 13. State is saved securely in Terraform Cloud\n```\n\n---\n\n### âœ… Day 18 Checklist\n\n- [x] Terraform Cloud benefits\n- [x] Account and organization setup\n- [x] Workspace configuration\n- [x] VCS integration (GitHub, GitLab)\n- [x] Remote runs\n- [x] Team collaboration and RBAC\n- [x] Run triggers\n- [x] Cost estimation\n- [x] Sentinel policies (Policy as Code)\n- [x] Private module registry\n- [x] Complete collaboration workflow\n\n**Next:** Best practices for project structure and organization!\n\n---\n\n## Day 19: Project Structure & Organization\n\n### ðŸŽ¯ Learning Objectives\n- Organize Terraform projects effectively\n- Structure multi-environment setups\n- Use naming conventions\n- Manage large codebases\n- Implement DRY principles\n\n---\n\n### ðŸ“ Project Structure Patterns\n\n#### **Pattern 1: Simple Single Environment**\n\n```\nterraform/\nâ”œâ”€â”€ main.tf           # Main resources\nâ”œâ”€â”€ variables.tf      # Input variables\nâ”œâ”€â”€ outputs.tf        # Output values\nâ”œâ”€â”€ providers.tf      # Provider configuration\nâ”œâ”€â”€ versions.tf       # Version constraints\nâ”œâ”€â”€ terraform.tfvars  # Variable values (gitignored)\nâ””â”€â”€ README.md         # Documentation\n```\n\n**Best for:**\n- Small projects\n- Single environment\n- Learning/prototyping\n\n---\n\n#### **Pattern 2: Multi-Environment with Workspaces**\n\n```\nterraform/\nâ”œâ”€â”€ main.tf\nâ”œâ”€â”€ variables.tf\nâ”œâ”€â”€ outputs.tf\nâ”œâ”€â”€ providers.tf\nâ”œâ”€â”€ versions.tf\nâ”œâ”€â”€ dev.tfvars        # Dev variables\nâ”œâ”€â”€ staging.tfvars    # Staging variables\nâ”œâ”€â”€ prod.tfvars       # Prod variables\nâ””â”€â”€ README.md\n```\n\n**Usage:**\n\n```bash\n# Development\nterraform workspace select dev\nterraform apply -var-file=\"dev.tfvars\"\n\n# Production\nterraform workspace select prod\nterraform apply -var-file=\"prod.tfvars\"\n```\n\n**Best for:**\n- Medium projects\n- Similar infrastructure across environments\n- Same team managing all environments\n\n---\n\n#### **Pattern 3: Separate Environment Directories**\n\n```\nterraform/\nâ”œâ”€â”€ modules/\nâ”‚   â”œâ”€â”€ vpc/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â””â”€â”€ outputs.tf\nâ”‚   â”œâ”€â”€ compute/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â””â”€â”€ outputs.tf\nâ”‚   â””â”€â”€ database/\nâ”‚       â”œâ”€â”€ main.tf\nâ”‚       â”œâ”€â”€ variables.tf\nâ”‚       â””â”€â”€ outputs.tf\nâ”œâ”€â”€ environments/\nâ”‚   â”œâ”€â”€ dev/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â”œâ”€â”€ outputs.tf\nâ”‚   â”‚   â”œâ”€â”€ backend.tf\nâ”‚   â”‚   â””â”€â”€ terraform.tfvars\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ main.tf\nâ”‚   â”‚   â”œâ”€â”€ variables.tf\nâ”‚   â”‚   â”œâ”€â”€ outputs.tf\nâ”‚   â”‚   â”œâ”€â”€ backend.tf\nâ”‚   â”‚   â””â”€â”€ terraform.tfvars\nâ”‚   â””â”€â”€ prod/\nâ”‚       â”œâ”€â”€ main.tf\nâ”‚       â”œâ”€â”€ variables.tf\nâ”‚       â”œâ”€â”€ outputs.tf\nâ”‚       â”œâ”€â”€ backend.tf\nâ”‚       â””â”€â”€ terraform.tfvars\nâ””â”€â”€ README.md\n```\n\n**Best for:**\n- Large projects\n- Different architectures per environment\n- Separate teams per environment\n- Strict isolation required\n\n---\n\n#### **Pattern 4: Layered Architecture**\n\n```\nterraform/\nâ”œâ”€â”€ 00-bootstrap/         # Account setup, state backend\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ”œâ”€â”€ 10-networking/        # VPC, subnets, routing\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â”œâ”€â”€ backend.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ”œâ”€â”€ 20-security/          # IAM, security groups\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â”œâ”€â”€ backend.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ”œâ”€â”€ 30-data/              # RDS, ElastiCache, S3\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â”œâ”€â”€ backend.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ”œâ”€â”€ 40-compute/           # EC2, ECS, Lambda\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â”œâ”€â”€ backend.tf\nâ”‚   â””â”€â”€ terraform.tfvars\nâ””â”€â”€ 50-applications/      # Application-specific resources\n    â”œâ”€â”€ main.tf\n    â”œâ”€â”€ backend.tf\n    â””â”€â”€ terraform.tfvars\n```\n\n**Deployment order:**\n\n```bash\n# 1. Bootstrap (run once)\ncd 00-bootstrap\nterraform apply\n\n# 2. Networking\ncd ../10-networking\nterraform apply\n\n# 3. Security\ncd ../20-security\nterraform apply\n\n# 4. Data layer\ncd ../30-data\nterraform apply\n\n# 5. Compute layer\ncd ../40-compute\nterraform apply\n\n# 6. Applications\ncd ../50-applications\nterraform apply\n```\n\n**Best for:**\n- Very large projects\n- Complex dependencies\n- Multiple teams owning different layers\n- Gradual infrastructure changes\n\n---\n\n### ðŸ“‹ File Organization Best Practices\n\n#### **Core Files**\n\n```hcl\n# ==================================================\n# File: versions.tf\n# ==================================================\n# Terraform and provider version constraints\n\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~> 3.5\"\n    }\n  }\n}\n\n# ==================================================\n# File: providers.tf\n# ==================================================\n# Provider configurations\n\nprovider \"aws\" {\n  region = var.region\n  \n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  alias  = \"us-west-2\"\n  region = \"us-west-2\"\n  \n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n    }\n  }\n}\n\n# ==================================================\n# File: backend.tf\n# ==================================================\n# Remote state backend\n\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"production/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n\n# ==================================================\n# File: variables.tf\n# ==================================================\n# Input variables\n\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"project_name\" {\n  description = \"Project name for resource naming\"\n  type        = string\n  validation {\n    condition     = length(var.project_name) > 0\n    error_message = \"Project name cannot be empty.\"\n  }\n}\n\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n  validation {\n    condition     = contains([\"dev\", \"staging\", \"prod\"], var.environment)\n    error_message = \"Environment must be dev, staging, or prod.\"\n  }\n}\n\n# ==================================================\n# File: locals.tf\n# ==================================================\n# Local values\n\nlocals {\n  name_prefix = \"${var.project_name}-${var.environment}\"\n  \n  common_tags = {\n    Project     = var.project_name\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n  }\n  \n  # Environment-specific configuration\n  config = {\n    dev = {\n      instance_count = 1\n      instance_type  = \"t3.micro\"\n      multi_az       = false\n    }\n    staging = {\n      instance_count = 2\n      instance_type  = \"t3.small\"\n      multi_az       = false\n    }\n    prod = {\n      instance_count = 5\n      instance_type  = \"t3.large\"\n      multi_az       = true\n    }\n  }\n  \n  env_config = local.config[var.environment]\n}\n\n# ==================================================\n# File: data.tf\n# ==================================================\n# Data sources\n\ndata \"aws_caller_identity\" \"current\" {}\n\ndata \"aws_region\" \"current\" {}\n\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\ndata \"aws_ami\" \"amazon_linux_2\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n  \n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n  }\n}\n\n# ==================================================\n# File: main.tf\n# ==================================================\n# Main resources\n\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  \n  project_name = var.project_name\n  environment  = var.environment\n  \n  vpc_cidr             = var.vpc_cidr\n  availability_zones   = var.availability_zones\n  public_subnet_cidrs  = var.public_subnet_cidrs\n  private_subnet_cidrs = var.private_subnet_cidrs\n}\n\nmodule \"compute\" {\n  source = \"./modules/compute\"\n  \n  project_name = var.project_name\n  environment  = var.environment\n  \n  vpc_id              = module.vpc.vpc_id\n  subnet_ids          = module.vpc.private_subnet_ids\n  instance_count      = local.env_config.instance_count\n  instance_type       = local.env_config.instance_type\n}\n\n# ==================================================\n# File: outputs.tf\n# ==================================================\n# Output values\n\noutput \"vpc_id\" {\n  description = \"VPC ID\"\n  value       = module.vpc.vpc_id\n}\n\noutput \"instance_ids\" {\n  description = \"EC2 instance IDs\"\n  value       = module.compute.instance_ids\n}\n\n# ==================================================\n# File: terraform.tfvars\n# ==================================================\n# Variable values (add to .gitignore!)\n\nregion       = \"us-east-1\"\nproject_name = \"myapp\"\nenvironment  = \"prod\"\n\nvpc_cidr             = \"10.0.0.0/16\"\navailability_zones   = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\npublic_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\nprivate_subnet_cidrs = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n```\n\n---\n\n### ðŸŽ¯ Naming Conventions\n\n#### **Resource Names**\n\n```hcl\n# Pattern: {project}-{environment}-{resource_type}-{description}\n\n# Good âœ…\nresource \"aws_vpc\" \"main\" {\n  cidr_block = \"10.0.0.0/16\"\n  \n  tags = {\n    Name = \"${var.project_name}-${var.environment}-vpc\"\n  }\n}\n\nresource \"aws_subnet\" \"public\" {\n  count = 3\n  \n  vpc_id     = aws_vpc.main.id\n  cidr_block = var.public_subnet_cidrs[count.index]\n  \n  tags = {\n    Name = \"${var.project_name}-${var.environment}-public-subnet-${count.index + 1}\"\n  }\n}\n\nresource \"aws_instance\" \"web\" {\n  count = 5\n  \n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t3.micro\"\n  \n  tags = {\n    Name = \"${var.project_name}-${var.environment}-web-${count.index + 1}\"\n  }\n}\n\n# Bad âŒ\nresource \"aws_vpc\" \"vpc1\" {  # Unclear name\n  cidr_block = \"10.0.0.0/16\"\n  \n  tags = {\n    Name = \"my-vpc\"  # Not environment-specific\n  }\n}\n```\n\n#### **Variable Names**\n\n```hcl\n# Good âœ…\nvariable \"vpc_cidr_block\" {\n  description = \"CIDR block for VPC\"\n  type        = string\n}\n\nvariable \"database_instance_class\" {\n  description = \"RDS instance class\"\n  type        = string\n}\n\nvariable \"enable_nat_gateway\" {\n  description = \"Enable NAT gateway for private subnets\"\n  type        = bool\n}\n\n# Bad âŒ\nvariable \"cidr\" {  # Too vague\n  type = string\n}\n\nvariable \"db_size\" {  # Unclear what \"size\" means\n  type = string\n}\n\nvariable \"nat\" {  # Boolean should start with \"enable_\" or \"is_\"\n  type = bool\n}\n```\n\n#### **Module Names**\n\n```hcl\n# Directory structure\nmodules/\nâ”œâ”€â”€ vpc-with-nat/           # Good: descriptive\nâ”œâ”€â”€ ec2-auto-scaling/       # Good: clear purpose\nâ”œâ”€â”€ rds-mysql-primary/      # Good: specific\nâ”œâ”€â”€ s3-static-website/      # Good: use case clear\nâ”œâ”€â”€ networking/             # OK: generic but acceptable\nâ””â”€â”€ compute/                # OK: generic but acceptable\n\n# Bad examples:\nâ”œâ”€â”€ module1/                # Bad: meaningless\nâ”œâ”€â”€ stuff/                  # Bad: too vague\nâ””â”€â”€ resources/              # Bad: too generic\n```\n\n---\n\n### ðŸ—ï¸ Module Organization\n\n#### **Module Structure**\n\n```\nmodules/\nâ””â”€â”€ vpc/\n    â”œâ”€â”€ README.md          # Module documentation\n    â”œâ”€â”€ main.tf            # Main resources\n    â”œâ”€â”€ variables.tf       # Input variables\n    â”œâ”€â”€ outputs.tf         # Output values\n    â”œâ”€â”€ versions.tf        # Provider requirements\n    â”œâ”€â”€ locals.tf          # Local values (optional)\n    â”œâ”€â”€ data.tf            # Data sources (optional)\n    â””â”€â”€ examples/          # Usage examples\n        â”œâ”€â”€ complete/\n        â”‚   â”œâ”€â”€ main.tf\n        â”‚   â””â”€â”€ README.md\n        â””â”€â”€ simple/\n            â”œâ”€â”€ main.tf\n            â””â”€â”€ README.md\n```\n\n#### **Module README Template**\n\n```markdown\n# VPC Module\n\n## Description\nCreates a production-ready VPC with public and private subnets across multiple availability zones.\n\n## Features\n- [x] Multi-AZ deployment\n- [x] NAT Gateway (optional single/multi)\n- [x] Internet Gateway for public subnets\n- [x] Automatic route table configuration\n- [x] VPC Flow Logs (optional)\n- [x] DNS support enabled\n\n## Requirements\n- Terraform >= 1.5.0\n- AWS Provider >= 5.0\n\n## Usage\n\n### Basic Example\n```hcl\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  \n  project_name = \"myapp\"\n  environment  = \"prod\"\n  \n  vpc_cidr             = \"10.0.0.0/16\"\n  availability_zones   = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  public_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  private_subnet_cidrs = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n}\n```\n\n### Complete Example\nSee [examples/complete](./examples/complete)\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|----------|\n| project_name | Project name | string | - | yes |\n| environment | Environment name | string | - | yes |\n| vpc_cidr | VPC CIDR block | string | \"10.0.0.0/16\" | no |\n| enable_nat_gateway | Enable NAT gateway | bool | true | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| vpc_id | VPC ID |\n| public_subnet_ids | Public subnet IDs |\n| private_subnet_ids | Private subnet IDs |\n\n## Cost Estimation\n- VPC: Free\n- Internet Gateway: Free\n- NAT Gateway: ~$32/month per AZ\n- VPC Flow Logs: ~$0.50/GB ingested\n\n**Total (3 AZs): ~$96/month**\n\n## Authors\nPlatform Team <platform@company.com>\n\n## License\nMIT\n```\n\n---\n\n### ðŸ” Secrets Management\n\n#### **âŒ Never Do This**\n\n```hcl\n# BAD - Hardcoded secrets\nresource \"aws_db_instance\" \"main\" {\n  username = \"admin\"\n  password = \"MyPassword123!\"  # âŒ Never hardcode!\n}\n\n# BAD - Secrets in tfvars (even if gitignored)\n# terraform.tfvars\ndatabase_password = \"MyPassword123!\"  # âŒ Still bad!\n```\n\n#### **âœ… Use These Instead**\n\n```hcl\n# Method 1: AWS Secrets Manager\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"prod/database/password\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  username = \"admin\"\n  password = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)[\"password\"]\n}\n\n# Method 2: Random password + store in Secrets Manager\nresource \"random_password\" \"db_password\" {\n  length  = 32\n  special = true\n}\n\nresource \"aws_secretsmanager_secret\" \"db_password\" {\n  name = \"prod/database/password\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id     = aws_secretsmanager_secret.db_password.id\n  secret_string = jsonencode({\n    username = \"admin\"\n    password = random_password.db_password.result\n  })\n}\n\nresource \"aws_db_instance\" \"main\" {\n  username = \"admin\"\n  password = random_password.db_password.result\n  \n  # Password is in state, but state should be encrypted\n}\n\n# Method 3: Environment variables (for Terraform Cloud)\nvariable \"database_password\" {\n  description = \"Database password\"\n  type        = string\n  sensitive   = true\n}\n\n# Set as environment variable:\n# export TF_VAR_database_password=\"xxx\"\n\n# Or in Terraform Cloud as sensitive variable\n```\n\n---\n\n### ðŸ“ Documentation Best Practices\n\n#### **Project README**\n\n```markdown\n# Production Infrastructure\n\nTerraform code for managing production infrastructure.\n\n## Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              AWS Cloud                  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  VPC (10.0.0.0/16)             â”‚    â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚\nâ”‚  â”‚  â”‚ Public  â”‚    â”‚  Private  â”‚ â”‚    â”‚\nâ”‚  â”‚  â”‚ Subnets â”‚â”€â”€â”€â–¶â”‚  Subnets  â”‚ â”‚    â”‚\nâ”‚  â”‚  â”‚  + ALB  â”‚    â”‚   + EC2   â”‚ â”‚    â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   + RDS   â”‚ â”‚    â”‚\nâ”‚  â”‚       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚\nâ”‚  â”‚       â”‚                        â”‚    â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                 â”‚    â”‚\nâ”‚  â”‚  â”‚   IGW    â”‚                 â”‚    â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Prerequisites\n\n- Terraform >= 1.5.0\n- AWS CLI configured\n- S3 bucket for state: `my-terraform-state`\n- DynamoDB table for locking: `terraform-state-lock`\n\n## Directory Structure\n\n```\nterraform/\nâ”œâ”€â”€ environments/\nâ”‚   â”œâ”€â”€ dev/       # Development environment\nâ”‚   â”œâ”€â”€ staging/   # Staging environment\nâ”‚   â””â”€â”€ prod/      # Production environment\nâ””â”€â”€ modules/       # Reusable modules\n```\n\n## Usage\n\n### Initial Setup\n\n```bash\n# 1. Clone repository\ngit clone git@github.com:company/terraform-infrastructure.git\ncd terraform-infrastructure\n\n# 2. Choose environment\ncd environments/prod\n\n# 3. Initialize Terraform\nterraform init\n\n# 4. Review plan\nterraform plan\n\n# 5. Apply changes\nterraform apply\n```\n\n### Making Changes\n\n```bash\n# 1. Create feature branch\ngit checkout -b feature/add-cache\n\n# 2. Make changes\n# ... edit files ...\n\n# 3. Validate\nterraform validate\nterraform fmt -recursive\n\n# 4. Plan\nterraform plan -out=tfplan\n\n# 5. Create PR\ngit add .\ngit commit -m \"Add ElastiCache cluster\"\ngit push origin feature/add-cache\n\n# 6. After approval, merge and apply\n```\n\n## Environments\n\n### Development\n- **Purpose**: Testing and development\n- **Scale**: Minimal (1 instance, t3.micro)\n- **Cost**: ~$50/month\n\n### Staging\n- **Purpose**: Pre-production testing\n- **Scale**: Medium (2 instances, t3.small)\n- **Cost**: ~$200/month\n\n### Production\n- **Purpose**: Live traffic\n- **Scale**: High (5+ instances, t3.large, Multi-AZ)\n- **Cost**: ~$1,500/month\n\n## State Management\n\nState is stored in S3:\n- Bucket: `my-terraform-state`\n- Locking: DynamoDB table `terraform-state-lock`\n- Encryption: AES256\n\n## Team\n\n- **Owner**: Platform Team\n- **On-call**: PagerDuty rotation\n- **Slack**: #infrastructure\n\n## Resources\n\n- [Terraform Docs](https://www.terraform.io/docs)\n- [AWS Provider Docs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)\n- [Internal Wiki](https://wiki.company.com/terraform)\n```\n\n---\n\n### âœ… Day 19 Checklist\n\n- [x] Project structure patterns\n- [x] File organization\n- [x] Naming conventions\n- [x] Module organization\n- [x] Secrets management\n- [x] Documentation best practices\n\n**Next:** Security best practices!\n\n---\n\n## Day 20: Security Best Practices\n\n### ðŸŽ¯ Learning Objectives\n- Secure Terraform state\n- Manage secrets properly\n- Implement least privilege IAM\n- Use security scanning tools\n- Follow compliance requirements\n\n---\n\n### ðŸ”’ State Security\n\n#### **Encrypt State at Rest**\n\n```hcl\n# S3 backend with encryption\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true                    # âœ… Enable encryption\n    kms_key_id     = \"arn:aws:kms:us-east-1:ACCOUNT:key/KEY_ID\"  # âœ… Use KMS\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n```\n\n#### **Restrict State Access**\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowTerraformStateRead\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::ACCOUNT:role/terraform-role\",\n          \"arn:aws:iam::ACCOUNT:user/terraform-ci\"\n        ]\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-terraform-state\",\n        \"arn:aws:s3:::my-terraform-state/*\"\n      ]\n    },\n    {\n      \"Sid\": \"AllowTerraformStateWrite\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::ACCOUNT:role/terraform-role\"\n      },\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::my-terraform-state/*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n        }\n      }\n    },\n    {\n      \"Sid\": \"DenyUnencryptedUploads\",\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::my-terraform-state/*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n        }\n      }\n    }\n  ]\n}\n```\n\n#### **Enable Versioning and Logging**\n\n```hcl\n# State bucket with security features\nresource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = \"my-terraform-state\"\n}\n\nresource \"aws_s3_bucket_versioning\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_logging\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  target_bucket = aws_s3_bucket.logs.id\n  target_prefix = \"terraform-state-logs/\"\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm     = \"aws:kms\"\n      kms_master_key_id = aws_kms_key.terraform_state.arn\n    }\n  }\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n```\n\n---\n\n### ðŸ”‘ Secrets Management\n\n#### **âŒ What NOT to Do**\n\n```hcl\n# âŒ Hardcoded credentials\nprovider \"aws\" {\n  access_key = \"AKIAIOSFODNN7EXAMPLE\"\n  secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n}\n\n# âŒ Secrets in variables\nvariable \"database_password\" {\n  default = \"MyP@ssw0rd123\"\n}\n\n# âŒ Secrets in terraform.tfvars (even gitignored)\ndatabase_password = \"MyP@ssw0rd123\"\n```\n\n#### **âœ… Proper Secrets Management**\n\n```hcl\n# Method 1: AWS Secrets Manager\ndata \"aws_secretsmanager_secret\" \"db_creds\" {\n  name = \"prod/database/credentials\"\n}\n\ndata \"aws_secretsmanager_secret_version\" \"db_creds\" {\n  secret_id = data.aws_secretsmanager_secret.db_creds.id\n}\n\nlocals {\n  db_creds = jsondecode(data.aws_secretsmanager_secret_version.db_creds.secret_string)\n}\n\nresource \"aws_db_instance\" \"main\" {\n  username = local.db_creds.username\n  password = local.db_creds.password\n}\n\n# Method 2: Generate and store secrets\nresource \"random_password\" \"db_password\" {\n  length  = 32\n  special = true\n}\n\nresource \"aws_secretsmanager_secret\" \"db_password\" {\n  name                    = \"prod/database/password\"\n  recovery_window_in_days = 30\n}\n\nresource \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = aws_secretsmanager_secret.db_password.id\n  secret_string = jsonencode({\n    username = \"admin\"\n    password = random_password.db_password.result\n    engine   = \"mysql\"\n  })\n}\n\nresource \"aws_db_instance\" \"main\" {\n  username = \"admin\"\n  password = random_password.db_password.result\n}\n\n# Method 3: AWS Systems Manager Parameter Store\nresource \"aws_ssm_parameter\" \"db_password\" {\n  name  = \"/prod/database/password\"\n  type  = \"SecureString\"\n  value = random_password.db_password.result\n  \n  tags = {\n    Environment = \"production\"\n  }\n}\n\ndata \"aws_ssm_parameter\" \"db_password\" {\n  name = \"/prod/database/password\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  username = \"admin\"\n  password = data.aws_ssm_parameter.db_password.value\n}\n```\n\n#### **Sensitive Variables**\n\n```hcl\n# Mark variables as sensitive\nvariable \"database_password\" {\n  description = \"Database password\"\n  type        = string\n  sensitive   = true  # âœ… Hides value in outputs\n}\n\nvariable \"api_key\" {\n  description = \"API key\"\n  type        = string\n  sensitive   = true\n}\n\n# Sensitive outputs\noutput \"database_password\" {\n  description = \"Database password\"\n  value       = random_password.db_password.result\n  sensitive   = true  # âœ… Hidden in terraform output\n}\n\n# Values are hidden in CLI output but STILL IN STATE FILE!\n# Must encrypt state!\n```\n\n---\n\n### ðŸ‘¤ IAM Best Practices\n\n#### **Least Privilege Policy**\n\n```hcl\n# Bad: Too broad âŒ\ndata \"aws_iam_policy_document\" \"terraform_bad\" {\n  statement {\n    effect    = \"Allow\"\n    actions   = [\"*\"]\n    resources = [\"*\"]\n  }\n}\n\n# Good: Specific permissions âœ…\ndata \"aws_iam_policy_document\" \"terraform_good\" {\n  # VPC management\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"ec2:CreateVpc\",\n      \"ec2:DeleteVpc\",\n      \"ec2:DescribeVpcs\",\n      \"ec2:ModifyVpcAttribute\",\n      \"ec2:CreateSubnet\",\n      \"ec2:DeleteSubnet\",\n      \"ec2:DescribeSubnets\",\n    ]\n    resources = [\"*\"]\n  }\n  \n  # S3 state bucket access\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"s3:GetObject\",\n      \"s3:PutObject\",\n      \"s3:DeleteObject\",\n      \"s3:ListBucket\",\n    ]\n    resources = [\n      \"arn:aws:s3:::my-terraform-state\",\n      \"arn:aws:s3:::my-terraform-state/*\",\n    ]\n  }\n  \n  # DynamoDB state locking\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"dynamodb:GetItem\",\n      \"dynamodb:PutItem\",\n      \"dynamodb:DeleteItem\",\n      \"dynamodb:DescribeTable\",\n    ]\n    resources = [\n      \"arn:aws:dynamodb:*:*:table/terraform-state-lock\",\n    ]\n  }\n}\n\nresource \"aws_iam_policy\" \"terraform\" {\n  name   = \"terraform-deployment\"\n  policy = data.aws_iam_policy_document.terraform_good.json\n}\n```\n\n#### **Assume Role Pattern**\n\n```hcl\n# Terraform role with specific permissions\nresource \"aws_iam_role\" \"terraform\" {\n  name = \"terraform-deployment\"\n  \n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          AWS = [\n            \"arn:aws:iam::ACCOUNT:user/terraform-ci\",\n            \"arn:aws:iam::ACCOUNT:role/GitHubActions\",\n          ]\n        }\n        Action = \"sts:AssumeRole\"\n        Condition = {\n          StringEquals = {\n            \"sts:ExternalId\" = \"terraform-deployment-key\"\n          }\n        }\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"terraform\" {\n  role       = aws_iam_role.terraform.name\n  policy_arn = aws_iam_policy.terraform.arn\n}\n\n# Use in Terraform\nprovider \"aws\" {\n  region = \"us-east-1\"\n  \n  assume_role {\n    role_arn     = \"arn:aws:iam::ACCOUNT:role/terraform-deployment\"\n    external_id  = \"terraform-deployment-key\"\n    session_name = \"terraform-session\"\n  }\n}\n```\n\n---\n\n### ðŸ›¡ï¸ Security Scanning\n\n#### **tfsec - Security Scanner**\n\n```bash\n# Install\nbrew install tfsec\n\n# Or\ncurl -s https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash\n\n# Scan current directory\ntfsec .\n\n# Output:\n# Result #1 HIGH S3 Bucket does not have encryption enabled\n#   /main.tf:10-15\n# \n# 10 resource \"aws_s3_bucket\" \"data\" {\n# 11   bucket = \"my-data-bucket\"\n# 12 }\n#\n# Fix: Add encryption configuration\n```\n\n**Fix security issues:**\n\n```hcl\n# Before (insecure)\nresource \"aws_s3_bucket\" \"data\" {\n  bucket = \"my-data-bucket\"\n}\n\n# After (secure)\nresource \"aws_s3_bucket\" \"data\" {\n  bucket = \"my-data-bucket\"\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n  \n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"data\" {\n  bucket = aws_s3_bucket.data.id\n  \n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n```\n\n#### **Checkov - Policy Scanner**\n\n```bash\n# Install\npip install checkov\n\n# Scan\ncheckov -d .\n\n# Output:\n# Check: CKV_AWS_18: \"Ensure S3 bucket has access logging enabled\"\n# FAILED for resource: aws_s3_bucket.data\n# Fix: Enable logging\n```\n\n**Integration in CI/CD:**\n\n```yaml\n# .github/workflows/security-scan.yml\nname: Security Scan\n\non: [pull_request]\n\njobs:\n  tfsec:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run tfsec\n        uses: aquasecurity/tfsec-action@v1.0.0\n        with:\n          soft_fail: false\n  \n  checkov:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Checkov\n        uses: bridgecrewio/checkov-action@master\n        with:\n          directory: .\n          soft_fail: false\n```\n\n---\n\n### ðŸ” Compliance and Auditing\n\n#### **Tagging Strategy**\n\n```hcl\n# Required tags for compliance\nlocals {\n  required_tags = {\n    Project     = var.project_name\n    Environment = var.environment\n    Owner       = var.owner_email\n    CostCenter  = var.cost_center\n    Compliance  = var.compliance_level  # \"pci\", \"hipaa\", \"sox\"\n    ManagedBy   = \"Terraform\"\n    CreatedDate = timestamp()\n  }\n}\n\n# Apply to all resources via provider\nprovider \"aws\" {\n  region = var.region\n  \n  default_tags {\n    tags = local.required_tags\n  }\n}\n\n# Enforce tagging with precondition\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = var.instance_type\n  \n  tags = merge(\n    local.required_tags,\n    {\n      Name = \"web-server\"\n    }\n  )\n  \n  lifecycle {\n    precondition {\n      condition     = length(keys(merge(local.required_tags, self.tags))) >= 7\n      error_message = \"All required tags must be present.\"\n    }\n  }\n}\n```\n\n#### **Audit Logging**\n\n```hcl\n# Enable CloudTrail for all API calls\nresource \"aws_cloudtrail\" \"main\" {\n  name                          = \"terraform-audit-trail\"\n  s3_bucket_name                = aws_s3_bucket.cloudtrail.id\n  include_global_service_events = true\n  is_multi_region_trail         = true\n  enable_log_file_validation    = true\n  \n  event_selector {\n    read_write_type           = \"All\"\n    include_management_events = true\n    \n    data_resource {\n      type   = \"AWS::S3::Object\"\n      values = [\"arn:aws:s3:::my-terraform-state/*\"]\n    }\n  }\n}\n\n# VPC Flow Logs\nresource \"aws_flow_log\" \"main\" {\n  vpc_id          = aws_vpc.main.id\n  traffic_type    = \"ALL\"\n  iam_role_arn    = aws_iam_role.flow_log.arn\n  log_destination = aws_cloudwatch_log_group.flow_log.arn\n}\n\n# S3 access logging\nresource \"aws_s3_bucket_logging\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  target_bucket = aws_s3_bucket.logs.id\n  target_prefix = \"terraform-state-access/\"\n}\n```\n\n---\n\n### âœ… Day 20 Checklist\n\n- [x] State encryption and access control\n- [x] Secrets management (Secrets Manager, SSM)\n- [x] Sensitive variables and outputs\n- [x] Least privilege IAM policies\n- [x] Security scanning (tfsec, Checkov)\n- [x] Compliance tagging\n- [x] Audit logging\n\n**Next:** Testing strategies for Terraform!\n\n---\n\n## Day 21: Testing Strategies\n\n### ðŸŽ¯ Learning Objectives\n- Test Terraform configurations\n- Use terraform validate and fmt\n- Implement automated testing\n- Use Terratest for integration tests\n- Set up pre-commit hooks\n\n---\n\n### âœ… Built-in Validation\n\n#### **terraform validate**\n\n```bash\n# Validate syntax and configuration\nterraform validate\n\n# Output on success:\n# Success! The configuration is valid.\n\n# Output on error:\n# Error: Invalid reference\n#   on main.tf line 15:\n#   Resource 'aws_subnet.public' not found.\n```\n\n#### **terraform fmt**\n\n```bash\n# Format code (dry run)\nterraform fmt -check -diff\n\n# Format code (apply changes)\nterraform fmt -recursive\n\n# Returns:\n# main.tf\n# modules/vpc/main.tf\n# modules/compute/main.tf\n```\n\n#### **terraform plan**\n\n```bash\n# Detailed plan\nterraform plan\n\n# Save plan for review\nterraform plan -out=tfplan\n\n# Review saved plan\nterraform show tfplan\n\n# JSON output for automated testing\nterraform plan -out=tfplan\nterraform show -json tfplan > plan.json\n```\n\n---\n\n### ðŸ§ª Testing Pyramid\n\n```plaintext\n        /\\\n       /  \\\n      / E2E\\         End-to-End Tests (Terratest)\n     /______\\\n    /        \\\n   / Integr. \\       Integration Tests (Kitchen-Terraform)\n  /___________\\\n /             \\\n/   Unit Tests  \\    Unit Tests (validate, fmt, tflint)\n/_________________\\\n```\n\n---\n\n### ðŸ” Static Analysis\n\n#### **TFLint - Linter**\n\n```bash\n# Install\nbrew install tflint\n\n# Or\ncurl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash\n\n# Initialize plugins\ntflint --init\n\n# Run linter\ntflint\n\n# Output:\n# 3 issue(s) found:\n# \n# Warning: terraform_unused_declarations (main.tf:10)\n# variable \"unused_var\" is declared but not used\n# \n# Error: aws_instance_invalid_type (main.tf:25)\n# \"t2.mega\" is an invalid instance type\n```\n\n**Configuration:**\n\n```hcl\n# .tflint.hcl\n\nplugin \"aws\" {\n  enabled = true\n  version = \"0.27.0\"\n  source  = \"github.com/terraform-linters/tflint-ruleset-aws\"\n}\n\nrule \"terraform_naming_convention\" {\n  enabled = true\n  \n  format = \"snake_case\"\n}\n\nrule \"terraform_unused_declarations\" {\n  enabled = true\n}\n\nrule \"terraform_deprecated_interpolation\" {\n  enabled = true\n}\n\nrule \"terraform_module_pinned_source\" {\n  enabled = true\n  style   = \"semver\"\n}\n```\n\n---\n\n### ðŸ§ª Terratest - Go-Based Testing\n\n**Install:**\n\n```bash\ngo mod init github.com/mycompany/terraform-tests\ngo get github.com/gruntwork-io/terratest/modules/terraform\n```\n\n**Test Structure:**\n\n```\ntests/\nâ”œâ”€â”€ go.mod\nâ”œâ”€â”€ go.sum\nâ””â”€â”€ terraform_basic_test.go\n```\n\n**Example Test:**\n\n```go\n// File: tests/terraform_basic_test.go\n\npackage test\n\nimport (\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestTerraformBasicExample(t *testing.T) {\n    t.Parallel()\n    \n    // Terraform options\n    terraformOptions := &terraform.Options{\n        // Path to Terraform code\n        TerraformDir: \"../examples/basic\",\n        \n        // Variables to pass\n        Vars: map[string]interface{}{\n            \"instance_type\": \"t2.micro\",\n            \"region\":        \"us-east-1\",\n        },\n        \n        // Disable colors in output\n        NoColor: true,\n    }\n    \n    // Clean up resources\n    defer terraform.Destroy(t, terraformOptions)\n    \n    // Run terraform init and apply\n    terraform.InitAndApply(t, terraformOptions)\n    \n    // Get outputs\n    instanceId := terraform.Output(t, terraformOptions, \"instance_id\")\n    publicIp := terraform.Output(t, terraformOptions, \"public_ip\")\n    \n    // Assertions\n    assert.NotEmpty(t, instanceId)\n    assert.NotEmpty(t, publicIp)\n    assert.Regexp(t, \"^i-[a-f0-9]+$\", instanceId)\n}\n```\n\n**Advanced Test:**\n\n```go\n// File: tests/terraform_vpc_test.go\n\npackage test\n\nimport (\n    \"fmt\"\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/aws\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestTerraformVPCExample(t *testing.T) {\n    t.Parallel()\n    \n    expectedRegion := \"us-east-1\"\n    expectedVpcCidr := \"10.0.0.0/16\"\n    \n    terraformOptions := &terraform.Options{\n        TerraformDir: \"../modules/vpc\",\n        \n        Vars: map[string]interface{}{\n            \"project_name\":       \"test\",\n            \"environment\":        \"test\",\n            \"vpc_cidr\":           expectedVpcCidr,\n            \"availability_zones\": []string{\"us-east-1a\", \"us-east-1b\"},\n            \"public_subnet_cidrs\": []string{\n                \"10.0.1.0/24\",\n                \"10.0.2.0/24\",\n            },\n            \"private_subnet_cidrs\": []string{\n                \"10.0.11.0/24\",\n                \"10.0.12.0/24\",\n            },\n        },\n    }\n    \n    defer terraform.Destroy(t, terraformOptions)\n    \n    terraform.InitAndApply(t, terraformOptions)\n    \n    // Get outputs\n    vpcId := terraform.Output(t, terraformOptions, \"vpc_id\")\n    publicSubnetIds := terraform.OutputList(t, terraformOptions, \"public_subnet_ids\")\n    privateSubnetIds := terraform.OutputList(t, terraformOptions, \"private_subnet_ids\")\n    \n    // Validate VPC exists\n    vpc := aws.GetVpcById(t, vpcId, expectedRegion)\n    assert.Equal(t, expectedVpcCidr, *vpc.CidrBlock)\n    assert.True(t, *vpc.EnableDnsHostnames)\n    \n    // Validate subnets\n    assert.Equal(t, 2, len(publicSubnetIds))\n    assert.Equal(t, 2, len(privateSubnetIds))\n    \n    // Validate subnet CIDR blocks\n    for i, subnetId := range publicSubnetIds {\n        subnet := aws.GetSubnetById(t, subnetId, expectedRegion)\n        expectedCidr := fmt.Sprintf(\"10.0.%d.0/24\", i+1)\n        assert.Equal(t, expectedCidr, *subnet.CidrBlock)\n    }\n    \n    // Validate NAT gateways exist\n    natGateways := aws.GetNatGatewaysInVpc(t, vpcId, expectedRegion)\n    assert.Equal(t, 2, len(natGateways))\n}\n```\n\n**Run tests:**\n\n```bash\n# Run all tests\ncd tests\ngo test -v -timeout 30m\n\n# Run specific test\ngo test -v -run TestTerraformVPCExample -timeout 30m\n\n# Run tests in parallel\ngo test -v -parallel 10 -timeout 30m\n```\n\n---\n\n### ðŸ”§ Pre-Commit Hooks\n\n**Install pre-commit:**\n\n```bash\n# macOS\nbrew install pre-commit\n\n# Linux\npip install pre-commit\n```\n\n**Configuration:**\n\n```yaml\n# File: .pre-commit-config.yaml\n\nrepos:\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\n    rev: v1.83.5\n    hooks:\n      - id: terraform_fmt\n      - id: terraform_validate\n      - id: terraform_docs\n        args:\n          - --hook-config=--path-to-file=README.md\n          - --hook-config=--add-to-existing-file=true\n          - --hook-config=--create-file-if-not-exist=true\n      - id: terraform_tflint\n        args:\n          - --args=--config=__GIT_WORKING_DIR__/.tflint.hcl\n      - id: terraform_tfsec\n        args:\n          - --args=--soft-fail\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n      - id: check-merge-conflict\n```\n\n**Install hooks:**\n\n```bash\n# Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n\n# Update hooks\npre-commit autoupdate\n```\n\n---\n\n### ðŸ“Š Test Coverage\n\n**Example test suite:**\n\n```go\n// File: tests/terraform_test.go\n\npackage test\n\nimport (\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\n// Test 1: Basic validation\nfunc TestValidation(t *testing.T) {\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"..\",\n    }\n    \n    // Should not error\n    terraform.Init(t, terraformOptions)\n    terraform.Validate(t, terraformOptions)\n}\n\n// Test 2: Plan shows no changes for existing state\nfunc TestIdempotency(t *testing.T) {\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"..\",\n    }\n    \n    defer terraform.Destroy(t, terraformOptions)\n    \n    // First apply\n    terraform.InitAndApply(t, terraformOptions)\n    \n    // Second plan should show no changes\n    planExitCode := terraform.PlanExitCode(t, terraformOptions)\n    assert.Equal(t, 0, planExitCode)\n}\n\n// Test 3: Outputs are correct type\nfunc TestOutputs(t *testing.T) {\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"..\",\n    }\n    \n    defer terraform.Destroy(t, terraformOptions)\n    \n    terraform.InitAndApply(t, terraformOptions)\n    \n    // String output\n    vpcId := terraform.Output(t, terraformOptions, \"vpc_id\")\n    assert.Regexp(t, \"^vpc-[a-f0-9]+$\", vpcId)\n    \n    // List output\n    subnetIds := terraform.OutputList(t, terraformOptions, \"subnet_ids\")\n    assert.Greater(t, len(subnetIds), 0)\n    \n    // Map output\n    tags := terraform.OutputMap(t, terraformOptions, \"common_tags\")\n    assert.Contains(t, tags, \"Environment\")\n}\n\n// Test 4: Security group rules are correct\nfunc TestSecurityGroups(t *testing.T) {\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"..\",\n    }\n    \n    defer terraform.Destroy(t, terraformOptions)\n    \n    terraform.InitAndApply(t, terraformOptions)\n    \n    sgId := terraform.Output(t, terraformOptions, \"security_group_id\")\n    \n    // Verify security group rules\n    region := terraform.Output(t, terraformOptions, \"region\")\n    sg := aws.GetSecurityGroupById(t, sgId, region)\n    \n    assert.Equal(t, 2, len(sg.IpPermissions))  // 2 ingress rules\n    assert.Equal(t, 1, len(sg.IpPermissionsEgress))  // 1 egress rule\n}\n```\n\n---\n\n### ðŸŽ¯ Complete Testing Workflow\n\n```bash\n#!/bin/bash\n# File: scripts/test.sh\n\nset -e\n\necho \"===== Running Terraform Tests =====\"\n\n# 1. Format check\necho \"1. Checking format...\"\nterraform fmt -check -recursive -diff\n\n# 2. Validation\necho \"2. Validating configuration...\"\nterraform validate\n\n# 3. Linting\necho \"3. Running TFLint...\"\ntflint --init\ntflint --recursive\n\n# 4. Security scanning\necho \"4. Running tfsec...\"\ntfsec .\n\necho \"5. Running Checkov...\"\ncheckov -d .\n\n# 6. Run Terratest (if Go is installed)\nif command -v go &> /dev/null; then\n    echo \"6. Running Terratest...\"\n    cd tests\n    go test -v -timeout 30m\n    cd ..\nfi\n\necho \"===== All Tests Passed! =====\"\n```\n\n---\n\n### âœ… Day 21 Checklist\n\n- [x] terraform validate and fmt\n- [x] terraform plan testing\n- [x] TFLint for static analysis\n- [x] Terratest for integration tests\n- [x] Pre-commit hooks\n- [x] Test coverage strategies\n- [x] Complete testing workflow\n\n**Next:** CI/CD integration!\n\n---\n\n## Day 22: CI/CD Integration\n\n### ðŸŽ¯ Learning Objectives\n- Set up Terraform CI/CD pipelines\n- Use GitHub Actions\n- Use GitLab CI\n- Use Jenkins\n- Implement approval workflows\n- Handle secrets in CI/CD\n\n---\n\n### ðŸ”„ CI/CD Workflow\n\n```plaintext\nDeveloper Workflow:\n1. Developer creates branch\n2. Makes Terraform changes\n3. Pushes to Git\n4. CI runs automatically:\n   - terraform fmt -check\n   - terraform validate\n   - tflint\n   - tfsec/checkov\n   - terraform plan\n5. Plan posted as PR comment\n6. Team reviews plan\n7. PR merged to main\n8. CD runs automatically:\n   - terraform plan\n   - Wait for approval\n   - terraform apply\n```\n\n---\n\n### ðŸ™ GitHub Actions\n\n#### **Complete Workflow**\n\n```yaml\n# File: .github/workflows/terraform.yml\n\nname: Terraform CI/CD\n\non:\n  pull_request:\n    branches: [main]\n    paths:\n      - 'terraform/**'\n      - '.github/workflows/terraform.yml'\n  push:\n    branches: [main]\n    paths:\n      - 'terraform/**'\n\nenv:\n  TF_VERSION: '1.7.0'\n  AWS_REGION: 'us-east-1'\n\njobs:\n  terraform-check:\n    name: Terraform Check\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: terraform\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n      \n      - name: Terraform Format\n        id: fmt\n        run: terraform fmt -check -recursive -diff\n        continue-on-error: true\n      \n      - name: Terraform Init\n        id: init\n        run: terraform init -backend=false\n      \n      - name: Terraform Validate\n        id: validate\n        run: terraform validate -no-color\n      \n      - name: TFLint\n        uses: terraform-linters/setup-tflint@v4\n        with:\n          tflint_version: latest\n      \n      - name: Run TFLint\n        run: |\n          tflint --init\n          tflint --format compact\n      \n      - name: tfsec\n        uses: aquasecurity/tfsec-action@v1.0.3\n        with:\n          working_directory: terraform\n          soft_fail: false\n      \n      - name: Checkov\n        uses: bridgecrewio/checkov-action@master\n        with:\n          directory: terraform\n          soft_fail: false\n          framework: terraform\n  \n  terraform-plan:\n    name: Terraform Plan\n    runs-on: ubuntu-latest\n    needs: terraform-check\n    if: github.event_name == 'pull_request'\n    defaults:\n      run:\n        working-directory: terraform\n    \n    permissions:\n      contents: read\n      pull-requests: write\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n      \n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n      \n      - name: Terraform Init\n        run: terraform init\n      \n      - name: Terraform Plan\n        id: plan\n        run: |\n          terraform plan -no-color -out=tfplan\n          terraform show -no-color tfplan > plan_output.txt\n        continue-on-error: true\n      \n      - name: Comment Plan on PR\n        uses: actions/github-script@v7\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          script: |\n            const fs = require('fs');\n            const plan = fs.readFileSync('terraform/plan_output.txt', 'utf8');\n            const output = `#### Terraform Plan ðŸ“–\n            <details><summary>Show Plan</summary>\n            \n            \\`\\`\\`terraform\n            ${plan}\n            \\`\\`\\`\n            \n            </details>\n            \n            *Pushed by: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`;\n            \n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            });\n      \n      - name: Plan Status\n        if: steps.plan.outcome == 'failure'\n        run: exit 1\n  \n  terraform-apply:\n    name: Terraform Apply\n    runs-on: ubuntu-latest\n    needs: terraform-check\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    defaults:\n      run:\n        working-directory: terraform\n    \n    environment:\n      name: production\n      url: https://console.aws.amazon.com\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n      \n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n      \n      - name: Terraform Init\n        run: terraform init\n      \n      - name: Terraform Plan\n        run: terraform plan -out=tfplan\n      \n      - name: Terraform Apply\n        run: terraform apply -auto-approve tfplan\n      \n      - name: Post-deployment tests\n        run: |\n          # Run smoke tests\n          echo \"Running post-deployment tests...\"\n          # Add your tests here\n```\n\n**OIDC Configuration for AWS:**\n\n```hcl\n# Configure GitHub OIDC provider in AWS\nresource \"aws_iam_openid_connect_provider\" \"github\" {\n  url = \"https://token.actions.githubusercontent.com\"\n  \n  client_id_list = [\"sts.amazonaws.com\"]\n  \n  thumbprint_list = [\n    \"6938fd4d98bab03faadb97b34396831e3780aea1\"\n  ]\n}\n\nresource \"aws_iam_role\" \"github_actions\" {\n  name = \"github-actions-terraform\"\n  \n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Federated = aws_iam_openid_connect_provider.github.arn\n        }\n        Action = \"sts:AssumeRoleWithWebIdentity\"\n        Condition = {\n          StringEquals = {\n            \"token.actions.githubusercontent.com:aud\" = \"sts.amazonaws.com\"\n          }\n          StringLike = {\n            \"token.actions.githubusercontent.com:sub\" = \"repo:myorg/myrepo:*\"\n          }\n        }\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"github_actions\" {\n  role       = aws_iam_role.github_actions.name\n  policy_arn = aws_iam_policy.terraform_deployment.arn\n}\n```\n\n---\n\n### ðŸ¦Š GitLab CI\n\n```yaml\n# File: .gitlab-ci.yml\n\nimage:\n  name: hashicorp/terraform:1.7.0\n  entrypoint: [\"\"]\n\nvariables:\n  TF_ROOT: ${CI_PROJECT_DIR}/terraform\n  TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/production\n\ncache:\n  paths:\n    - ${TF_ROOT}/.terraform\n\nbefore_script:\n  - cd ${TF_ROOT}\n\nstages:\n  - validate\n  - plan\n  - apply\n\nvalidate:\n  stage: validate\n  script:\n    - terraform fmt -check -recursive\n    - terraform init -backend=false\n    - terraform validate\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n\nsecurity_scan:\n  stage: validate\n  image: aquasec/tfsec:latest\n  script:\n    - tfsec ${TF_ROOT}\n  allow_failure: true\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n\nplan:\n  stage: plan\n  script:\n    - terraform init\n    - terraform plan -out=tfplan\n    - terraform show -no-color tfplan > plan_output.txt\n  artifacts:\n    paths:\n      - ${TF_ROOT}/tfplan\n      - ${TF_ROOT}/plan_output.txt\n    expire_in: 1 week\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n\napply:\n  stage: apply\n  script:\n    - terraform init\n    - terraform apply -auto-approve tfplan\n  dependencies:\n    - plan\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n      when: manual\n  only:\n    - main\n```\n\n---\n\n### ðŸ”¨ Jenkins\n\n```groovy\n// File: Jenkinsfile\n\npipeline {\n    agent any\n    \n    parameters {\n        choice(\n            name: 'ACTION',\n            choices: ['plan', 'apply', 'destroy'],\n            description: 'Terraform action to perform'\n        )\n        choice(\n            name: 'ENVIRONMENT',\n            choices: ['dev', 'staging', 'prod'],\n            description: 'Environment to deploy'\n        )\n    }\n    \n    environment {\n        AWS_CREDENTIALS = credentials('aws-credentials')\n        TF_VERSION = '1.7.0'\n        TF_ROOT = 'terraform'\n    }\n    \n    stages {\n        stage('Setup') {\n            steps {\n                script {\n                    // Install Terraform\n                    sh '''\n                        if ! command -v terraform &> /dev/null; then\n                            wget https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip\n                            unzip terraform_${TF_VERSION}_linux_amd64.zip\n                            sudo mv terraform /usr/local/bin/\n                        fi\n                    '''\n                }\n            }\n        }\n        \n        stage('Validate') {\n            steps {\n                dir(\"${TF_ROOT}\") {\n                    sh 'terraform fmt -check -recursive'\n                    sh 'terraform init -backend=false'\n                    sh 'terraform validate'\n                }\n            }\n        }\n        \n        stage('Security Scan') {\n            steps {\n                script {\n                    // Run tfsec\n                    sh 'docker run --rm -v $(pwd):/src aquasec/tfsec /src/${TF_ROOT}'\n                }\n            }\n        }\n        \n        stage('Plan') {\n            steps {\n                dir(\"${TF_ROOT}\") {\n                    withCredentials([\n                        [\n                            $class: 'AmazonWebServicesCredentialsBinding',\n                            credentialsId: 'aws-credentials',\n                            accessKeyVariable: 'AWS_ACCESS_KEY_ID',\n                            secretKeyVariable: 'AWS_SECRET_ACCESS_KEY'\n                        ]\n                    ]) {\n                        sh 'terraform init'\n                        sh 'terraform plan -out=tfplan'\n                        sh 'terraform show -no-color tfplan > plan_output.txt'\n                    }\n                }\n                \n                archiveArtifacts artifacts: \"${TF_ROOT}/plan_output.txt\"\n            }\n        }\n        \n        stage('Approval') {\n            when {\n                expression { params.ACTION == 'apply' }\n                expression { params.ENVIRONMENT == 'prod' }\n            }\n            steps {\n                script {\n                    def plan = readFile(\"${TF_ROOT}/plan_output.txt\")\n                    \n                    input(\n                        message: \"Review the plan and approve to proceed with apply:\",\n                        parameters: [\n                            text(\n                                name: 'PLAN',\n                                defaultValue: plan,\n                                description: 'Terraform Plan Output'\n                            )\n                        ]\n                    )\n                }\n            }\n        }\n        \n        stage('Apply') {\n            when {\n                expression { params.ACTION == 'apply' }\n            }\n            steps {\n                dir(\"${TF_ROOT}\") {\n                    withCredentials([\n                        [\n                            $class: 'AmazonWebServicesCredentialsBinding',\n                            credentialsId: 'aws-credentials',\n                            accessKeyVariable: 'AWS_ACCESS_KEY_ID',\n                            secretKeyVariable: 'AWS_SECRET_ACCESS_KEY'\n                        ]\n                    ]) {\n                        sh 'terraform apply -auto-approve tfplan'\n                    }\n                }\n            }\n        }\n        \n        stage('Destroy') {\n            when {\n                expression { params.ACTION == 'destroy' }\n            }\n            steps {\n                input message: 'Are you sure you want to destroy?', ok: 'Destroy'\n                \n                dir(\"${TF_ROOT}\") {\n                    withCredentials([\n                        [\n                            $class: 'AmazonWebServicesCredentialsBinding',\n                            credentialsId: 'aws-credentials',\n                            accessKeyVariable: 'AWS_ACCESS_KEY_ID',\n                            secretKeyVariable: 'AWS_SECRET_ACCESS_KEY'\n                        ]\n                    ]) {\n                        sh 'terraform destroy -auto-approve'\n                    }\n                }\n            }\n        }\n    }\n    \n    post {\n        always {\n            cleanWs()\n        }\n        success {\n            slackSend(\n                color: 'good',\n                message: \"Terraform ${params.ACTION} succeeded for ${params.ENVIRONMENT}\"\n            )\n        }\n        failure {\n            slackSend(\n                color: 'danger',\n                message: \"Terraform ${params.ACTION} failed for ${params.ENVIRONMENT}\"\n            )\n        }\n    }\n}\n```\n\n---\n\n### ðŸ” Secrets Management in CI/CD\n\n#### **GitHub Actions Secrets**\n\n```yaml\n# Use GitHub Secrets\nsteps:\n  - name: Configure AWS\n    env:\n      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    run: terraform apply\n```\n\n#### **Terraform Cloud Variables**\n\n```hcl\nterraform {\n  cloud {\n    organization = \"my-org\"\n    workspaces {\n      name = \"production\"\n    }\n  }\n}\n\n# Variables set in Terraform Cloud UI as sensitive\n# - AWS_ACCESS_KEY_ID (sensitive)\n# - AWS_SECRET_ACCESS_KEY (sensitive)\n# - database_password (sensitive)\n```\n\n#### **AWS Secrets Manager Integration**\n\n```yaml\nsteps:\n  - name: Get secrets from AWS Secrets Manager\n    uses: aws-actions/aws-secretsmanager-get-secrets@v1\n    with:\n      secret-ids: |\n        prod/database/password\n        prod/api/keys\n      parse-json-secrets: true\n  \n  - name: Use secrets\n    run: |\n      terraform apply \\\n        -var=\"db_password=${{ env.PROD_DATABASE_PASSWORD_PASSWORD }}\"\n```\n\n---\n\n### âœ… Day 22 Checklist\n\n- [x] CI/CD workflow design\n- [x] GitHub Actions integration\n- [x] GitLab CI configuration\n- [x] Jenkins pipeline\n- [x] Approval workflows\n- [x] Secrets management in CI/CD\n- [x] OIDC authentication\n\n**Next:** Real-World Projects!\n\n---\n\n# ðŸš€ PART 6: REAL-WORLD PROJECTS\n\n## Project 1: Simple Web Server\n\n### ðŸ“‹ Project Overview\n\nDeploy a simple web server on AWS with:\n- EC2 instance running Apache\n- Security group allowing HTTP/HTTPS\n- Elastic IP for static IP\n- User data for automatic configuration\n\n**Architecture:**\n\n```plaintext\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         Internet            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚ Elastic IP   â”‚\n      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚ Security Groupâ”‚\n      â”‚  Port 80, 443 â”‚\n      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  EC2 Instance â”‚\n      â”‚    Apache     â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n### ðŸ“ Project Structure\n\n```\nproject-1-simple-web-server/\nâ”œâ”€â”€ main.tf\nâ”œâ”€â”€ variables.tf\nâ”œâ”€â”€ outputs.tf\nâ”œâ”€â”€ terraform.tfvars\nâ”œâ”€â”€ user_data.sh\nâ””â”€â”€ README.md\n```\n\n---\n\n### ðŸ’» Complete Code\n\n```hcl\n# ==================================================\n# File: variables.tf\n# ==================================================\n\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"project_name\" {\n  description = \"Project name for tagging\"\n  type        = string\n  default     = \"simple-web\"\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nvariable \"allowed_ips\" {\n  description = \"IP addresses allowed to access the server\"\n  type        = list(string)\n  default     = [\"0.0.0.0/0\"]  # Change to your IP for security\n}\n\n# ==================================================\n# File: main.tf\n# ==================================================\n\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n  \n  default_tags {\n    tags = {\n      Project   = var.project_name\n      ManagedBy = \"Terraform\"\n    }\n  }\n}\n\n# Data sources\ndata \"aws_ami\" \"amazon_linux_2\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n  \n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n  }\n  \n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\n# Security Group\nresource \"aws_security_group\" \"web\" {\n  name        = \"${var.project_name}-sg\"\n  description = \"Security group for web server\"\n  \n  # HTTP\n  ingress {\n    description = \"HTTP from anywhere\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = var.allowed_ips\n  }\n  \n  # HTTPS\n  ingress {\n    description = \"HTTPS from anywhere\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = var.allowed_ips\n  }\n  \n  # SSH (optional - for management)\n  ingress {\n    description = \"SSH from allowed IPs\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = var.allowed_ips\n  }\n  \n  # Allow all outbound\n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${var.project_name}-sg\"\n  }\n}\n\n# EC2 Instance\nresource \"aws_instance\" \"web\" {\n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = var.instance_type\n  vpc_security_group_ids = [aws_security_group.web.id]\n  \n  user_data = file(\"${path.module}/user_data.sh\")\n  \n  root_block_device {\n    volume_size = 8\n    volume_type = \"gp3\"\n    encrypted   = true\n  }\n  \n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 1\n  }\n  \n  tags = {\n    Name = \"${var.project_name}-server\"\n  }\n}\n\n# Elastic IP\nresource \"aws_eip\" \"web\" {\n  instance = aws_instance.web.id\n  domain   = \"vpc\"\n  \n  tags = {\n    Name = \"${var.project_name}-eip\"\n  }\n}\n\n# ==================================================\n# File: outputs.tf\n# ==================================================\n\noutput \"instance_id\" {\n  description = \"EC2 instance ID\"\n  value       = aws_instance.web.id\n}\n\noutput \"public_ip\" {\n  description = \"Public IP address (Elastic IP)\"\n  value       = aws_eip.web.public_ip\n}\n\noutput \"web_url\" {\n  description = \"URL to access the web server\"\n  value       = \"http://${aws_eip.web.public_ip}\"\n}\n\noutput \"ssh_command\" {\n  description = \"SSH command to connect\"\n  value       = \"ssh -i your-key.pem ec2-user@${aws_eip.web.public_ip}\"\n}\n\n# ==================================================\n# File: user_data.sh\n# ==================================================\n\n#!/bin/bash\nset -e\n\n# Update system\nyum update -y\n\n# Install Apache\nyum install -y httpd\n\n# Create simple webpage\ncat > /var/www/html/index.html <<'EOF'\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Simple Web Server</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n            max-width: 800px;\n            margin: 50px auto;\n            padding: 20px;\n            background: #f5f5f5;\n        }\n        .container {\n            background: white;\n            padding: 30px;\n            border-radius: 10px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        h1 { color: #333; }\n        .info { background: #e8f4f8; padding: 15px; border-radius: 5px; }\n        .success { color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>ðŸš€ Simple Web Server</h1>\n        <p class=\"success\">âœ“ Apache is running successfully!</p>\n        <div class=\"info\">\n            <h3>Server Information</h3>\n            <p><strong>Hostname:</strong> $(hostname)</p>\n            <p><strong>IP Address:</strong> $(hostname -I)</p>\n            <p><strong>Date:</strong> $(date)</p>\n        </div>\n        <p>This server was deployed using Terraform! ðŸŽ‰</p>\n    </div>\n</body>\n</html>\nEOF\n\n# Start and enable Apache\nsystemctl start httpd\nsystemctl enable httpd\n\n# Configure firewall (if firewalld is running)\nif systemctl is-active --quiet firewalld; then\n    firewall-cmd --permanent --add-service=http\n    firewall-cmd --permanent --add-service=https\n    firewall-cmd --reload\nfi\n\necho \"Web server setup complete!\"\n\n# ==================================================\n# File: terraform.tfvars\n# ==================================================\n\nregion        = \"us-east-1\"\nproject_name  = \"simple-web\"\ninstance_type = \"t2.micro\"\n\n# IMPORTANT: Change this to your IP for security\n# Get your IP: curl ifconfig.me\nallowed_ips = [\"0.0.0.0/0\"]  # Allow from anywhere (not recommended for prod)\n# allowed_ips = [\"YOUR.IP.ADDRESS.HERE/32\"]  # Restrict to your IP\n\n# ==================================================\n# File: README.md\n# ==================================================\n```\n\n```markdown\n# Project 1: Simple Web Server\n\n## Description\nDeploy a simple web server on AWS using Terraform.\n\n## Architecture\n- EC2 instance running Amazon Linux 2\n- Apache web server\n- Elastic IP for static IP address\n- Security group with HTTP/HTTPS access\n\n## Prerequisites\n- AWS account\n- AWS CLI configured\n- Terraform >= 1.5.0\n\n## Deployment\n\n1. **Initialize Terraform:**\n   ```bash\n   terraform init\n   ```\n\n2. **Review the plan:**\n   ```bash\n   terraform plan\n   ```\n\n3. **Deploy:**\n   ```bash\n   terraform apply\n   ```\n\n4. **Access the web server:**\n   ```bash\n   # Get the URL from output\n   terraform output web_url\n   \n   # Open in browser\n   open $(terraform output -raw web_url)\n   ```\n\n## Outputs\n- `instance_id` - EC2 instance ID\n- `public_ip` - Elastic IP address\n- `web_url` - URL to access the server\n- `ssh_command` - SSH command to connect\n\n## Cost Estimate\n- EC2 t2.micro: ~$8.50/month\n- Elastic IP: Free (when attached)\n- **Total: ~$8.50/month**\n\n## Cleanup\n```bash\nterraform destroy\n```\n\n## Security Notes\n- Change `allowed_ips` in `terraform.tfvars` to your IP\n- Add SSH key for EC2 access\n- Enable HTTPS with SSL certificate for production\n```\n\n---\n\n### ðŸ§ª Testing\n\n```bash\n# Deploy\nterraform apply -auto-approve\n\n# Get URL\nURL=$(terraform output -raw web_url)\n\n# Test HTTP access\ncurl -I $URL\n\n# Expected output:\n# HTTP/1.1 200 OK\n# Server: Apache/2.4.x\n\n# Verify content\ncurl $URL | grep \"Apache is running\"\n\n# Cleanup\nterraform destroy -auto-approve\n```\n\n---\n\n## Project 2: Complete VPC & Networking\n\n### ðŸ“‹ Project Overview\n\nCreate a production-ready VPC with:\n- Public and private subnets across 3 AZs\n- Internet Gateway\n- NAT Gateways (one per AZ)\n- Route tables\n- Bastion host for secure access\n- Network ACLs\n\n**Architecture:**\n\n```plaintext\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  AWS Cloud                        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  VPC (10.0.0.0/16)                         â”‚  â”‚\nâ”‚  â”‚                                             â”‚  â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”â”‚  â”‚\nâ”‚  â”‚  â”‚   AZ-1a     â”‚  â”‚   AZ-1b     â”‚  â”‚AZ-1câ”‚â”‚  â”‚\nâ”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚     â”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”Œâ”€â”€â”€â”â”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”‚ Public  â”‚ â”‚  â”‚ â”‚ Public  â”‚ â”‚  â”‚â”‚Pubâ”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”‚10.0.1.0 â”‚ â”‚  â”‚ â”‚10.0.2.0 â”‚ â”‚  â”‚â”‚licâ”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚â””â”€â”¬â”€â”˜â”‚â”‚  â”‚\nâ”‚  â”‚  â”‚      â”‚NAT   â”‚  â”‚      â”‚NAT   â”‚  â”‚  â”‚  â”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚  â”‚â”Œâ”€â–¼â”€â”â”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”‚ Private â”‚ â”‚  â”‚ â”‚ Private â”‚ â”‚  â”‚â”‚Priâ”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â”‚10.0.11.0â”‚ â”‚  â”‚ â”‚10.0.12.0â”‚ â”‚  â”‚â”‚vatâ”‚â”‚  â”‚\nâ”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚â””â”€â”€â”€â”˜â”‚â”‚  â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜â”‚  â”‚\nâ”‚  â”‚         â”‚                  â”‚               â”‚  â”‚\nâ”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚\nâ”‚  â”‚                    â”‚                       â”‚  â”‚\nâ”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\nâ”‚  â”‚              â”‚    IGW     â”‚                â”‚  â”‚\nâ”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n### ðŸ“ Project Structure\n\n```\nproject-2-vpc-networking/\nâ”œâ”€â”€ main.tf\nâ”œâ”€â”€ variables.tf\nâ”œâ”€â”€ outputs.tf\nâ”œâ”€â”€ terraform.tfvars\nâ”œâ”€â”€ vpc.tf\nâ”œâ”€â”€ subnets.tf\nâ”œâ”€â”€ routing.tf\nâ”œâ”€â”€ nat.tf\nâ”œâ”€â”€ bastion.tf\nâ””â”€â”€ README.md\n```\n\n---\n\n### ðŸ’» Complete Code\n\n```hcl\n# ==================================================\n# File: variables.tf\n# ==================================================\n\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"project_name\" {\n  description = \"Project name\"\n  type        = string\n  default     = \"vpc-project\"\n}\n\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n  default     = \"prod\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR block\"\n  type        = string\n  default     = \"10.0.0.0/16\"\n}\n\nvariable \"availability_zones\" {\n  description = \"Availability zones\"\n  type        = list(string)\n  default     = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n}\n\nvariable \"public_subnet_cidrs\" {\n  description = \"Public subnet CIDR blocks\"\n  type        = list(string)\n  default     = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n}\n\nvariable \"private_subnet_cidrs\" {\n  description = \"Private subnet CIDR blocks\"\n  type        = list(string)\n  default     = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n}\n\nvariable \"enable_nat_gateway\" {\n  description = \"Enable NAT Gateway\"\n  type        = bool\n  default     = true\n}\n\nvariable \"single_nat_gateway\" {\n  description = \"Use single NAT Gateway for all AZs\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enable_dns_hostnames\" {\n  description = \"Enable DNS hostnames\"\n  type        = bool\n  default     = true\n}\n\nvariable \"enable_dns_support\" {\n  description = \"Enable DNS support\"\n  type        = bool\n  default     = true\n}\n\nvariable \"enable_flow_logs\" {\n  description = \"Enable VPC Flow Logs\"\n  type        = bool\n  default     = true\n}\n\nvariable \"bastion_instance_type\" {\n  description = \"Bastion host instance type\"\n  type        = string\n  default     = \"t3.micro\"\n}\n\nvariable \"bastion_key_name\" {\n  description = \"SSH key name for bastion host\"\n  type        = string\n  default     = \"\"\n}\n\n# ==================================================\n# File: main.tf\n# ==================================================\n\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n  \n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n    }\n  }\n}\n\nlocals {\n  name_prefix = \"${var.project_name}-${var.environment}\"\n}\n\n# ==================================================\n# File: vpc.tf\n# ==================================================\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = var.enable_dns_hostnames\n  enable_dns_support   = var.enable_dns_support\n  \n  tags = {\n    Name = \"${local.name_prefix}-vpc\"\n  }\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  \n  tags = {\n    Name = \"${local.name_prefix}-igw\"\n  }\n}\n\n# VPC Flow Logs\nresource \"aws_cloudwatch_log_group\" \"flow_logs\" {\n  count = var.enable_flow_logs ? 1 : 0\n  \n  name              = \"/aws/vpc/${local.name_prefix}\"\n  retention_in_days = 7\n  \n  tags = {\n    Name = \"${local.name_prefix}-flow-logs\"\n  }\n}\n\nresource \"aws_iam_role\" \"flow_logs\" {\n  count = var.enable_flow_logs ? 1 : 0\n  \n  name = \"${local.name_prefix}-flow-logs-role\"\n  \n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"vpc-flow-logs.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy\" \"flow_logs\" {\n  count = var.enable_flow_logs ? 1 : 0\n  \n  name = \"${local.name_prefix}-flow-logs-policy\"\n  role = aws_iam_role.flow_logs[0].id\n  \n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"logs:CreateLogGroup\",\n          \"logs:CreateLogStream\",\n          \"logs:PutLogEvents\",\n          \"logs:DescribeLogGroups\",\n          \"logs:DescribeLogStreams\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n\nresource \"aws_flow_log\" \"main\" {\n  count = var.enable_flow_logs ? 1 : 0\n  \n  vpc_id          = aws_vpc.main.id\n  traffic_type    = \"ALL\"\n  iam_role_arn    = aws_iam_role.flow_logs[0].arn\n  log_destination = aws_cloudwatch_log_group.flow_logs[0].arn\n  \n  tags = {\n    Name = \"${local.name_prefix}-flow-log\"\n  }\n}\n\n# ==================================================\n# File: subnets.tf\n# ==================================================\n\n# Public Subnets\nresource \"aws_subnet\" \"public\" {\n  count = length(var.public_subnet_cidrs)\n  \n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = var.public_subnet_cidrs[count.index]\n  availability_zone       = var.availability_zones[count.index]\n  map_public_ip_on_launch = true\n  \n  tags = {\n    Name = \"${local.name_prefix}-public-${var.availability_zones[count.index]}\"\n    Type = \"public\"\n  }\n}\n\n# Private Subnets\nresource \"aws_subnet\" \"private\" {\n  count = length(var.private_subnet_cidrs)\n  \n  vpc_id            = aws_vpc.main.id\n  cidr_block        = var.private_subnet_cidrs[count.index]\n  availability_zone = var.availability_zones[count.index]\n  \n  tags = {\n    Name = \"${local.name_prefix}-private-${var.availability_zones[count.index]}\"\n    Type = \"private\"\n  }\n}\n\n# ==================================================\n# File: nat.tf\n# ==================================================\n\n# Elastic IPs for NAT Gateways\nresource \"aws_eip\" \"nat\" {\n  count = var.enable_nat_gateway ? (var.single_nat_gateway ? 1 : length(var.availability_zones)) : 0\n  \n  domain = \"vpc\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-nat-eip-${count.index + 1}\"\n  }\n  \n  depends_on = [aws_internet_gateway.main]\n}\n\n# NAT Gateways\nresource \"aws_nat_gateway\" \"main\" {\n  count = var.enable_nat_gateway ? (var.single_nat_gateway ? 1 : length(var.availability_zones)) : 0\n  \n  allocation_id = aws_eip.nat[count.index].id\n  subnet_id     = aws_subnet.public[count.index].id\n  \n  tags = {\n    Name = \"${local.name_prefix}-nat-${count.index + 1}\"\n  }\n  \n  depends_on = [aws_internet_gateway.main]\n}\n\n# ==================================================\n# File: routing.tf\n# ==================================================\n\n# Public Route Table\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  \n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-public-rt\"\n  }\n}\n\n# Public Route Table Associations\nresource \"aws_route_table_association\" \"public\" {\n  count = length(aws_subnet.public)\n  \n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = aws_route_table.public.id\n}\n\n# Private Route Tables\nresource \"aws_route_table\" \"private\" {\n  count = var.enable_nat_gateway ? length(var.availability_zones) : 1\n  \n  vpc_id = aws_vpc.main.id\n  \n  tags = {\n    Name = \"${local.name_prefix}-private-rt-${count.index + 1}\"\n  }\n}\n\n# Private Routes (to NAT Gateway)\nresource \"aws_route\" \"private_nat\" {\n  count = var.enable_nat_gateway ? length(var.availability_zones) : 0\n  \n  route_table_id         = aws_route_table.private[count.index].id\n  destination_cidr_block = \"0.0.0.0/0\"\n  nat_gateway_id         = var.single_nat_gateway ? aws_nat_gateway.main[0].id : aws_nat_gateway.main[count.index].id\n}\n\n# Private Route Table Associations\nresource \"aws_route_table_association\" \"private\" {\n  count = length(aws_subnet.private)\n  \n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = var.enable_nat_gateway ? aws_route_table.private[var.single_nat_gateway ? 0 : count.index].id : aws_route_table.private[0].id\n}\n\n# ==================================================\n# File: bastion.tf\n# ==================================================\n\ndata \"aws_ami\" \"amazon_linux_2\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n  \n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n  }\n}\n\n# Bastion Security Group\nresource \"aws_security_group\" \"bastion\" {\n  name        = \"${local.name_prefix}-bastion-sg\"\n  description = \"Security group for bastion host\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    description = \"SSH from anywhere\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]  # Restrict this in production!\n  }\n  \n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-bastion-sg\"\n  }\n}\n\n# Bastion Host\nresource \"aws_instance\" \"bastion\" {\n  count = var.bastion_key_name != \"\" ? 1 : 0\n  \n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = var.bastion_instance_type\n  key_name               = var.bastion_key_name\n  subnet_id              = aws_subnet.public[0].id\n  vpc_security_group_ids = [aws_security_group.bastion.id]\n  \n  root_block_device {\n    volume_size = 8\n    volume_type = \"gp3\"\n    encrypted   = true\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-bastion\"\n  }\n}\n\n# Elastic IP for Bastion\nresource \"aws_eip\" \"bastion\" {\n  count = var.bastion_key_name != \"\" ? 1 : 0\n  \n  instance = aws_instance.bastion[0].id\n  domain   = \"vpc\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-bastion-eip\"\n  }\n}\n\n# ==================================================\n# File: outputs.tf\n# ==================================================\n\noutput \"vpc_id\" {\n  description = \"VPC ID\"\n  value       = aws_vpc.main.id\n}\n\noutput \"vpc_cidr\" {\n  description = \"VPC CIDR block\"\n  value       = aws_vpc.main.cidr_block\n}\n\noutput \"public_subnet_ids\" {\n  description = \"Public subnet IDs\"\n  value       = aws_subnet.public[*].id\n}\n\noutput \"private_subnet_ids\" {\n  description = \"Private subnet IDs\"\n  value       = aws_subnet.private[*].id\n}\n\noutput \"public_subnet_cidrs\" {\n  description = \"Public subnet CIDR blocks\"\n  value       = aws_subnet.public[*].cidr_block\n}\n\noutput \"private_subnet_cidrs\" {\n  description = \"Private subnet CIDR blocks\"\n  value       = aws_subnet.private[*].cidr_block\n}\n\noutput \"internet_gateway_id\" {\n  description = \"Internet Gateway ID\"\n  value       = aws_internet_gateway.main.id\n}\n\noutput \"nat_gateway_ids\" {\n  description = \"NAT Gateway IDs\"\n  value       = aws_nat_gateway.main[*].id\n}\n\noutput \"nat_gateway_ips\" {\n  description = \"NAT Gateway public IPs\"\n  value       = aws_eip.nat[*].public_ip\n}\n\noutput \"bastion_public_ip\" {\n  description = \"Bastion host public IP\"\n  value       = length(aws_eip.bastion) > 0 ? aws_eip.bastion[0].public_ip : null\n}\n\noutput \"bastion_ssh_command\" {\n  description = \"SSH command for bastion\"\n  value       = length(aws_instance.bastion) > 0 ? \"ssh -i ${var.bastion_key_name}.pem ec2-user@${aws_eip.bastion[0].public_ip}\" : \"No bastion host deployed\"\n}\n\n# ==================================================\n# File: terraform.tfvars\n# ==================================================\n\nregion             = \"us-east-1\"\nproject_name       = \"vpc-project\"\nenvironment        = \"prod\"\nvpc_cidr           = \"10.0.0.0/16\"\navailability_zones = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n\npublic_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\nprivate_subnet_cidrs = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n\nenable_nat_gateway   = true\nsingle_nat_gateway   = false  # Use one NAT per AZ for HA\nenable_dns_hostnames = true\nenable_dns_support   = true\nenable_flow_logs     = true\n\nbastion_instance_type = \"t3.micro\"\nbastion_key_name      = \"\"  # Set your SSH key name\n```\n\n---\n\n### ðŸ§ª Testing\n\n```bash\n# Deploy\nterraform apply -auto-approve\n\n# Verify VPC\naws ec2 describe-vpcs --vpc-ids $(terraform output -raw vpc_id)\n\n# Verify subnets\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=$(terraform output -raw vpc_id)\"\n\n# Test connectivity from bastion (if deployed)\nBASTION_IP=$(terraform output -raw bastion_public_ip)\nssh -i your-key.pem ec2-user@$BASTION_IP\n\n# Cleanup\nterraform destroy -auto-approve\n```\n\n---\n\n### ðŸ’° Cost Estimate\n\n**Monthly costs:**\n- VPC: Free\n- Internet Gateway: Free\n- NAT Gateways (3): ~$96/month ($32 each)\n- Elastic IPs (NAT): Free (attached)\n- Bastion (t3.micro): ~$7.50/month\n- VPC Flow Logs: ~$10/month\n\n**Total: ~$113.50/month**\n\n---\n\n## Project 3: Multi-Tier Application\n\nComplete 3-tier architecture with Application Load Balancer, Auto Scaling, and RDS.\n\n*Due to length, I'll provide the key structure:*\n\n```plaintext\nArchitecture:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     Application Load Balancer      â”‚\nâ”‚         (Public Subnets)           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Auto Scaling Group (Web Tier)   â”‚\nâ”‚         (Private Subnets)          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         RDS MySQL (Multi-AZ)       â”‚\nâ”‚      (Private DB Subnets)          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Key components:**\n- VPC with public, private, and database subnets\n- Application Load Balancer\n- Auto Scaling Group (2-10 instances)\n- Launch Template with user data\n- RDS MySQL Multi-AZ\n- ElastiCache Redis cluster\n- CloudWatch alarms\n- SNS notifications\n\n---\n\n## Project 3: Multi-Tier Application\n\n### ðŸ“‹ Project Overview\n\nDeploy a complete 3-tier application with:\n- Application Load Balancer (public)\n- Auto Scaling Group with Launch Template\n- RDS MySQL (Multi-AZ)\n- ElastiCache Redis\n- CloudWatch monitoring\n- SNS notifications\n\n**Architecture:**\n\n```plaintext\nInternet\n   â”‚\n   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Route 53 DNS    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  CloudFront CDN  â”‚ (Optional)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Application Load Balancer       â”‚\nâ”‚  (Public Subnets across 3 AZs)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Auto Scaling Group             â”‚\nâ”‚   EC2 Instances (Web/App Tier)   â”‚\nâ”‚   (Private Subnets across 3 AZs) â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â”‚                    â”‚\n     â–¼                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ RDS MySQL   â”‚    â”‚ ElastiCache  â”‚\nâ”‚ Multi-AZ    â”‚    â”‚ Redis        â”‚\nâ”‚ (DB Subnet) â”‚    â”‚ (Cache Layer)â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n### ðŸ“ Project Structure\n\n```\nproject-3-multi-tier-app/\nâ”œâ”€â”€ main.tf\nâ”œâ”€â”€ variables.tf\nâ”œâ”€â”€ outputs.tf\nâ”œâ”€â”€ terraform.tfvars\nâ”œâ”€â”€ vpc.tf\nâ”œâ”€â”€ security-groups.tf\nâ”œâ”€â”€ alb.tf\nâ”œâ”€â”€ asg.tf\nâ”œâ”€â”€ rds.tf\nâ”œâ”€â”€ elasticache.tf\nâ”œâ”€â”€ cloudwatch.tf\nâ”œâ”€â”€ sns.tf\nâ”œâ”€â”€ user_data.sh\nâ””â”€â”€ README.md\n```\n\n---\n\n### ðŸ’» Complete Code\n\n```hcl\n# ==================================================\n# File: variables.tf\n# ==================================================\n\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"project_name\" {\n  description = \"Project name\"\n  type        = string\n  default     = \"multi-tier-app\"\n}\n\nvariable \"environment\" {\n  description = \"Environment\"\n  type        = string\n  default     = \"prod\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR\"\n  type        = string\n  default     = \"10.0.0.0/16\"\n}\n\nvariable \"availability_zones\" {\n  description = \"Availability zones\"\n  type        = list(string)\n  default     = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n}\n\n# ASG Configuration\nvariable \"min_size\" {\n  description = \"Minimum instances\"\n  type        = number\n  default     = 2\n}\n\nvariable \"max_size\" {\n  description = \"Maximum instances\"\n  type        = number\n  default     = 10\n}\n\nvariable \"desired_capacity\" {\n  description = \"Desired instances\"\n  type        = number\n  default     = 3\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t3.medium\"\n}\n\n# Database Configuration\nvariable \"db_instance_class\" {\n  description = \"RDS instance class\"\n  type        = string\n  default     = \"db.t3.medium\"\n}\n\nvariable \"db_name\" {\n  description = \"Database name\"\n  type        = string\n  default     = \"appdb\"\n}\n\nvariable \"db_username\" {\n  description = \"Database username\"\n  type        = string\n  default     = \"admin\"\n}\n\nvariable \"db_multi_az\" {\n  description = \"Enable Multi-AZ\"\n  type        = bool\n  default     = true\n}\n\n# ElastiCache Configuration\nvariable \"redis_node_type\" {\n  description = \"Redis node type\"\n  type        = string\n  default     = \"cache.t3.micro\"\n}\n\nvariable \"redis_num_cache_nodes\" {\n  description = \"Number of cache nodes\"\n  type        = number\n  default     = 2\n}\n\nvariable \"alert_email\" {\n  description = \"Email for alerts\"\n  type        = string\n}\n\n# ==================================================\n# File: main.tf\n# ==================================================\n\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~> 3.5\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n  \n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n    }\n  }\n}\n\nlocals {\n  name_prefix = \"${var.project_name}-${var.environment}\"\n  \n  public_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  private_subnet_cidrs = [\"10.0.11.0/24\", \"10.0.12.0/24\", \"10.0.13.0/24\"]\n  database_subnet_cidrs = [\"10.0.21.0/24\", \"10.0.22.0/24\", \"10.0.23.0/24\"]\n}\n\ndata \"aws_ami\" \"amazon_linux_2\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n  \n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n  }\n}\n\n# ==================================================\n# File: vpc.tf\n# ==================================================\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n  \n  tags = {\n    Name = \"${local.name_prefix}-vpc\"\n  }\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  \n  tags = {\n    Name = \"${local.name_prefix}-igw\"\n  }\n}\n\n# Public Subnets\nresource \"aws_subnet\" \"public\" {\n  count = length(local.public_subnet_cidrs)\n  \n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = local.public_subnet_cidrs[count.index]\n  availability_zone       = var.availability_zones[count.index]\n  map_public_ip_on_launch = true\n  \n  tags = {\n    Name = \"${local.name_prefix}-public-${count.index + 1}\"\n    Type = \"public\"\n  }\n}\n\n# Private Subnets (for application)\nresource \"aws_subnet\" \"private\" {\n  count = length(local.private_subnet_cidrs)\n  \n  vpc_id            = aws_vpc.main.id\n  cidr_block        = local.private_subnet_cidrs[count.index]\n  availability_zone = var.availability_zones[count.index]\n  \n  tags = {\n    Name = \"${local.name_prefix}-private-${count.index + 1}\"\n    Type = \"private\"\n  }\n}\n\n# Database Subnets\nresource \"aws_subnet\" \"database\" {\n  count = length(local.database_subnet_cidrs)\n  \n  vpc_id            = aws_vpc.main.id\n  cidr_block        = local.database_subnet_cidrs[count.index]\n  availability_zone = var.availability_zones[count.index]\n  \n  tags = {\n    Name = \"${local.name_prefix}-database-${count.index + 1}\"\n    Type = \"database\"\n  }\n}\n\n# NAT Gateways\nresource \"aws_eip\" \"nat\" {\n  count = length(var.availability_zones)\n  domain = \"vpc\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-nat-eip-${count.index + 1}\"\n  }\n}\n\nresource \"aws_nat_gateway\" \"main\" {\n  count = length(var.availability_zones)\n  \n  allocation_id = aws_eip.nat[count.index].id\n  subnet_id     = aws_subnet.public[count.index].id\n  \n  tags = {\n    Name = \"${local.name_prefix}-nat-${count.index + 1}\"\n  }\n  \n  depends_on = [aws_internet_gateway.main]\n}\n\n# Route Tables\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  \n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-public-rt\"\n  }\n}\n\nresource \"aws_route_table_association\" \"public\" {\n  count = length(aws_subnet.public)\n  \n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = aws_route_table.public.id\n}\n\nresource \"aws_route_table\" \"private\" {\n  count = length(var.availability_zones)\n  \n  vpc_id = aws_vpc.main.id\n  \n  route {\n    cidr_block     = \"0.0.0.0/0\"\n    nat_gateway_id = aws_nat_gateway.main[count.index].id\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-private-rt-${count.index + 1}\"\n  }\n}\n\nresource \"aws_route_table_association\" \"private\" {\n  count = length(aws_subnet.private)\n  \n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = aws_route_table.private[count.index].id\n}\n\n# ==================================================\n# File: security-groups.tf\n# ==================================================\n\n# ALB Security Group\nresource \"aws_security_group\" \"alb\" {\n  name        = \"${local.name_prefix}-alb-sg\"\n  description = \"Security group for ALB\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    description = \"HTTP from internet\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  ingress {\n    description = \"HTTPS from internet\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-alb-sg\"\n  }\n}\n\n# Application Security Group\nresource \"aws_security_group\" \"app\" {\n  name        = \"${local.name_prefix}-app-sg\"\n  description = \"Security group for application servers\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    description     = \"HTTP from ALB\"\n    from_port       = 80\n    to_port         = 80\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.alb.id]\n  }\n  \n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-app-sg\"\n  }\n}\n\n# Database Security Group\nresource \"aws_security_group\" \"database\" {\n  name        = \"${local.name_prefix}-database-sg\"\n  description = \"Security group for RDS\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    description     = \"MySQL from app servers\"\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.app.id]\n  }\n  \n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-database-sg\"\n  }\n}\n\n# Cache Security Group\nresource \"aws_security_group\" \"cache\" {\n  name        = \"${local.name_prefix}-cache-sg\"\n  description = \"Security group for ElastiCache\"\n  vpc_id      = aws_vpc.main.id\n  \n  ingress {\n    description     = \"Redis from app servers\"\n    from_port       = 6379\n    to_port         = 6379\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.app.id]\n  }\n  \n  egress {\n    description = \"Allow all outbound\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  \n  tags = {\n    Name = \"${local.name_prefix}-cache-sg\"\n  }\n}\n\n# ==================================================\n# File: alb.tf\n# ==================================================\n\nresource \"aws_lb\" \"main\" {\n  name               = \"${local.name_prefix}-alb\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb.id]\n  subnets            = aws_subnet.public[*].id\n  \n  enable_deletion_protection = false\n  enable_http2              = true\n  \n  tags = {\n    Name = \"${local.name_prefix}-alb\"\n  }\n}\n\nresource \"aws_lb_target_group\" \"app\" {\n  name     = \"${local.name_prefix}-tg\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n  \n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    unhealthy_threshold = 3\n    timeout             = 5\n    interval            = 30\n    path                = \"/health\"\n    matcher             = \"200\"\n  }\n  \n  deregistration_delay = 30\n  \n  tags = {\n    Name = \"${local.name_prefix}-tg\"\n  }\n}\n\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.main.arn\n  port              = 80\n  protocol          = \"HTTP\"\n  \n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.app.arn\n  }\n}\n\n# ==================================================\n# File: asg.tf\n# ==================================================\n\nresource \"aws_launch_template\" \"app\" {\n  name_prefix   = \"${local.name_prefix}-lt-\"\n  image_id      = data.aws_ami.amazon_linux_2.id\n  instance_type = var.instance_type\n  \n  vpc_security_group_ids = [aws_security_group.app.id]\n  \n  user_data = base64encode(templatefile(\"${path.module}/user_data.sh\", {\n    db_endpoint    = aws_db_instance.main.endpoint\n    redis_endpoint = aws_elasticache_cluster.main.cache_nodes[0].address\n    db_name        = var.db_name\n    db_username    = var.db_username\n    region         = var.region\n  }))\n  \n  iam_instance_profile {\n    name = aws_iam_instance_profile.app.name\n  }\n  \n  monitoring {\n    enabled = true\n  }\n  \n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 1\n  }\n  \n  tag_specifications {\n    resource_type = \"instance\"\n    \n    tags = {\n      Name = \"${local.name_prefix}-app-instance\"\n    }\n  }\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_autoscaling_group\" \"app\" {\n  name                = \"${local.name_prefix}-asg\"\n  vpc_zone_identifier = aws_subnet.private[*].id\n  target_group_arns   = [aws_lb_target_group.app.arn]\n  \n  min_size         = var.min_size\n  max_size         = var.max_size\n  desired_capacity = var.desired_capacity\n  \n  health_check_type         = \"ELB\"\n  health_check_grace_period = 300\n  \n  launch_template {\n    id      = aws_launch_template.app.id\n    version = \"$Latest\"\n  }\n  \n  tag {\n    key                 = \"Name\"\n    value               = \"${local.name_prefix}-app-instance\"\n    propagate_at_launch = true\n  }\n  \n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [desired_capacity]\n  }\n}\n\n# IAM Role for EC2 instances\nresource \"aws_iam_role\" \"app\" {\n  name = \"${local.name_prefix}-app-role\"\n  \n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"ec2.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"app_ssm\" {\n  role       = aws_iam_role.app.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"app_cloudwatch\" {\n  role       = aws_iam_role.app.name\n  policy_arn = \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\"\n}\n\nresource \"aws_iam_instance_profile\" \"app\" {\n  name = \"${local.name_prefix}-app-profile\"\n  role = aws_iam_role.app.name\n}\n\n# Auto Scaling Policies\nresource \"aws_autoscaling_policy\" \"scale_up\" {\n  name                   = \"${local.name_prefix}-scale-up\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n  adjustment_type        = \"ChangeInCapacity\"\n  scaling_adjustment     = 2\n  cooldown               = 300\n}\n\nresource \"aws_autoscaling_policy\" \"scale_down\" {\n  name                   = \"${local.name_prefix}-scale-down\"\n  autoscaling_group_name = aws_autoscaling_group.app.name\n  adjustment_type        = \"ChangeInCapacity\"\n  scaling_adjustment     = -1\n  cooldown               = 300\n}\n\n# ==================================================\n# File: rds.tf\n# ==================================================\n\nresource \"random_password\" \"db_password\" {\n  length  = 32\n  special = true\n}\n\nresource \"aws_secretsmanager_secret\" \"db_password\" {\n  name                    = \"${local.name_prefix}-db-password\"\n  recovery_window_in_days = 7\n}\n\nresource \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = aws_secretsmanager_secret.db_password.id\n  secret_string = jsonencode({\n    username = var.db_username\n    password = random_password.db_password.result\n    engine   = \"mysql\"\n    host     = aws_db_instance.main.endpoint\n    port     = 3306\n    dbname   = var.db_name\n  })\n}\n\nresource \"aws_db_subnet_group\" \"main\" {\n  name       = \"${local.name_prefix}-db-subnet-group\"\n  subnet_ids = aws_subnet.database[*].id\n  \n  tags = {\n    Name = \"${local.name_prefix}-db-subnet-group\"\n  }\n}\n\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"${local.name_prefix}-db\"\n  engine         = \"mysql\"\n  engine_version = \"8.0.35\"\n  instance_class = var.db_instance_class\n  \n  allocated_storage     = 100\n  max_allocated_storage = 1000\n  storage_type          = \"gp3\"\n  storage_encrypted     = true\n  \n  db_name  = var.db_name\n  username = var.db_username\n  password = random_password.db_password.result\n  \n  multi_az               = var.db_multi_az\n  db_subnet_group_name   = aws_db_subnet_group.main.name\n  vpc_security_group_ids = [aws_security_group.database.id]\n  \n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"mon:04:00-mon:05:00\"\n  \n  enabled_cloudwatch_logs_exports = [\"error\", \"general\", \"slowquery\"]\n  \n  skip_final_snapshot = false\n  final_snapshot_identifier = \"${local.name_prefix}-db-final-snapshot\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-db\"\n  }\n}\n\n# ==================================================\n# File: elasticache.tf\n# ==================================================\n\nresource \"aws_elasticache_subnet_group\" \"main\" {\n  name       = \"${local.name_prefix}-cache-subnet-group\"\n  subnet_ids = aws_subnet.private[*].id\n  \n  tags = {\n    Name = \"${local.name_prefix}-cache-subnet-group\"\n  }\n}\n\nresource \"aws_elasticache_cluster\" \"main\" {\n  cluster_id           = \"${local.name_prefix}-redis\"\n  engine               = \"redis\"\n  node_type            = var.redis_node_type\n  num_cache_nodes      = var.redis_num_cache_nodes\n  parameter_group_name = \"default.redis7\"\n  engine_version       = \"7.0\"\n  port                 = 6379\n  \n  subnet_group_name    = aws_elasticache_subnet_group.main.name\n  security_group_ids   = [aws_security_group.cache.id]\n  \n  snapshot_retention_limit = 5\n  snapshot_window          = \"03:00-05:00\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-redis\"\n  }\n}\n\n# ==================================================\n# File: cloudwatch.tf\n# ==================================================\n\n# High CPU Alarm\nresource \"aws_cloudwatch_metric_alarm\" \"high_cpu\" {\n  alarm_name          = \"${local.name_prefix}-high-cpu\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"CPUUtilization\"\n  namespace           = \"AWS/EC2\"\n  period              = 300\n  statistic           = \"Average\"\n  threshold           = 75\n  alarm_description   = \"Triggers when CPU exceeds 75%\"\n  alarm_actions       = [aws_sns_topic.alerts.arn, aws_autoscaling_policy.scale_up.arn]\n  \n  dimensions = {\n    AutoScalingGroupName = aws_autoscaling_group.app.name\n  }\n}\n\n# Low CPU Alarm\nresource \"aws_cloudwatch_metric_alarm\" \"low_cpu\" {\n  alarm_name          = \"${local.name_prefix}-low-cpu\"\n  comparison_operator = \"LessThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"CPUUtilization\"\n  namespace           = \"AWS/EC2\"\n  period              = 300\n  statistic           = \"Average\"\n  threshold           = 25\n  alarm_description   = \"Triggers when CPU below 25%\"\n  alarm_actions       = [aws_autoscaling_policy.scale_down.arn]\n  \n  dimensions = {\n    AutoScalingGroupName = aws_autoscaling_group.app.name\n  }\n}\n\n# Database CPU Alarm\nresource \"aws_cloudwatch_metric_alarm\" \"db_cpu\" {\n  alarm_name          = \"${local.name_prefix}-db-high-cpu\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"CPUUtilization\"\n  namespace           = \"AWS/RDS\"\n  period              = 300\n  statistic           = \"Average\"\n  threshold           = 80\n  alarm_description   = \"Database CPU is high\"\n  alarm_actions       = [aws_sns_topic.alerts.arn]\n  \n  dimensions = {\n    DBInstanceIdentifier = aws_db_instance.main.id\n  }\n}\n\n# ==================================================\n# File: sns.tf\n# ==================================================\n\nresource \"aws_sns_topic\" \"alerts\" {\n  name = \"${local.name_prefix}-alerts\"\n  \n  tags = {\n    Name = \"${local.name_prefix}-alerts\"\n  }\n}\n\nresource \"aws_sns_topic_subscription\" \"alerts_email\" {\n  topic_arn = aws_sns_topic.alerts.arn\n  protocol  = \"email\"\n  endpoint  = var.alert_email\n}\n\n# ==================================================\n# File: outputs.tf\n# ==================================================\n\noutput \"alb_dns_name\" {\n  description = \"ALB DNS name\"\n  value       = aws_lb.main.dns_name\n}\n\noutput \"alb_url\" {\n  description = \"Application URL\"\n  value       = \"http://${aws_lb.main.dns_name}\"\n}\n\noutput \"database_endpoint\" {\n  description = \"RDS endpoint\"\n  value       = aws_db_instance.main.endpoint\n  sensitive   = true\n}\n\noutput \"redis_endpoint\" {\n  description = \"Redis endpoint\"\n  value       = aws_elasticache_cluster.main.cache_nodes[0].address\n}\n\noutput \"asg_name\" {\n  description = \"Auto Scaling Group name\"\n  value       = aws_autoscaling_group.app.name\n}\n\n# ==================================================\n# File: user_data.sh\n# ==================================================\n\n#!/bin/bash\nset -e\n\n# Update system\nyum update -y\n\n# Install dependencies\nyum install -y httpd php php-mysqlnd php-redis amazon-cloudwatch-agent\n\n# Get DB password from Secrets Manager\nDB_PASSWORD=$(aws secretsmanager get-secret-value \\\n  --secret-id ${db_name}-db-password \\\n  --region ${region} \\\n  --query SecretString \\\n  --output text | jq -r .password)\n\n# Configure application\ncat > /var/www/html/config.php <<EOF\n<?php\ndefine('DB_HOST', '${db_endpoint}');\ndefine('DB_USER', '${db_username}');\ndefine('DB_PASS', '$DB_PASSWORD');\ndefine('DB_NAME', '${db_name}');\ndefine('REDIS_HOST', '${redis_endpoint}');\n?>\nEOF\n\n# Create sample application\ncat > /var/www/html/index.php <<'EOF'\n<?php\nrequire_once 'config.php';\n\n// Database connection\n$mysqli = new mysqli(DB_HOST, DB_USER, DB_PASS, DB_NAME);\nif ($mysqli->connect_error) {\n    die(\"Database connection failed: \" . $mysqli->connect_error);\n}\n\n// Redis connection\n$redis = new Redis();\n$redis->connect(REDIS_HOST, 6379);\n\n// Page view counter\n$views = $redis->incr('page_views');\n\n?>\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Multi-Tier Application</title>\n    <style>\n        body { font-family: Arial; max-width: 800px; margin: 50px auto; }\n        .status { color: #28a745; font-weight: bold; }\n    </style>\n</head>\n<body>\n    <h1>ðŸš€ Multi-Tier Application</h1>\n    <div class=\"status\">âœ“ Application is running!</div>\n    <p><strong>Page Views:</strong> <?php echo $views; ?></p>\n    <p><strong>Database:</strong> Connected to MySQL</p>\n    <p><strong>Cache:</strong> Connected to Redis</p>\n    <p><strong>Server:</strong> <?php echo gethostname(); ?></p>\n</body>\n</html>\nEOF\n\n# Health check endpoint\ncat > /var/www/html/health <<'EOF'\nOK\nEOF\n\n# Start Apache\nsystemctl start httpd\nsystemctl enable httpd\n\n# Configure CloudWatch agent\ncat > /opt/aws/amazon-cloudwatch-agent/etc/config.json <<'EOF'\n{\n  \"metrics\": {\n    \"namespace\": \"MultiTierApp\",\n    \"metrics_collected\": {\n      \"mem\": {\n        \"measurement\": [{\"name\": \"mem_used_percent\"}],\n        \"metrics_collection_interval\": 60\n      },\n      \"disk\": {\n        \"measurement\": [{\"name\": \"used_percent\"}],\n        \"metrics_collection_interval\": 60,\n        \"resources\": [\"*\"]\n      }\n    }\n  }\n}\nEOF\n\n# Start CloudWatch agent\n/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \\\n  -a fetch-config \\\n  -m ec2 \\\n  -s \\\n  -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json\n\necho \"Application setup complete!\"\n\n# ==================================================\n# File: terraform.tfvars\n# ==================================================\n\nregion      = \"us-east-1\"\nproject_name = \"multi-tier-app\"\nenvironment = \"prod\"\n\nvpc_cidr           = \"10.0.0.0/16\"\navailability_zones = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n\n# Auto Scaling\nmin_size         = 2\nmax_size         = 10\ndesired_capacity = 3\ninstance_type    = \"t3.medium\"\n\n# Database\ndb_instance_class = \"db.t3.medium\"\ndb_name          = \"appdb\"\ndb_username      = \"admin\"\ndb_multi_az      = true\n\n# Cache\nredis_node_type       = \"cache.t3.micro\"\nredis_num_cache_nodes = 2\n\n# Alerts\nalert_email = \"your-email@example.com\"\n```\n\n---\n\n### ðŸ’° Cost Estimate\n\n**Monthly costs:**\n- ALB: ~$22/month\n- EC2 (3x t3.medium): ~$90/month\n- RDS Multi-AZ (db.t3.medium): ~$120/month\n- ElastiCache (2 nodes): ~$30/month\n- NAT Gateways (3): ~$96/month\n- Data transfer: ~$50/month\n\n**Total: ~$408/month**\n\n---\n\n## Project 4: EKS Kubernetes Cluster\n\n### ðŸ“‹ Project Overview\n\nDeploy production-ready EKS cluster with:\n- EKS Control Plane\n- Managed Node Groups\n- VPC with private subnets\n- IAM roles and policies\n- OIDC provider\n- Kubernetes addons (CoreDNS, kube-proxy, VPC-CNI)\n\n**Architecture:**\n\n```plaintext\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     EKS Control Plane              â”‚\nâ”‚     (Managed by AWS)               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Worker Nodes (Managed Node Group) â”‚\nâ”‚  - Auto Scaling                    â”‚\nâ”‚  - Private Subnets                 â”‚\nâ”‚  - Multi-AZ                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Application Pods                  â”‚\nâ”‚  - Load Balancer Controller        â”‚\nâ”‚  - Metrics Server                  â”‚\nâ”‚  - Cluster Autoscaler              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n### ðŸ’» Key Configuration\n\n```hcl\n# Simplified EKS setup - Key components\n\nresource \"aws_eks_cluster\" \"main\" {\n  name     = \"${local.name_prefix}-eks\"\n  role_arn = aws_iam_role.eks_cluster.arn\n  version  = \"1.28\"\n  \n  vpc_config {\n    subnet_ids              = aws_subnet.private[*].id\n    endpoint_private_access = true\n    endpoint_public_access  = true\n    public_access_cidrs     = [\"0.0.0.0/0\"]\n  }\n  \n  enabled_cluster_log_types = [\"api\", \"audit\", \"authenticator\", \"controllerManager\", \"scheduler\"]\n}\n\nresource \"aws_eks_node_group\" \"main\" {\n  cluster_name    = aws_eks_cluster.main.name\n  node_group_name = \"${local.name_prefix}-node-group\"\n  node_role_arn   = aws_iam_role.eks_node.arn\n  subnet_ids      = aws_subnet.private[*].id\n  \n  scaling_config {\n    desired_size = 3\n    max_size     = 10\n    min_size     = 2\n  }\n  \n  instance_types = [\"t3.medium\"]\n  capacity_type  = \"ON_DEMAND\"\n  \n  update_config {\n    max_unavailable = 1\n  }\n}\n```\n\n---\n\n## Project 5: Multi-Cloud Deployment\n\n### ðŸ“‹ Project Overview\n\nDeploy resources across AWS, Azure, and GCP simultaneously:\n- AWS: VPC + EC2\n- Azure: VNet + VM\n- GCP: VPC + Compute Instance\n- Unified monitoring and management\n\n**Architecture:**\n\n```plaintext\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   AWS    â”‚   â”‚  Azure   â”‚   â”‚   GCP    â”‚\nâ”‚          â”‚   â”‚          â”‚   â”‚          â”‚\nâ”‚  VPC     â”‚   â”‚  VNet    â”‚   â”‚  VPC     â”‚\nâ”‚  EC2     â”‚   â”‚  VM      â”‚   â”‚  CE      â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n     â”‚              â”‚              â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              Terraform\n```\n\n---\n\n### ðŸ’» Multi-Provider Configuration\n\n```hcl\n# Providers for all three clouds\n\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 3.0\"\n    }\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\n# AWS Resources\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.amazon_linux_2.id\n  instance_type = \"t2.micro\"\n  \n  tags = {\n    Name  = \"multi-cloud-aws\"\n    Cloud = \"AWS\"\n  }\n}\n\n# Azure Resources\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_virtual_machine\" \"web\" {\n  name                = \"multi-cloud-azure\"\n  location            = \"East US\"\n  resource_group_name = azurerm_resource_group.main.name\n  vm_size             = \"Standard_B1s\"\n  \n  tags = {\n    Cloud = \"Azure\"\n  }\n}\n\n# GCP Resources\nprovider \"google\" {\n  project = \"my-project\"\n  region  = \"us-central1\"\n}\n\nresource \"google_compute_instance\" \"web\" {\n  name         = \"multi-cloud-gcp\"\n  machine_type = \"e2-micro\"\n  zone         = \"us-central1-a\"\n  \n  labels = {\n    cloud = \"gcp\"\n  }\n}\n```\n\n---\n\n# ðŸ“š BONUS SECTIONS\n\n## Bonus 1: Advanced State Management & Surgery\n\n### ðŸ”§ State File Deep Dive\n\nThe Terraform state file (`.tfstate`) is JSON-formatted and contains:\n\n**State File Structure:**\n\n```json\n{\n  \"version\": 4,\n  \"terraform_version\": \"1.5.0\",\n  \"serial\": 15,\n  \"lineage\": \"abc123-def456-ghi789\",\n  \"outputs\": {},\n  \"resources\": [\n    {\n      \"mode\": \"managed\",\n      \"type\": \"aws_instance\",\n      \"name\": \"web\",\n      \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n      \"instances\": [\n        {\n          \"schema_version\": 1,\n          \"attributes\": {\n            \"id\": \"i-0123456789abcdef0\",\n            \"ami\": \"ami-12345678\",\n            \"instance_type\": \"t2.micro\"\n          },\n          \"private\": \"eyJzY2hlbWF...\",\n          \"dependencies\": []\n        }\n      ]\n    }\n  ]\n}\n```\n\n---\n\n### ðŸš¨ State Surgery Techniques\n\n**âš ï¸ WARNING:** Always backup state before surgery!\n\n```bash\n# 1. Backup current state\nterraform state pull > terraform.tfstate.backup\n\n# 2. List all resources\nterraform state list\n\n# 3. Show specific resource details\nterraform state show aws_instance.web\n\n# 4. Remove resource from state (doesn't delete actual resource)\nterraform state rm aws_instance.web\n\n# 5. Move resource to different address\nterraform state mv aws_instance.web aws_instance.server\n\n# 6. Replace provider in state\nterraform state replace-provider hashicorp/aws registry.terraform.io/hashicorp/aws\n\n# 7. Import existing resource\nterraform import aws_instance.web i-0123456789abcdef0\n```\n\n---\n\n### ðŸ”„ State Recovery Scenarios\n\n**Scenario 1: Corrupted State File**\n\n```bash\n# Step 1: Check if backup exists\nls -la terraform.tfstate.backup\n\n# Step 2: Restore from backup\ncp terraform.tfstate.backup terraform.tfstate\n\n# Step 3: Verify\nterraform plan\n\n# If no backup, restore from remote backend\nterraform state pull > terraform.tfstate\n```\n\n**Scenario 2: Lost State File**\n\n```bash\n# Option 1: Recreate state with imports\n# Create empty main.tf with resource definitions\nterraform init\n\n# Import each resource\nterraform import aws_vpc.main vpc-12345678\nterraform import aws_subnet.public[0] subnet-11111111\nterraform import aws_subnet.public[1] subnet-22222222\n\n# Option 2: Use terraform refresh (if resources exist)\nterraform refresh\n```\n\n**Scenario 3: State Drift (Manual Changes)**\n\n```bash\n# Detect drift\nterraform plan -detailed-exitcode\n# Exit code 2 = changes detected\n\n# Option 1: Update state to match reality\nterraform refresh\n\n# Option 2: Update infrastructure to match state\nterraform apply\n\n# Option 3: Accept drift for specific attribute\nresource \"aws_instance\" \"web\" {\n  # ... \n  \n  lifecycle {\n    ignore_changes = [ami]  # Ignore AMI changes\n  }\n}\n```\n\n---\n\n### ðŸ› ï¸ Advanced State Manipulation\n\n**Bulk State Operations:**\n\n```bash\n#!/bin/bash\n# Script: bulk_state_operations.sh\n\n# Move all resources with prefix\nfor resource in $(terraform state list | grep \"aws_subnet\"); do\n    new_name=$(echo $resource | sed 's/public/private/g')\n    terraform state mv \"$resource\" \"$new_name\"\ndone\n\n# Remove all resources of specific type\nterraform state list | grep \"aws_route_table_association\" | \\\n    xargs -I {} terraform state rm {}\n\n# Export state to JSON for analysis\nterraform state pull | jq . > state_analysis.json\n\n# Count resources by type\nterraform state list | cut -d. -f1 | sort | uniq -c\n```\n\n---\n\n### ðŸ” State Encryption Best Practices\n\n```hcl\n# S3 backend with full encryption\n\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    kms_key_id     = \"arn:aws:kms:us-east-1:123456789012:key/abc-def-ghi\"\n    dynamodb_table = \"terraform-locks\"\n    \n    # Additional security\n    acl                  = \"private\"\n    skip_metadata_api_check = true\n    skip_credentials_validation = false\n  }\n}\n\n# Prevent accidental state deletion\nresource \"aws_s3_bucket_versioning\" \"state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  \n  rule {\n    id     = \"keep-versions\"\n    status = \"Enabled\"\n    \n    noncurrent_version_expiration {\n      noncurrent_days = 90\n    }\n  }\n}\n```\n\n---\n\n### ðŸ“Š State Analysis & Monitoring\n\n```bash\n# Analyze state file size\nterraform state pull | wc -c\n\n# Count total resources\nterraform state list | wc -l\n\n# Resources by type\nterraform state list | cut -d. -f1 | sort | uniq -c | sort -rn\n\n# Find large resources\nterraform state show aws_instance.large_data | wc -c\n\n# Check for sensitive data in state\nterraform state pull | grep -i \"password\\|secret\\|key\"\n\n# State lineage tracking\nterraform state pull | jq -r '.lineage'\n\n# State serial number (version)\nterraform state pull | jq -r '.serial'\n```\n\n---\n\n## Bonus 2: Comprehensive Troubleshooting Guide\n\n### ðŸ› Common Errors & Solutions\n\n#### Error 1: Provider Version Conflicts\n\n**Error Message:**\n```\nError: Failed to query available provider packages\nCould not retrieve the list of available versions for provider hashicorp/aws\n```\n\n**Solution:**\n```hcl\n# Fix: Lock provider versions\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"  # Lock to major version\n    }\n  }\n}\n\n# Then run:\n# terraform init -upgrade\n```\n\n---\n\n#### Error 2: State Lock Errors\n\n**Error Message:**\n```\nError: Error acquiring the state lock\nLock Info:\n  ID:        abc-123-def-456\n  Path:      s3-bucket/terraform.tfstate\n  Operation: OperationTypeApply\n  Who:       user@hostname\n  Version:   1.5.0\n  Created:   2024-01-15 10:30:00\n```\n\n**Solution:**\n```bash\n# Option 1: Wait for lock to release (if operation is running)\n# Wait 5-10 minutes\n\n# Option 2: Force unlock (DANGEROUS - only if operation crashed)\nterraform force-unlock abc-123-def-456\n\n# Option 3: Check DynamoDB for stale lock\naws dynamodb get-item \\\n    --table-name terraform-locks \\\n    --key '{\"LockID\": {\"S\": \"my-bucket/terraform.tfstate-md5\"}}'\n\n# Option 4: Delete stale lock from DynamoDB\naws dynamodb delete-item \\\n    --table-name terraform-locks \\\n    --key '{\"LockID\": {\"S\": \"my-bucket/terraform.tfstate-md5\"}}'\n```\n\n---\n\n#### Error 3: Resource Already Exists\n\n**Error Message:**\n```\nError: creating EC2 Instance: ResourceAlreadyExists: Instance i-123 already exists\n```\n\n**Solution:**\n```bash\n# Import existing resource\nterraform import aws_instance.web i-0123456789abcdef0\n\n# Or remove from state if it shouldn't be managed\nterraform state rm aws_instance.web\n```\n\n---\n\n#### Error 4: Dependency Cycle\n\n**Error Message:**\n```\nError: Cycle: aws_subnet.private, aws_nat_gateway.main, aws_route_table.private\n```\n\n**Solution:**\n```hcl\n# Break cycle with depends_on\n\nresource \"aws_subnet\" \"private\" {\n  # ... configuration\n  \n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_route_table\" \"private\" {\n  # Don't reference NAT gateway directly\n  # Use depends_on instead\n  \n  depends_on = [aws_nat_gateway.main]\n}\n```\n\n---\n\n#### Error 5: Authentication Errors\n\n**Error Message:**\n```\nError: error configuring Terraform AWS Provider: no valid credential sources\n```\n\n**Solution:**\n```bash\n# Check AWS credentials\naws sts get-caller-identity\n\n# Set credentials\nexport AWS_ACCESS_KEY_ID=\"your-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n\n# Or use AWS SSO\naws sso login --profile my-profile\nexport AWS_PROFILE=my-profile\n\n# Verify Terraform can authenticate\nterraform init\n```\n\n---\n\n### ðŸ” Debugging Techniques\n\n**Enable Debug Logging:**\n\n```bash\n# Trace-level logging (most verbose)\nexport TF_LOG=TRACE\nexport TF_LOG_PATH=./terraform-trace.log\nterraform apply\n\n# Different log levels\nexport TF_LOG=DEBUG   # Debug information\nexport TF_LOG=INFO    # Info messages\nexport TF_LOG=WARN    # Warnings only\nexport TF_LOG=ERROR   # Errors only\n\n# Component-specific logging\nexport TF_LOG_CORE=TRACE     # Core operations\nexport TF_LOG_PROVIDER=TRACE # Provider operations\n\n# Disable logging\nunset TF_LOG TF_LOG_PATH\n```\n\n**Analyze Plan Output:**\n\n```bash\n# Save plan to file\nterraform plan -out=tfplan\n\n# Show plan in JSON\nterraform show -json tfplan | jq . > plan.json\n\n# Analyze what's changing\nterraform show tfplan | grep \"will be created\"\nterraform show tfplan | grep \"will be destroyed\"\nterraform show tfplan | grep \"will be updated\"\n\n# Show specific resource changes\nterraform show tfplan | grep -A 10 \"aws_instance.web\"\n```\n\n**Resource Graph Analysis:**\n\n```bash\n# Generate dependency graph\nterraform graph | dot -Tpng > graph.png\n\n# Show only specific resource dependencies\nterraform graph -draw-cycles | grep aws_instance\n\n# Analyze cycle issues\nterraform graph -type=plan | grep -A 5 -B 5 cycle\n```\n\n---\n\n### ðŸ›¡ï¸ Disaster Recovery Procedures\n\n**Procedure 1: Complete Infrastructure Loss**\n\n```bash\n#!/bin/bash\n# disaster_recovery.sh\n\nset -e\n\necho \"=== Terraform Disaster Recovery ===\"\n\n# Step 1: Verify state backup exists\necho \"Checking state backup...\"\nif [ ! -f \"terraform.tfstate.backup\" ]; then\n    echo \"ERROR: No backup found!\"\n    exit 1\nfi\n\n# Step 2: Restore state\necho \"Restoring state from backup...\"\ncp terraform.tfstate.backup terraform.tfstate\n\n# Step 3: Refresh state\necho \"Refreshing state...\"\nterraform refresh -lock=false\n\n# Step 4: Check what needs recreation\necho \"Checking infrastructure status...\"\nterraform plan -out=recovery.tfplan\n\n# Step 5: Apply if needed\nread -p \"Apply recovery plan? (yes/no): \" confirm\nif [ \"$confirm\" = \"yes\" ]; then\n    terraform apply recovery.tfplan\n    echo \"Recovery complete!\"\nfi\n```\n\n**Procedure 2: State File Corruption**\n\n```bash\n# Attempt recovery from remote backend versions\naws s3api list-object-versions \\\n    --bucket my-terraform-state \\\n    --prefix prod/terraform.tfstate\n\n# Download previous version\naws s3api get-object \\\n    --bucket my-terraform-state \\\n    --key prod/terraform.tfstate \\\n    --version-id <VERSION_ID> \\\n    terraform.tfstate.recovered\n\n# Validate recovered state\nterraform state list -state=terraform.tfstate.recovered\n\n# Replace current state if valid\nmv terraform.tfstate.recovered terraform.tfstate\n```\n\n---\n\n## Bonus 3: Terraform Certification Prep\n\n### ðŸ“œ HashiCorp Certified: Terraform Associate (003)\n\n#### Exam Objectives Mapping\n\n**1. Understand Infrastructure as Code (IaC) concepts**\n- âœ… See Part 1: Day 1 (Introduction)\n- Benefits of IaC, IaC vs manual provisioning\n\n**2. Understand Terraform's purpose (vs other IaC)**\n- âœ… See Part 1: Day 1\n- Terraform vs CloudFormation, Ansible, Pulumi\n\n**3. Understand Terraform basics**\n- âœ… See Part 1: Days 2-4\n- Installation, providers, resources, CLI workflow\n\n**4. Use the Terraform CLI (outside of core workflow)**\n- âœ… See Part 1: Day 3 & Part 2: Testing\n- `terraform fmt`, `validate`, `taint`, `import`, `state`\n\n**5. Interact with Terraform modules**\n- âœ… See Part 1: Day 11\n- Module sources, inputs, outputs, versioning\n\n**6. Navigate Terraform workflow**\n- âœ… See Part 1: Day 3\n- `init`, `plan`, `apply`, `destroy`\n\n**7. Implement and maintain state**\n- âœ… See Part 2: Days 13-15\n- Local vs remote state, locking, workspaces\n\n**8. Read, generate, and modify configuration**\n- âœ… See Part 1: Days 5-10\n- Resources, variables, outputs, expressions, functions\n\n**9. Understand Terraform Cloud and Enterprise capabilities**\n- âœ… See Part 2: Day 18\n- Remote state, VCS integration, Sentinel, cost estimation\n\n---\n\n### ðŸ“ Sample Exam Questions\n\n**Question 1:**\nWhich command should you run to preview changes without modifying infrastructure?\n\nA) `terraform apply -dry-run`\nB) `terraform plan`\nC) `terraform preview`\nD) `terraform validate`\n\n**Answer: B** - `terraform plan` shows changes without applying them.\n\n---\n\n**Question 2:**\nWhat is the purpose of the `terraform.tfstate.backup` file?\n\nA) It's automatically uploaded to Terraform Cloud\nB) It contains the previous state before the last apply\nC) It's used for disaster recovery only\nD) It stores encrypted secrets\n\n**Answer: B** - It's the previous state file, created before modifications.\n\n---\n\n**Question 3:**\nWhich meta-argument can you use to create multiple similar resources?\n\nA) `loop`\nB) `replicate`\nC) `count` or `for_each`\nD) `multiply`\n\n**Answer: C** - Use `count` or `for_each` for multiple instances.\n\n---\n\n**Question 4:**\nWhat does `terraform refresh` do?\n\nA) Updates state to match real infrastructure\nB) Downloads latest provider versions\nC) Clears the Terraform cache\nD) Reloads configuration files\n\n**Answer: A** - Queries infrastructure and updates state file.\n\n---\n\n**Question 5:**\nWhich backend supports state locking?\n\nA) local\nB) s3 (with DynamoDB)\nC) http\nD) All of the above\n\n**Answer: D** - All listed backends support locking (S3 requires DynamoDB table).\n\n---\n\n### ðŸŽ¯ Study Tips\n\n**Week 1-2: Fundamentals**\n- Install Terraform and practice basic commands\n- Create simple EC2/VPC resources\n- Practice `init`, `plan`, `apply`, `destroy` workflow\n- Understand state file structure\n\n**Week 3-4: Advanced Concepts**\n- Work with modules (create and consume)\n- Practice state management (`state list`, `mv`, `rm`, `import`)\n- Set up remote state with S3/DynamoDB\n- Use workspaces for multiple environments\n\n**Week 5-6: Best Practices**\n- Implement proper project structure\n- Practice variable validation and sensitive data handling\n- Work with `count`, `for_each`, `dynamic` blocks\n- Use provisioners and lifecycle rules\n\n**Week 7-8: Exam Preparation**\n- Review official Terraform documentation\n- Take practice exams\n- Build complete projects (like Projects 1-5 in this guide)\n- Review all exam objectives\n\n**Resources:**\n- ðŸ“š Official Study Guide: https://developer.hashicorp.com/terraform/tutorials/certification\n- ðŸŽ“ Practice Exams: HashiCorp Learn platform\n- ðŸ“– This Complete Guide (all sections!)\n\n---\n\n## Bonus 4: Ultimate Terraform Cheat Sheet\n\n### ðŸš€ Essential Commands\n\n```bash\n# INITIALIZATION\nterraform init                  # Initialize working directory\nterraform init -upgrade         # Upgrade providers\nterraform init -backend-config=\"bucket=mybucket\"  # Configure backend\n\n# PLANNING\nterraform plan                  # Show execution plan\nterraform plan -out=tfplan      # Save plan to file\nterraform plan -destroy         # Plan destruction\nterraform plan -target=aws_instance.web  # Plan specific resource\n\n# APPLYING\nterraform apply                 # Apply changes\nterraform apply tfplan          # Apply saved plan\nterraform apply -auto-approve   # Skip confirmation\nterraform apply -parallelism=20 # Adjust parallel operations\n\n# DESTROYING\nterraform destroy               # Destroy all resources\nterraform destroy -target=aws_instance.web  # Destroy specific resource\nterraform destroy -auto-approve # Skip confirmation\n\n# STATE MANAGEMENT\nterraform state list            # List all resources\nterraform state show aws_instance.web  # Show resource details\nterraform state mv SOURCE DEST  # Move/rename resource\nterraform state rm aws_instance.web    # Remove from state\nterraform state pull            # Download remote state\nterraform state push            # Upload state\n\n# IMPORTING\nterraform import aws_instance.web i-12345  # Import existing resource\n\n# WORKSPACES\nterraform workspace list        # List workspaces\nterraform workspace new dev     # Create workspace\nterraform workspace select dev  # Switch workspace\nterraform workspace delete dev  # Delete workspace\n\n# VALIDATION & FORMATTING\nterraform fmt                   # Format code\nterraform fmt -recursive        # Format all files\nterraform validate              # Validate configuration\nterraform validate -json        # JSON output\n\n# OUTPUTS\nterraform output                # Show all outputs\nterraform output instance_ip    # Show specific output\nterraform output -json          # JSON format\n\n# CONSOLE & TESTING\nterraform console               # Interactive console\nterraform test                  # Run tests (1.6+)\n\n# TROUBLESHOOTING\nterraform refresh               # Update state\nterraform taint aws_instance.web  # Mark for recreation\nterraform untaint aws_instance.web # Remove taint\nterraform force-unlock LOCK_ID  # Force unlock state\nterraform show                  # Show current state\nterraform graph                 # Generate dependency graph\n```\n\n---\n\n### ðŸ“¦ HCL Syntax Quick Reference\n\n**Resource Block:**\n\n```hcl\nresource \"provider_type\" \"name\" {\n  argument1 = \"value\"\n  argument2 = 123\n  \n  nested_block {\n    key = \"value\"\n  }\n  \n  dynamic \"dynamic_block\" {\n    for_each = var.list\n    content {\n      name = dynamic_block.value\n    }\n  }\n  \n  count = 3\n  # OR\n  for_each = toset([\"a\", \"b\", \"c\"])\n  \n  lifecycle {\n    create_before_destroy = true\n    prevent_destroy       = false\n    ignore_changes        = [tags]\n  }\n  \n  depends_on = [resource_type.other]\n  provider   = aws.alternate\n}\n```\n\n**Variable Block:**\n\n```hcl\nvariable \"name\" {\n  description = \"Description\"\n  type        = string\n  default     = \"default_value\"\n  sensitive   = false\n  nullable    = false\n  \n  validation {\n    condition     = length(var.name) > 3\n    error_message = \"Must be > 3 characters\"\n  }\n}\n```\n\n**Output Block:**\n\n```hcl\noutput \"name\" {\n  description = \"Description\"\n  value       = resource.attribute\n  sensitive   = false\n  depends_on  = [resource.other]\n}\n```\n\n**Locals Block:**\n\n```hcl\nlocals {\n  common_tags = {\n    Project     = \"MyProject\"\n    Environment = var.environment\n  }\n  \n  instance_count = var.environment == \"prod\" ? 3 : 1\n}\n```\n\n---\n\n### ðŸ”§ Common Functions Reference\n\n**String Functions:**\n```hcl\nupper(\"hello\")              # \"HELLO\"\nlower(\"HELLO\")              # \"hello\"\ntitle(\"hello world\")        # \"Hello World\"\ntrim(\"  text  \", \" \")       # \"text\"\ntrimspace(\"  text  \")       # \"text\"\nreplace(\"hello\", \"l\", \"r\")  # \"herro\"\njoin(\",\", [\"a\", \"b\"])       # \"a,b\"\nsplit(\",\", \"a,b,c\")         # [\"a\", \"b\", \"c\"]\nformat(\"Hello %s\", \"World\") # \"Hello World\"\n```\n\n**Numeric Functions:**\n```hcl\nmin(1, 2, 3)                # 1\nmax(1, 2, 3)                # 3\nabs(-5)                     # 5\nceil(4.3)                   # 5\nfloor(4.7)                  # 4\n```\n\n**Collection Functions:**\n```hcl\nlength([1, 2, 3])           # 3\nelement([1, 2, 3], 0)       # 1\nconcat([1, 2], [3, 4])      # [1, 2, 3, 4]\ncontains([1, 2], 1)         # true\nmerge({a=1}, {b=2})         # {a=1, b=2}\nkeys({a=1, b=2})            # [\"a\", \"b\"]\nvalues({a=1, b=2})          # [1, 2]\nlookup({a=1}, \"a\", 0)       # 1\n```\n\n**Type Conversion:**\n```hcl\ntostring(123)               # \"123\"\ntonumber(\"123\")             # 123\ntobool(\"true\")              # true\ntolist([1, 2])              # [1, 2]\ntoset([1, 1, 2])           # [1, 2]\ntomap({a=1})                # {a=1}\n```\n\n**Date/Time:**\n```hcl\ntimestamp()                 # \"2024-01-15T10:30:00Z\"\nformatdate(\"YYYY-MM-DD\", timestamp())  # \"2024-01-15\"\n```\n\n**Encoding:**\n```hcl\nbase64encode(\"text\")        # \"dGV4dA==\"\nbase64decode(\"dGV4dA==\")    # \"text\"\njsonencode({a=1})           # \"{\\\"a\\\":1}\"\njsondecode(\"{\\\"a\\\":1}\")     # {a=1}\n```\n\n**Filesystem:**\n```hcl\nfile(\"path/to/file\")        # Read file\nfileexists(\"path\")          # Check if exists\ntemplatefile(\"template.tpl\", {var = \"value\"})\n```\n\n**IP Network:**\n```hcl\ncidrsubnet(\"10.0.0.0/16\", 8, 0)  # \"10.0.0.0/24\"\ncidrhost(\"10.0.0.0/24\", 5)        # \"10.0.0.5\"\n```\n\n---\n\n### ðŸŽ¨ Common Patterns\n\n**Conditional Resource Creation:**\n\n```hcl\nresource \"aws_instance\" \"example\" {\n  count = var.create_instance ? 1 : 0\n  # configuration\n}\n```\n\n**Multiple Instances with Count:**\n\n```hcl\nresource \"aws_subnet\" \"public\" {\n  count             = length(var.availability_zones)\n  cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index)\n  availability_zone = var.availability_zones[count.index]\n}\n```\n\n**Multiple Instances with for_each:**\n\n```hcl\nresource \"aws_instance\" \"servers\" {\n  for_each = var.server_config\n  \n  instance_type = each.value.type\n  ami           = each.value.ami\n  \n  tags = {\n    Name = each.key\n  }\n}\n```\n\n**Dynamic Blocks:**\n\n```hcl\nresource \"aws_security_group\" \"example\" {\n  dynamic \"ingress\" {\n    for_each = var.ingress_rules\n    content {\n      from_port   = ingress.value.port\n      to_port     = ingress.value.port\n      protocol    = \"tcp\"\n      cidr_blocks = ingress.value.cidr_blocks\n    }\n  }\n}\n```\n\n---\n\n### âš¡ Performance Tips\n\n```bash\n# Increase parallelism (default is 10)\nterraform apply -parallelism=50\n\n# Reduce output verbosity\nterraform apply -compact-warnings\n\n# Target specific resources\nterraform apply -target=module.vpc\n\n# Refresh only (don't apply)\nterraform apply -refresh-only\n\n# Skip refresh during plan\nterraform plan -refresh=false\n```\n\n---\n\n### ðŸ” Security Checklist\n\n```hcl\n# âœ… DO:\n- Store secrets in AWS Secrets Manager/SSM\n- Use remote state with encryption\n- Enable state locking\n- Use .gitignore for sensitive files\n- Implement least-privilege IAM\n- Enable MFA for production\n- Use OIDC instead of static credentials\n- Scan code with tfsec/Checkov\n\n# âŒ DON'T:\n- Hardcode secrets in .tf files\n- Commit .tfstate files to Git\n- Use root AWS credentials\n- Disable state encryption\n- Skip security scanning\n- Use wildcard IAM permissions\n```\n\n---\n\n### ðŸ“‹ Common Variable Types\n\n```hcl\n# String\nvariable \"name\" {\n  type = string\n}\n\n# Number\nvariable \"count\" {\n  type = number\n}\n\n# Bool\nvariable \"enabled\" {\n  type = bool\n}\n\n# List\nvariable \"zones\" {\n  type = list(string)\n}\n\n# Map\nvariable \"tags\" {\n  type = map(string)\n}\n\n# Object\nvariable \"config\" {\n  type = object({\n    name = string\n    size = number\n  })\n}\n\n# Tuple\nvariable \"tuple_example\" {\n  type = tuple([string, number, bool])\n}\n\n# Set\nvariable \"unique_items\" {\n  type = set(string)\n}\n\n# Any (not recommended)\nvariable \"flexible\" {\n  type = any\n}\n```\n\n---\n\n### ðŸŽ¯ Best Practices Summary\n\n1. **Version Control Everything** (except state)\n2. **Use Remote State** (S3/Terraform Cloud)\n3. **Enable State Locking** (DynamoDB)\n4. **Structure Code** (modules, environments)\n5. **Validate & Format** (pre-commit hooks)\n6. **Use Workspaces** (dev/staging/prod)\n7. **Implement Security Scanning** (tfsec, Checkov)\n8. **Document Everything** (README, comments)\n9. **Test Infrastructure** (Terratest)\n10. **Automate CI/CD** (GitHub Actions)\n\n---\n\n## ðŸŽ“ Conclusion\n\nCongratulations! ðŸŽ‰ You've completed the **Terraform Complete Mastery Guide from Zero to Hero**!\n\n### What You've Learned:\n\n**Part 1 (Days 1-11):**\n- âœ… Terraform fundamentals and installation\n- âœ… HCL syntax and 100+ functions\n- âœ… Providers (AWS, Azure, GCP)\n- âœ… Resources, variables, outputs\n- âœ… Advanced features (count, for_each, dynamic blocks)\n- âœ… Modules and reusable infrastructure\n\n**Part 2 (Days 12-22):**\n- âœ… Workspaces and remote state\n- âœ… State management and locking\n- âœ… Provisioners and lifecycle rules\n- âœ… Terraform Cloud collaboration\n- âœ… Project structure and organization\n- âœ… Security best practices\n- âœ… Testing strategies\n- âœ… CI/CD integration\n\n**Real-World Projects:**\n- âœ… Project 1: Simple Web Server\n- âœ… Project 2: VPC & Networking\n- âœ… Project 3: Multi-Tier Application\n- âœ… Project 4: EKS Kubernetes Cluster\n- âœ… Project 5: Multi-Cloud Deployment\n\n**Bonus Content:**\n- âœ… State management surgery\n- âœ… Troubleshooting guide\n- âœ… Certification preparation\n- âœ… Complete cheat sheet\n\n---\n\n### ðŸš€ Next Steps:\n\n1. **Practice**: Build your own infrastructure projects\n2. **Contribute**: Create and share Terraform modules\n3. **Certify**: Get HashiCorp Certified: Terraform Associate\n4. **Automate**: Implement CI/CD pipelines\n5. **Scale**: Apply patterns to production workloads\n\n---\n\n### ðŸ“š Additional Resources:\n\n- **Official Docs**: https://developer.hashicorp.com/terraform\n- **Registry**: https://registry.terraform.io\n- **Community**: https://discuss.hashicorp.com\n- **GitHub**: https://github.com/hashicorp/terraform\n- **Certification**: https://www.hashicorp.com/certification/terraform-associate\n\n---\n\n### ðŸ’¡ Remember:\n\n> \"Infrastructure as Code is not about the tools,\n> it's about the mindset of treating infrastructure\n> as software with all the associated disciplines:\n> version control, testing, and continuous delivery.\"\n\n---\n\n**Happy Terraforming! ðŸŒ**\n\n*This guide was created with â¤ï¸ for the DevOps and Cloud Engineering community.*\n\n---\n\n**Document Information:**\n- **Title**: Terraform Complete Mastery Guide - Part 2\n- **Version**: 2.0\n- **Last Updated**: December 2024\n- **Author**: Tech Mastery Notebooks\n- **Terraform Version**: 1.5.0 - 1.7.0\n- **Total Lines**: ~15,000+ lines of comprehensive content\n\n---\n\n*End of Part 2*\n"}