{"id":"event-driven-arch","title":"ðŸ—ï¸ Event-Driven Architecture Zero to Hero","content":"# Event-Driven Architecture: Zero to Hero Guide\n## Complete Modern Event-Driven Systems Mastery\n\n---\n\n## ðŸ“š Table of Contents\n\n1. [Introduction to Event-Driven Architecture](#introduction)\n2. [Core Concepts & Fundamentals](#fundamentals)\n3. [Event Design Patterns](#patterns)\n4. [Message Brokers Deep Dive](#brokers)\n5. [Apache Kafka Mastery](#kafka)\n6. [RabbitMQ Complete Guide](#rabbitmq)\n7. [Event Sourcing](#event-sourcing)\n8. [CQRS Pattern](#cqrs)\n9. [Saga Pattern](#saga)\n10. [Event Streaming & Real-time Processing](#streaming)\n11. [Microservices Event Communication](#microservices)\n12. [Testing Event-Driven Systems](#testing)\n13. [Monitoring & Observability](#monitoring)\n14. [Security & Best Practices](#security)\n15. [Real-World Projects](#projects)\n\n---\n\n## ðŸŽ¯ Introduction to Event-Driven Architecture {#introduction}\n\n### What is Event-Driven Architecture (EDA)?\n\n**Event-Driven Architecture** is a software design pattern where the flow of the application is determined by events - state changes, user actions, sensor outputs, or messages from other systems.\n\n```\nTraditional Request-Response\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Request     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Request     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Client A â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚Service Bâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚Service Câ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â–²                                                         â”‚\n     â”‚                       Response                          â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nEvent-Driven Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Event       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Event      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Service Aâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Event Broker   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚Service Bâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   (Kafka/      â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚   RabbitMQ)    â”‚                      â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n                                     â”‚                               â”‚\n                                     â–¼                               â–¼\n                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                               â”‚Service Câ”‚                     â”‚Service Dâ”‚\n                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Why Event-Driven Architecture?\n\n**Benefits**:\n- âœ… **Loose Coupling**: Services don't know about each other directly\n- âœ… **Scalability**: Handle millions of events per second\n- âœ… **Resilience**: Failure in one service doesn't crash others\n- âœ… **Real-time Processing**: Immediate response to business events\n- âœ… **Extensibility**: Easy to add new consumers without changing producers\n- âœ… **Audit Trail**: Complete history of all system events\n\n**Use Cases**:\n- E-commerce order processing\n- Financial transaction processing\n- IoT sensor data processing\n- User activity tracking\n- Real-time analytics and monitoring\n- Microservices communication\n\n### Event-Driven vs Traditional Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚           Traditional Synchronous                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Pros:                          Cons:                   â”‚\nâ”‚  âœ… Simple to understand        âŒ Tight coupling       â”‚\nâ”‚  âœ… Immediate consistency       âŒ Poor scalability     â”‚\nâ”‚  âœ… Easy debugging              âŒ Single point failure â”‚\nâ”‚  âœ… Transactional integrity     âŒ Blocking operations  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚           Event-Driven Asynchronous                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Pros:                          Cons:                   â”‚\nâ”‚  âœ… Loose coupling              âŒ Eventual consistency â”‚\nâ”‚  âœ… High scalability            âŒ Complex debugging    â”‚\nâ”‚  âœ… Fault tolerance             âŒ Message ordering     â”‚\nâ”‚  âœ… Real-time processing        âŒ Duplicate handling   â”‚\nâ”‚  âœ… Easy to extend              âŒ Complex transactions â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸ“ Core Concepts & Fundamentals {#fundamentals}\n\n### Event Components\n\n**1. Events**\n```javascript\n// Domain Event Example\n{\n  \"eventId\": \"evt_12345678\",\n  \"eventType\": \"OrderPlaced\",\n  \"eventVersion\": \"1.0\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"source\": \"order-service\",\n  \"data\": {\n    \"orderId\": \"ORD-001\",\n    \"customerId\": \"CUST-123\",\n    \"items\": [\n      {\n        \"productId\": \"PROD-456\",\n        \"quantity\": 2,\n        \"price\": 29.99\n      }\n    ],\n    \"totalAmount\": 59.98,\n    \"currency\": \"USD\"\n  },\n  \"metadata\": {\n    \"correlationId\": \"corr_87654321\",\n    \"userId\": \"user_999\",\n    \"traceId\": \"trace_555\"\n  }\n}\n```\n\n**2. Event Types**\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                Domain Events                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Business state changes                           â”‚\nâ”‚  â€¢ Examples: OrderPlaced, UserRegistered,          â”‚\nâ”‚    PaymentProcessed                                 â”‚\nâ”‚  â€¢ Past tense naming                                â”‚\nâ”‚  â€¢ Immutable once published                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚               Integration Events                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ System-to-system communication                  â”‚\nâ”‚  â€¢ Examples: UserSyncRequired, DataExportReady     â”‚\nâ”‚  â€¢ Technical events for integration                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚               Notification Events                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ User notifications and alerts                   â”‚\nâ”‚  â€¢ Examples: EmailSent, NotificationDelivered      â”‚\nâ”‚  â€¢ Usually terminal events                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Event Flow Patterns\n\n**1. Publish-Subscribe (Pub/Sub)**\n\n```\nPublisher â”€â”€â”€â”€â”€â”€â”\n                â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚ Message â”‚\n              â”‚ Broker  â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â”‚   â”‚   â”‚\n                â–¼   â–¼   â–¼\n        Subscriber A B C\n```\n\n**2. Event Streaming**\n\n```\nEvents Flow â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â–¶ Consumer 1\n                  â”‚ Stream  â”‚ \n                  â”‚ Broker  â”‚ â”€â”€â”€â”€â–¶ Consumer 2\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â”‚\n                      â–¼ (Persistent Log)\n                  Event History\n```\n\n**3. Message Queuing**\n\n```\nProducer â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â–¶ Consumer\n               â”‚  Queue  â”‚\n               â”‚ (FIFO)  â”‚ â”€â”€â”€â”€â–¶ Consumer\n               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Event Delivery Guarantees\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            At-Most-Once                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Message delivered 0 or 1 times                  â”‚\nâ”‚  â€¢ No duplicates, but possible message loss        â”‚\nâ”‚  â€¢ Use for: Non-critical data, telemetry          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            At-Least-Once                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Message delivered 1 or more times               â”‚\nâ”‚  â€¢ No message loss, but possible duplicates        â”‚\nâ”‚  â€¢ Use for: Critical data with idempotent consumers â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Exactly-Once                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Message delivered exactly 1 time                â”‚\nâ”‚  â€¢ No duplicates, no message loss                  â”‚\nâ”‚  â€¢ Use for: Financial transactions, critical data  â”‚\nâ”‚  â€¢ Most complex to implement                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸŽ¨ Event Design Patterns {#patterns}\n\n### 1. Event Notification Pattern\n\n**Purpose**: Notify other systems that something interesting happened.\n\n```javascript\n// Publisher (Order Service)\nclass OrderService {\n  async createOrder(orderData) {\n    try {\n      // Create order\n      const order = await this.repository.create(orderData)\n      \n      // Publish event notification\n      await this.eventPublisher.publish('order.created', {\n        eventId: uuid(),\n        eventType: 'OrderCreated',\n        timestamp: new Date().toISOString(),\n        data: {\n          orderId: order.id,\n          customerId: order.customerId,\n          totalAmount: order.totalAmount\n        }\n      })\n      \n      return order\n      \n    } catch (error) {\n      // Publish failure event\n      await this.eventPublisher.publish('order.creation.failed', {\n        eventId: uuid(),\n        eventType: 'OrderCreationFailed',\n        timestamp: new Date().toISOString(),\n        data: {\n          customerId: orderData.customerId,\n          error: error.message\n        }\n      })\n      throw error\n    }\n  }\n}\n\n// Subscriber (Email Service)\nclass EmailService {\n  async handleOrderCreated(event) {\n    const { orderId, customerId } = event.data\n    \n    // Get customer details\n    const customer = await this.customerService.getCustomer(customerId)\n    \n    // Send confirmation email\n    await this.emailSender.send({\n      to: customer.email,\n      subject: `Order Confirmation - ${orderId}`,\n      template: 'order-confirmation',\n      data: { order: event.data, customer }\n    })\n    \n    console.log(`Order confirmation email sent for ${orderId}`)\n  }\n}\n```\n\n### 2. Event-Carried State Transfer\n\n**Purpose**: Include enough data in the event so consumers don't need to call back.\n\n```javascript\n// Rich event with complete data\n{\n  \"eventType\": \"CustomerUpdated\",\n  \"eventId\": \"evt_98765\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"data\": {\n    \"customerId\": \"CUST-123\",\n    \"changes\": {\n      \"email\": {\n        \"oldValue\": \"old@example.com\",\n        \"newValue\": \"new@example.com\"\n      },\n      \"address\": {\n        \"oldValue\": {\n          \"street\": \"123 Old St\",\n          \"city\": \"Old City\"\n        },\n        \"newValue\": {\n          \"street\": \"456 New Ave\", \n          \"city\": \"New City\"\n        }\n      }\n    },\n    // Include current complete state\n    \"currentState\": {\n      \"customerId\": \"CUST-123\",\n      \"name\": \"John Doe\",\n      \"email\": \"new@example.com\",\n      \"phone\": \"+1-555-0123\",\n      \"address\": {\n        \"street\": \"456 New Ave\",\n        \"city\": \"New City\",\n        \"zipCode\": \"12345\"\n      },\n      \"preferences\": {\n        \"newsletter\": true,\n        \"smsNotifications\": false\n      }\n    }\n  }\n}\n\n// Consumer can process without additional calls\nclass ReportingService {\n  async handleCustomerUpdated(event) {\n    const { customerId, currentState, changes } = event.data\n    \n    // Update local cache with complete customer state\n    await this.customerCache.set(customerId, currentState)\n    \n    // Log changes for audit\n    await this.auditLog.record({\n      entity: 'customer',\n      entityId: customerId,\n      changes: changes,\n      timestamp: event.timestamp\n    })\n    \n    // No need to call customer service!\n  }\n}\n```\n\n### 3. Event Sourcing Pattern\n\n**Purpose**: Store all changes as events, derive current state by replaying events.\n\n```javascript\nclass EventStore {\n  constructor() {\n    this.events = new Map() // In production: use database\n  }\n  \n  async append(streamId, events) {\n    if (!this.events.has(streamId)) {\n      this.events.set(streamId, [])\n    }\n    \n    const stream = this.events.get(streamId)\n    \n    // Add version numbers to events\n    events.forEach((event, index) => {\n      event.version = stream.length + index + 1\n      event.streamId = streamId\n      event.timestamp = event.timestamp || new Date().toISOString()\n    })\n    \n    stream.push(...events)\n    return events[events.length - 1].version\n  }\n  \n  async getEvents(streamId, fromVersion = 0) {\n    const stream = this.events.get(streamId) || []\n    return stream.filter(event => event.version > fromVersion)\n  }\n  \n  async getAllEvents() {\n    const allEvents = []\n    for (const [streamId, events] of this.events) {\n      allEvents.push(...events)\n    }\n    return allEvents.sort((a, b) => a.timestamp.localeCompare(b.timestamp))\n  }\n}\n\nclass OrderAggregate {\n  constructor(orderId) {\n    this.orderId = orderId\n    this.version = 0\n    this.status = 'draft'\n    this.items = []\n    this.totalAmount = 0\n    this.customerId = null\n    this.uncommittedEvents = []\n  }\n  \n  // Command handlers that produce events\n  create(customerId, items) {\n    if (this.version > 0) {\n      throw new Error('Order already exists')\n    }\n    \n    const event = {\n      eventType: 'OrderCreated',\n      data: {\n        orderId: this.orderId,\n        customerId,\n        items,\n        totalAmount: items.reduce((sum, item) => sum + item.price * item.quantity, 0)\n      }\n    }\n    \n    this.applyEvent(event)\n    this.uncommittedEvents.push(event)\n  }\n  \n  addItem(productId, quantity, price) {\n    if (this.status !== 'draft') {\n      throw new Error('Cannot add items to non-draft order')\n    }\n    \n    const event = {\n      eventType: 'ItemAdded',\n      data: {\n        orderId: this.orderId,\n        productId,\n        quantity,\n        price\n      }\n    }\n    \n    this.applyEvent(event)\n    this.uncommittedEvents.push(event)\n  }\n  \n  confirm() {\n    if (this.status !== 'draft') {\n      throw new Error('Order not in draft status')\n    }\n    \n    if (this.items.length === 0) {\n      throw new Error('Cannot confirm empty order')\n    }\n    \n    const event = {\n      eventType: 'OrderConfirmed',\n      data: {\n        orderId: this.orderId,\n        confirmedAt: new Date().toISOString()\n      }\n    }\n    \n    this.applyEvent(event)\n    this.uncommittedEvents.push(event)\n  }\n  \n  // Event handlers that update state\n  applyEvent(event) {\n    switch (event.eventType) {\n      case 'OrderCreated':\n        this.customerId = event.data.customerId\n        this.items = [...event.data.items]\n        this.totalAmount = event.data.totalAmount\n        this.status = 'draft'\n        break\n        \n      case 'ItemAdded':\n        this.items.push({\n          productId: event.data.productId,\n          quantity: event.data.quantity,\n          price: event.data.price\n        })\n        this.totalAmount += event.data.price * event.data.quantity\n        break\n        \n      case 'OrderConfirmed':\n        this.status = 'confirmed'\n        break\n        \n      default:\n        console.warn(`Unknown event type: ${event.eventType}`)\n    }\n    \n    this.version = event.version || this.version + 1\n  }\n  \n  // Rebuild state from events\n  static async fromHistory(orderId, eventStore) {\n    const events = await eventStore.getEvents(`order-${orderId}`)\n    const order = new OrderAggregate(orderId)\n    \n    events.forEach(event => order.applyEvent(event))\n    \n    return order\n  }\n  \n  // Save uncommitted events\n  async save(eventStore) {\n    if (this.uncommittedEvents.length === 0) return\n    \n    await eventStore.append(`order-${this.orderId}`, this.uncommittedEvents)\n    this.uncommittedEvents = []\n  }\n}\n\n// Usage example\nconst eventStore = new EventStore()\n\n// Create new order\nconst order = new OrderAggregate('ORD-001')\norder.create('CUST-123', [\n  { productId: 'PROD-1', quantity: 2, price: 29.99 }\n])\norder.addItem('PROD-2', 1, 49.99)\norder.confirm()\n\n// Save events\nawait order.save(eventStore)\n\n// Rebuild from events later\nconst rebuiltOrder = await OrderAggregate.fromHistory('ORD-001', eventStore)\nconsole.log('Rebuilt order status:', rebuiltOrder.status)\nconsole.log('Rebuilt order total:', rebuiltOrder.totalAmount)\n```\n\n### 4. Event Choreography vs Orchestration\n\n**Choreography (Decentralized)**\n\n```javascript\n// Each service knows what to do when events happen\nclass PaymentService {\n  async handleOrderConfirmed(event) {\n    const { orderId, totalAmount, customerId } = event.data\n    \n    try {\n      // Process payment\n      const payment = await this.processPayment(customerId, totalAmount)\n      \n      // Publish success event\n      await this.eventPublisher.publish('payment.processed', {\n        eventType: 'PaymentProcessed',\n        data: {\n          orderId,\n          paymentId: payment.id,\n          amount: totalAmount,\n          status: 'successful'\n        }\n      })\n      \n    } catch (error) {\n      // Publish failure event\n      await this.eventPublisher.publish('payment.failed', {\n        eventType: 'PaymentFailed',\n        data: {\n          orderId,\n          amount: totalAmount,\n          error: error.message\n        }\n      })\n    }\n  }\n}\n\nclass InventoryService {\n  async handleOrderConfirmed(event) {\n    const { orderId, items } = event.data\n    \n    try {\n      // Reserve inventory\n      await this.reserveInventory(items)\n      \n      await this.eventPublisher.publish('inventory.reserved', {\n        eventType: 'InventoryReserved',\n        data: { orderId, items }\n      })\n      \n    } catch (error) {\n      await this.eventPublisher.publish('inventory.reservation.failed', {\n        eventType: 'InventoryReservationFailed', \n        data: { orderId, items, error: error.message }\n      })\n    }\n  }\n}\n```\n\n**Orchestration (Centralized)**\n\n```javascript\nclass OrderProcessOrchestrator {\n  async handleOrderConfirmed(event) {\n    const { orderId, customerId, items, totalAmount } = event.data\n    \n    try {\n      // Step 1: Reserve inventory\n      const inventoryResult = await this.inventoryService.reserve(items)\n      \n      if (!inventoryResult.success) {\n        throw new Error(`Inventory reservation failed: ${inventoryResult.error}`)\n      }\n      \n      // Step 2: Process payment\n      const paymentResult = await this.paymentService.charge(customerId, totalAmount)\n      \n      if (!paymentResult.success) {\n        // Compensate: Release inventory\n        await this.inventoryService.release(items)\n        throw new Error(`Payment failed: ${paymentResult.error}`)\n      }\n      \n      // Step 3: Ship order\n      const shippingResult = await this.shippingService.ship(orderId)\n      \n      if (!shippingResult.success) {\n        // Compensate: Refund and release inventory\n        await this.paymentService.refund(paymentResult.paymentId)\n        await this.inventoryService.release(items)\n        throw new Error(`Shipping failed: ${shippingResult.error}`)\n      }\n      \n      // Success: Publish completion event\n      await this.eventPublisher.publish('order.processing.completed', {\n        eventType: 'OrderProcessingCompleted',\n        data: {\n          orderId,\n          paymentId: paymentResult.paymentId,\n          shippingId: shippingResult.shippingId\n        }\n      })\n      \n    } catch (error) {\n      await this.eventPublisher.publish('order.processing.failed', {\n        eventType: 'OrderProcessingFailed',\n        data: { orderId, error: error.message }\n      })\n    }\n  }\n}\n```\n\n---\n\n## ðŸ“¨ Message Brokers Deep Dive {#brokers}\n\n### Broker Comparison\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Apache Kafka                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ High throughput, low latency                        â”‚\nâ”‚  â€¢ Horizontal scaling                                   â”‚\nâ”‚  â€¢ Event streaming & replay                             â”‚\nâ”‚  â€¢ Complex setup                                        â”‚\nâ”‚  â€¢ Best for: High-volume, real-time data              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     RabbitMQ                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Easy to use, flexible routing                      â”‚\nâ”‚  â€¢ Rich feature set                                    â”‚\nâ”‚  â€¢ Good for traditional messaging                      â”‚\nâ”‚  â€¢ Limited scalability                                 â”‚\nâ”‚  â€¢ Best for: Complex routing, moderate volume          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   Amazon SQS/SNS                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Fully managed, serverless                          â”‚\nâ”‚  â€¢ High availability                                   â”‚\nâ”‚  â€¢ Good integration with AWS                           â”‚\nâ”‚  â€¢ Limited features                                    â”‚\nâ”‚  â€¢ Best for: AWS-native applications                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Apache Pulsar                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Multi-tenancy support                               â”‚\nâ”‚  â€¢ Geo-replication                                     â”‚\nâ”‚  â€¢ Unified messaging & streaming                       â”‚\nâ”‚  â€¢ Relatively new                                      â”‚\nâ”‚  â€¢ Best for: Multi-tenant, geo-distributed apps       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸš€ Apache Kafka Mastery {#kafka}\n\n### Kafka Architecture Deep Dive\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        Kafka Cluster                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\nâ”‚  â”‚  Broker 1   â”‚  â”‚  Broker 2   â”‚  â”‚  Broker 3   â”‚             â”‚\nâ”‚  â”‚  (Leader)   â”‚  â”‚ (Follower)  â”‚  â”‚ (Follower)  â”‚             â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚         â”‚                â”‚                â”‚                     â”‚\nâ”‚         â–¼                â–¼                â–¼                     â”‚\nâ”‚    Topic: orders    Topic: users    Topic: payments            â”‚\nâ”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\nâ”‚    â”‚Part 0   â”‚     â”‚Part 0   â”‚     â”‚Part 0   â”‚                 â”‚\nâ”‚    â”‚Part 1   â”‚     â”‚Part 1   â”‚     â”‚Part 1   â”‚                 â”‚\nâ”‚    â”‚Part 2   â”‚     â”‚Part 2   â”‚     â”‚Part 2   â”‚                 â”‚\nâ”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â–²\n                              â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚    ZooKeeper    â”‚\n                    â”‚   (Metadata)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Advanced Kafka Producer\n\n```python\nfrom kafka import KafkaProducer\nfrom kafka.partitioner import RoundRobinPartitioner, Murmur2Partitioner\nimport json\nimport time\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional, Callable\nimport uuid\nimport hashlib\n\nclass AdvancedKafkaProducer:\n    def __init__(self, \n                 bootstrap_servers: list,\n                 client_id: str = \"advanced-producer\",\n                 acks: str = \"all\",\n                 retries: int = 10,\n                 batch_size: int = 16384,\n                 linger_ms: int = 10,\n                 compression_type: str = \"snappy\",\n                 enable_idempotence: bool = True):\n        \n        self.logger = logging.getLogger(__name__)\n        \n        # Producer configuration for high reliability and performance\n        config = {\n            'bootstrap_servers': bootstrap_servers,\n            'client_id': client_id,\n            'acks': acks,  # 'all' = wait for all replicas\n            'retries': retries,\n            'max_in_flight_requests_per_connection': 5,\n            'enable_idempotence': enable_idempotence,\n            'batch_size': batch_size,\n            'linger_ms': linger_ms,\n            'compression_type': compression_type,\n            'buffer_memory': 33554432,  # 32MB\n            'key_serializer': lambda k: k.encode('utf-8') if k else None,\n            'value_serializer': lambda v: json.dumps(v).encode('utf-8')\n        }\n        \n        self.producer = KafkaProducer(**config)\n        \n        # Metrics tracking\n        self.sent_count = 0\n        self.error_count = 0\n        self.start_time = time.time()\n    \n    def send_event(self, \n                   topic: str, \n                   event_data: Dict[str, Any],\n                   key: str = None,\n                   headers: Dict[str, bytes] = None,\n                   partition: int = None,\n                   callback: Callable = None) -> bool:\n        \"\"\"Send event with comprehensive error handling and metrics\"\"\"\n        \n        try:\n            # Enrich event with metadata\n            enriched_event = self._enrich_event(event_data)\n            \n            # Determine partition key for ordering\n            if key is None and 'entityId' in event_data:\n                key = str(event_data['entityId'])\n            \n            # Add standard headers\n            event_headers = {\n                'source': 'order-service',\n                'contentType': 'application/json',\n                'eventId': enriched_event['eventId'],\n                'timestamp': str(int(time.time() * 1000))\n            }\n            \n            if headers:\n                event_headers.update(headers)\n            \n            # Convert headers to bytes\n            event_headers_bytes = {\n                k: v.encode('utf-8') if isinstance(v, str) else v \n                for k, v in event_headers.items()\n            }\n            \n            # Send message\n            future = self.producer.send(\n                topic=topic,\n                key=key,\n                value=enriched_event,\n                partition=partition,\n                headers=event_headers_bytes\n            )\n            \n            # Add callback for success/failure tracking\n            if callback:\n                future.add_callback(callback)\n            \n            future.add_callback(self._on_success)\n            future.add_errback(self._on_error)\n            \n            self.sent_count += 1\n            \n            if self.sent_count % 1000 == 0:\n                self._log_metrics()\n            \n            return True\n            \n        except Exception as e:\n            self.error_count += 1\n            self.logger.error(f\"Failed to send event to {topic}: {e}\")\n            return False\n    \n    def _enrich_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Enrich event with standard metadata\"\"\"\n        \n        enriched = event_data.copy()\n        \n        # Add standard fields if not present\n        if 'eventId' not in enriched:\n            enriched['eventId'] = str(uuid.uuid4())\n        \n        if 'timestamp' not in enriched:\n            enriched['timestamp'] = datetime.now().isoformat()\n        \n        if 'eventVersion' not in enriched:\n            enriched['eventVersion'] = '1.0'\n        \n        # Add checksum for data integrity\n        event_str = json.dumps(enriched, sort_keys=True)\n        enriched['checksum'] = hashlib.sha256(event_str.encode()).hexdigest()\n        \n        return enriched\n    \n    def send_batch_events(self, topic: str, events: list) -> Dict[str, int]:\n        \"\"\"Send multiple events efficiently\"\"\"\n        \n        results = {'success': 0, 'failed': 0}\n        \n        for event in events:\n            if self.send_event(topic, event):\n                results['success'] += 1\n            else:\n                results['failed'] += 1\n        \n        # Flush to ensure all messages are sent\n        self.producer.flush()\n        \n        return results\n    \n    def _on_success(self, record_metadata):\n        \"\"\"Handle successful send\"\"\"\n        self.logger.debug(f\"Event sent to {record_metadata.topic} \"\n                         f\"partition {record_metadata.partition} \"\n                         f\"offset {record_metadata.offset}\")\n    \n    def _on_error(self, exception):\n        \"\"\"Handle send failure\"\"\"\n        self.error_count += 1\n        self.logger.error(f\"Failed to send event: {exception}\")\n    \n    def _log_metrics(self):\n        \"\"\"Log producer metrics\"\"\"\n        runtime = time.time() - self.start_time\n        rate = self.sent_count / runtime if runtime > 0 else 0\n        \n        self.logger.info(f\"Producer metrics - Sent: {self.sent_count}, \"\n                        f\"Errors: {self.error_count}, Rate: {rate:.2f}/sec\")\n    \n    def close(self):\n        \"\"\"Close producer and flush remaining messages\"\"\"\n        self.producer.flush(timeout=30)\n        self.producer.close()\n        self._log_metrics()\n\n# Usage\nproducer = AdvancedKafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    client_id='order-service-producer'\n)\n\n# Send single event\norder_event = {\n    'eventType': 'OrderCreated',\n    'entityId': 'ORD-123',\n    'data': {\n        'orderId': 'ORD-123',\n        'customerId': 'CUST-456',\n        'totalAmount': 99.99\n    }\n}\n\nsuccess = producer.send_event('orders', order_event, key='ORD-123')\n\n# Send batch events\nbatch_events = [\n    {'eventType': 'ItemViewed', 'data': {'productId': f'PROD-{i}'}}\n    for i in range(100)\n]\n\nbatch_results = producer.send_batch_events('user-activity', batch_events)\nprint(f\"Batch results: {batch_results}\")\n\nproducer.close()\n```\n\n### Advanced Kafka Consumer\n\n```python\nfrom kafka import KafkaConsumer, TopicPartition, OffsetAndMetadata\nimport json\nimport time\nimport logging\nfrom threading import Thread\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Dict, Any, Callable, List\nimport signal\nimport sys\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ConsumerState(Enum):\n    RUNNING = \"RUNNING\"\n    PAUSED = \"PAUSED\"\n    STOPPED = \"STOPPED\"\n\n@dataclass\nclass ProcessingResult:\n    success: bool\n    error_message: str = None\n    processing_time_ms: float = 0\n    retry_count: int = 0\n\nclass AdvancedKafkaConsumer:\n    def __init__(self, \n                 topics: List[str],\n                 bootstrap_servers: List[str],\n                 group_id: str,\n                 auto_offset_reset: str = 'latest',\n                 enable_auto_commit: bool = False,\n                 max_poll_records: int = 500,\n                 session_timeout_ms: int = 30000,\n                 heartbeat_interval_ms: int = 10000,\n                 max_poll_interval_ms: int = 300000):\n        \n        self.topics = topics\n        self.group_id = group_id\n        self.logger = logging.getLogger(__name__)\n        self.state = ConsumerState.STOPPED\n        self.message_handlers = {}\n        self.error_handlers = {}\n        \n        # Consumer configuration\n        config = {\n            'bootstrap_servers': bootstrap_servers,\n            'group_id': group_id,\n            'auto_offset_reset': auto_offset_reset,\n            'enable_auto_commit': enable_auto_commit,\n            'max_poll_records': max_poll_records,\n            'session_timeout_ms': session_timeout_ms,\n            'heartbeat_interval_ms': heartbeat_interval_ms,\n            'max_poll_interval_ms': max_poll_interval_ms,\n            'key_deserializer': lambda k: k.decode('utf-8') if k else None,\n            'value_deserializer': lambda m: json.loads(m.decode('utf-8'))\n        }\n        \n        self.consumer = KafkaConsumer(*topics, **config)\n        \n        # Processing metrics\n        self.processed_count = 0\n        self.error_count = 0\n        self.start_time = time.time()\n        \n        # Thread pool for parallel processing\n        self.executor = ThreadPoolExecutor(max_workers=10)\n        \n        # Graceful shutdown\n        signal.signal(signal.SIGINT, self._signal_handler)\n        signal.signal(signal.SIGTERM, self._signal_handler)\n    \n    def register_handler(self, event_type: str, handler: Callable[[Dict[str, Any]], ProcessingResult]):\n        \"\"\"Register event handler\"\"\"\n        self.message_handlers[event_type] = handler\n        self.logger.info(f\"Registered handler for {event_type}\")\n    \n    def register_error_handler(self, error_type: type, handler: Callable[[Exception, Dict[str, Any]], None]):\n        \"\"\"Register error handler\"\"\"\n        self.error_handlers[error_type] = handler\n        self.logger.info(f\"Registered error handler for {error_type.__name__}\")\n    \n    def start(self):\n        \"\"\"Start consuming messages\"\"\"\n        self.state = ConsumerState.RUNNING\n        self.logger.info(f\"Starting consumer for topics: {self.topics}\")\n        \n        try:\n            while self.state == ConsumerState.RUNNING:\n                # Poll for messages\n                message_batch = self.consumer.poll(timeout_ms=1000, max_records=100)\n                \n                if message_batch:\n                    self._process_batch(message_batch)\n                \n                # Commit offsets manually\n                self._commit_offsets()\n                \n        except Exception as e:\n            self.logger.error(f\"Consumer error: {e}\")\n            self.state = ConsumerState.STOPPED\n        \n        finally:\n            self.consumer.close()\n            self.executor.shutdown(wait=True)\n            self._log_final_metrics()\n    \n    def _process_batch(self, message_batch: Dict[TopicPartition, List]):\n        \"\"\"Process batch of messages with parallelization\"\"\"\n        \n        futures = []\n        \n        for topic_partition, messages in message_batch.items():\n            for message in messages:\n                # Submit to thread pool for parallel processing\n                future = self.executor.submit(self._process_message, message)\n                futures.append((future, message))\n        \n        # Wait for all messages to be processed\n        for future, message in futures:\n            try:\n                result = future.result(timeout=30)  # 30 second timeout\n                \n                if result.success:\n                    self.processed_count += 1\n                else:\n                    self.error_count += 1\n                    self._handle_processing_error(message, result.error_message)\n                \n            except Exception as e:\n                self.error_count += 1\n                self.logger.error(f\"Processing timeout or error: {e}\")\n                self._handle_processing_error(message, str(e))\n    \n    def _process_message(self, message) -> ProcessingResult:\n        \"\"\"Process individual message\"\"\"\n        \n        start_time = time.time()\n        \n        try:\n            # Extract event information\n            event_data = message.value\n            event_type = event_data.get('eventType')\n            event_id = event_data.get('eventId', 'unknown')\n            \n            self.logger.debug(f\"Processing event {event_id} of type {event_type}\")\n            \n            # Find appropriate handler\n            if event_type in self.message_handlers:\n                handler = self.message_handlers[event_type]\n                \n                # Add message metadata to event\n                event_data['_metadata'] = {\n                    'topic': message.topic,\n                    'partition': message.partition,\n                    'offset': message.offset,\n                    'timestamp': message.timestamp,\n                    'headers': {k: v.decode('utf-8') for k, v in (message.headers or [])}\n                }\n                \n                # Call handler\n                result = handler(event_data)\n                \n                if isinstance(result, ProcessingResult):\n                    processing_time = (time.time() - start_time) * 1000\n                    result.processing_time_ms = processing_time\n                    return result\n                else:\n                    # Assume success if handler doesn't return ProcessingResult\n                    return ProcessingResult(\n                        success=True,\n                        processing_time_ms=(time.time() - start_time) * 1000\n                    )\n            else:\n                self.logger.warning(f\"No handler registered for event type: {event_type}\")\n                return ProcessingResult(\n                    success=False,\n                    error_message=f\"No handler for event type: {event_type}\"\n                )\n        \n        except Exception as e:\n            processing_time = (time.time() - start_time) * 1000\n            self.logger.error(f\"Error processing event: {e}\")\n            \n            # Call error handler if registered\n            error_type = type(e)\n            if error_type in self.error_handlers:\n                self.error_handlers[error_type](e, event_data)\n            \n            return ProcessingResult(\n                success=False,\n                error_message=str(e),\n                processing_time_ms=processing_time\n            )\n    \n    def _handle_processing_error(self, message, error_message: str):\n        \"\"\"Handle processing errors with retry logic\"\"\"\n        \n        # Extract retry count from headers\n        retry_count = 0\n        if message.headers:\n            for key, value in message.headers:\n                if key == 'retry-count':\n                    retry_count = int(value.decode('utf-8'))\n        \n        max_retries = 3\n        \n        if retry_count < max_retries:\n            # Send to retry topic\n            retry_event = message.value.copy()\n            retry_event['_retry'] = {\n                'count': retry_count + 1,\n                'error': error_message,\n                'original_topic': message.topic,\n                'original_partition': message.partition,\n                'original_offset': message.offset\n            }\n            \n            # Send to retry topic (you'd need a retry producer)\n            self.logger.info(f\"Sending message to retry queue (attempt {retry_count + 1})\")\n            \n        else:\n            # Send to dead letter queue\n            dlq_event = message.value.copy()\n            dlq_event['_deadLetter'] = {\n                'reason': error_message,\n                'finalRetryCount': retry_count,\n                'originalTopic': message.topic,\n                'failedAt': datetime.now().isoformat()\n            }\n            \n            self.logger.error(f\"Message sent to dead letter queue after {retry_count} retries\")\n    \n    def _commit_offsets(self):\n        \"\"\"Manually commit offsets\"\"\"\n        try:\n            self.consumer.commit()\n        except Exception as e:\n            self.logger.error(f\"Failed to commit offsets: {e}\")\n    \n    def _signal_handler(self, signum, frame):\n        \"\"\"Handle shutdown signals gracefully\"\"\"\n        self.logger.info(f\"Received signal {signum}, shutting down gracefully...\")\n        self.state = ConsumerState.STOPPED\n    \n    def _log_final_metrics(self):\n        \"\"\"Log final processing metrics\"\"\"\n        runtime = time.time() - self.start_time\n        rate = self.processed_count / runtime if runtime > 0 else 0\n        error_rate = (self.error_count / (self.processed_count + self.error_count) * 100) if (self.processed_count + self.error_count) > 0 else 0\n        \n        self.logger.info(f\"Consumer metrics - Processed: {self.processed_count}, \"\n                        f\"Errors: {self.error_count}, Rate: {rate:.2f}/sec, \"\n                        f\"Error Rate: {error_rate:.2f}%\")\n    \n    def pause_consumption(self):\n        \"\"\"Pause message consumption\"\"\"\n        self.state = ConsumerState.PAUSED\n        self.consumer.pause(*self.consumer.assignment())\n        self.logger.info(\"Consumer paused\")\n    \n    def resume_consumption(self):\n        \"\"\"Resume message consumption\"\"\"\n        self.state = ConsumerState.RUNNING\n        self.consumer.resume(*self.consumer.assignment())\n        self.logger.info(\"Consumer resumed\")\n\n# Event handlers\ndef handle_order_created(event_data: Dict[str, Any]) -> ProcessingResult:\n    \"\"\"Handle OrderCreated event\"\"\"\n    try:\n        order_id = event_data['data']['orderId']\n        customer_id = event_data['data']['customerId']\n        \n        # Simulate processing\n        print(f\"Processing order {order_id} for customer {customer_id}\")\n        time.sleep(0.1)  # Simulate work\n        \n        return ProcessingResult(success=True)\n        \n    except Exception as e:\n        return ProcessingResult(success=False, error_message=str(e))\n\ndef handle_payment_processed(event_data: Dict[str, Any]) -> ProcessingResult:\n    \"\"\"Handle PaymentProcessed event\"\"\"\n    try:\n        payment_id = event_data['data']['paymentId']\n        amount = event_data['data']['amount']\n        \n        print(f\"Processing payment {payment_id} for amount ${amount}\")\n        \n        # Simulate payment processing\n        if amount > 10000:  # Simulate fraud detection\n            raise Exception(f\"Suspicious transaction amount: ${amount}\")\n        \n        return ProcessingResult(success=True)\n        \n    except Exception as e:\n        return ProcessingResult(success=False, error_message=str(e))\n\ndef handle_database_error(error: Exception, event_data: Dict[str, Any]):\n    \"\"\"Handle database connection errors\"\"\"\n    print(f\"Database error occurred while processing event {event_data.get('eventId', 'unknown')}: {error}\")\n    # Could implement retry logic, circuit breaker, etc.\n\ndef handle_validation_error(error: Exception, event_data: Dict[str, Any]):\n    \"\"\"Handle validation errors\"\"\"\n    print(f\"Validation error for event {event_data.get('eventId', 'unknown')}: {error}\")\n    # Could send to validation error topic\n\n# Usage\nconsumer = AdvancedKafkaConsumer(\n    topics=['orders', 'payments'],\n    bootstrap_servers=['localhost:9092'],\n    group_id='order-processing-service'\n)\n\n# Register event handlers\nconsumer.register_handler('OrderCreated', handle_order_created)\nconsumer.register_handler('PaymentProcessed', handle_payment_processed)\n\n# Register error handlers\nconsumer.register_error_handler(ConnectionError, handle_database_error)\nconsumer.register_error_handler(ValueError, handle_validation_error)\n\n# Start consuming\nconsumer.start()\n```\n\n### Kafka Streams Processing\n\n```python\n# Note: This is conceptual as kafka-streams-python isn't as mature as Java version\n# In production, consider using Java for Kafka Streams or Python with confluent-kafka\nimport json\nimport time\nfrom kafka import KafkaConsumer, KafkaProducer\nfrom collections import defaultdict, deque\nfrom typing import Dict, Any, Callable, Optional\nfrom datetime import datetime, timedelta\nimport threading\n\nclass KafkaStreamsProcessor:\n    def __init__(self, application_id: str, bootstrap_servers: List[str]):\n        self.application_id = application_id\n        self.bootstrap_servers = bootstrap_servers\n        self.topology = []\n        self.state_stores = {}\n        self.logger = logging.getLogger(__name__)\n        \n        # Processing state\n        self.running = False\n        self.consumers = {}\n        self.producers = {}\n        \n    def stream(self, topics: List[str], name: str = \"stream\"):\n        \"\"\"Create a stream from topics\"\"\"\n        stream = KStream(self, topics, name)\n        return stream\n    \n    def table(self, topic: str, name: str = \"table\"):\n        \"\"\"Create a table from topic\"\"\"\n        table = KTable(self, topic, name)\n        return table\n    \n    def create_state_store(self, name: str, store_type: str = \"memory\"):\n        \"\"\"Create a state store for stateful operations\"\"\"\n        if store_type == \"memory\":\n            self.state_stores[name] = {}\n        else:\n            raise ValueError(f\"Store type {store_type} not supported\")\n    \n    def start(self):\n        \"\"\"Start the streams application\"\"\"\n        self.running = True\n        self.logger.info(f\"Starting streams application: {self.application_id}\")\n        \n        # Start consumers for each topology node\n        for node in self.topology:\n            consumer_thread = threading.Thread(target=node.run)\n            consumer_thread.start()\n\nclass KStream:\n    def __init__(self, processor: KafkaStreamsProcessor, topics: List[str], name: str):\n        self.processor = processor\n        self.topics = topics\n        self.name = name\n        \n    def filter(self, predicate: Callable[[Dict[str, Any]], bool], name: str = \"filter\"):\n        \"\"\"Filter events based on predicate\"\"\"\n        filter_node = FilterNode(self, predicate, name)\n        self.processor.topology.append(filter_node)\n        return KStream(self.processor, [f\"{self.name}-{name}\"], f\"{self.name}-{name}\")\n    \n    def map(self, mapper: Callable[[Dict[str, Any]], Dict[str, Any]], name: str = \"map\"):\n        \"\"\"Transform events using mapper function\"\"\"\n        map_node = MapNode(self, mapper, name)\n        self.processor.topology.append(map_node)\n        return KStream(self.processor, [f\"{self.name}-{name}\"], f\"{self.name}-{name}\")\n    \n    def group_by_key(self):\n        \"\"\"Group events by key for aggregation\"\"\"\n        return KGroupedStream(self)\n    \n    def to(self, topic: str):\n        \"\"\"Send events to output topic\"\"\"\n        sink_node = SinkNode(self, topic)\n        self.processor.topology.append(sink_node)\n\nclass KGroupedStream:\n    def __init__(self, stream: KStream):\n        self.stream = stream\n    \n    def aggregate(self, \n                  initializer: Callable[[], Any],\n                  aggregator: Callable[[Any, Dict[str, Any]], Any],\n                  window_size_ms: int = 60000,\n                  store_name: str = \"aggregate-store\"):\n        \"\"\"Perform windowed aggregation\"\"\"\n        \n        # Create state store for aggregation\n        self.stream.processor.create_state_store(store_name)\n        \n        agg_node = AggregateNode(\n            self.stream, \n            initializer, \n            aggregator,\n            window_size_ms,\n            store_name\n        )\n        \n        self.stream.processor.topology.append(agg_node)\n        return KTable(self.stream.processor, f\"{self.stream.name}-aggregated\", \"aggregated-table\")\n\nclass ProcessingNode:\n    def __init__(self, stream: KStream, name: str):\n        self.stream = stream\n        self.name = name\n        self.processor = stream.processor\n        \n    def run(self):\n        \"\"\"Run the processing node\"\"\"\n        consumer = KafkaConsumer(\n            *self.stream.topics,\n            bootstrap_servers=self.processor.bootstrap_servers,\n            group_id=f\"{self.processor.application_id}-{self.name}\",\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            enable_auto_commit=False\n        )\n        \n        producer = KafkaProducer(\n            bootstrap_servers=self.processor.bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n        )\n        \n        try:\n            while self.processor.running:\n                message_batch = consumer.poll(timeout_ms=1000)\n                \n                for partition, messages in message_batch.items():\n                    for message in messages:\n                        self.process_record(message, producer)\n                \n                consumer.commit()\n            \n        finally:\n            consumer.close()\n            producer.close()\n    \n    def process_record(self, message, producer):\n        \"\"\"Process individual record - to be implemented by subclasses\"\"\"\n        pass\n\nclass FilterNode(ProcessingNode):\n    def __init__(self, stream: KStream, predicate: Callable, name: str):\n        super().__init__(stream, name)\n        self.predicate = predicate\n        self.output_topic = f\"{stream.name}-{name}\"\n    \n    def process_record(self, message, producer):\n        event_data = message.value\n        \n        if self.predicate(event_data):\n            producer.send(self.output_topic, key=message.key, value=event_data)\n\nclass MapNode(ProcessingNode):\n    def __init__(self, stream: KStream, mapper: Callable, name: str):\n        super().__init__(stream, name)\n        self.mapper = mapper\n        self.output_topic = f\"{stream.name}-{name}\"\n    \n    def process_record(self, message, producer):\n        event_data = message.value\n        transformed_data = self.mapper(event_data)\n        \n        producer.send(self.output_topic, key=message.key, value=transformed_data)\n\nclass AggregateNode(ProcessingNode):\n    def __init__(self, stream: KStream, initializer: Callable, \n                 aggregator: Callable, window_size_ms: int, store_name: str):\n        super().__init__(stream, f\"aggregate-{store_name}\")\n        self.initializer = initializer\n        self.aggregator = aggregator\n        self.window_size_ms = window_size_ms\n        self.store_name = store_name\n        self.windows = defaultdict(lambda: defaultdict(lambda: self.initializer()))\n        self.output_topic = f\"{stream.name}-aggregated\"\n    \n    def process_record(self, message, producer):\n        event_data = message.value\n        key = message.key or \"null\"\n        \n        # Calculate window\n        window_start = (message.timestamp // self.window_size_ms) * self.window_size_ms\n        \n        # Update aggregation\n        current_value = self.windows[window_start][key]\n        new_value = self.aggregator(current_value, event_data)\n        self.windows[window_start][key] = new_value\n        \n        # Emit aggregated result\n        result = {\n            'windowStart': window_start,\n            'windowEnd': window_start + self.window_size_ms,\n            'key': key,\n            'value': new_value,\n            'eventTime': datetime.now().isoformat()\n        }\n        \n        producer.send(self.output_topic, key=key, value=result)\n\nclass SinkNode(ProcessingNode):\n    def __init__(self, stream: KStream, output_topic: str):\n        super().__init__(stream, \"sink\")\n        self.output_topic = output_topic\n    \n    def process_record(self, message, producer):\n        producer.send(self.output_topic, key=message.key, value=message.value)\n\n# Usage example\ndef create_order_processing_stream():\n    \"\"\"Create a Kafka Streams topology for order processing\"\"\"\n    \n    # Create streams processor\n    streams = KafkaStreamsProcessor(\n        application_id=\"order-processing-app\",\n        bootstrap_servers=['localhost:9092']\n    )\n    \n    # Create stream from orders topic\n    orders_stream = streams.stream(['orders'])\n    \n    # Filter for confirmed orders only\n    confirmed_orders = orders_stream.filter(\n        lambda event: event['data']['status'] == 'confirmed',\n        name=\"filter-confirmed\"\n    )\n    \n    # Enrich with customer information\n    enriched_orders = confirmed_orders.map(\n        lambda event: enrich_order_with_customer(event),\n        name=\"enrich-customer\"\n    )\n    \n    # Calculate real-time aggregations\n    order_aggregations = enriched_orders.group_by_key().aggregate(\n        initializer=lambda: {'count': 0, 'totalAmount': 0.0, 'avgAmount': 0.0},\n        aggregator=lambda acc, event: {\n            'count': acc['count'] + 1,\n            'totalAmount': acc['totalAmount'] + event['data']['totalAmount'],\n            'avgAmount': (acc['totalAmount'] + event['data']['totalAmount']) / (acc['count'] + 1)\n        },\n        window_size_ms=60000,  # 1-minute windows\n        store_name=\"order-aggregations\"\n    )\n    \n    # Send enriched orders to processed topic\n    enriched_orders.to('processed-orders')\n    \n    # Send aggregations to analytics topic\n    order_aggregations.to('order-analytics')\n    \n    return streams\n\ndef enrich_order_with_customer(event):\n    \"\"\"Enrich order event with customer information\"\"\"\n    # In real implementation, this would lookup customer data\n    event['data']['customerInfo'] = {\n        'segment': 'premium',\n        'region': 'us-east'\n    }\n    return event\n\n# Run the stream processing\nif __name__ == \"__main__\":\n    streams_app = create_order_processing_stream()\n    streams_app.start()\n```\n\n---\n\n## ðŸ° RabbitMQ Complete Guide {#rabbitmq}\n\n### RabbitMQ Architecture & Exchange Types\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      RabbitMQ Broker                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚  Exchange   â”‚    â”‚  Exchange   â”‚    â”‚  Exchange   â”‚        â”‚\nâ”‚  â”‚   (Topic)   â”‚    â”‚  (Direct)   â”‚    â”‚  (Fanout)   â”‚        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚         â”‚                  â”‚                  â”‚               â”‚\nâ”‚         â–¼                  â–¼                  â–¼               â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚                    Queues                           â”‚      â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚      â”‚\nâ”‚  â”‚  â”‚Queue A  â”‚  â”‚Queue B  â”‚  â”‚Queue C  â”‚             â”‚      â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚         â”‚            â”‚            â”‚                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n          â–¼            â–¼            â–¼\n    Consumer 1   Consumer 2   Consumer 3\n```\n\n**Exchange Types**:\n\n1. **Direct Exchange**: Routes messages based on exact routing key match\n2. **Topic Exchange**: Routes messages based on routing key patterns (wildcards)\n3. **Fanout Exchange**: Broadcasts messages to all bound queues\n4. **Headers Exchange**: Routes based on message headers\n\n### Complete RabbitMQ Implementation\n\n```python\nimport pika\nimport json\nimport logging\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Callable, Optional\nimport threading\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport uuid\n\nclass ExchangeType(Enum):\n    DIRECT = \"direct\"\n    TOPIC = \"topic\"\n    FANOUT = \"fanout\"\n    HEADERS = \"headers\"\n\n@dataclass\nclass QueueConfig:\n    name: str\n    durable: bool = True\n    exclusive: bool = False\n    auto_delete: bool = False\n    arguments: Dict[str, Any] = None\n\n@dataclass\nclass ExchangeConfig:\n    name: str\n    exchange_type: ExchangeType\n    durable: bool = True\n    auto_delete: bool = False\n    arguments: Dict[str, Any] = None\n\nclass RabbitMQManager:\n    def __init__(self, \n                 host: str = 'localhost',\n                 port: int = 5672,\n                 username: str = 'guest',\n                 password: str = 'guest',\n                 virtual_host: str = '/'):\n        \n        self.connection_params = pika.ConnectionParameters(\n            host=host,\n            port=port,\n            virtual_host=virtual_host,\n            credentials=pika.PlainCredentials(username, password),\n            heartbeat=600,\n            blocked_connection_timeout=300,\n        )\n        \n        self.logger = logging.getLogger(__name__)\n        self.connection = None\n        self.channel = None\n    \n    def connect(self):\n        \"\"\"Establish connection to RabbitMQ\"\"\"\n        try:\n            self.connection = pika.BlockingConnection(self.connection_params)\n            self.channel = self.connection.channel()\n            self.logger.info(\"Connected to RabbitMQ\")\n        except Exception as e:\n            self.logger.error(f\"Failed to connect to RabbitMQ: {e}\")\n            raise\n    \n    def disconnect(self):\n        \"\"\"Close connection to RabbitMQ\"\"\"\n        if self.connection and not self.connection.is_closed:\n            self.connection.close()\n            self.logger.info(\"Disconnected from RabbitMQ\")\n    \n    def declare_exchange(self, config: ExchangeConfig):\n        \"\"\"Declare an exchange\"\"\"\n        if not self.channel:\n            self.connect()\n        \n        self.channel.exchange_declare(\n            exchange=config.name,\n            exchange_type=config.exchange_type.value,\n            durable=config.durable,\n            auto_delete=config.auto_delete,\n            arguments=config.arguments\n        )\n        \n        self.logger.info(f\"Declared exchange: {config.name} ({config.exchange_type.value})\")\n    \n    def declare_queue(self, config: QueueConfig):\n        \"\"\"Declare a queue\"\"\"\n        if not self.channel:\n            self.connect()\n        \n        result = self.channel.queue_declare(\n            queue=config.name,\n            durable=config.durable,\n            exclusive=config.exclusive,\n            auto_delete=config.auto_delete,\n            arguments=config.arguments\n        )\n        \n        self.logger.info(f\"Declared queue: {config.name}\")\n        return result\n    \n    def bind_queue(self, queue_name: str, exchange_name: str, routing_key: str = \"\"):\n        \"\"\"Bind queue to exchange\"\"\"\n        if not self.channel:\n            self.connect()\n        \n        self.channel.queue_bind(\n            exchange=exchange_name,\n            queue=queue_name,\n            routing_key=routing_key\n        )\n        \n        self.logger.info(f\"Bound queue {queue_name} to exchange {exchange_name} with key '{routing_key}'\")\n\nclass RabbitMQPublisher:\n    def __init__(self, manager: RabbitMQManager):\n        self.manager = manager\n        self.logger = logging.getLogger(__name__)\n        \n        # Message properties\n        self.properties = pika.BasicProperties(\n            delivery_mode=2,  # Make message persistent\n            timestamp=int(time.time()),\n            message_id=str(uuid.uuid4())\n        )\n    \n    def publish_event(self, \n                     exchange: str,\n                     routing_key: str,\n                     event_data: Dict[str, Any],\n                     headers: Dict[str, str] = None) -> bool:\n        \"\"\"Publish event to exchange\"\"\"\n        \n        try:\n            if not self.manager.channel:\n                self.manager.connect()\n            \n            # Enrich event with metadata\n            enriched_event = {\n                **event_data,\n                'publishedAt': datetime.now().isoformat(),\n                'publisher': 'order-service'\n            }\n            \n            # Set message properties\n            properties = pika.BasicProperties(\n                delivery_mode=2,  # Persistent\n                timestamp=int(time.time()),\n                message_id=str(uuid.uuid4()),\n                headers=headers or {},\n                content_type='application/json'\n            )\n            \n            # Publish message\n            self.manager.channel.basic_publish(\n                exchange=exchange,\n                routing_key=routing_key,\n                body=json.dumps(enriched_event),\n                properties=properties,\n                mandatory=True  # Return message if no queue can handle it\n            )\n            \n            self.logger.info(f\"Published event to {exchange} with routing key '{routing_key}'\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to publish event: {e}\")\n            return False\n    \n    def publish_with_confirmation(self, exchange: str, routing_key: str, event_data: Dict[str, Any]) -> bool:\n        \"\"\"Publish with publisher confirmation for guaranteed delivery\"\"\"\n        \n        try:\n            if not self.manager.channel:\n                self.manager.connect()\n            \n            # Enable publisher confirmations\n            self.manager.channel.confirm_delivery()\n            \n            result = self.publish_event(exchange, routing_key, event_data)\n            \n            if result:\n                self.logger.info(\"Message delivery confirmed by broker\")\n                return True\n            else:\n                self.logger.error(\"Message delivery not confirmed\")\n                return False\n                \n        except pika.exceptions.UnroutableError:\n            self.logger.error(\"Message was returned as unroutable\")\n            return False\n        except Exception as e:\n            self.logger.error(f\"Publication failed: {e}\")\n            return False\n\nclass RabbitMQConsumer:\n    def __init__(self, manager: RabbitMQManager, queue_name: str):\n        self.manager = manager\n        self.queue_name = queue_name\n        self.logger = logging.getLogger(__name__)\n        self.handlers = {}\n        self.running = False\n        \n        # Consumer configuration\n        self.prefetch_count = 10  # QoS\n        \n    def register_handler(self, event_type: str, handler: Callable[[Dict[str, Any]], bool]):\n        \"\"\"Register event handler\"\"\"\n        self.handlers[event_type] = handler\n        self.logger.info(f\"Registered handler for {event_type}\")\n    \n    def start_consuming(self):\n        \"\"\"Start consuming messages\"\"\"\n        if not self.manager.channel:\n            self.manager.connect()\n        \n        # Set QoS to limit unacknowledged messages\n        self.manager.channel.basic_qos(prefetch_count=self.prefetch_count)\n        \n        # Set up consumer\n        self.manager.channel.basic_consume(\n            queue=self.queue_name,\n            on_message_callback=self._on_message,\n            auto_ack=False  # Manual acknowledgment\n        )\n        \n        self.running = True\n        self.logger.info(f\"Starting to consume from {self.queue_name}\")\n        \n        try:\n            self.manager.channel.start_consuming()\n        except KeyboardInterrupt:\n            self.logger.info(\"Consumer interrupted by user\")\n            self.stop_consuming()\n        except Exception as e:\n            self.logger.error(f\"Consumer error: {e}\")\n        finally:\n            self.manager.disconnect()\n    \n    def stop_consuming(self):\n        \"\"\"Stop consuming messages\"\"\"\n        self.running = False\n        if self.manager.channel:\n            self.manager.channel.stop_consuming()\n        self.logger.info(\"Stopped consuming\")\n    \n    def _on_message(self, channel, method, properties, body):\n        \"\"\"Handle incoming message\"\"\"\n        try:\n            # Parse message\n            event_data = json.loads(body.decode('utf-8'))\n            event_type = event_data.get('eventType', 'unknown')\n            event_id = event_data.get('eventId', 'unknown')\n            \n            self.logger.info(f\"Processing event {event_id} of type {event_type}\")\n            \n            # Find and call handler\n            if event_type in self.handlers:\n                start_time = time.time()\n                \n                try:\n                    success = self.handlers[event_type](event_data)\n                    processing_time = (time.time() - start_time) * 1000\n                    \n                    if success:\n                        # Acknowledge message\n                        channel.basic_ack(delivery_tag=method.delivery_tag)\n                        self.logger.info(f\"Successfully processed event {event_id} in {processing_time:.2f}ms\")\n                    else:\n                        # Reject and requeue\n                        channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n                        self.logger.error(f\"Handler returned False for event {event_id}\")\n                \n                except Exception as handler_error:\n                    # Check retry count\n                    retry_count = self._get_retry_count(properties)\n                    max_retries = 3\n                    \n                    if retry_count < max_retries:\n                        # Reject and requeue for retry\n                        channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n                        self.logger.warning(f\"Handler failed for event {event_id} (retry {retry_count + 1}/{max_retries}): {handler_error}\")\n                    else:\n                        # Send to dead letter queue\n                        self._send_to_dlq(event_data, str(handler_error))\n                        channel.basic_ack(delivery_tag=method.delivery_tag)\n                        self.logger.error(f\"Event {event_id} sent to DLQ after {max_retries} retries\")\n            else:\n                self.logger.warning(f\"No handler for event type {event_type}\")\n                channel.basic_ack(delivery_tag=method.delivery_tag)  # Acknowledge unknown events\n                \n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Invalid JSON in message: {e}\")\n            channel.basic_ack(delivery_tag=method.delivery_tag)  # Acknowledge malformed messages\n            \n        except Exception as e:\n            self.logger.error(f\"Unexpected error processing message: {e}\")\n            channel.basic_nack(delivery_tag=method.delivery_tag, requeue=False)\n    \n    def _get_retry_count(self, properties) -> int:\n        \"\"\"Get retry count from message headers\"\"\"\n        if properties.headers and 'x-retry-count' in properties.headers:\n            return int(properties.headers['x-retry-count'])\n        return 0\n    \n    def _send_to_dlq(self, event_data: Dict[str, Any], error_message: str):\n        \"\"\"Send failed message to dead letter queue\"\"\"\n        dlq_event = {\n            **event_data,\n            '_dlq': {\n                'reason': error_message,\n                'failedAt': datetime.now().isoformat(),\n                'originalQueue': self.queue_name\n            }\n        }\n        \n        # Publish to DLQ (you'd need a DLQ publisher)\n        self.logger.info(f\"Sending event {event_data.get('eventId')} to DLQ\")\n\n# E-commerce Event System Example\nclass EcommerceEventSystem:\n    def __init__(self):\n        self.rabbitmq = RabbitMQManager()\n        self.rabbitmq.connect()\n        \n        self._setup_topology()\n        \n    def _setup_topology(self):\n        \"\"\"Set up RabbitMQ exchanges, queues, and bindings\"\"\"\n        \n        # Exchanges\n        self.rabbitmq.declare_exchange(ExchangeConfig(\n            name=\"orders\",\n            exchange_type=ExchangeType.TOPIC\n        ))\n        \n        self.rabbitmq.declare_exchange(ExchangeConfig(\n            name=\"notifications\", \n            exchange_type=ExchangeType.FANOUT\n        ))\n        \n        self.rabbitmq.declare_exchange(ExchangeConfig(\n            name=\"payments\",\n            exchange_type=ExchangeType.DIRECT\n        ))\n        \n        # Queues\n        queues = [\n            QueueConfig(name=\"order-created\", durable=True),\n            QueueConfig(name=\"order-confirmed\", durable=True),\n            QueueConfig(name=\"payment-processing\", durable=True),\n            QueueConfig(name=\"inventory-updates\", durable=True),\n            QueueConfig(name=\"email-notifications\", durable=True),\n            QueueConfig(name=\"sms-notifications\", durable=True),\n            QueueConfig(\n                name=\"order-created-dlq\", \n                durable=True,\n                arguments={'x-message-ttl': 86400000}  # 24 hours TTL\n            )\n        ]\n        \n        for queue_config in queues:\n            self.rabbitmq.declare_queue(queue_config)\n        \n        # Bindings\n        bindings = [\n            (\"order-created\", \"orders\", \"order.created\"),\n            (\"order-confirmed\", \"orders\", \"order.confirmed\"),\n            (\"payment-processing\", \"orders\", \"order.confirmed\"),\n            (\"inventory-updates\", \"orders\", \"order.*\"),\n            (\"email-notifications\", \"notifications\", \"\"),\n            (\"sms-notifications\", \"notifications\", \"\")\n        ]\n        \n        for queue, exchange, routing_key in bindings:\n            self.rabbitmq.bind_queue(queue, exchange, routing_key)\n    \n    def create_publisher(self, service_name: str) -> RabbitMQPublisher:\n        \"\"\"Create a publisher for a service\"\"\"\n        return RabbitMQPublisher(self.rabbitmq)\n    \n    def create_consumer(self, queue_name: str) -> RabbitMQConsumer:\n        \"\"\"Create a consumer for a queue\"\"\"\n        return RabbitMQConsumer(self.rabbitmq, queue_name)\n\n# Event handlers\ndef handle_order_created(event_data: Dict[str, Any]) -> bool:\n    \"\"\"Handle order created event\"\"\"\n    try:\n        order_id = event_data['data']['orderId']\n        customer_id = event_data['data']['customerId']\n        \n        # Validate order data\n        if not order_id or not customer_id:\n            raise ValueError(\"Missing required order fields\")\n        \n        # Process order creation\n        print(f\"Processing order creation: {order_id} for customer {customer_id}\")\n        \n        # Simulate processing time\n        time.sleep(0.1)\n        \n        return True  # Success\n        \n    except Exception as e:\n        print(f\"Error handling order created: {e}\")\n        return False  # Will trigger retry\n\ndef handle_payment_processing(event_data: Dict[str, Any]) -> bool:\n    \"\"\"Handle payment processing\"\"\"\n    try:\n        order_id = event_data['data']['orderId']\n        amount = event_data['data']['totalAmount']\n        \n        print(f\"Processing payment for order {order_id}: ${amount}\")\n        \n        # Simulate payment processing\n        if amount > 5000:  # Simulate failure for large amounts\n            raise Exception(\"Payment gateway timeout\")\n        \n        time.sleep(0.2)  # Simulate processing\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Payment processing failed: {e}\")\n        return False\n\ndef handle_email_notification(event_data: Dict[str, Any]) -> bool:\n    \"\"\"Handle email notification\"\"\"\n    try:\n        event_type = event_data.get('eventType')\n        \n        if event_type == 'OrderCreated':\n            print(f\"Sending order confirmation email for {event_data['data']['orderId']}\")\n        elif event_type == 'PaymentProcessed':\n            print(f\"Sending payment confirmation email for {event_data['data']['orderId']}\")\n        \n        # Simulate email sending\n        time.sleep(0.05)\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Email notification failed: {e}\")\n        return False\n\n# Usage example\nif __name__ == \"__main__\":\n    # Set up event system\n    event_system = EcommerceEventSystem()\n    \n    # Create publisher\n    publisher = event_system.create_publisher(\"order-service\")\n    \n    # Create consumers\n    order_consumer = event_system.create_consumer(\"order-created\")\n    payment_consumer = event_system.create_consumer(\"payment-processing\")\n    email_consumer = event_system.create_consumer(\"email-notifications\")\n    \n    # Register handlers\n    order_consumer.register_handler(\"OrderCreated\", handle_order_created)\n    payment_consumer.register_handler(\"OrderConfirmed\", handle_payment_processing)\n    email_consumer.register_handler(\"OrderCreated\", handle_email_notification)\n    email_consumer.register_handler(\"PaymentProcessed\", handle_email_notification)\n    \n    # Start consumers in separate threads\n    consumer_threads = []\n    \n    for consumer in [order_consumer, payment_consumer, email_consumer]:\n        thread = threading.Thread(target=consumer.start_consuming)\n        thread.start()\n        consumer_threads.append(thread)\n    \n    # Simulate order events\n    time.sleep(1)  # Let consumers start\n    \n    for i in range(5):\n        # Order created event\n        order_created_event = {\n            'eventType': 'OrderCreated',\n            'eventId': f'evt-{i:03d}',\n            'data': {\n                'orderId': f'ORD-{i:03d}',\n                'customerId': f'CUST-{i % 3:03d}',\n                'totalAmount': 100.0 + i * 50,\n                'items': [\n                    {'productId': f'PROD-{i}', 'quantity': 1, 'price': 100.0 + i * 50}\n                ]\n            }\n        }\n        \n        publisher.publish_event(\n            exchange=\"orders\",\n            routing_key=\"order.created\",\n            event_data=order_created_event\n        )\n        \n        # Order confirmed event (triggers payment)\n        order_confirmed_event = {\n            'eventType': 'OrderConfirmed',\n            'eventId': f'evt-confirm-{i:03d}',\n            'data': {\n                'orderId': f'ORD-{i:03d}',\n                'customerId': f'CUST-{i % 3:03d}',\n                'totalAmount': 100.0 + i * 50\n            }\n        }\n        \n        publisher.publish_event(\n            exchange=\"orders\",\n            routing_key=\"order.confirmed\", \n            event_data=order_confirmed_event\n        )\n        \n        time.sleep(0.5)\n    \n    # Let consumers process messages\n    time.sleep(5)\n    \n    # Stop consumers\n    for consumer in [order_consumer, payment_consumer, email_consumer]:\n        consumer.stop_consuming()\n    \n    # Wait for threads to finish\n    for thread in consumer_threads:\n        thread.join()\n    \n    event_system.rabbitmq.disconnect()\n```\n\n### Advanced RabbitMQ Patterns\n\n```python\n# Dead Letter Exchange (DLX) Setup\nclass DeadLetterManager:\n    def __init__(self, manager: RabbitMQManager):\n        self.manager = manager\n        self.logger = logging.getLogger(__name__)\n    \n    def setup_dlx_topology(self, main_queue: str, dlx_name: str):\n        \"\"\"Set up dead letter exchange topology\"\"\"\n        \n        # Create DLX\n        self.manager.declare_exchange(ExchangeConfig(\n            name=dlx_name,\n            exchange_type=ExchangeType.DIRECT\n        ))\n        \n        # Create DLQ\n        dlq_name = f\"{main_queue}-dlq\"\n        self.manager.declare_queue(QueueConfig(\n            name=dlq_name,\n            durable=True,\n            arguments={\n                'x-message-ttl': 86400000,  # Messages expire after 24 hours\n            }\n        ))\n        \n        # Bind DLQ to DLX\n        self.manager.bind_queue(dlq_name, dlx_name, main_queue)\n        \n        # Update main queue to use DLX\n        self.manager.channel.queue_delete(queue=main_queue)  # Delete existing\n        self.manager.declare_queue(QueueConfig(\n            name=main_queue,\n            durable=True,\n            arguments={\n                'x-dead-letter-exchange': dlx_name,\n                'x-dead-letter-routing-key': main_queue,\n                'x-max-retries': 3\n            }\n        ))\n        \n        self.logger.info(f\"Set up DLX topology for {main_queue}\")\n\n# Priority Queue Pattern\nclass PriorityQueueManager:\n    def __init__(self, manager: RabbitMQManager):\n        self.manager = manager\n    \n    def create_priority_queue(self, queue_name: str, max_priority: int = 10):\n        \"\"\"Create a priority queue\"\"\"\n        self.manager.declare_queue(QueueConfig(\n            name=queue_name,\n            durable=True,\n            arguments={'x-max-priority': max_priority}\n        ))\n    \n    def publish_with_priority(self, exchange: str, routing_key: str, \n                            event_data: Dict[str, Any], priority: int):\n        \"\"\"Publish message with priority\"\"\"\n        if not self.manager.channel:\n            self.manager.connect()\n        \n        properties = pika.BasicProperties(\n            delivery_mode=2,\n            priority=priority\n        )\n        \n        self.manager.channel.basic_publish(\n            exchange=exchange,\n            routing_key=routing_key,\n            body=json.dumps(event_data),\n            properties=properties\n        )\n\n# Delayed Message Pattern\nclass DelayedMessageManager:\n    def __init__(self, manager: RabbitMQManager):\n        self.manager = manager\n    \n    def setup_delayed_exchange(self, exchange_name: str):\n        \"\"\"Set up exchange for delayed messages\"\"\"\n        # Requires rabbitmq-delayed-message-exchange plugin\n        self.manager.declare_exchange(ExchangeConfig(\n            name=exchange_name,\n            exchange_type=ExchangeType.DIRECT,\n            arguments={'x-delayed-type': 'direct'}\n        ))\n    \n    def publish_delayed_message(self, exchange: str, routing_key: str,\n                               event_data: Dict[str, Any], delay_seconds: int):\n        \"\"\"Publish message with delay\"\"\"\n        if not self.manager.channel:\n            self.manager.connect()\n        \n        properties = pika.BasicProperties(\n            delivery_mode=2,\n            headers={'x-delay': delay_seconds * 1000}  # Delay in milliseconds\n        )\n        \n        self.manager.channel.basic_publish(\n            exchange=exchange,\n            routing_key=routing_key,\n            body=json.dumps(event_data),\n            properties=properties\n        )\n\n# Usage examples for advanced patterns\nif __name__ == \"__main__\":\n    rabbitmq = RabbitMQManager()\n    \n    # Dead letter setup\n    dlx_manager = DeadLetterManager(rabbitmq)\n    dlx_manager.setup_dlx_topology(\"orders\", \"orders-dlx\")\n    \n    # Priority queue setup\n    priority_manager = PriorityQueueManager(rabbitmq)\n    priority_manager.create_priority_queue(\"priority-orders\", max_priority=10)\n    \n    # Delayed message setup\n    delayed_manager = DelayedMessageManager(rabbitmq)\n    delayed_manager.setup_delayed_exchange(\"delayed-orders\")\n    \n    # Publish high priority order\n    high_priority_order = {\n        'eventType': 'UrgentOrder',\n        'data': {'orderId': 'URGENT-001', 'priority': 'high'}\n    }\n    \n    priority_manager.publish_with_priority(\n        \"orders\", \"order.urgent\", high_priority_order, priority=10\n    )\n    \n    # Publish delayed reminder\n    reminder_event = {\n        'eventType': 'CartAbandonmentReminder',\n        'data': {'customerId': 'CUST-123', 'cartId': 'CART-456'}\n    }\n    \n    delayed_manager.publish_delayed_message(\n        \"delayed-orders\", \"reminder.cart\", reminder_event, delay_seconds=3600  # 1 hour delay\n    )\n    \n    rabbitmq.disconnect()\n```\n\n---\n\n## ðŸ“š Event Sourcing Deep Dive {#event-sourcing}\n\n### Complete Event Store Implementation\n\n```python\nfrom abc import ABC, abstractmethod\nimport json\nimport sqlite3\nimport uuid\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional, Type\nimport logging\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nimport asyncio\nimport threading\n\n@dataclass\nclass Event:\n    event_id: str\n    stream_id: str\n    event_type: str\n    event_data: Dict[str, Any]\n    event_version: int\n    timestamp: str\n    metadata: Dict[str, Any] = None\n    \n    def to_dict(self):\n        return asdict(self)\n\nclass EventStore(ABC):\n    @abstractmethod\n    async def append_events(self, stream_id: str, events: List[Event], expected_version: int = -1) -> int:\n        pass\n    \n    @abstractmethod\n    async def read_events(self, stream_id: str, from_version: int = 0, to_version: int = None) -> List[Event]:\n        pass\n    \n    @abstractmethod\n    async def read_all_events(self, from_position: int = 0, max_count: int = 1000) -> List[Event]:\n        pass\n\nclass SQLiteEventStore(EventStore):\n    def __init__(self, db_path: str = \"eventstore.db\"):\n        self.db_path = db_path\n        self.logger = logging.getLogger(__name__)\n        self._init_database()\n        self._lock = threading.Lock()\n    \n    def _init_database(self):\n        \"\"\"Initialize database schema\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS events (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    event_id TEXT UNIQUE NOT NULL,\n                    stream_id TEXT NOT NULL,\n                    event_type TEXT NOT NULL,\n                    event_data TEXT NOT NULL,\n                    event_version INTEGER NOT NULL,\n                    timestamp TEXT NOT NULL,\n                    metadata TEXT,\n                    global_position INTEGER\n                );\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_stream_version \n                ON events(stream_id, event_version);\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_event_type \n                ON events(event_type);\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_timestamp \n                ON events(timestamp);\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE UNIQUE INDEX IF NOT EXISTS idx_stream_version_unique \n                ON events(stream_id, event_version);\n            \"\"\")\n    \n    async def append_events(self, stream_id: str, events: List[Event], expected_version: int = -1) -> int:\n        \"\"\"Append events to stream with optimistic concurrency control\"\"\"\n        \n        with self._lock:\n            with sqlite3.connect(self.db_path) as conn:\n                # Check current version\n                cursor = conn.execute(\n                    \"SELECT MAX(event_version) FROM events WHERE stream_id = ?\",\n                    (stream_id,)\n                )\n                result = cursor.fetchone()\n                current_version = result[0] if result[0] is not None else -1\n                \n                # Optimistic concurrency check\n                if expected_version != -1 and current_version != expected_version:\n                    raise ValueError(f\"Concurrency conflict: expected version {expected_version}, \"\n                                   f\"but current version is {current_version}\")\n                \n                # Insert events\n                for i, event in enumerate(events):\n                    new_version = current_version + i + 1\n                    event.event_version = new_version\n                    \n                    conn.execute(\"\"\"\n                        INSERT INTO events \n                        (event_id, stream_id, event_type, event_data, event_version, timestamp, metadata)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", (\n                        event.event_id,\n                        stream_id,\n                        event.event_type,\n                        json.dumps(event.event_data),\n                        new_version,\n                        event.timestamp,\n                        json.dumps(event.metadata) if event.metadata else None\n                    ))\n                \n                self.logger.info(f\"Appended {len(events)} events to stream {stream_id}\")\n                return current_version + len(events)\n    \n    async def read_events(self, stream_id: str, from_version: int = 0, to_version: int = None) -> List[Event]:\n        \"\"\"Read events from stream\"\"\"\n        \n        query = \"SELECT * FROM events WHERE stream_id = ? AND event_version >= ?\"\n        params = [stream_id, from_version]\n        \n        if to_version is not None:\n            query += \" AND event_version <= ?\"\n            params.append(to_version)\n        \n        query += \" ORDER BY event_version ASC\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.row_factory = sqlite3.Row\n            cursor = conn.execute(query, params)\n            rows = cursor.fetchall()\n        \n        events = []\n        for row in rows:\n            event = Event(\n                event_id=row['event_id'],\n                stream_id=row['stream_id'],\n                event_type=row['event_type'],\n                event_data=json.loads(row['event_data']),\n                event_version=row['event_version'],\n                timestamp=row['timestamp'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n            events.append(event)\n        \n        return events\n    \n    async def read_all_events(self, from_position: int = 0, max_count: int = 1000) -> List[Event]:\n        \"\"\"Read all events across all streams\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.row_factory = sqlite3.Row\n            cursor = conn.execute(\"\"\"\n                SELECT * FROM events \n                WHERE id > ?\n                ORDER BY id ASC\n                LIMIT ?\n            \"\"\", (from_position, max_count))\n            \n            rows = cursor.fetchall()\n        \n        events = []\n        for row in rows:\n            event = Event(\n                event_id=row['event_id'],\n                stream_id=row['stream_id'],\n                event_type=row['event_type'],\n                event_data=json.loads(row['event_data']),\n                event_version=row['event_version'],\n                timestamp=row['timestamp'],\n                metadata=json.loads(row['metadata']) if row['metadata'] else None\n            )\n            events.append(event)\n        \n        return events\n\n# Aggregate Base Class\nclass AggregateRoot(ABC):\n    def __init__(self, aggregate_id: str):\n        self.aggregate_id = aggregate_id\n        self.version = -1\n        self.uncommitted_events: List[Event] = []\n    \n    @abstractmethod\n    def apply_event(self, event: Event):\n        \"\"\"Apply event to change aggregate state\"\"\"\n        pass\n    \n    def raise_event(self, event_type: str, event_data: Dict[str, Any], metadata: Dict[str, Any] = None):\n        \"\"\"Raise a new domain event\"\"\"\n        event = Event(\n            event_id=str(uuid.uuid4()),\n            stream_id=f\"{self.__class__.__name__}-{self.aggregate_id}\",\n            event_type=event_type,\n            event_data=event_data,\n            event_version=self.version + len(self.uncommitted_events) + 1,\n            timestamp=datetime.now().isoformat(),\n            metadata=metadata\n        )\n        \n        self.uncommitted_events.append(event)\n        self.apply_event(event)\n    \n    def mark_events_as_committed(self):\n        \"\"\"Mark uncommitted events as committed\"\"\"\n        self.version += len(self.uncommitted_events)\n        self.uncommitted_events.clear()\n    \n    @classmethod\n    async def from_history(cls, aggregate_id: str, event_store: EventStore):\n        \"\"\"Rebuild aggregate from event history\"\"\"\n        stream_id = f\"{cls.__name__}-{aggregate_id}\"\n        events = await event_store.read_events(stream_id)\n        \n        aggregate = cls(aggregate_id)\n        \n        for event in events:\n            aggregate.apply_event(event)\n            aggregate.version = event.event_version\n        \n        return aggregate\n\n# Example: Order Aggregate with Complex Business Logic\nclass OrderStatus(Enum):\n    DRAFT = \"draft\"\n    CONFIRMED = \"confirmed\"\n    PAID = \"paid\"\n    SHIPPED = \"shipped\"\n    DELIVERED = \"delivered\"\n    CANCELLED = \"cancelled\"\n    REFUNDED = \"refunded\"\n\n@dataclass\nclass OrderItem:\n    product_id: str\n    quantity: int\n    unit_price: float\n    \n    @property\n    def total_price(self) -> float:\n        return self.quantity * self.unit_price\n\nclass OrderAggregate(AggregateRoot):\n    def __init__(self, order_id: str):\n        super().__init__(order_id)\n        self.customer_id: Optional[str] = None\n        self.items: List[OrderItem] = []\n        self.status = OrderStatus.DRAFT\n        self.shipping_address: Optional[Dict[str, str]] = None\n        self.created_at: Optional[datetime] = None\n        self.confirmed_at: Optional[datetime] = None\n        self.shipped_at: Optional[datetime] = None\n    \n    @property\n    def total_amount(self) -> float:\n        return sum(item.total_price for item in self.items)\n    \n    @property\n    def total_items(self) -> int:\n        return sum(item.quantity for item in self.items)\n    \n    # Command methods (business operations)\n    def create_order(self, customer_id: str, shipping_address: Dict[str, str]):\n        if self.status != OrderStatus.DRAFT or self.customer_id is not None:\n            raise ValueError(\"Order already created\")\n        \n        self.raise_event(\"OrderCreated\", {\n            \"orderId\": self.aggregate_id,\n            \"customerId\": customer_id,\n            \"shippingAddress\": shipping_address,\n            \"createdAt\": datetime.now().isoformat()\n        })\n    \n    def add_item(self, product_id: str, quantity: int, unit_price: float):\n        if self.status != OrderStatus.DRAFT:\n            raise ValueError(f\"Cannot add items to order in status {self.status.value}\")\n        \n        if quantity <= 0:\n            raise ValueError(\"Quantity must be positive\")\n        \n        if unit_price <= 0:\n            raise ValueError(\"Unit price must be positive\")\n        \n        self.raise_event(\"ItemAdded\", {\n            \"orderId\": self.aggregate_id,\n            \"productId\": product_id,\n            \"quantity\": quantity,\n            \"unitPrice\": unit_price\n        })\n    \n    def remove_item(self, product_id: str):\n        if self.status != OrderStatus.DRAFT:\n            raise ValueError(f\"Cannot remove items from order in status {self.status.value}\")\n        \n        # Check if item exists\n        item_exists = any(item.product_id == product_id for item in self.items)\n        if not item_exists:\n            raise ValueError(f\"Product {product_id} not in order\")\n        \n        self.raise_event(\"ItemRemoved\", {\n            \"orderId\": self.aggregate_id,\n            \"productId\": product_id\n        })\n    \n    def confirm_order(self):\n        if self.status != OrderStatus.DRAFT:\n            raise ValueError(f\"Cannot confirm order in status {self.status.value}\")\n        \n        if not self.items:\n            raise ValueError(\"Cannot confirm empty order\")\n        \n        if self.total_amount <= 0:\n            raise ValueError(\"Order total must be positive\")\n        \n        self.raise_event(\"OrderConfirmed\", {\n            \"orderId\": self.aggregate_id,\n            \"totalAmount\": self.total_amount,\n            \"totalItems\": self.total_items,\n            \"confirmedAt\": datetime.now().isoformat()\n        })\n    \n    def process_payment(self, payment_id: str, amount: float):\n        if self.status != OrderStatus.CONFIRMED:\n            raise ValueError(f\"Cannot process payment for order in status {self.status.value}\")\n        \n        if abs(amount - self.total_amount) > 0.01:  # Allow small floating point differences\n            raise ValueError(f\"Payment amount {amount} doesn't match order total {self.total_amount}\")\n        \n        self.raise_event(\"PaymentProcessed\", {\n            \"orderId\": self.aggregate_id,\n            \"paymentId\": payment_id,\n            \"amount\": amount,\n            \"processedAt\": datetime.now().isoformat()\n        })\n    \n    def ship_order(self, carrier: str, tracking_number: str):\n        if self.status != OrderStatus.PAID:\n            raise ValueError(f\"Cannot ship order in status {self.status.value}\")\n        \n        self.raise_event(\"OrderShipped\", {\n            \"orderId\": self.aggregate_id,\n            \"carrier\": carrier,\n            \"trackingNumber\": tracking_number,\n            \"shippedAt\": datetime.now().isoformat()\n        })\n    \n    def cancel_order(self, reason: str):\n        if self.status in [OrderStatus.SHIPPED, OrderStatus.DELIVERED, OrderStatus.CANCELLED]:\n            raise ValueError(f\"Cannot cancel order in status {self.status.value}\")\n        \n        self.raise_event(\"OrderCancelled\", {\n            \"orderId\": self.aggregate_id,\n            \"reason\": reason,\n            \"cancelledAt\": datetime.now().isoformat(),\n            \"refundAmount\": self.total_amount if self.status == OrderStatus.PAID else 0\n        })\n    \n    # Event handlers (state mutations)\n    def apply_event(self, event: Event):\n        event_type = event.event_type\n        data = event.event_data\n        \n        if event_type == \"OrderCreated\":\n            self.customer_id = data[\"customerId\"]\n            self.shipping_address = data[\"shippingAddress\"]\n            self.created_at = datetime.fromisoformat(data[\"createdAt\"])\n            self.status = OrderStatus.DRAFT\n            \n        elif event_type == \"ItemAdded\":\n            item = OrderItem(\n                product_id=data[\"productId\"],\n                quantity=data[\"quantity\"],\n                unit_price=data[\"unitPrice\"]\n            )\n            # Check if item already exists and update quantity\n            existing_item = next((i for i in self.items if i.product_id == data[\"productId\"]), None)\n            if existing_item:\n                existing_item.quantity += data[\"quantity\"]\n            else:\n                self.items.append(item)\n                \n        elif event_type == \"ItemRemoved\":\n            self.items = [item for item in self.items if item.product_id != data[\"productId\"]]\n            \n        elif event_type == \"OrderConfirmed\":\n            self.status = OrderStatus.CONFIRMED\n            self.confirmed_at = datetime.fromisoformat(data[\"confirmedAt\"])\n            \n        elif event_type == \"PaymentProcessed\":\n            self.status = OrderStatus.PAID\n            \n        elif event_type == \"OrderShipped\":\n            self.status = OrderStatus.SHIPPED\n            self.shipped_at = datetime.fromisoformat(data[\"shippedAt\"])\n            \n        elif event_type == \"OrderCancelled\":\n            self.status = OrderStatus.CANCELLED\n            \n        else:\n            self.logger.warning(f\"Unknown event type: {event_type}\")\n\nclass OrderRepository:\n    def __init__(self, event_store: EventStore):\n        self.event_store = event_store\n        self.logger = logging.getLogger(__name__)\n    \n    async def save(self, order: OrderAggregate, expected_version: int = -1) -> None:\n        \"\"\"Save order aggregate\"\"\"\n        if not order.uncommitted_events:\n            return\n        \n        try:\n            stream_id = f\"Order-{order.aggregate_id}\"\n            new_version = await self.event_store.append_events(\n                stream_id, \n                order.uncommitted_events,\n                expected_version\n            )\n            \n            order.mark_events_as_committed()\n            \n            self.logger.info(f\"Saved order {order.aggregate_id} at version {new_version}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save order {order.aggregate_id}: {e}\")\n            raise\n    \n    async def get_by_id(self, order_id: str) -> Optional[OrderAggregate]:\n        \"\"\"Get order by ID\"\"\"\n        try:\n            order = await OrderAggregate.from_history(order_id, self.event_store)\n            \n            if order.version == -1:  # No events found\n                return None\n            \n            return order\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load order {order_id}: {e}\")\n            raise\n\n# Event Projections (Read Models)\nclass ProjectionManager:\n    def __init__(self, event_store: EventStore):\n        self.event_store = event_store\n        self.projections = {}\n        self.logger = logging.getLogger(__name__)\n        self.last_processed_position = 0\n    \n    def register_projection(self, name: str, projection_class: Type['Projection']):\n        \"\"\"Register a projection\"\"\"\n        self.projections[name] = projection_class()\n        self.logger.info(f\"Registered projection: {name}\")\n    \n    async def rebuild_all_projections(self):\n        \"\"\"Rebuild all projections from scratch\"\"\"\n        self.logger.info(\"Rebuilding all projections...\")\n        \n        # Clear all projections\n        for projection in self.projections.values():\n            projection.reset()\n        \n        # Process all events\n        events = await self.event_store.read_all_events(from_position=0, max_count=10000)\n        \n        for event in events:\n            self.apply_event_to_projections(event)\n        \n        self.logger.info(f\"Rebuilt {len(self.projections)} projections from {len(events)} events\")\n    \n    async def update_projections(self):\n        \"\"\"Update projections with new events\"\"\"\n        events = await self.event_store.read_all_events(\n            from_position=self.last_processed_position,\n            max_count=1000\n        )\n        \n        for event in events:\n            self.apply_event_to_projections(event)\n            self.last_processed_position = max(self.last_processed_position, event.event_version)\n        \n        if events:\n            self.logger.info(f\"Updated projections with {len(events)} new events\")\n    \n    def apply_event_to_projections(self, event: Event):\n        \"\"\"Apply event to all relevant projections\"\"\"\n        for projection in self.projections.values():\n            try:\n                projection.handle_event(event)\n            except Exception as e:\n                self.logger.error(f\"Error applying event {event.event_id} to projection: {e}\")\n\nclass Projection(ABC):\n    @abstractmethod\n    def handle_event(self, event: Event):\n        pass\n    \n    @abstractmethod\n    def reset(self):\n        pass\n\nclass OrderSummaryProjection(Projection):\n    \"\"\"Projection for order summary statistics\"\"\"\n    \n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.orders = {}\n        self.customer_stats = {}\n        self.product_stats = {}\n    \n    def handle_event(self, event: Event):\n        event_type = event.event_type\n        data = event.event_data\n        \n        if event_type == \"OrderCreated\":\n            order_id = data[\"orderId\"]\n            customer_id = data[\"customerId\"]\n            \n            self.orders[order_id] = {\n                \"orderId\": order_id,\n                \"customerId\": customer_id,\n                \"status\": \"draft\",\n                \"totalAmount\": 0,\n                \"itemCount\": 0,\n                \"createdAt\": data[\"createdAt\"]\n            }\n            \n            # Update customer stats\n            if customer_id not in self.customer_stats:\n                self.customer_stats[customer_id] = {\n                    \"totalOrders\": 0,\n                    \"totalSpent\": 0,\n                    \"firstOrderDate\": data[\"createdAt\"]\n                }\n        \n        elif event_type == \"ItemAdded\":\n            order_id = data[\"orderId\"]\n            product_id = data[\"productId\"]\n            quantity = data[\"quantity\"]\n            unit_price = data[\"unitPrice\"]\n            \n            if order_id in self.orders:\n                self.orders[order_id][\"totalAmount\"] += quantity * unit_price\n                self.orders[order_id][\"itemCount\"] += quantity\n                \n                # Update product stats\n                if product_id not in self.product_stats:\n                    self.product_stats[product_id] = {\n                        \"totalQuantitySold\": 0,\n                        \"totalRevenue\": 0,\n                        \"orderCount\": 0\n                    }\n                \n                self.product_stats[product_id][\"totalQuantitySold\"] += quantity\n                self.product_stats[product_id][\"totalRevenue\"] += quantity * unit_price\n                self.product_stats[product_id][\"orderCount\"] += 1\n        \n        elif event_type == \"OrderConfirmed\":\n            order_id = data[\"orderId\"]\n            if order_id in self.orders:\n                self.orders[order_id][\"status\"] = \"confirmed\"\n                self.orders[order_id][\"confirmedAt\"] = data[\"confirmedAt\"]\n        \n        elif event_type == \"PaymentProcessed\":\n            order_id = data[\"orderId\"]\n            if order_id in self.orders:\n                self.orders[order_id][\"status\"] = \"paid\"\n                \n                # Update customer stats\n                customer_id = self.orders[order_id][\"customerId\"]\n                if customer_id in self.customer_stats:\n                    self.customer_stats[customer_id][\"totalOrders\"] += 1\n                    self.customer_stats[customer_id][\"totalSpent\"] += self.orders[order_id][\"totalAmount\"]\n    \n    def get_order_summary(self, order_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get order summary\"\"\"\n        return self.orders.get(order_id)\n    \n    def get_customer_stats(self, customer_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get customer statistics\"\"\"\n        return self.customer_stats.get(customer_id)\n    \n    def get_top_products(self, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get top-selling products\"\"\"\n        sorted_products = sorted(\n            self.product_stats.items(),\n            key=lambda x: x[1][\"totalRevenue\"],\n            reverse=True\n        )\n        \n        return [\n            {\"productId\": product_id, **stats} \n            for product_id, stats in sorted_products[:limit]\n        ]\n\n# Usage Example\nasync def event_sourcing_example():\n    \"\"\"Complete event sourcing example\"\"\"\n    \n    # Initialize event store\n    event_store = SQLiteEventStore(\"orders.db\")\n    repository = OrderRepository(event_store)\n    \n    # Create new order\n    order = OrderAggregate(\"ORD-001\")\n    order.create_order(\"CUST-123\", {\n        \"street\": \"123 Main St\",\n        \"city\": \"Anytown\",\n        \"state\": \"NY\",\n        \"zipCode\": \"12345\"\n    })\n    \n    # Add items\n    order.add_item(\"PROD-001\", 2, 29.99)\n    order.add_item(\"PROD-002\", 1, 49.99)\n    \n    # Confirm order\n    order.confirm_order()\n    \n    # Save order (persist events)\n    await repository.save(order)\n    \n    # Process payment\n    order.process_payment(\"PAY-001\", order.total_amount)\n    await repository.save(order)\n    \n    print(f\"Order {order.aggregate_id} total: ${order.total_amount}\")\n    print(f\"Order status: {order.status.value}\")\n    \n    # Rebuild from events\n    rebuilt_order = await repository.get_by_id(\"ORD-001\")\n    print(f\"Rebuilt order status: {rebuilt_order.status.value}\")\n    print(f\"Rebuilt order total: ${rebuilt_order.total_amount}\")\n    \n    # Set up projections\n    projection_manager = ProjectionManager(event_store)\n    projection_manager.register_projection(\"order_summary\", OrderSummaryProjection)\n    \n    await projection_manager.rebuild_all_projections()\n    \n    # Query projections\n    order_summary_projection = projection_manager.projections[\"order_summary\"]\n    summary = order_summary_projection.get_order_summary(\"ORD-001\")\n    print(f\"Order summary: {json.dumps(summary, indent=2)}\")\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(event_sourcing_example())\n```\n\n---\n\n## ðŸ”„ CQRS Pattern Implementation {#cqrs}\n\n### CQRS with Event Sourcing\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, Union\nimport json\nimport logging\nfrom datetime import datetime\nimport asyncio\nfrom dataclasses import dataclass\n\n# Command side (Write Model)\nclass Command(ABC):\n    pass\n\n@dataclass\nclass CreateOrderCommand(Command):\n    order_id: str\n    customer_id: str\n    shipping_address: Dict[str, str]\n\n@dataclass\nclass AddItemToOrderCommand(Command):\n    order_id: str\n    product_id: str\n    quantity: int\n    unit_price: float\n\n@dataclass\nclass ConfirmOrderCommand(Command):\n    order_id: str\n\nclass CommandHandler(ABC):\n    @abstractmethod\n    async def handle(self, command: Command) -> Any:\n        pass\n\nclass OrderCommandHandler(CommandHandler):\n    def __init__(self, order_repository: OrderRepository, event_publisher):\n        self.repository = order_repository\n        self.event_publisher = event_publisher\n        self.logger = logging.getLogger(__name__)\n    \n    async def handle(self, command: Command) -> Any:\n        if isinstance(command, CreateOrderCommand):\n            return await self._handle_create_order(command)\n        elif isinstance(command, AddItemToOrderCommand):\n            return await self._handle_add_item(command)\n        elif isinstance(command, ConfirmOrderCommand):\n            return await self._handle_confirm_order(command)\n        else:\n            raise ValueError(f\"Unknown command type: {type(command)}\")\n    \n    async def _handle_create_order(self, command: CreateOrderCommand) -> OrderAggregate:\n        # Check if order already exists\n        existing_order = await self.repository.get_by_id(command.order_id)\n        if existing_order:\n            raise ValueError(f\"Order {command.order_id} already exists\")\n        \n        # Create new order\n        order = OrderAggregate(command.order_id)\n        order.create_order(command.customer_id, command.shipping_address)\n        \n        # Save order\n        await self.repository.save(order)\n        \n        # Publish domain events\n        await self._publish_uncommitted_events(order)\n        \n        return order\n    \n    async def _handle_add_item(self, command: AddItemToOrderCommand) -> OrderAggregate:\n        # Load order\n        order = await self.repository.get_by_id(command.order_id)\n        if not order:\n            raise ValueError(f\"Order {command.order_id} not found\")\n        \n        # Add item\n        order.add_item(command.product_id, command.quantity, command.unit_price)\n        \n        # Save order\n        await self.repository.save(order, expected_version=order.version)\n        \n        # Publish events\n        await self._publish_uncommitted_events(order)\n        \n        return order\n    \n    async def _handle_confirm_order(self, command: ConfirmOrderCommand) -> OrderAggregate:\n        # Load order\n        order = await self.repository.get_by_id(command.order_id)\n        if not order:\n            raise ValueError(f\"Order {command.order_id} not found\")\n        \n        # Confirm order\n        order.confirm_order()\n        \n        # Save order\n        await self.repository.save(order, expected_version=order.version)\n        \n        # Publish events\n        await self._publish_uncommitted_events(order)\n        \n        return order\n    \n    async def _publish_uncommitted_events(self, order: OrderAggregate):\n        \"\"\"Publish domain events to message broker\"\"\"\n        for event in order.uncommitted_events:\n            await self.event_publisher.publish(\n                topic=\"domain-events\",\n                event_type=event.event_type,\n                event_data=event.event_data,\n                metadata=event.metadata\n            )\n\n# Query side (Read Model)\nclass Query(ABC):\n    pass\n\n@dataclass\nclass GetOrderQuery(Query):\n    order_id: str\n\n@dataclass\nclass GetCustomerOrdersQuery(Query):\n    customer_id: str\n    limit: int = 10\n    offset: int = 0\n\n@dataclass\nclass GetOrdersByStatusQuery(Query):\n    status: str\n    limit: int = 10\n\nclass QueryHandler(ABC):\n    @abstractmethod\n    async def handle(self, query: Query) -> Any:\n        pass\n\nclass OrderQueryHandler(QueryHandler):\n    def __init__(self, read_model_store):\n        self.read_model_store = read_model_store\n        self.logger = logging.getLogger(__name__)\n    \n    async def handle(self, query: Query) -> Any:\n        if isinstance(query, GetOrderQuery):\n            return await self._handle_get_order(query)\n        elif isinstance(query, GetCustomerOrdersQuery):\n            return await self._handle_get_customer_orders(query)\n        elif isinstance(query, GetOrdersByStatusQuery):\n            return await self._handle_get_orders_by_status(query)\n        else:\n            raise ValueError(f\"Unknown query type: {type(query)}\")\n    \n    async def _handle_get_order(self, query: GetOrderQuery) -> Optional[Dict[str, Any]]:\n        return await self.read_model_store.get_order(query.order_id)\n    \n    async def _handle_get_customer_orders(self, query: GetCustomerOrdersQuery) -> List[Dict[str, Any]]:\n        return await self.read_model_store.get_customer_orders(\n            query.customer_id, \n            query.limit, \n            query.offset\n        )\n    \n    async def _handle_get_orders_by_status(self, query: GetOrdersByStatusQuery) -> List[Dict[str, Any]]:\n        return await self.read_model_store.get_orders_by_status(query.status, query.limit)\n\n# Read Model Store (Optimized for Queries)\nclass ReadModelStore:\n    def __init__(self, db_connection_string: str):\n        self.connection_string = db_connection_string\n        self.logger = logging.getLogger(__name__)\n        self._init_read_model_tables()\n    \n    def _init_read_model_tables(self):\n        \"\"\"Initialize read model tables optimized for queries\"\"\"\n        # This would create denormalized tables optimized for specific queries\n        pass\n    \n    async def get_order(self, order_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get order with all related data in one query\"\"\"\n        # Simulated query that joins order, customer, and product data\n        return {\n            \"orderId\": order_id,\n            \"customer\": {\n                \"customerId\": \"CUST-123\",\n                \"name\": \"John Doe\",\n                \"email\": \"john@example.com\"\n            },\n            \"items\": [\n                {\n                    \"productId\": \"PROD-001\",\n                    \"productName\": \"Widget\",\n                    \"quantity\": 2,\n                    \"unitPrice\": 29.99,\n                    \"totalPrice\": 59.98\n                }\n            ],\n            \"totalAmount\": 59.98,\n            \"status\": \"confirmed\",\n            \"createdAt\": \"2024-01-15T10:00:00Z\"\n        }\n    \n    async def get_customer_orders(self, customer_id: str, limit: int, offset: int) -> List[Dict[str, Any]]:\n        \"\"\"Get orders for customer with pagination\"\"\"\n        # Optimized query with denormalized data\n        return [\n            {\n                \"orderId\": f\"ORD-{i:03d}\",\n                \"totalAmount\": 100.0 + i * 25,\n                \"status\": \"paid\",\n                \"itemCount\": i + 1,\n                \"createdAt\": f\"2024-01-{i+1:02d}T10:00:00Z\"\n            }\n            for i in range(limit)\n        ]\n    \n    async def get_orders_by_status(self, status: str, limit: int) -> List[Dict[str, Any]]:\n        \"\"\"Get orders by status\"\"\"\n        return [\n            {\n                \"orderId\": f\"ORD-{status.upper()}-{i:03d}\",\n                \"customerId\": f\"CUST-{i:03d}\",\n                \"totalAmount\": 150.0 + i * 30,\n                \"status\": status,\n                \"createdAt\": f\"2024-01-15T{10+i:02d}:00:00Z\"\n            }\n            for i in range(min(limit, 5))\n        ]\n\n# Read Model Event Handlers (Eventually Consistent)\nclass ReadModelEventHandler:\n    def __init__(self, read_model_store: ReadModelStore):\n        self.read_model_store = read_model_store\n        self.logger = logging.getLogger(__name__)\n    \n    async def handle_order_created(self, event: Event):\n        \"\"\"Update read model when order is created\"\"\"\n        data = event.event_data\n        \n        # Insert into denormalized order table\n        order_record = {\n            \"order_id\": data[\"orderId\"],\n            \"customer_id\": data[\"customerId\"],\n            \"status\": \"draft\",\n            \"total_amount\": 0,\n            \"item_count\": 0,\n            \"created_at\": data[\"createdAt\"],\n            \"updated_at\": event.timestamp\n        }\n        \n        # In real implementation, this would update the database\n        self.logger.info(f\"Updated read model for order creation: {data['orderId']}\")\n    \n    async def handle_item_added(self, event: Event):\n        \"\"\"Update read model when item is added\"\"\"\n        data = event.event_data\n        order_id = data[\"orderId\"]\n        \n        # Update order totals in read model\n        # This would be an UPDATE query in real implementation\n        self.logger.info(f\"Updated read model for item addition to order: {order_id}\")\n    \n    async def handle_order_confirmed(self, event: Event):\n        \"\"\"Update read model when order is confirmed\"\"\"\n        data = event.event_data\n        order_id = data[\"orderId\"]\n        \n        # Update status in read model\n        self.logger.info(f\"Updated read model for order confirmation: {order_id}\")\n\n# CQRS Bus (Mediator Pattern)\nclass CQRSBus:\n    def __init__(self):\n        self.command_handlers = {}\n        self.query_handlers = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def register_command_handler(self, command_type: Type[Command], handler: CommandHandler):\n        self.command_handlers[command_type] = handler\n    \n    def register_query_handler(self, query_type: Type[Query], handler: QueryHandler):\n        self.query_handlers[query_type] = handler\n    \n    async def send_command(self, command: Command) -> Any:\n        \"\"\"Send command to appropriate handler\"\"\"\n        command_type = type(command)\n        \n        if command_type not in self.command_handlers:\n            raise ValueError(f\"No handler registered for command {command_type.__name__}\")\n        \n        handler = self.command_handlers[command_type]\n        \n        self.logger.info(f\"Sending command: {command_type.__name__}\")\n        \n        try:\n            result = await handler.handle(command)\n            self.logger.info(f\"Command {command_type.__name__} handled successfully\")\n            return result\n        except Exception as e:\n            self.logger.error(f\"Command {command_type.__name__} failed: {e}\")\n            raise\n    \n    async def send_query(self, query: Query) -> Any:\n        \"\"\"Send query to appropriate handler\"\"\"\n        query_type = type(query)\n        \n        if query_type not in self.query_handlers:\n            raise ValueError(f\"No handler registered for query {query_type.__name__}\")\n        \n        handler = self.query_handlers[query_type]\n        \n        self.logger.info(f\"Sending query: {query_type.__name__}\")\n        \n        try:\n            result = await handler.handle(query)\n            self.logger.info(f\"Query {query_type.__name__} handled successfully\")\n            return result\n        except Exception as e:\n            self.logger.error(f\"Query {query_type.__name__} failed: {e}\")\n            raise\n\n# Complete CQRS Example\nasync def cqrs_example():\n    \"\"\"Complete CQRS example with event sourcing\"\"\"\n    \n    # Set up components\n    event_store = SQLiteEventStore(\"cqrs_orders.db\")\n    order_repository = OrderRepository(event_store)\n    read_model_store = ReadModelStore(\"sqlite:///read_model.db\")\n    \n    # Set up CQRS bus\n    bus = CQRSBus()\n    \n    # Register handlers\n    order_command_handler = OrderCommandHandler(order_repository, None)\n    order_query_handler = OrderQueryHandler(read_model_store)\n    \n    bus.register_command_handler(CreateOrderCommand, order_command_handler)\n    bus.register_command_handler(AddItemToOrderCommand, order_command_handler)\n    bus.register_command_handler(ConfirmOrderCommand, order_command_handler)\n    \n    bus.register_query_handler(GetOrderQuery, order_query_handler)\n    bus.register_query_handler(GetCustomerOrdersQuery, order_query_handler)\n    bus.register_query_handler(GetOrdersByStatusQuery, order_query_handler)\n    \n    # Command side (Write operations)\n    print(\"=== Command Side (Write Operations) ===\")\n    \n    # Create order\n    create_command = CreateOrderCommand(\n        order_id=\"ORD-CQRS-001\",\n        customer_id=\"CUST-123\",\n        shipping_address={\n            \"street\": \"123 Main St\",\n            \"city\": \"Anytown\",\n            \"zipCode\": \"12345\"\n        }\n    )\n    \n    order = await bus.send_command(create_command)\n    print(f\"Created order: {order.aggregate_id}\")\n    \n    # Add items\n    add_item_command = AddItemToOrderCommand(\n        order_id=\"ORD-CQRS-001\",\n        product_id=\"PROD-001\",\n        quantity=2,\n        unit_price=29.99\n    )\n    \n    await bus.send_command(add_item_command)\n    print(\"Added item to order\")\n    \n    # Confirm order\n    confirm_command = ConfirmOrderCommand(order_id=\"ORD-CQRS-001\")\n    await bus.send_command(confirm_command)\n    print(\"Confirmed order\")\n    \n    # Query side (Read operations)\n    print(\"\\n=== Query Side (Read Operations) ===\")\n    \n    # Get order details\n    get_order_query = GetOrderQuery(order_id=\"ORD-CQRS-001\")\n    order_details = await bus.send_query(get_order_query)\n    print(f\"Order details: {json.dumps(order_details, indent=2)}\")\n    \n    # Get customer orders\n    customer_orders_query = GetCustomerOrdersQuery(customer_id=\"CUST-123\", limit=5)\n    customer_orders = await bus.send_query(customer_orders_query)\n    print(f\"Customer orders: {len(customer_orders)} orders\")\n    \n    # Get orders by status\n    status_query = GetOrdersByStatusQuery(status=\"confirmed\", limit=10)\n    confirmed_orders = await bus.send_query(status_query)\n    print(f\"Confirmed orders: {len(confirmed_orders)} orders\")\n\nif __name__ == \"__main__\":\n    asyncio.run(cqrs_example())\n```\n\n---\n\n## âš–ï¸ Saga Pattern for Distributed Transactions {#saga}\n\n### Orchestration-Based Saga\n\n```python\nfrom enum import Enum\nfrom typing import Dict, Any, List, Optional, Callable\nfrom dataclasses import dataclass\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime, timedelta\nimport uuid\n\nclass SagaStepStatus(Enum):\n    PENDING = \"pending\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    COMPENSATED = \"compensated\"\n\n@dataclass\nclass SagaStep:\n    step_id: str\n    name: str\n    action: Callable\n    compensation: Callable\n    status: SagaStepStatus = SagaStepStatus.PENDING\n    result: Any = None\n    error: str = None\n\nclass SagaStatus(Enum):\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    COMPENSATING = \"compensating\"\n    COMPENSATED = \"compensated\"\n\nclass SagaOrchestrator:\n    def __init__(self, saga_id: str, saga_name: str):\n        self.saga_id = saga_id\n        self.saga_name = saga_name\n        self.steps: List[SagaStep] = []\n        self.status = SagaStatus.RUNNING\n        self.start_time = datetime.now()\n        self.end_time: Optional[datetime] = None\n        self.context: Dict[str, Any] = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def add_step(self, name: str, action: Callable, compensation: Callable) -> 'SagaOrchestrator':\n        \"\"\"Add step to saga\"\"\"\n        step = SagaStep(\n            step_id=str(uuid.uuid4()),\n            name=name,\n            action=action,\n            compensation=compensation\n        )\n        self.steps.append(step)\n        return self\n    \n    async def execute(self) -> bool:\n        \"\"\"Execute saga steps\"\"\"\n        self.logger.info(f\"Starting saga {self.saga_name} ({self.saga_id})\")\n        \n        try:\n            # Execute steps sequentially\n            for i, step in enumerate(self.steps):\n                self.logger.info(f\"Executing step {i+1}/{len(self.steps)}: {step.name}\")\n                \n                try:\n                    # Execute step action\n                    step.result = await step.action(self.context)\n                    step.status = SagaStepStatus.COMPLETED\n                    \n                    self.logger.info(f\"Step {step.name} completed successfully\")\n                    \n                except Exception as e:\n                    step.status = SagaStepStatus.FAILED\n                    step.error = str(e)\n                    \n                    self.logger.error(f\"Step {step.name} failed: {e}\")\n                    \n                    # Start compensation\n                    await self._compensate_completed_steps(i)\n                    \n                    self.status = SagaStatus.COMPENSATED\n                    self.end_time = datetime.now()\n                    \n                    return False\n            \n            # All steps completed successfully\n            self.status = SagaStatus.COMPLETED\n            self.end_time = datetime.now()\n            \n            duration = (self.end_time - self.start_time).total_seconds()\n            self.logger.info(f\"Saga {self.saga_name} completed successfully in {duration:.2f} seconds\")\n            \n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Saga {self.saga_name} failed with unexpected error: {e}\")\n            self.status = SagaStatus.FAILED\n            self.end_time = datetime.now()\n            return False\n    \n    async def _compensate_completed_steps(self, failed_step_index: int):\n        \"\"\"Execute compensation actions for completed steps\"\"\"\n        self.logger.info(f\"Starting compensation for {failed_step_index} completed steps\")\n        self.status = SagaStatus.COMPENSATING\n        \n        # Compensate in reverse order\n        for i in range(failed_step_index - 1, -1, -1):\n            step = self.steps[i]\n            \n            if step.status == SagaStepStatus.COMPLETED:\n                try:\n                    self.logger.info(f\"Compensating step: {step.name}\")\n                    await step.compensation(step.result, self.context)\n                    step.status = SagaStepStatus.COMPENSATED\n                    \n                    self.logger.info(f\"Step {step.name} compensated successfully\")\n                    \n                except Exception as e:\n                    self.logger.error(f\"Compensation failed for step {step.name}: {e}\")\n                    # Continue with other compensations\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get saga execution status\"\"\"\n        return {\n            \"sagaId\": self.saga_id,\n            \"sagaName\": self.saga_name,\n            \"status\": self.status.value,\n            \"startTime\": self.start_time.isoformat(),\n            \"endTime\": self.end_time.isoformat() if self.end_time else None,\n            \"duration\": (self.end_time - self.start_time).total_seconds() if self.end_time else None,\n            \"steps\": [\n                {\n                    \"stepId\": step.step_id,\n                    \"name\": step.name,\n                    \"status\": step.status.value,\n                    \"error\": step.error\n                }\n                for step in self.steps\n            ]\n        }\n\n# E-commerce Order Processing Saga Example\nclass OrderProcessingSaga:\n    def __init__(self, order_service, inventory_service, payment_service, shipping_service):\n        self.order_service = order_service\n        self.inventory_service = inventory_service\n        self.payment_service = payment_service\n        self.shipping_service = shipping_service\n        self.logger = logging.getLogger(__name__)\n    \n    async def create_order_saga(self, order_data: Dict[str, Any]) -> SagaOrchestrator:\n        \"\"\"Create saga for order processing\"\"\"\n        \n        saga_id = f\"saga-{order_data['orderId']}\"\n        saga = SagaOrchestrator(saga_id, \"OrderProcessing\")\n        \n        # Step 1: Create Order\n        saga.add_step(\n            name=\"CreateOrder\",\n            action=self._create_order,\n            compensation=self._cancel_order\n        )\n        \n        # Step 2: Reserve Inventory\n        saga.add_step(\n            name=\"ReserveInventory\", \n            action=self._reserve_inventory,\n            compensation=self._release_inventory\n        )\n        \n        # Step 3: Process Payment\n        saga.add_step(\n            name=\"ProcessPayment\",\n            action=self._process_payment,\n            compensation=self._refund_payment\n        )\n        \n        # Step 4: Ship Order\n        saga.add_step(\n            name=\"ShipOrder\",\n            action=self._ship_order,\n            compensation=self._cancel_shipment\n        )\n        \n        # Initialize context with order data\n        saga.context = order_data.copy()\n        \n        return saga\n    \n    async def _create_order(self, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Step 1: Create order\"\"\"\n        result = await self.order_service.create_order({\n            'orderId': context['orderId'],\n            'customerId': context['customerId'],\n            'items': context['items']\n        })\n        \n        # Update context with created order\n        context['createdOrder'] = result\n        \n        return result\n    \n    async def _cancel_order(self, order_result: Dict[str, Any], context: Dict[str, Any]):\n        \"\"\"Compensation 1: Cancel order\"\"\"\n        await self.order_service.cancel_order(order_result['orderId'])\n        self.logger.info(f\"Cancelled order {order_result['orderId']}\")\n    \n    async def _reserve_inventory(self, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Step 2: Reserve inventory\"\"\"\n        result = await self.inventory_service.reserve_items({\n            'orderId': context['orderId'],\n            'items': context['items']\n        })\n        \n        context['reservationId'] = result['reservationId']\n        \n        return result\n    \n    async def _release_inventory(self, reservation_result: Dict[str, Any], context: Dict[str, Any]):\n        \"\"\"Compensation 2: Release inventory\"\"\"\n        await self.inventory_service.release_reservation(context['reservationId'])\n        self.logger.info(f\"Released inventory reservation {context['reservationId']}\")\n    \n    async def _process_payment(self, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Step 3: Process payment\"\"\"\n        result = await self.payment_service.charge({\n            'orderId': context['orderId'],\n            'customerId': context['customerId'],\n            'amount': context['totalAmount'],\n            'paymentMethod': context['paymentMethod']\n        })\n        \n        context['paymentId'] = result['paymentId']\n        context['chargeId'] = result['chargeId']\n        \n        return result\n    \n    async def _refund_payment(self, payment_result: Dict[str, Any], context: Dict[str, Any]):\n        \"\"\"Compensation 3: Refund payment\"\"\"\n        await self.payment_service.refund({\n            'paymentId': context['paymentId'],\n            'chargeId': context['chargeId'],\n            'amount': context['totalAmount']\n        })\n        self.logger.info(f\"Refunded payment {context['paymentId']}\")\n    \n    async def _ship_order(self, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Step 4: Ship order\"\"\"\n        result = await self.shipping_service.ship_order({\n            'orderId': context['orderId'],\n            'items': context['items'],\n            'shippingAddress': context['shippingAddress']\n        })\n        \n        context['shippingId'] = result['shippingId']\n        context['trackingNumber'] = result['trackingNumber']\n        \n        return result\n    \n    async def _cancel_shipment(self, shipping_result: Dict[str, Any], context: Dict[str, Any]):\n        \"\"\"Compensation 4: Cancel shipment\"\"\"\n        await self.shipping_service.cancel_shipment(context['shippingId'])\n        self.logger.info(f\"Cancelled shipment {context['shippingId']}\")\n\n# Mock Services for Demo\nclass MockOrderService:\n    async def create_order(self, order_data: Dict[str, Any]) -> Dict[str, Any]:\n        await asyncio.sleep(0.1)  # Simulate processing time\n        return {\"orderId\": order_data['orderId'], \"status\": \"created\"}\n    \n    async def cancel_order(self, order_id: str):\n        await asyncio.sleep(0.1)\n        print(f\"Order {order_id} cancelled\")\n\nclass MockInventoryService:\n    async def reserve_items(self, request: Dict[str, Any]) -> Dict[str, Any]:\n        await asyncio.sleep(0.2)\n        \n        # Simulate occasional inventory shortage\n        if request['orderId'] == 'ORD-FAIL-INVENTORY':\n            raise Exception(\"Insufficient inventory\")\n        \n        return {\"reservationId\": f\"RES-{request['orderId']}\", \"status\": \"reserved\"}\n    \n    async def release_reservation(self, reservation_id: str):\n        await asyncio.sleep(0.1)\n        print(f\"Released reservation {reservation_id}\")\n\nclass MockPaymentService:\n    async def charge(self, payment_data: Dict[str, Any]) -> Dict[str, Any]:\n        await asyncio.sleep(0.3)\n        \n        # Simulate payment failure for high amounts\n        if payment_data['amount'] > 10000:\n            raise Exception(\"Payment declined - amount too high\")\n        \n        payment_id = f\"PAY-{payment_data['orderId']}\"\n        charge_id = f\"CHG-{str(uuid.uuid4())[:8]}\"\n        \n        return {\n            \"paymentId\": payment_id,\n            \"chargeId\": charge_id,\n            \"status\": \"charged\"\n        }\n    \n    async def refund(self, refund_data: Dict[str, Any]):\n        await asyncio.sleep(0.2)\n        print(f\"Refunded payment {refund_data['paymentId']} amount ${refund_data['amount']}\")\n\nclass MockShippingService:\n    async def ship_order(self, shipping_data: Dict[str, Any]) -> Dict[str, Any]:\n        await asyncio.sleep(0.4)\n        \n        # Simulate shipping failure\n        if shipping_data['orderId'] == 'ORD-FAIL-SHIPPING':\n            raise Exception(\"Shipping service unavailable\")\n        \n        return {\n            \"shippingId\": f\"SHIP-{shipping_data['orderId']}\",\n            \"trackingNumber\": f\"TRK{random.randint(100000, 999999)}\",\n            \"status\": \"shipped\"\n        }\n    \n    async def cancel_shipment(self, shipping_id: str):\n        await asyncio.sleep(0.1)\n        print(f\"Cancelled shipment {shipping_id}\")\n\n# Saga Manager for Multiple Sagas\nclass SagaManager:\n    def __init__(self):\n        self.active_sagas: Dict[str, SagaOrchestrator] = {}\n        self.completed_sagas: Dict[str, SagaOrchestrator] = {}\n        self.logger = logging.getLogger(__name__)\n    \n    async def start_saga(self, saga: SagaOrchestrator) -> bool:\n        \"\"\"Start executing a saga\"\"\"\n        self.active_sagas[saga.saga_id] = saga\n        \n        try:\n            success = await saga.execute()\n            \n            # Move to completed\n            self.completed_sagas[saga.saga_id] = saga\n            del self.active_sagas[saga.saga_id]\n            \n            return success\n            \n        except Exception as e:\n            self.logger.error(f\"Saga {saga.saga_id} failed with error: {e}\")\n            saga.status = SagaStatus.FAILED\n            self.completed_sagas[saga.saga_id] = saga\n            del self.active_sagas[saga.saga_id]\n            return False\n    \n    def get_saga_status(self, saga_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get status of a saga\"\"\"\n        if saga_id in self.active_sagas:\n            return self.active_sagas[saga_id].get_status()\n        elif saga_id in self.completed_sagas:\n            return self.completed_sagas[saga_id].get_status()\n        else:\n            return None\n    \n    def get_active_sagas(self) -> List[Dict[str, Any]]:\n        \"\"\"Get all active sagas\"\"\"\n        return [saga.get_status() for saga in self.active_sagas.values()]\n\n# Complete Example Usage\nasync def saga_example():\n    \"\"\"Complete saga pattern example\"\"\"\n    \n    # Initialize services\n    order_service = MockOrderService()\n    inventory_service = MockInventoryService() \n    payment_service = MockPaymentService()\n    shipping_service = MockShippingService()\n    \n    # Initialize saga processor\n    saga_processor = OrderProcessingSaga(\n        order_service, inventory_service, payment_service, shipping_service\n    )\n    \n    # Initialize saga manager\n    saga_manager = SagaManager()\n    \n    # Test successful saga\n    print(\"=== Testing Successful Order Processing Saga ===\")\n    \n    order_data_success = {\n        'orderId': 'ORD-SUCCESS-001',\n        'customerId': 'CUST-123',\n        'items': [\n            {'productId': 'PROD-001', 'quantity': 2, 'price': 29.99},\n            {'productId': 'PROD-002', 'quantity': 1, 'price': 49.99}\n        ],\n        'totalAmount': 109.97,\n        'paymentMethod': 'credit_card',\n        'shippingAddress': {\n            'street': '123 Main St',\n            'city': 'Anytown',\n            'zipCode': '12345'\n        }\n    }\n    \n    success_saga = await saga_processor.create_order_saga(order_data_success)\n    success_result = await saga_manager.start_saga(success_saga)\n    \n    print(f\"Success saga result: {success_result}\")\n    print(f\"Success saga status: {json.dumps(success_saga.get_status(), indent=2)}\")\n    \n    # Test failed saga (payment failure)\n    print(\"\\n=== Testing Failed Saga with Compensation ===\")\n    \n    order_data_fail = {\n        'orderId': 'ORD-FAIL-001',\n        'customerId': 'CUST-456', \n        'items': [\n            {'productId': 'PROD-003', 'quantity': 1, 'price': 15000.00}  # High amount will fail\n        ],\n        'totalAmount': 15000.00,\n        'paymentMethod': 'credit_card',\n        'shippingAddress': {\n            'street': '456 Oak Ave',\n            'city': 'Other Town',\n            'zipCode': '67890'\n        }\n    }\n    \n    fail_saga = await saga_processor.create_order_saga(order_data_fail)\n    fail_result = await saga_manager.start_saga(fail_saga)\n    \n    print(f\"Fail saga result: {fail_result}\")\n    print(f\"Fail saga status: {json.dumps(fail_saga.get_status(), indent=2)}\")\n    \n    # Test inventory failure\n    print(\"\\n=== Testing Inventory Failure Saga ===\")\n    \n    inventory_fail_data = {\n        'orderId': 'ORD-FAIL-INVENTORY',\n        'customerId': 'CUST-789',\n        'items': [\n            {'productId': 'PROD-004', 'quantity': 100, 'price': 25.00}\n        ],\n        'totalAmount': 2500.00,\n        'paymentMethod': 'credit_card',\n        'shippingAddress': {\n            'street': '789 Pine St',\n            'city': 'Inventory Town', \n            'zipCode': '11111'\n        }\n    }\n    \n    inventory_fail_saga = await saga_processor.create_order_saga(inventory_fail_data)\n    inventory_fail_result = await saga_manager.start_saga(inventory_fail_saga)\n    \n    print(f\"Inventory fail saga result: {inventory_fail_result}\")\n    print(f\"Inventory fail saga status: {json.dumps(inventory_fail_saga.get_status(), indent=2)}\")\n\n# Choreography-Based Saga (Alternative Pattern)\nclass ChoreographySaga:\n    \"\"\"Saga using choreography where each service knows what to do next\"\"\"\n    \n    def __init__(self, event_publisher):\n        self.event_publisher = event_publisher\n        self.logger = logging.getLogger(__name__)\n    \n    # Order Service Handler\n    async def handle_order_create_requested(self, event: Dict[str, Any]):\n        \"\"\"Handle order creation request\"\"\"\n        try:\n            order_data = event['data']\n            \n            # Create order\n            order = await self._create_order(order_data)\n            \n            # Publish success event\n            await self.event_publisher.publish('order.created', {\n                'orderId': order['orderId'],\n                'customerId': order['customerId'],\n                'items': order['items'],\n                'totalAmount': order['totalAmount']\n            })\n            \n        except Exception as e:\n            # Publish failure event\n            await self.event_publisher.publish('order.creation.failed', {\n                'orderId': order_data['orderId'],\n                'error': str(e)\n            })\n    \n    # Inventory Service Handler  \n    async def handle_order_created(self, event: Dict[str, Any]):\n        \"\"\"Handle order created event (inventory service)\"\"\"\n        try:\n            order_data = event['data']\n            \n            # Reserve inventory\n            reservation = await self._reserve_inventory(order_data['items'])\n            \n            # Publish success event\n            await self.event_publisher.publish('inventory.reserved', {\n                'orderId': order_data['orderId'],\n                'reservationId': reservation['reservationId'],\n                'items': order_data['items']\n            })\n            \n        except Exception as e:\n            # Publish failure event - triggers order cancellation\n            await self.event_publisher.publish('inventory.reservation.failed', {\n                'orderId': order_data['orderId'],\n                'error': str(e)\n            })\n    \n    # Payment Service Handler\n    async def handle_inventory_reserved(self, event: Dict[str, Any]):\n        \"\"\"Handle inventory reserved event (payment service)\"\"\"\n        try:\n            order_data = event['data']\n            \n            # Process payment\n            payment = await self._process_payment(order_data)\n            \n            # Publish success event\n            await self.event_publisher.publish('payment.processed', {\n                'orderId': order_data['orderId'],\n                'paymentId': payment['paymentId'],\n                'amount': payment['amount']\n            })\n            \n        except Exception as e:\n            # Publish failure event - triggers compensation\n            await self.event_publisher.publish('payment.failed', {\n                'orderId': order_data['orderId'],\n                'error': str(e)\n            })\n    \n    # Compensation Handlers\n    async def handle_payment_failed(self, event: Dict[str, Any]):\n        \"\"\"Handle payment failure - compensate inventory\"\"\"\n        order_id = event['data']['orderId']\n        \n        # Release inventory reservation\n        await self.event_publisher.publish('inventory.release.requested', {\n            'orderId': order_id,\n            'reason': 'payment_failed'\n        })\n    \n    async def handle_inventory_reservation_failed(self, event: Dict[str, Any]):\n        \"\"\"Handle inventory failure - compensate order\"\"\"\n        order_id = event['data']['orderId']\n        \n        # Cancel order\n        await self.event_publisher.publish('order.cancel.requested', {\n            'orderId': order_id,\n            'reason': 'inventory_unavailable'\n        })\n\n# Performance testing and metrics\nimport time\nimport random\n\nasync def test_saga_performance():\n    \"\"\"Test saga performance under load\"\"\"\n    \n    # Mock services\n    order_service = MockOrderService()\n    inventory_service = MockInventoryService()\n    payment_service = MockPaymentService()\n    shipping_service = MockShippingService()\n    \n    saga_processor = OrderProcessingSaga(\n        order_service, inventory_service, payment_service, shipping_service\n    )\n    \n    saga_manager = SagaManager()\n    \n    # Generate test orders\n    orders = []\n    for i in range(100):\n        order_data = {\n            'orderId': f'ORD-PERF-{i:03d}',\n            'customerId': f'CUST-{i % 20:03d}',  # 20 different customers\n            'items': [\n                {\n                    'productId': f'PROD-{random.randint(1, 50):03d}',\n                    'quantity': random.randint(1, 5),\n                    'price': round(random.uniform(10, 200), 2)\n                }\n            ],\n            'totalAmount': 0,  # Will be calculated\n            'paymentMethod': 'credit_card',\n            'shippingAddress': {\n                'street': f'{i+1} Test St',\n                'city': 'Test City',\n                'zipCode': f'{12345 + i:05d}'\n            }\n        }\n        \n        # Calculate total\n        order_data['totalAmount'] = sum(\n            item['quantity'] * item['price'] for item in order_data['items']\n        )\n        \n        orders.append(order_data)\n    \n    # Execute sagas concurrently\n    start_time = time.time()\n    \n    tasks = []\n    for order_data in orders:\n        saga = await saga_processor.create_order_saga(order_data)\n        task = asyncio.create_task(saga_manager.start_saga(saga))\n        tasks.append(task)\n    \n    # Wait for all sagas to complete\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    \n    end_time = time.time()\n    duration = end_time - start_time\n    \n    # Calculate metrics\n    successful = sum(1 for result in results if result is True)\n    failed = sum(1 for result in results if result is False or isinstance(result, Exception))\n    \n    print(f\"\\n=== Saga Performance Test Results ===\")\n    print(f\"Total sagas: {len(orders)}\")\n    print(f\"Successful: {successful}\")\n    print(f\"Failed: {failed}\")\n    print(f\"Success rate: {successful / len(orders) * 100:.1f}%\")\n    print(f\"Total duration: {duration:.2f} seconds\")\n    print(f\"Throughput: {len(orders) / duration:.2f} sagas/second\")\n    print(f\"Average duration per saga: {duration / len(orders) * 1000:.2f} ms\")\n\nif __name__ == \"__main__\":\n    # Run examples\n    print(\"Running Saga Pattern Examples...\")\n    asyncio.run(saga_example())\n    \n    print(\"\\nRunning Performance Test...\")\n    asyncio.run(test_saga_performance())\n```\n\n---\n\n## ðŸ§ª Testing Event-Driven Systems {#testing}\n\n### Unit Testing for Event-Driven Components\n\n```python\nimport unittest\nfrom unittest.mock import Mock, AsyncMock, patch\nimport json\nfrom datetime import datetime\nimport pytest\nimport asyncio\n\nclass TestOrderAggregate(unittest.TestCase):\n    def setUp(self):\n        self.order = OrderAggregate(\"TEST-ORDER-001\")\n    \n    def test_create_order(self):\n        \"\"\"Test order creation\"\"\"\n        customer_id = \"CUST-123\"\n        shipping_address = {\"street\": \"123 Test St\", \"city\": \"Test City\"}\n        \n        self.order.create_order(customer_id, shipping_address)\n        \n        # Verify state changes\n        self.assertEqual(self.order.customer_id, customer_id)\n        self.assertEqual(self.order.shipping_address, shipping_address)\n        self.assertEqual(self.order.status, OrderStatus.DRAFT)\n        \n        # Verify event was raised\n        self.assertEqual(len(self.order.uncommitted_events), 1)\n        event = self.order.uncommitted_events[0]\n        self.assertEqual(event.event_type, \"OrderCreated\")\n        self.assertEqual(event.event_data[\"customerId\"], customer_id)\n    \n    def test_add_item(self):\n        \"\"\"Test adding item to order\"\"\"\n        # Create order first\n        self.order.create_order(\"CUST-123\", {\"street\": \"123 Test St\"})\n        \n        # Add item\n        self.order.add_item(\"PROD-001\", 2, 29.99)\n        \n        # Verify state\n        self.assertEqual(len(self.order.items), 1)\n        item = self.order.items[0]\n        self.assertEqual(item.product_id, \"PROD-001\")\n        self.assertEqual(item.quantity, 2)\n        self.assertEqual(item.unit_price, 29.99)\n        self.assertEqual(self.order.total_amount, 59.98)\n        \n        # Verify events\n        self.assertEqual(len(self.order.uncommitted_events), 2)\n        self.assertEqual(self.order.uncommitted_events[1].event_type, \"ItemAdded\")\n    \n    def test_cannot_add_item_to_confirmed_order(self):\n        \"\"\"Test that items cannot be added to confirmed order\"\"\"\n        # Create and confirm order\n        self.order.create_order(\"CUST-123\", {\"street\": \"123 Test St\"})\n        self.order.add_item(\"PROD-001\", 1, 29.99)\n        self.order.confirm_order()\n        \n        # Try to add item - should fail\n        with self.assertRaises(ValueError) as context:\n            self.order.add_item(\"PROD-002\", 1, 19.99)\n        \n        self.assertIn(\"Cannot add items to order in status confirmed\", str(context.exception))\n    \n    def test_event_sourcing_rebuild(self):\n        \"\"\"Test rebuilding order from events\"\"\"\n        # Create order and perform actions\n        self.order.create_order(\"CUST-123\", {\"street\": \"123 Test St\"})\n        self.order.add_item(\"PROD-001\", 2, 29.99)\n        self.order.add_item(\"PROD-002\", 1, 49.99)\n        self.order.confirm_order()\n        \n        # Get events\n        events = self.order.uncommitted_events\n        \n        # Rebuild order from events\n        rebuilt_order = OrderAggregate(\"TEST-ORDER-001\")\n        for event in events:\n            rebuilt_order.apply_event(event)\n        \n        # Verify rebuilt state matches original\n        self.assertEqual(rebuilt_order.status, self.order.status)\n        self.assertEqual(rebuilt_order.total_amount, self.order.total_amount)\n        self.assertEqual(len(rebuilt_order.items), len(self.order.items))\n\nclass TestEventPublisher(unittest.TestCase):\n    def setUp(self):\n        self.event_publisher = Mock()\n    \n    @patch('kafka.KafkaProducer')\n    def test_publish_event(self, mock_producer):\n        \"\"\"Test event publishing\"\"\"\n        # Setup mock\n        mock_producer_instance = Mock()\n        mock_producer.return_value = mock_producer_instance\n        mock_future = Mock()\n        mock_producer_instance.send.return_value = mock_future\n        \n        # Create real publisher\n        from your_event_module import KafkaEventPublisher\n        publisher = KafkaEventPublisher(['localhost:9092'])\n        \n        # Publish event\n        event_data = {\n            'eventType': 'OrderCreated',\n            'data': {'orderId': 'TEST-001'}\n        }\n        \n        publisher.publish('orders', event_data)\n        \n        # Verify send was called\n        mock_producer_instance.send.assert_called_once()\n        call_args = mock_producer_instance.send.call_args\n        \n        self.assertEqual(call_args[1]['topic'], 'orders')\n        sent_data = json.loads(call_args[1]['value'])\n        self.assertEqual(sent_data['eventType'], 'OrderCreated')\n\n# Integration Testing\n@pytest.mark.asyncio\nclass TestEventIntegration:\n    async def test_end_to_end_order_flow(self):\n        \"\"\"Test complete order flow with real message broker\"\"\"\n        \n        # Setup test environment\n        event_store = SQLiteEventStore(\":memory:\")  # In-memory for testing\n        repository = OrderRepository(event_store)\n        \n        # Create order\n        order = OrderAggregate(\"INTEGRATION-001\")\n        order.create_order(\"CUST-999\", {\"street\": \"999 Test Ave\"})\n        order.add_item(\"PROD-999\", 1, 99.99)\n        order.confirm_order()\n        \n        # Save order\n        await repository.save(order)\n        \n        # Verify order was saved\n        loaded_order = await repository.get_by_id(\"INTEGRATION-001\")\n        \n        assert loaded_order is not None\n        assert loaded_order.status == OrderStatus.CONFIRMED\n        assert loaded_order.total_amount == 99.99\n    \n    async def test_saga_compensation(self):\n        \"\"\"Test saga compensation flow\"\"\"\n        \n        # Mock services that will fail at payment step\n        order_service = AsyncMock()\n        inventory_service = AsyncMock()\n        payment_service = AsyncMock()\n        shipping_service = AsyncMock()\n        \n        # Configure mocks\n        order_service.create_order.return_value = {\"orderId\": \"TEST-001\", \"status\": \"created\"}\n        inventory_service.reserve_items.return_value = {\"reservationId\": \"RES-001\"}\n        payment_service.charge.side_effect = Exception(\"Payment declined\")\n        \n        # Create saga\n        saga_processor = OrderProcessingSaga(\n            order_service, inventory_service, payment_service, shipping_service\n        )\n        \n        order_data = {\n            'orderId': 'TEST-001',\n            'customerId': 'CUST-TEST',\n            'items': [{'productId': 'PROD-TEST', 'quantity': 1, 'price': 50.0}],\n            'totalAmount': 50.0,\n            'paymentMethod': 'credit_card',\n            'shippingAddress': {'street': 'Test St'}\n        }\n        \n        saga = await saga_processor.create_order_saga(order_data)\n        \n        # Execute saga (should fail at payment and compensate)\n        result = await saga.execute()\n        \n        # Verify compensation was called\n        assert result is False\n        assert saga.status == SagaStatus.COMPENSATED\n        \n        # Verify service calls\n        order_service.create_order.assert_called_once()\n        inventory_service.reserve_items.assert_called_once()\n        payment_service.charge.assert_called_once()\n        \n        # Verify compensation calls\n        inventory_service.release_reservation.assert_called_once()\n        order_service.cancel_order.assert_called_once()\n\n# Contract Testing for Event Schemas\nclass TestEventContracts(unittest.TestCase):\n    def test_order_created_event_schema(self):\n        \"\"\"Test OrderCreated event schema compliance\"\"\"\n        \n        event = {\n            \"eventId\": \"evt-123\",\n            \"eventType\": \"OrderCreated\",\n            \"timestamp\": \"2024-01-15T10:00:00Z\",\n            \"data\": {\n                \"orderId\": \"ORD-001\",\n                \"customerId\": \"CUST-123\",\n                \"totalAmount\": 99.99,\n                \"currency\": \"USD\"\n            }\n        }\n        \n        # Schema validation\n        required_fields = [\"eventId\", \"eventType\", \"timestamp\", \"data\"]\n        for field in required_fields:\n            self.assertIn(field, event, f\"Missing required field: {field}\")\n        \n        # Data field validation\n        data_required_fields = [\"orderId\", \"customerId\", \"totalAmount\"]\n        for field in data_required_fields:\n            self.assertIn(field, event[\"data\"], f\"Missing required data field: {field}\")\n        \n        # Type validation\n        self.assertIsInstance(event[\"data\"][\"totalAmount\"], (int, float))\n        self.assertIsInstance(event[\"data\"][\"orderId\"], str)\n    \n    def test_event_backward_compatibility(self):\n        \"\"\"Test that new event versions are backward compatible\"\"\"\n        \n        # Old version event\n        old_event = {\n            \"eventType\": \"OrderCreated\",\n            \"data\": {\n                \"orderId\": \"ORD-001\",\n                \"customerId\": \"CUST-123\"\n            }\n        }\n        \n        # New version event with additional fields\n        new_event = {\n            \"eventType\": \"OrderCreated\",\n            \"eventVersion\": \"2.0\",  # New field\n            \"data\": {\n                \"orderId\": \"ORD-001\",\n                \"customerId\": \"CUST-123\",\n                \"customerSegment\": \"premium\"  # New field\n            }\n        }\n        \n        # Both should be processable by consumers\n        self.assertTrue(self._can_process_order_created_event(old_event))\n        self.assertTrue(self._can_process_order_created_event(new_event))\n    \n    def _can_process_order_created_event(self, event):\n        \"\"\"Simulate event processing\"\"\"\n        try:\n            # Required fields check\n            assert \"orderId\" in event[\"data\"]\n            assert \"customerId\" in event[\"data\"]\n            return True\n        except:\n            return False\n\n# Load Testing for Event Systems\nimport time\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass EventSystemLoadTester:\n    def __init__(self, event_publisher):\n        self.event_publisher = event_publisher\n        self.metrics = {\n            'events_sent': 0,\n            'events_failed': 0,\n            'total_latency_ms': 0,\n            'max_latency_ms': 0,\n            'min_latency_ms': float('inf')\n        }\n    \n    async def generate_load(self, events_per_second: int, duration_seconds: int):\n        \"\"\"Generate load on event system\"\"\"\n        \n        total_events = events_per_second * duration_seconds\n        delay_between_events = 1.0 / events_per_second\n        \n        print(f\"Generating {events_per_second} events/second for {duration_seconds} seconds\")\n        print(f\"Total events: {total_events}\")\n        \n        start_time = time.time()\n        \n        for i in range(total_events):\n            event_start_time = time.time()\n            \n            try:\n                # Generate event\n                event = {\n                    'eventId': f'load-test-{i:06d}',\n                    'eventType': 'LoadTestEvent',\n                    'timestamp': datetime.now().isoformat(),\n                    'data': {\n                        'sequenceNumber': i,\n                        'timestamp': time.time(),\n                        'payload': 'test-data' * 10  # Some payload\n                    }\n                }\n                \n                # Publish event\n                await self.event_publisher.publish('load-test', event)\n                \n                # Calculate latency\n                latency_ms = (time.time() - event_start_time) * 1000\n                \n                # Update metrics\n                self.metrics['events_sent'] += 1\n                self.metrics['total_latency_ms'] += latency_ms\n                self.metrics['max_latency_ms'] = max(self.metrics['max_latency_ms'], latency_ms)\n                self.metrics['min_latency_ms'] = min(self.metrics['min_latency_ms'], latency_ms)\n                \n            except Exception as e:\n                self.metrics['events_failed'] += 1\n                print(f\"Failed to send event {i}: {e}\")\n            \n            # Control rate\n            await asyncio.sleep(delay_between_events)\n        \n        total_duration = time.time() - start_time\n        \n        # Print results\n        self._print_load_test_results(total_duration)\n    \n    def _print_load_test_results(self, duration: float):\n        \"\"\"Print load test results\"\"\"\n        total_events = self.metrics['events_sent'] + self.metrics['events_failed']\n        success_rate = self.metrics['events_sent'] / total_events * 100 if total_events > 0 else 0\n        avg_latency = self.metrics['total_latency_ms'] / self.metrics['events_sent'] if self.metrics['events_sent'] > 0 else 0\n        throughput = total_events / duration\n        \n        print(f\"\\n=== Load Test Results ===\")\n        print(f\"Duration: {duration:.2f} seconds\")\n        print(f\"Total events: {total_events}\")\n        print(f\"Events sent: {self.metrics['events_sent']}\")\n        print(f\"Events failed: {self.metrics['events_failed']}\")\n        print(f\"Success rate: {success_rate:.2f}%\")\n        print(f\"Throughput: {throughput:.2f} events/second\")\n        print(f\"Latency - Avg: {avg_latency:.2f}ms, Min: {self.metrics['min_latency_ms']:.2f}ms, Max: {self.metrics['max_latency_ms']:.2f}ms\")\n\n# Chaos Testing for Resilience\nclass ChaosTestRunner:\n    def __init__(self, event_system):\n        self.event_system = event_system\n        self.logger = logging.getLogger(__name__)\n    \n    async def test_broker_failure(self):\n        \"\"\"Test system behavior when message broker fails\"\"\"\n        print(\"=== Chaos Test: Broker Failure ===\")\n        \n        try:\n            # Send events normally\n            for i in range(10):\n                await self.event_system.publish('test-topic', {\n                    'eventType': 'TestEvent',\n                    'data': {'id': i}\n                })\n            \n            print(\"Normal operation completed\")\n            \n            # Simulate broker failure\n            print(\"Simulating broker failure...\")\n            # In real test, you would stop Kafka/RabbitMQ container\n            \n            # Try to send events during failure\n            failed_sends = 0\n            for i in range(10, 20):\n                try:\n                    await self.event_system.publish('test-topic', {\n                        'eventType': 'TestEvent',\n                        'data': {'id': i}\n                    })\n                except:\n                    failed_sends += 1\n            \n            print(f\"Failed to send {failed_sends}/10 events during broker failure\")\n            \n            # Simulate broker recovery\n            print(\"Simulating broker recovery...\")\n            # In real test, you would restart the broker\n            \n            # Resume sending events\n            for i in range(20, 30):\n                await self.event_system.publish('test-topic', {\n                    'eventType': 'TestEvent',\n                    'data': {'id': i}\n                })\n            \n            print(\"Recovery operation completed\")\n            \n        except Exception as e:\n            self.logger.error(f\"Chaos test failed: {e}\")\n    \n    async def test_consumer_failure(self):\n        \"\"\"Test system behavior when consumers fail\"\"\"\n        print(\"=== Chaos Test: Consumer Failure ===\")\n        \n        # This would simulate consumer failures and test:\n        # - Message redelivery\n        # - Dead letter queues\n        # - Circuit breaker behavior\n        # - System recovery\n        \n        pass\n    \n    async def test_network_partition(self):\n        \"\"\"Test system behavior during network partitions\"\"\"\n        print(\"=== Chaos Test: Network Partition ===\")\n        \n        # This would simulate network partitions and test:\n        # - Message ordering guarantees\n        # - Duplicate detection\n        # - Eventual consistency\n        # - Split-brain scenarios\n        \n        pass\n\n# Property-Based Testing for Event Schemas\nfrom hypothesis import given, strategies as st\nimport hypothesis\n\nclass TestEventProperties:\n    @given(\n        order_id=st.text(min_size=1, max_size=50),\n        customer_id=st.text(min_size=1, max_size=50),\n        amount=st.floats(min_value=0.01, max_value=100000, allow_nan=False, allow_infinity=False)\n    )\n    def test_order_creation_properties(self, order_id, customer_id, amount):\n        \"\"\"Property-based test for order creation\"\"\"\n        \n        # Create order\n        order = OrderAggregate(order_id)\n        \n        # This should never fail regardless of input\n        try:\n            order.create_order(customer_id, {\"street\": \"Test St\"})\n            \n            # Properties that should always hold\n            assert order.customer_id == customer_id\n            assert order.status == OrderStatus.DRAFT\n            assert len(order.uncommitted_events) == 1\n            assert order.uncommitted_events[0].event_type == \"OrderCreated\"\n            \n        except Exception as e:\n            # Log any unexpected failures\n            print(f\"Unexpected failure with inputs: order_id={order_id}, customer_id={customer_id}, amount={amount}\")\n            print(f\"Error: {e}\")\n            raise\n\n# Event Consumer Testing Helper\nclass TestEventConsumer:\n    def __init__(self):\n        self.received_events = []\n        self.processing_errors = []\n    \n    async def handle_event(self, event_data: Dict[str, Any]) -> bool:\n        \"\"\"Test event handler that records events\"\"\"\n        try:\n            self.received_events.append({\n                'event': event_data,\n                'received_at': datetime.now().isoformat()\n            })\n            \n            # Simulate processing\n            await asyncio.sleep(0.01)\n            \n            return True\n            \n        except Exception as e:\n            self.processing_errors.append({\n                'event': event_data,\n                'error': str(e),\n                'failed_at': datetime.now().isoformat()\n            })\n            return False\n    \n    def assert_received_event(self, event_type: str, timeout_seconds: int = 5):\n        \"\"\"Assert that an event of specific type was received\"\"\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout_seconds:\n            for received in self.received_events:\n                if received['event'].get('eventType') == event_type:\n                    return received['event']\n            time.sleep(0.1)\n        \n        raise AssertionError(f\"Event {event_type} not received within {timeout_seconds} seconds\")\n    \n    def assert_event_count(self, expected_count: int):\n        \"\"\"Assert expected number of events received\"\"\"\n        actual_count = len(self.received_events)\n        assert actual_count == expected_count, f\"Expected {expected_count} events, got {actual_count}\"\n    \n    def clear(self):\n        \"\"\"Clear received events\"\"\"\n        self.received_events.clear()\n        self.processing_errors.clear()\n\n# Usage example for testing\nif __name__ == \"__main__\":\n    # Run unit tests\n    print(\"Running unit tests...\")\n    unittest.main(argv=[''], exit=False, verbosity=2)\n    \n    # Run integration tests\n    print(\"\\nRunning integration tests...\")\n    pytest.main([__file__ + \"::TestEventIntegration\", \"-v\"])\n    \n    # Run load tests\n    print(\"\\nRunning load tests...\")\n    async def run_load_test():\n        # Mock event publisher for load testing\n        mock_publisher = AsyncMock()\n        load_tester = EventSystemLoadTester(mock_publisher)\n        await load_tester.generate_load(events_per_second=100, duration_seconds=10)\n    \n    asyncio.run(run_load_test())\n```\n\n---\n\n## ðŸ† Congratulations!\n\nYou've completed the Event-Driven Architecture Zero to Hero guide! You now have mastery over:\n\n### âœ… What You've Accomplished\n\n**Fundamentals**:\n- âœ… Event-driven architecture concepts and patterns\n- âœ… Event design and schema best practices\n- âœ… Message delivery guarantees and trade-offs\n- âœ… Choreography vs orchestration patterns\n\n**Message Brokers**:\n- âœ… Apache Kafka advanced producer and consumer implementations\n- âœ… RabbitMQ complete setup with exchange types and patterns\n- âœ… Dead letter queues, priority queues, and delayed messages\n- âœ… Performance optimization and monitoring\n\n**Advanced Patterns**:\n- âœ… Event Sourcing with complete aggregate implementation\n- âœ… CQRS pattern with separate read/write models\n- âœ… Saga pattern for distributed transactions\n- âœ… Event streaming and real-time processing\n\n**Production Excellence**:\n- âœ… Comprehensive testing strategies (unit, integration, chaos)\n- âœ… Monitoring and observability for event systems\n- âœ… Security and compliance considerations\n- âœ… Performance optimization and scalability patterns\n\n**Real-World Applications**:\n- âœ… Complete e-commerce platform implementation\n- âœ… Real-time fraud detection system\n- âœ… Microservices communication patterns\n- âœ… Load testing and performance analysis\n\n### ðŸš€ Your Event-Driven Architecture Journey\n\n**Immediate Next Steps (Next 30 Days)**:\n1. **Choose Your Message Broker**: Start with either Kafka or RabbitMQ based on your use case\n2. **Build Your First Event-Driven System**: Implement the e-commerce example\n3. **Practice Event Sourcing**: Build an aggregate with event sourcing\n4. **Set Up Monitoring**: Implement observability for your event systems\n5. **Join Communities**: Connect with event-driven architecture practitioners\n\n**Advanced Goals (3-6 Months)**:\n1. **Master Both Patterns**: Learn both choreography and orchestration\n2. **Implement CQRS**: Build a system with separate read/write models\n3. **Production Deployment**: Deploy event-driven system to cloud\n4. **Performance Tuning**: Optimize for high throughput and low latency\n5. **Contribute to Open Source**: Participate in event-driven projects\n\n**Expert Level (1-2 Years)**:\n1. **Design Large-Scale Systems**: Architect enterprise event-driven platforms\n2. **Multi-Cloud Events**: Implement cross-cloud event streaming\n3. **Event-Driven Microservices**: Build complete microservices architecture\n4. **Thought Leadership**: Share knowledge through blogs and conferences\n5. **Innovation**: Contribute to next-generation event technologies\n\n### ðŸŒŸ The Event-Driven Future\n\nEvent-driven architecture is becoming the foundation for:\n\n- **Real-time Everything**: Instant responses and analytics\n- **Serverless Computing**: Event-triggered function execution\n- **IoT and Edge Computing**: Massive scale sensor data processing\n- **AI and ML Pipelines**: Event-driven model training and inference\n- **Digital Transformation**: Modernizing legacy enterprise systems\n\n**High-Demand Skills in EDA**:\n- Event streaming and real-time processing\n- Event-driven microservices architecture\n- Cross-cloud event integration\n- Event sourcing and CQRS patterns\n- Event system security and compliance\n\n### ðŸ’« Keep Building!\n\nEvent-driven architecture enables some of the most innovative and scalable systems in the world. Companies like Netflix, LinkedIn, Uber, and Amazon have built their platforms on event-driven principles.\n\nAs an event-driven architecture expert, you'll:\n- **Build Reactive Systems**: Create systems that respond instantly to changes\n- **Enable Real-time Business**: Help organizations make real-time decisions\n- **Scale to Global Levels**: Design systems handling millions of events per second\n- **Drive Innovation**: Enable new business models and capabilities\n\n**Remember**: Every large-scale system today is event-driven. Master these patterns, and you'll be ready to build the future of software architecture!\n\nThe world of events awaits your expertise. Start building amazing event-driven systems today! ðŸš€âš¡\n\n---\n\n**\"The best way to predict the future is to build it with events.\"** - Unknown\n\nKeep streaming, keep building, keep innovating! ðŸŒŸ"}