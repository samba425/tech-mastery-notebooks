{"id":"computer-vision","title":"üëÅÔ∏è Computer Vision Complete Guide","content":"# üëÅÔ∏è Computer Vision - Zero to Hero Complete Guide\n\n> **From Image Processing to Object Detection & Beyond**\n\n---\n\n## üéØ Complete Computer Vision Mastery\n\n**What You'll Build:**\n- üì∑ Image classification systems\n- üéØ Object detection (YOLO, Faster R-CNN)\n- üñºÔ∏è Image segmentation\n- üë§ Face recognition\n- üé® Image generation\n- üöÄ Production CV systems\n\n**Time:** 6-8 weeks | **Salary:** $120K-$200K+\n\n---\n\n## üìö Quick Navigation\n\n1. **Image Basics** - Loading, processing, augmentation\n2. **CNNs Deep Dive** - ResNet, VGG, EfficientNet\n3. **Object Detection** - YOLO, R-CNN, RetinaNet\n4. **Segmentation** - U-Net, Mask R-CNN\n5. **Face Recognition** - FaceNet, ArcFace\n6. **Transfer Learning** - Fine-tuning pre-trained models\n7. **Production** - Deployment, optimization\n\n---\n\n## Part 1: Image Fundamentals\n\n### **Loading & Processing Images:**\n\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load image (OpenCV)\nimg_cv = cv2.imread('image.jpg')\nimg_cv_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n\n# Load image (PIL)\nimg_pil = Image.open('image.jpg')\nimg_array = np.array(img_pil)\n\n# Basic operations\ngray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\nresized = cv2.resize(img_cv, (224, 224))\ncropped = img_cv[100:300, 100:300]\n\n# Display\nplt.imshow(img_cv_rgb)\nplt.axis('off')\nplt.show()\n```\n\n### **Data Augmentation:**\n\n```python\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport albumentations as A\n\n# Keras augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    zoom_range=0.2,\n    fill_mode='nearest'\n)\n\n# Albumentations (more powerful)\ntransform = A.Compose([\n    A.Rotate(limit=45),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n])\n\naugmented = transform(image=img_array)['image']\n```\n\n---\n\n## Part 2: CNN Architectures\n\n### **ResNet from Scratch:**\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = torch.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ResNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n        \n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n    \n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Usage\nmodel = ResNet(num_classes=1000)\nx = torch.randn(1, 3, 224, 224)\noutput = model(x)\nprint(output.shape)  # [1, 1000]\n```\n\n### **Using Pre-trained Models:**\n\n```python\nfrom torchvision import models, transforms\nimport torch\n\n# Load pre-trained ResNet\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\n\n# Preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# Inference\nfrom PIL import Image\nimg = Image.open('dog.jpg')\nimg_tensor = transform(img).unsqueeze(0)\n\nwith torch.no_grad():\n    output = model(img_tensor)\n    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n    top5_prob, top5_idx = torch.topk(probabilities, 5)\n\n# Load ImageNet labels\nimport json\nwith open('imagenet_classes.json') as f:\n    labels = json.load(f)\n\nfor prob, idx in zip(top5_prob, top5_idx):\n    print(f\"{labels[idx]}: {prob.item():.2%}\")\n```\n\n---\n\n## Part 3: Object Detection\n\n### **YOLO (You Only Look Once):**\n\n```python\nimport cv2\nfrom ultralytics import YOLO\n\n# Load YOLOv8\nmodel = YOLO('yolov8n.pt')  # nano model\n\n# Detect objects\nimg = cv2.imread('street.jpg')\nresults = model(img)\n\n# Draw boxes\nfor result in results:\n    boxes = result.boxes\n    for box in boxes:\n        # Get box coordinates\n        x1, y1, x2, y2 = map(int, box.xyxy[0])\n        confidence = box.conf[0]\n        class_id = int(box.cls[0])\n        label = model.names[class_id]\n        \n        # Draw\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        cv2.putText(img, f'{label} {confidence:.2f}',\n                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,\n                   0.5, (0, 255, 0), 2)\n\ncv2.imshow('Detections', img)\ncv2.waitKey(0)\n```\n\n### **Training Custom YOLO:**\n\n```python\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO('yolov8n.pt')\n\n# Train on custom dataset\n# Dataset structure:\n# dataset/\n#   ‚îú‚îÄ‚îÄ train/\n#   ‚îÇ   ‚îú‚îÄ‚îÄ images/\n#   ‚îÇ   ‚îî‚îÄ‚îÄ labels/\n#   ‚îî‚îÄ‚îÄ val/\n#       ‚îú‚îÄ‚îÄ images/\n#       ‚îî‚îÄ‚îÄ labels/\n\nmodel.train(\n    data='dataset.yaml',  # Dataset config\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    name='custom_yolo'\n)\n\n# Validate\nmetrics = model.val()\n\n# Export\nmodel.export(format='onnx')\n```\n\n### **Faster R-CNN:**\n\n```python\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Load pre-trained model\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify for custom classes\nnum_classes = 10  # Your number of classes + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Training loop\nimport torch\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    for images, targets in train_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    \n    lr_scheduler.step()\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {losses.item():.4f}')\n```\n\n---\n\n## Part 4: Image Segmentation\n\n### **U-Net Architecture:**\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.enc1 = self.conv_block(in_channels, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n        \n        # Decoder\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n        self.dec4 = self.conv_block(1024, 512)\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.dec3 = self.conv_block(512, 256)\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.dec2 = self.conv_block(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.dec1 = self.conv_block(128, 64)\n        \n        self.out = nn.Conv2d(64, out_channels, 1)\n        \n        self.pool = nn.MaxPool2d(2)\n    \n    def conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool(enc1))\n        enc3 = self.enc3(self.pool(enc2))\n        enc4 = self.enc4(self.pool(enc3))\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(self.pool(enc4))\n        \n        # Decoder\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat([dec4, enc4], dim=1)\n        dec4 = self.dec4(dec4)\n        \n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat([dec3, enc3], dim=1)\n        dec3 = self.dec3(dec3)\n        \n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat([dec2, enc2], dim=1)\n        dec2 = self.dec2(dec2)\n        \n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat([dec1, enc1], dim=1)\n        dec1 = self.dec1(dec1)\n        \n        return torch.sigmoid(self.out(dec1))\n\n# Usage\nmodel = UNet(in_channels=3, out_channels=1)\nx = torch.randn(1, 3, 256, 256)\noutput = model(x)\nprint(output.shape)  # [1, 1, 256, 256]\n```\n\n---\n\n## Part 5: Face Recognition\n\n### **FaceNet Implementation:**\n\n```python\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\nimport torch\nfrom PIL import Image\n\n# Initialize models\nmtcnn = MTCNN(image_size=160, margin=0, device='cuda')\nresnet = InceptionResnetV1(pretrained='vggface2').eval()\n\n# Detect and extract faces\nimg = Image.open('person.jpg')\nfaces = mtcnn(img)  # Returns cropped faces\n\n# Get embeddings\nif faces is not None:\n    embeddings = resnet(faces.unsqueeze(0))\n    print(f\"Embedding shape: {embeddings.shape}\")  # [1, 512]\n\n# Face verification\ndef verify_faces(img1_path, img2_path, threshold=0.6):\n    \"\"\"Check if two images contain the same person\"\"\"\n    img1 = Image.open(img1_path)\n    img2 = Image.open(img2_path)\n    \n    face1 = mtcnn(img1)\n    face2 = mtcnn(img2)\n    \n    if face1 is None or face2 is None:\n        return False, \"Face not detected\"\n    \n    emb1 = resnet(face1.unsqueeze(0))\n    emb2 = resnet(face2.unsqueeze(0))\n    \n    # Calculate distance\n    distance = (emb1 - emb2).norm().item()\n    \n    is_same = distance < threshold\n    return is_same, distance\n\n# Example\nsame, dist = verify_faces('person1.jpg', 'person2.jpg')\nprint(f\"Same person: {same}, Distance: {dist:.4f}\")\n```\n\n---\n\n## Part 6: Transfer Learning\n\n### **Fine-tuning Pre-trained Models:**\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms, datasets\nfrom torch.utils.data import DataLoader\n\n# Load pre-trained model\nmodel = models.resnet50(pretrained=True)\n\n# Freeze all layers except final\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)  # 10 classes\n\n# Only train final layer initially\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load dataset\ntrain_dataset = datasets.ImageFolder('data/train', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# Phase 1: Train only final layer\nfor epoch in range(5):\n    model.train()\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n\n# Phase 2: Unfreeze and fine-tune all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n# Continue training...\n```\n\n---\n\n## Part 7: Production Deployment\n\n### **FastAPI Computer Vision API:**\n\n```python\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import JSONResponse\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport io\nimport json\n\napp = FastAPI(title=\"Computer Vision API\")\n\n# Load model at startup\n@app.on_event(\"startup\")\nasync def load_model():\n    global model, transform, labels\n    \n    model = models.resnet50(pretrained=True)\n    model.eval()\n    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    with open('imagenet_labels.json') as f:\n        labels = json.load(f)\n\n@app.post(\"/classify\")\nasync def classify_image(file: UploadFile = File(...)):\n    \"\"\"Classify uploaded image\"\"\"\n    # Read image\n    contents = await file.read()\n    image = Image.open(io.BytesIO(contents)).convert('RGB')\n    \n    # Transform\n    img_tensor = transform(image).unsqueeze(0)\n    \n    # Predict\n    with torch.no_grad():\n        outputs = model(img_tensor)\n        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n        top5_prob, top5_idx = torch.topk(probabilities, 5)\n    \n    # Format results\n    results = []\n    for prob, idx in zip(top5_prob, top5_idx):\n        results.append({\n            \"class\": labels[idx.item()],\n            \"probability\": prob.item()\n        })\n    \n    return JSONResponse(content={\"predictions\": results})\n\n@app.post(\"/detect\")\nasync def detect_objects(file: UploadFile = File(...)):\n    \"\"\"Detect objects in image\"\"\"\n    # Load YOLO or Faster R-CNN\n    # Similar implementation\n    pass\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n---\n\n## üìö Complete Projects\n\n### **Project 1: Image Classifier**\n- Build from scratch\n- Train on custom dataset\n- Deploy as web app\n\n### **Project 2: Object Detector**\n- Train YOLOv8\n- Real-time detection\n- Mobile deployment\n\n### **Project 3: Face Recognition System**\n- Face detection + recognition\n- Database of known faces\n- Production API\n\n---\n\n## üíº Career Path\n\n- **CV Engineer:** $110K-$150K\n- **Senior CV Engineer:** $150K-$200K\n- **Computer Vision Architect:** $200K-$300K+\n\n---\n\n## üéØ Learning Path (6-8 weeks)\n\n```\nWeek 1-2: CNNs & Image Processing\nWeek 3-4: Object Detection\nWeek 5-6: Segmentation & Face Recognition\nWeek 7-8: Transfer Learning & Production\n```\n\n---\n\n*Computer Vision Complete Guide*\n*From Image Processing to Production Systems*\n*Career-ready in 6-8 weeks* üöÄ\n\n"}