{"id":"data-engineering","title":"ðŸ—„ï¸ Data Engineering Zero to Hero","content":"# Data Engineering: Zero to Hero Guide\n## Complete Modern Data Engineering Mastery\n\n---\n\n## ðŸ“š Table of Contents\n\n1. [Introduction to Data Engineering](#introduction)\n2. [Data Engineering Fundamentals](#fundamentals)\n3. [Programming Languages for DE](#programming)\n4. [Data Collection & Ingestion](#ingestion)\n5. [Data Storage Solutions](#storage)\n6. [Data Processing](#processing)\n7. [Data Pipelines & Orchestration](#pipelines)\n8. [Big Data Technologies](#bigdata)\n9. [Cloud Data Platforms](#cloud)\n10. [Stream Processing](#streaming)\n11. [Data Quality & Governance](#quality)\n12. [Monitoring & Observability](#monitoring)\n13. [Data Security & Compliance](#security)\n14. [Best Practices & Patterns](#best-practices)\n15. [Real-World Projects](#projects)\n16. [Career Path & Skills](#career)\n\n---\n\n## ðŸŽ¯ Introduction to Data Engineering {#introduction}\n\n### What is Data Engineering?\n\n**Data Engineering** is the practice of designing and building systems for collecting, storing, and analyzing data at scale. Data engineers create the infrastructure and tools that enable data scientists and analysts to work with data effectively.\n\n```\nTraditional Software Engineering\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  User Input â†’ Processing â†’ Output  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nData Engineering\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Raw Data â†’ Collect â†’ Transform â†’ Store â†’ Process â†’ Insights    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### The Data Engineering Lifecycle\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Generation â”‚â”€â”€â”€â–¶â”‚  Ingestion   â”‚â”€â”€â”€â–¶â”‚ Transformationâ”‚â”€â”€â–¶â”‚   Serving    â”‚\nâ”‚             â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚\nâ”‚ â€¢ IoT       â”‚    â”‚ â€¢ Batch      â”‚    â”‚ â€¢ ETL/ELT    â”‚    â”‚ â€¢ Analytics  â”‚\nâ”‚ â€¢ APIs      â”‚    â”‚ â€¢ Streaming  â”‚    â”‚ â€¢ Data Prep  â”‚    â”‚ â€¢ ML/AI      â”‚\nâ”‚ â€¢ Databases â”‚    â”‚ â€¢ Real-time  â”‚    â”‚ â€¢ Validation â”‚    â”‚ â€¢ Dashboards â”‚\nâ”‚ â€¢ Files     â”‚    â”‚ â€¢ Scheduled  â”‚    â”‚ â€¢ Enrichment â”‚    â”‚ â€¢ APIs       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                   â”‚   Storage    â”‚\n                   â”‚              â”‚\n                   â”‚ â€¢ Data Lake  â”‚\n                   â”‚ â€¢ Warehouse  â”‚\n                   â”‚ â€¢ NoSQL      â”‚\n                   â”‚ â€¢ Object     â”‚\n                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Why Data Engineering Matters\n\n- **Data Volume Growth**: 90% of world's data created in last 2 years\n- **Business Intelligence**: Data-driven decisions are crucial\n- **Machine Learning**: Models need clean, reliable data\n- **Real-time Insights**: Businesses need instant analytics\n- **Compliance**: GDPR, CCPA require proper data handling\n\n### Data Engineer vs Data Scientist vs Data Analyst\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                Data Engineer                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Focus: Infrastructure, pipelines, systems             â”‚\nâ”‚  Skills: Python, SQL, Docker, Cloud, ETL               â”‚\nâ”‚  Tools: Airflow, Spark, Kafka, AWS/GCP/Azure          â”‚\nâ”‚  Goal: Enable data access and reliability              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                Data Scientist                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Focus: Models, algorithms, insights                   â”‚\nâ”‚  Skills: Python, R, Statistics, ML, Domain expertise  â”‚\nâ”‚  Tools: Jupyter, scikit-learn, TensorFlow, PyTorch    â”‚\nâ”‚  Goal: Extract insights and build predictive models    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                Data Analyst                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Focus: Reporting, visualization, business insights    â”‚\nâ”‚  Skills: SQL, Excel, Tableau, Power BI, Statistics    â”‚\nâ”‚  Tools: Tableau, Power BI, Looker, Excel              â”‚\nâ”‚  Goal: Answer business questions with data             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸ“ Data Engineering Fundamentals {#fundamentals}\n\n### Core Concepts\n\n**1. ETL vs ELT**\n\n```\nETL (Extract, Transform, Load)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Extract â”‚â”€â”€â”€â–¶â”‚ Transform â”‚â”€â”€â”€â–¶â”‚   Load   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ€¢ Traditional approach\nâ€¢ Transform before loading\nâ€¢ Good for structured data\nâ€¢ Less storage needed\n\nELT (Extract, Load, Transform)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Extract â”‚â”€â”€â”€â–¶â”‚   Load   â”‚â”€â”€â”€â–¶â”‚ Transform â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ€¢ Modern cloud approach\nâ€¢ Load first, transform later\nâ€¢ Good for big data\nâ€¢ More flexible\n```\n\n**2. Batch vs Stream Processing**\n\n```\nBatch Processing\nâ€¢ Process large volumes of data at scheduled intervals\nâ€¢ High latency, high throughput\nâ€¢ Examples: Daily reports, monthly analytics\nâ€¢ Tools: Apache Spark, Hadoop MapReduce\n\nStream Processing\nâ€¢ Process data as it arrives in real-time\nâ€¢ Low latency, continuous processing\nâ€¢ Examples: Fraud detection, real-time analytics\nâ€¢ Tools: Apache Kafka, Apache Storm, Apache Flink\n```\n\n**3. Data Warehouse vs Data Lake**\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Data Warehouse              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Structured data only                â”‚\nâ”‚  â€¢ Schema-on-write                     â”‚\nâ”‚  â€¢ High performance queries           â”‚\nâ”‚  â€¢ Expensive storage                   â”‚\nâ”‚  â€¢ Examples: Snowflake, Redshift      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚             Data Lake                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â€¢ Any data format (structured,       â”‚\nâ”‚    semi-structured, unstructured)     â”‚\nâ”‚  â€¢ Schema-on-read                     â”‚\nâ”‚  â€¢ Cheap storage                      â”‚\nâ”‚  â€¢ Examples: AWS S3, Azure Data Lake  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Data Types and Formats\n\n**Structured Data**\n- Relational databases (MySQL, PostgreSQL)\n- CSV files\n- Excel spreadsheets\n\n**Semi-Structured Data**\n- JSON documents\n- XML files\n- Parquet files\n- Avro files\n\n**Unstructured Data**\n- Text documents\n- Images, videos\n- Log files\n- Social media posts\n\n### Common File Formats\n\n```python\n# CSV - Simple, widely supported\nimport pandas as pd\ndf = pd.read_csv('data.csv')\n\n# JSON - Human readable, nested data\nimport json\nwith open('data.json', 'r') as f:\n    data = json.load(f)\n\n# Parquet - Columnar, compressed, fast\ndf = pd.read_parquet('data.parquet')\ndf.to_parquet('output.parquet')\n\n# Avro - Schema evolution, streaming\nfrom avro import schema, datafile, io\n\n# Delta Lake - ACID transactions, versioning\nimport delta\ndf.write.format(\"delta\").save(\"delta-table\")\n```\n\n---\n\n## ðŸ’» Programming Languages for Data Engineering {#programming}\n\n### Python for Data Engineering\n\n**Why Python?**\n- Extensive libraries (pandas, numpy, boto3)\n- Great for data manipulation\n- Cloud SDK support\n- Easy to learn and read\n\n**Essential Python Libraries**\n\n```python\n# Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Database Connectivity\nimport psycopg2  # PostgreSQL\nimport pymongo   # MongoDB\nimport sqlite3   # SQLite\n\n# Cloud SDKs\nimport boto3     # AWS\nfrom google.cloud import bigquery  # GCP\nfrom azure.identity import DefaultAzureCredential  # Azure\n\n# Data Processing\nimport dask      # Parallel computing\nfrom pyspark.sql import SparkSession  # Big data processing\n\n# API and Web\nimport requests  # HTTP requests\nfrom flask import Flask  # Web framework\nimport asyncio   # Async programming\n\n# Data Validation\nfrom pydantic import BaseModel\nimport great_expectations as ge\n```\n\n**Example: Data Pipeline with Python**\n\n```python\nimport pandas as pd\nimport requests\nfrom sqlalchemy import create_engine\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass DataPipeline:\n    def __init__(self, db_url):\n        self.engine = create_engine(db_url)\n    \n    def extract_from_api(self, url):\n        \"\"\"Extract data from REST API\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            data = response.json()\n            logger.info(f\"Extracted {len(data)} records from API\")\n            return pd.DataFrame(data)\n        except Exception as e:\n            logger.error(f\"API extraction failed: {e}\")\n            raise\n    \n    def transform_data(self, df):\n        \"\"\"Transform and clean data\"\"\"\n        # Remove duplicates\n        df = df.drop_duplicates()\n        \n        # Handle missing values\n        df = df.fillna(0)\n        \n        # Data type conversions\n        df['created_at'] = pd.to_datetime(df['created_at'])\n        df['amount'] = pd.to_numeric(df['amount'])\n        \n        # Add calculated fields\n        df['year'] = df['created_at'].dt.year\n        df['month'] = df['created_at'].dt.month\n        \n        logger.info(f\"Transformed {len(df)} records\")\n        return df\n    \n    def load_to_database(self, df, table_name):\n        \"\"\"Load data to database\"\"\"\n        try:\n            df.to_sql(\n                table_name, \n                self.engine, \n                if_exists='append', \n                index=False\n            )\n            logger.info(f\"Loaded {len(df)} records to {table_name}\")\n        except Exception as e:\n            logger.error(f\"Database load failed: {e}\")\n            raise\n    \n    def run_pipeline(self, api_url, table_name):\n        \"\"\"Run complete ETL pipeline\"\"\"\n        logger.info(\"Starting ETL pipeline\")\n        \n        # Extract\n        raw_data = self.extract_from_api(api_url)\n        \n        # Transform\n        clean_data = self.transform_data(raw_data)\n        \n        # Load\n        self.load_to_database(clean_data, table_name)\n        \n        logger.info(\"ETL pipeline completed successfully\")\n\n# Usage\npipeline = DataPipeline('postgresql://user:pass@localhost:5432/db')\npipeline.run_pipeline('https://api.example.com/data', 'sales_data')\n```\n\n### SQL Mastery for Data Engineers\n\n**Advanced SQL Concepts**\n\n```sql\n-- Window Functions\nSELECT \n    customer_id,\n    order_date,\n    amount,\n    SUM(amount) OVER (PARTITION BY customer_id ORDER BY order_date) as running_total,\n    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rank_by_amount,\n    LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_amount\nFROM orders;\n\n-- Common Table Expressions (CTEs)\nWITH monthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) as month,\n        SUM(amount) as total_sales\n    FROM orders\n    GROUP BY DATE_TRUNC('month', order_date)\n),\nsales_growth AS (\n    SELECT \n        month,\n        total_sales,\n        LAG(total_sales) OVER (ORDER BY month) as previous_month_sales,\n        (total_sales - LAG(total_sales) OVER (ORDER BY month)) / \n        LAG(total_sales) OVER (ORDER BY month) * 100 as growth_rate\n    FROM monthly_sales\n)\nSELECT * FROM sales_growth WHERE growth_rate > 10;\n\n-- Data Quality Checks\nSELECT \n    'duplicate_check' as check_name,\n    COUNT(*) - COUNT(DISTINCT email) as duplicate_count\nFROM users\nUNION ALL\nSELECT \n    'null_check' as check_name,\n    COUNT(*) as null_count\nFROM users \nWHERE email IS NULL\nUNION ALL\nSELECT \n    'format_check' as check_name,\n    COUNT(*) as invalid_email_count\nFROM users \nWHERE email NOT LIKE '%@%.%';\n\n-- Dynamic SQL for Data Profiling\nSELECT \n    column_name,\n    data_type,\n    COUNT(*) as total_records,\n    COUNT(column_name) as non_null_records,\n    COUNT(DISTINCT column_name) as distinct_values,\n    COUNT(*) - COUNT(column_name) as null_count,\n    (COUNT(*) - COUNT(column_name)) * 100.0 / COUNT(*) as null_percentage\nFROM information_schema.columns \nCROSS JOIN your_table\nGROUP BY column_name, data_type;\n```\n\n### Bash/Shell Scripting\n\n```bash\n#!/bin/bash\n# data_pipeline.sh - Automated data processing script\n\n# Set variables\nDATA_DIR=\"/data/raw\"\nPROCESSED_DIR=\"/data/processed\"\nLOG_FILE=\"/logs/pipeline.log\"\nDATE=$(date +%Y%m%d)\n\n# Function to log messages\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" >> $LOG_FILE\n}\n\n# Check if directories exist\nif [ ! -d \"$DATA_DIR\" ]; then\n    log_message \"ERROR: Data directory $DATA_DIR does not exist\"\n    exit 1\nfi\n\nlog_message \"Starting data pipeline for $DATE\"\n\n# Download data from S3\naws s3 sync s3://my-bucket/raw-data/ $DATA_DIR/ \\\n    --exclude \"*\" \\\n    --include \"*.csv\" \\\n    --include \"*.json\"\n\nif [ $? -eq 0 ]; then\n    log_message \"Data download completed successfully\"\nelse\n    log_message \"ERROR: Data download failed\"\n    exit 1\nfi\n\n# Process each CSV file\nfor file in $DATA_DIR/*.csv; do\n    if [ -f \"$file\" ]; then\n        filename=$(basename \"$file\" .csv)\n        log_message \"Processing file: $filename\"\n        \n        # Run Python processing script\n        python3 /scripts/process_data.py \"$file\" \"$PROCESSED_DIR/${filename}_processed.csv\"\n        \n        if [ $? -eq 0 ]; then\n            log_message \"Successfully processed: $filename\"\n            # Move original file to archive\n            mv \"$file\" \"/data/archive/${filename}_${DATE}.csv\"\n        else\n            log_message \"ERROR: Failed to process $filename\"\n        fi\n    fi\ndone\n\n# Upload processed data\naws s3 sync $PROCESSED_DIR/ s3://my-bucket/processed-data/ \\\n    --delete\n\nlog_message \"Data pipeline completed for $DATE\"\n\n# Cleanup old files (keep last 7 days)\nfind /data/archive -name \"*.csv\" -mtime +7 -delete\nfind /logs -name \"*.log\" -mtime +30 -delete\n```\n\n---\n\n## ðŸ“¥ Data Collection & Ingestion {#ingestion}\n\n### Data Sources\n\n**1. Databases**\n- OLTP systems (PostgreSQL, MySQL, Oracle)\n- NoSQL (MongoDB, Cassandra, DynamoDB)\n- Data warehouses (Snowflake, Redshift, BigQuery)\n\n**2. APIs and Web Services**\n- REST APIs\n- GraphQL endpoints\n- Web scraping\n- Real-time feeds\n\n**3. Files and Object Storage**\n- CSV, JSON, Parquet files\n- AWS S3, GCS, Azure Blob\n- FTP/SFTP servers\n- Network drives\n\n**4. Streaming Sources**\n- Message queues (Apache Kafka, RabbitMQ)\n- IoT sensors\n- Application logs\n- Social media streams\n\n### Batch Data Ingestion\n\n**Example: Database to Data Lake ETL**\n\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport boto3\nfrom datetime import datetime, timedelta\nimport logging\n\nclass BatchIngestionPipeline:\n    def __init__(self, source_db_url, s3_bucket):\n        self.source_engine = create_engine(source_db_url)\n        self.s3_client = boto3.client('s3')\n        self.bucket = s3_bucket\n        self.logger = logging.getLogger(__name__)\n    \n    def extract_incremental_data(self, table_name, timestamp_column, last_run_time):\n        \"\"\"Extract data incrementally based on timestamp\"\"\"\n        query = f\"\"\"\n        SELECT * FROM {table_name}\n        WHERE {timestamp_column} > '{last_run_time}'\n        AND {timestamp_column} <= '{datetime.now()}'\n        ORDER BY {timestamp_column}\n        \"\"\"\n        \n        df = pd.read_sql(query, self.source_engine)\n        self.logger.info(f\"Extracted {len(df)} records from {table_name}\")\n        return df\n    \n    def upload_to_s3(self, df, s3_key):\n        \"\"\"Upload DataFrame to S3 as Parquet\"\"\"\n        # Convert to parquet in memory\n        parquet_buffer = df.to_parquet(index=False)\n        \n        # Upload to S3\n        self.s3_client.put_object(\n            Bucket=self.bucket,\n            Key=s3_key,\n            Body=parquet_buffer\n        )\n        \n        self.logger.info(f\"Uploaded data to s3://{self.bucket}/{s3_key}\")\n    \n    def run_incremental_load(self, table_config):\n        \"\"\"Run incremental load for a table\"\"\"\n        table_name = table_config['name']\n        timestamp_column = table_config['timestamp_column']\n        last_run_time = table_config['last_run_time']\n        \n        # Extract incremental data\n        df = self.extract_incremental_data(\n            table_name, \n            timestamp_column, \n            last_run_time\n        )\n        \n        if not df.empty:\n            # Partition by date for better query performance\n            df['partition_date'] = pd.to_datetime(df[timestamp_column]).dt.date\n            \n            # Upload each partition separately\n            for date, group in df.groupby('partition_date'):\n                s3_key = f\"raw-data/{table_name}/year={date.year}/month={date.month:02d}/day={date.day:02d}/data.parquet\"\n                self.upload_to_s3(group.drop('partition_date', axis=1), s3_key)\n        \n        return df[timestamp_column].max() if not df.empty else last_run_time\n\n# Configuration\ntables_config = [\n    {\n        'name': 'orders',\n        'timestamp_column': 'created_at',\n        'last_run_time': '2024-01-01 00:00:00'\n    },\n    {\n        'name': 'customers', \n        'timestamp_column': 'updated_at',\n        'last_run_time': '2024-01-01 00:00:00'\n    }\n]\n\n# Run pipeline\npipeline = BatchIngestionPipeline(\n    'postgresql://user:pass@localhost:5432/ecommerce',\n    'my-data-lake-bucket'\n)\n\nfor table_config in tables_config:\n    last_processed_time = pipeline.run_incremental_load(table_config)\n    # Update last_run_time for next execution\n    table_config['last_run_time'] = last_processed_time\n```\n\n### Real-time Data Ingestion with Apache Kafka\n\n**Producer Example**\n\n```python\nfrom kafka import KafkaProducer\nimport json\nimport time\nimport random\nfrom datetime import datetime\n\nclass EventProducer:\n    def __init__(self, bootstrap_servers, topic):\n        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n            key_serializer=lambda k: k.encode('utf-8') if k else None\n        )\n        self.topic = topic\n    \n    def produce_user_events(self):\n        \"\"\"Simulate user activity events\"\"\"\n        user_actions = ['login', 'view_product', 'add_to_cart', 'purchase', 'logout']\n        \n        while True:\n            event = {\n                'user_id': random.randint(1, 1000),\n                'action': random.choice(user_actions),\n                'timestamp': datetime.now().isoformat(),\n                'session_id': f\"session_{random.randint(10000, 99999)}\",\n                'product_id': random.randint(1, 100) if random.random() > 0.3 else None,\n                'amount': round(random.uniform(10, 1000), 2) if random.random() > 0.8 else None\n            }\n            \n            # Use user_id as partition key for ordering\n            self.producer.send(\n                self.topic, \n                key=str(event['user_id']),\n                value=event\n            )\n            \n            print(f\"Produced: {event}\")\n            time.sleep(random.uniform(0.1, 2))  # Random delay\n\n# Usage\nproducer = EventProducer(['localhost:9092'], 'user-events')\nproducer.produce_user_events()\n```\n\n**Consumer Example**\n\n```python\nfrom kafka import KafkaConsumer\nimport json\nfrom datetime import datetime\nimport boto3\n\nclass EventConsumer:\n    def __init__(self, bootstrap_servers, topic, group_id):\n        self.consumer = KafkaConsumer(\n            topic,\n            bootstrap_servers=bootstrap_servers,\n            group_id=group_id,\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            auto_offset_reset='latest',\n            enable_auto_commit=True\n        )\n        self.s3_client = boto3.client('s3')\n        self.buffer = []\n        self.buffer_size = 100\n        \n    def process_batch(self, events):\n        \"\"\"Process a batch of events\"\"\"\n        # Transform events\n        processed_events = []\n        for event in events:\n            processed_event = {\n                **event,\n                'processed_at': datetime.now().isoformat(),\n                'hour': datetime.fromisoformat(event['timestamp']).hour\n            }\n            processed_events.append(processed_event)\n        \n        # Save to S3\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        key = f\"events/raw/date={datetime.now().date()}/batch_{timestamp}.json\"\n        \n        self.s3_client.put_object(\n            Bucket='my-events-bucket',\n            Key=key,\n            Body=json.dumps(processed_events, indent=2)\n        )\n        \n        print(f\"Processed batch of {len(events)} events to s3://my-events-bucket/{key}\")\n    \n    def consume_events(self):\n        \"\"\"Consume events with micro-batching\"\"\"\n        for message in self.consumer:\n            event = message.value\n            self.buffer.append(event)\n            \n            # Process when buffer is full\n            if len(self.buffer) >= self.buffer_size:\n                self.process_batch(self.buffer)\n                self.buffer = []\n\n# Usage\nconsumer = EventConsumer(['localhost:9092'], 'user-events', 'analytics-group')\nconsumer.consume_events()\n```\n\n### API Data Ingestion\n\n```python\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport time\nimport logging\nfrom typing import List, Dict, Optional\n\nclass APIDataIngestion:\n    def __init__(self, base_url: str, api_key: str):\n        self.base_url = base_url\n        self.headers = {\n            'Authorization': f'Bearer {api_key}',\n            'Content-Type': 'application/json'\n        }\n        self.session = requests.Session()\n        self.session.headers.update(self.headers)\n        self.logger = logging.getLogger(__name__)\n    \n    def make_request(self, endpoint: str, params: Dict = None, retries: int = 3) -> Optional[Dict]:\n        \"\"\"Make HTTP request with retry logic\"\"\"\n        url = f\"{self.base_url}/{endpoint}\"\n        \n        for attempt in range(retries):\n            try:\n                response = self.session.get(url, params=params, timeout=30)\n                response.raise_for_status()\n                return response.json()\n            \n            except requests.exceptions.RequestException as e:\n                self.logger.warning(f\"Request failed (attempt {attempt + 1}): {e}\")\n                if attempt == retries - 1:\n                    self.logger.error(f\"All retries failed for {url}\")\n                    raise\n                time.sleep(2 ** attempt)  # Exponential backoff\n    \n    def paginated_fetch(self, endpoint: str, params: Dict = None) -> List[Dict]:\n        \"\"\"Fetch all data using pagination\"\"\"\n        all_data = []\n        page = 1\n        params = params or {}\n        \n        while True:\n            params['page'] = page\n            params['limit'] = 100  # API limit\n            \n            response = self.make_request(endpoint, params)\n            \n            if not response or not response.get('data'):\n                break\n            \n            all_data.extend(response['data'])\n            \n            # Check if more pages exist\n            if len(response['data']) < params['limit']:\n                break\n            \n            page += 1\n            time.sleep(0.1)  # Rate limiting\n        \n        self.logger.info(f\"Fetched {len(all_data)} records from {endpoint}\")\n        return all_data\n    \n    def fetch_orders(self, start_date: str, end_date: str) -> pd.DataFrame:\n        \"\"\"Fetch orders data for date range\"\"\"\n        params = {\n            'start_date': start_date,\n            'end_date': end_date,\n            'status': 'all'\n        }\n        \n        orders = self.paginated_fetch('orders', params)\n        \n        if orders:\n            df = pd.DataFrame(orders)\n            df['created_at'] = pd.to_datetime(df['created_at'])\n            df['order_value'] = pd.to_numeric(df['order_value'])\n            return df\n        \n        return pd.DataFrame()\n    \n    def incremental_sync(self, last_sync_time: datetime) -> pd.DataFrame:\n        \"\"\"Perform incremental sync since last sync time\"\"\"\n        end_time = datetime.now()\n        \n        df = self.fetch_orders(\n            start_date=last_sync_time.isoformat(),\n            end_date=end_time.isoformat()\n        )\n        \n        return df\n\n# Usage\napi_client = APIDataIngestion('https://api.ecommerce.com/v1', 'your-api-key')\n\n# Full historical load\nhistorical_data = api_client.fetch_orders('2024-01-01', '2024-12-31')\n\n# Incremental load\nlast_sync = datetime.now() - timedelta(hours=1)\nincremental_data = api_client.incremental_sync(last_sync)\n```\n\n---\n\n## ðŸ—„ï¸ Data Storage Solutions {#storage}\n\n### Relational Databases (OLTP)\n\n**PostgreSQL for Transactional Data**\n\n```sql\n-- Create optimized table for high-frequency inserts\nCREATE TABLE user_events (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL,\n    event_type VARCHAR(50) NOT NULL,\n    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    properties JSONB,\n    session_id UUID NOT NULL\n);\n\n-- Indexes for performance\nCREATE INDEX CONCURRENTLY idx_user_events_user_id_timestamp \nON user_events (user_id, timestamp DESC);\n\nCREATE INDEX CONCURRENTLY idx_user_events_event_type \nON user_events (event_type);\n\n-- JSONB index for property queries\nCREATE INDEX CONCURRENTLY idx_user_events_properties_gin \nON user_events USING GIN (properties);\n\n-- Partitioning for large tables\nCREATE TABLE user_events_2024_01 PARTITION OF user_events\nFOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n-- Bulk insert optimization\nCOPY user_events (user_id, event_type, properties, session_id)\nFROM '/path/to/data.csv'\nWITH (FORMAT csv, HEADER true);\n```\n\n### NoSQL Databases\n\n**MongoDB for Document Storage**\n\n```python\nfrom pymongo import MongoClient\nfrom datetime import datetime\nimport pandas as pd\n\nclass MongoDataStore:\n    def __init__(self, connection_string, database_name):\n        self.client = MongoClient(connection_string)\n        self.db = self.client[database_name]\n    \n    def insert_product_catalog(self, products_df):\n        \"\"\"Insert product catalog with nested data\"\"\"\n        products = []\n        for _, row in products_df.iterrows():\n            product = {\n                \"product_id\": row['product_id'],\n                \"name\": row['name'],\n                \"category\": {\n                    \"main\": row['main_category'],\n                    \"sub\": row['sub_category']\n                },\n                \"pricing\": {\n                    \"base_price\": float(row['base_price']),\n                    \"currency\": row['currency'],\n                    \"discount\": float(row.get('discount', 0))\n                },\n                \"attributes\": {\n                    \"brand\": row.get('brand'),\n                    \"color\": row.get('color'),\n                    \"size\": row.get('size'),\n                    \"weight\": row.get('weight')\n                },\n                \"inventory\": {\n                    \"stock_level\": int(row['stock_level']),\n                    \"warehouse\": row['warehouse'],\n                    \"reorder_level\": int(row.get('reorder_level', 10))\n                },\n                \"metadata\": {\n                    \"created_at\": datetime.now(),\n                    \"updated_at\": datetime.now(),\n                    \"active\": bool(row.get('active', True))\n                }\n            }\n            products.append(product)\n        \n        # Bulk insert\n        result = self.db.products.insert_many(products)\n        return len(result.inserted_ids)\n    \n    def create_indexes(self):\n        \"\"\"Create indexes for optimal query performance\"\"\"\n        # Compound index for category filtering\n        self.db.products.create_index([\n            (\"category.main\", 1),\n            (\"category.sub\", 1),\n            (\"pricing.base_price\", 1)\n        ])\n        \n        # Text index for search\n        self.db.products.create_index([\n            (\"name\", \"text\"),\n            (\"attributes.brand\", \"text\")\n        ])\n        \n        # TTL index for automatic deletion\n        self.db.user_sessions.create_index(\n            [(\"created_at\", 1)], \n            expireAfterSeconds=3600  # 1 hour\n        )\n    \n    def aggregate_sales_data(self):\n        \"\"\"Complex aggregation pipeline\"\"\"\n        pipeline = [\n            # Match orders from last 30 days\n            {\n                \"$match\": {\n                    \"order_date\": {\n                        \"$gte\": datetime.now() - pd.Timedelta(days=30)\n                    }\n                }\n            },\n            # Unwind order items\n            {\"$unwind\": \"$items\"},\n            \n            # Group by product and calculate metrics\n            {\n                \"$group\": {\n                    \"_id\": \"$items.product_id\",\n                    \"total_quantity\": {\"$sum\": \"$items.quantity\"},\n                    \"total_revenue\": {\"$sum\": {\"$multiply\": [\"$items.price\", \"$items.quantity\"]}},\n                    \"order_count\": {\"$sum\": 1},\n                    \"avg_order_size\": {\"$avg\": \"$items.quantity\"}\n                }\n            },\n            \n            # Lookup product details\n            {\n                \"$lookup\": {\n                    \"from\": \"products\",\n                    \"localField\": \"_id\",\n                    \"foreignField\": \"product_id\",\n                    \"as\": \"product_info\"\n                }\n            },\n            \n            # Sort by revenue\n            {\"$sort\": {\"total_revenue\": -1}},\n            \n            # Limit to top 100\n            {\"$limit\": 100}\n        ]\n        \n        return list(self.db.orders.aggregate(pipeline))\n\n# Usage\nmongo_store = MongoDataStore('mongodb://localhost:27017', 'ecommerce')\nmongo_store.create_indexes()\n\n# Insert product data\nproducts_df = pd.read_csv('products.csv')\ninserted_count = mongo_store.insert_product_catalog(products_df)\nprint(f\"Inserted {inserted_count} products\")\n```\n\n### Data Warehouses\n\n**Snowflake Data Warehouse**\n\n```python\nimport snowflake.connector\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nclass SnowflakeWarehouse:\n    def __init__(self, account, user, password, warehouse, database, schema):\n        self.connection_params = {\n            'account': account,\n            'user': user,\n            'password': password,\n            'warehouse': warehouse,\n            'database': database,\n            'schema': schema\n        }\n        \n        self.engine = create_engine(\n            f'snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}'\n        )\n    \n    def create_fact_table(self):\n        \"\"\"Create optimized fact table\"\"\"\n        create_table_sql = \"\"\"\n        CREATE OR REPLACE TABLE fact_sales (\n            sale_id NUMBER AUTOINCREMENT PRIMARY KEY,\n            date_key DATE NOT NULL,\n            customer_key NUMBER NOT NULL,\n            product_key NUMBER NOT NULL,\n            store_key NUMBER NOT NULL,\n            quantity NUMBER(10,2),\n            unit_price NUMBER(10,2),\n            total_amount NUMBER(12,2),\n            discount_amount NUMBER(10,2),\n            tax_amount NUMBER(10,2),\n            created_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\n        CLUSTER BY (date_key, customer_key);\n        \"\"\"\n        \n        with self.engine.connect() as conn:\n            conn.execute(create_table_sql)\n    \n    def bulk_load_from_s3(self, s3_path, table_name):\n        \"\"\"Load data from S3 using COPY command\"\"\"\n        copy_sql = f\"\"\"\n        COPY INTO {table_name}\n        FROM '{s3_path}'\n        CREDENTIALS = (AWS_KEY_ID = 'your_key' AWS_SECRET_KEY = 'your_secret')\n        FILE_FORMAT = (TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1)\n        ON_ERROR = 'SKIP_FILE';\n        \"\"\"\n        \n        with self.engine.connect() as conn:\n            result = conn.execute(copy_sql)\n            return result.fetchone()\n    \n    def create_materialized_view(self):\n        \"\"\"Create materialized view for fast analytics\"\"\"\n        create_view_sql = \"\"\"\n        CREATE OR REPLACE MATERIALIZED VIEW mv_daily_sales AS\n        SELECT \n            date_key,\n            COUNT(*) as transaction_count,\n            SUM(total_amount) as total_revenue,\n            AVG(total_amount) as avg_order_value,\n            SUM(quantity) as total_units_sold,\n            COUNT(DISTINCT customer_key) as unique_customers\n        FROM fact_sales\n        GROUP BY date_key;\n        \"\"\"\n        \n        with self.engine.connect() as conn:\n            conn.execute(create_view_sql)\n    \n    def optimize_table(self, table_name):\n        \"\"\"Optimize table performance\"\"\"\n        optimize_sql = f\"\"\"\n        -- Analyze table statistics\n        ALTER TABLE {table_name} SET TABLE_STATS = TRUE;\n        \n        -- Cluster table\n        ALTER TABLE {table_name} CLUSTER BY (date_key, customer_key);\n        \n        -- Optimize micro-partitions\n        ALTER TABLE {table_name} SET ENABLE_SCHEMA_EVOLUTION = TRUE;\n        \"\"\"\n        \n        with self.engine.connect() as conn:\n            conn.execute(optimize_sql)\n\n# Usage\nwarehouse = SnowflakeWarehouse(\n    account='your-account.snowflakecomputing.com',\n    user='your-user',\n    password='your-password',\n    warehouse='COMPUTE_WH',\n    database='ANALYTICS',\n    schema='SALES'\n)\n\nwarehouse.create_fact_table()\nwarehouse.bulk_load_from_s3('s3://my-bucket/sales-data/', 'fact_sales')\nwarehouse.create_materialized_view()\n```\n\n### Data Lakes\n\n**AWS S3 Data Lake Architecture**\n\n```python\nimport boto3\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nclass S3DataLake:\n    def __init__(self, bucket_name, aws_access_key_id, aws_secret_access_key, region):\n        self.bucket_name = bucket_name\n        self.s3_client = boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            region_name=region\n        )\n        \n    def write_partitioned_data(self, df, table_name, partition_cols):\n        \"\"\"Write data with partitioning for better query performance\"\"\"\n        \n        # Add partition columns if not exist\n        if 'year' not in df.columns and 'date' in df.columns:\n            df['year'] = pd.to_datetime(df['date']).dt.year\n        if 'month' not in df.columns and 'date' in df.columns:\n            df['month'] = pd.to_datetime(df['date']).dt.month\n        if 'day' not in df.columns and 'date' in df.columns:\n            df['day'] = pd.to_datetime(df['date']).dt.day\n        \n        # Group by partition columns\n        for partition_values, group_df in df.groupby(partition_cols):\n            # Create partition path\n            partition_path = '/'.join([\n                f\"{col}={val}\" for col, val in zip(partition_cols, partition_values)\n            ])\n            \n            # Create S3 key\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n            s3_key = f\"data-lake/{table_name}/{partition_path}/data_{timestamp}.parquet\"\n            \n            # Convert to parquet and upload\n            parquet_buffer = group_df.drop(partition_cols, axis=1).to_parquet(index=False)\n            \n            self.s3_client.put_object(\n                Bucket=self.bucket_name,\n                Key=s3_key,\n                Body=parquet_buffer,\n                Metadata={\n                    'table': table_name,\n                    'partition': partition_path,\n                    'records': str(len(group_df)),\n                    'created_at': datetime.now().isoformat()\n                }\n            )\n            \n            print(f\"Written {len(group_df)} records to s3://{self.bucket_name}/{s3_key}\")\n    \n    def read_partitioned_data(self, table_name, filters=None):\n        \"\"\"Read partitioned data with optional filters\"\"\"\n        prefix = f\"data-lake/{table_name}/\"\n        \n        # List all objects\n        paginator = self.s3_client.get_paginator('list_objects_v2')\n        page_iterator = paginator.paginate(Bucket=self.bucket_name, Prefix=prefix)\n        \n        dataframes = []\n        \n        for page in page_iterator:\n            if 'Contents' in page:\n                for obj in page['Contents']:\n                    key = obj['Key']\n                    \n                    # Apply partition filters\n                    if filters and not self._matches_filters(key, filters):\n                        continue\n                    \n                    # Read parquet file\n                    response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n                    df = pd.read_parquet(response['Body'])\n                    \n                    # Extract partition values from path\n                    partition_info = self._extract_partition_info(key)\n                    for col, val in partition_info.items():\n                        df[col] = val\n                    \n                    dataframes.append(df)\n        \n        if dataframes:\n            return pd.concat(dataframes, ignore_index=True)\n        else:\n            return pd.DataFrame()\n    \n    def _matches_filters(self, key, filters):\n        \"\"\"Check if S3 key matches partition filters\"\"\"\n        for col, val in filters.items():\n            if f\"{col}={val}\" not in key:\n                return False\n        return True\n    \n    def _extract_partition_info(self, key):\n        \"\"\"Extract partition information from S3 key\"\"\"\n        parts = key.split('/')\n        partition_info = {}\n        \n        for part in parts:\n            if '=' in part:\n                col, val = part.split('=', 1)\n                # Try to convert to appropriate type\n                try:\n                    if val.isdigit():\n                        val = int(val)\n                    elif val.replace('.', '').isdigit():\n                        val = float(val)\n                except:\n                    pass\n                partition_info[col] = val\n        \n        return partition_info\n    \n    def create_athena_table(self, table_name, schema, partition_cols):\n        \"\"\"Create Athena table for querying data lake\"\"\"\n        \n        # Build column definitions\n        columns = []\n        for col, dtype in schema.items():\n            if col not in partition_cols:\n                athena_type = self._pandas_to_athena_type(dtype)\n                columns.append(f\"`{col}` {athena_type}\")\n        \n        columns_str = ',\\n  '.join(columns)\n        \n        # Build partition definitions\n        partitions = []\n        for col in partition_cols:\n            dtype = schema.get(col, 'string')\n            athena_type = self._pandas_to_athena_type(dtype)\n            partitions.append(f\"`{col}` {athena_type}\")\n        \n        partitions_str = ',\\n  '.join(partitions)\n        \n        create_table_sql = f\"\"\"\n        CREATE EXTERNAL TABLE `{table_name}` (\n          {columns_str}\n        )\n        PARTITIONED BY (\n          {partitions_str}\n        )\n        STORED AS PARQUET\n        LOCATION 's3://{self.bucket_name}/data-lake/{table_name}/'\n        TBLPROPERTIES ('has_encrypted_data'='false');\n        \"\"\"\n        \n        return create_table_sql\n    \n    def _pandas_to_athena_type(self, pandas_dtype):\n        \"\"\"Convert pandas dtype to Athena data type\"\"\"\n        dtype_mapping = {\n            'object': 'string',\n            'int64': 'bigint',\n            'int32': 'int',\n            'float64': 'double',\n            'float32': 'float',\n            'bool': 'boolean',\n            'datetime64[ns]': 'timestamp'\n        }\n        return dtype_mapping.get(str(pandas_dtype), 'string')\n\n# Usage\ndata_lake = S3DataLake('my-data-lake-bucket', 'access-key', 'secret-key', 'us-east-1')\n\n# Write partitioned sales data\nsales_df = pd.read_csv('sales_data.csv')\ndata_lake.write_partitioned_data(sales_df, 'sales', ['year', 'month', 'day'])\n\n# Read data with filters\nfiltered_data = data_lake.read_partitioned_data(\n    'sales', \n    filters={'year': 2024, 'month': 1}\n)\n\n# Generate Athena table DDL\nschema = {\n    'sale_id': 'int64',\n    'customer_id': 'int64', \n    'amount': 'float64',\n    'date': 'datetime64[ns]',\n    'year': 'int64',\n    'month': 'int64',\n    'day': 'int64'\n}\n\ncreate_table_sql = data_lake.create_athena_table('sales', schema, ['year', 'month', 'day'])\nprint(create_table_sql)\n```\n\n---\n\n## âš™ï¸ Data Processing {#processing}\n\n### Apache Spark for Big Data Processing\n\n**Setting Up PySpark**\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport os\n\n# Configure Spark\nos.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n\ndef create_spark_session(app_name=\"DataProcessing\"):\n    \"\"\"Create optimized Spark session\"\"\"\n    spark = SparkSession.builder \\\n        .appName(app_name) \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n        .getOrCreate()\n    \n    # Set log level\n    spark.sparkContext.setLogLevel(\"WARN\")\n    return spark\n\nclass SparkDataProcessor:\n    def __init__(self):\n        self.spark = create_spark_session()\n    \n    def read_from_sources(self):\n        \"\"\"Read from multiple data sources\"\"\"\n        \n        # Read from CSV\n        customers_df = self.spark.read \\\n            .option(\"header\", \"true\") \\\n            .option(\"inferSchema\", \"true\") \\\n            .csv(\"s3a://my-bucket/customers/*.csv\")\n        \n        # Read from Parquet (partitioned)\n        orders_df = self.spark.read \\\n            .parquet(\"s3a://my-bucket/orders/year=2024/\")\n        \n        # Read from Delta Lake\n        products_df = self.spark.read \\\n            .format(\"delta\") \\\n            .load(\"s3a://my-bucket/products/\")\n        \n        # Read from Database\n        user_events_df = self.spark.read \\\n            .format(\"jdbc\") \\\n            .option(\"url\", \"jdbc:postgresql://localhost:5432/events\") \\\n            .option(\"dbtable\", \"user_events\") \\\n            .option(\"user\", \"user\") \\\n            .option(\"password\", \"password\") \\\n            .option(\"driver\", \"org.postgresql.Driver\") \\\n            .load()\n        \n        return customers_df, orders_df, products_df, user_events_df\n    \n    def advanced_transformations(self, df):\n        \"\"\"Demonstrate advanced Spark transformations\"\"\"\n        \n        # Window functions\n        from pyspark.sql.window import Window\n        \n        window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n        \n        transformed_df = df \\\n            .withColumn(\"row_number\", row_number().over(window_spec)) \\\n            .withColumn(\"running_total\", sum(\"amount\").over(window_spec)) \\\n            .withColumn(\"prev_order_amount\", lag(\"amount\", 1).over(window_spec)) \\\n            .withColumn(\"days_since_last_order\", \n                       datediff(col(\"order_date\"), \n                               lag(\"order_date\", 1).over(window_spec))) \\\n            .withColumn(\"customer_lifetime_value\", \n                       sum(\"amount\").over(Window.partitionBy(\"customer_id\"))) \\\n            .withColumn(\"order_rank\", \n                       rank().over(Window.partitionBy(\"customer_id\")\n                                  .orderBy(desc(\"amount\"))))\n        \n        # Complex aggregations\n        summary_df = transformed_df.groupBy(\"customer_id\") \\\n            .agg(\n                count(\"*\").alias(\"total_orders\"),\n                sum(\"amount\").alias(\"total_spent\"),\n                avg(\"amount\").alias(\"avg_order_value\"),\n                max(\"order_date\").alias(\"last_order_date\"),\n                min(\"order_date\").alias(\"first_order_date\"),\n                stddev(\"amount\").alias(\"order_value_stddev\"),\n                collect_list(\"product_category\").alias(\"categories_purchased\")\n            )\n        \n        # Add calculated columns\n        final_df = summary_df \\\n            .withColumn(\"customer_tenure_days\",\n                       datediff(col(\"last_order_date\"), col(\"first_order_date\"))) \\\n            .withColumn(\"order_frequency\", \n                       col(\"total_orders\") / (col(\"customer_tenure_days\") + 1)) \\\n            .withColumn(\"customer_segment\",\n                       when(col(\"total_spent\") > 1000, \"Premium\")\n                       .when(col(\"total_spent\") > 500, \"Standard\")\n                       .otherwise(\"Basic\"))\n        \n        return final_df\n    \n    def data_quality_checks(self, df, table_name):\n        \"\"\"Comprehensive data quality validation\"\"\"\n        \n        total_records = df.count()\n        \n        quality_report = {\n            'table_name': table_name,\n            'total_records': total_records,\n            'checks': {}\n        }\n        \n        # Null checks\n        for column in df.columns:\n            null_count = df.filter(col(column).isNull()).count()\n            null_percentage = (null_count / total_records) * 100\n            \n            quality_report['checks'][f'{column}_null_check'] = {\n                'null_count': null_count,\n                'null_percentage': round(null_percentage, 2),\n                'status': 'PASS' if null_percentage < 5 else 'FAIL'\n            }\n        \n        # Duplicate checks\n        distinct_records = df.distinct().count()\n        duplicate_count = total_records - distinct_records\n        \n        quality_report['checks']['duplicate_check'] = {\n            'duplicate_count': duplicate_count,\n            'duplicate_percentage': round((duplicate_count / total_records) * 100, 2),\n            'status': 'PASS' if duplicate_count == 0 else 'WARN'\n        }\n        \n        # Outlier detection (for numeric columns)\n        numeric_columns = [field.name for field in df.schema.fields \n                          if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))]\n        \n        for column in numeric_columns:\n            stats = df.select(column).describe().collect()\n            mean_val = float([row for row in stats if row[0] == 'mean'][0][1])\n            stddev_val = float([row for row in stats if row[0] == 'stddev'][0][1])\n            \n            # Count values beyond 3 standard deviations\n            outlier_count = df.filter(\n                (col(column) > mean_val + 3 * stddev_val) |\n                (col(column) < mean_val - 3 * stddev_val)\n            ).count()\n            \n            quality_report['checks'][f'{column}_outlier_check'] = {\n                'outlier_count': outlier_count,\n                'outlier_percentage': round((outlier_count / total_records) * 100, 2),\n                'status': 'PASS' if outlier_count < total_records * 0.01 else 'WARN'\n            }\n        \n        return quality_report\n    \n    def optimize_and_write(self, df, output_path, partition_columns=None):\n        \"\"\"Optimize DataFrame and write with best practices\"\"\"\n        \n        # Optimize partitioning\n        if partition_columns:\n            # Repartition based on partition columns for better performance\n            df = df.repartition(*[col(c) for c in partition_columns])\n        else:\n            # Auto-optimize partitions\n            df = df.coalesce(200)  # Reasonable number of partitions\n        \n        # Cache if the DataFrame will be used multiple times\n        df.cache()\n        \n        # Write with optimization\n        writer = df.write \\\n            .mode(\"overwrite\") \\\n            .option(\"compression\", \"snappy\") \\\n            .option(\"maxRecordsPerFile\", 1000000)\n        \n        if partition_columns:\n            writer = writer.partitionBy(*partition_columns)\n        \n        writer.parquet(output_path)\n        \n        print(f\"Data written to {output_path} with {df.rdd.getNumPartitions()} partitions\")\n\n# Example usage\nprocessor = SparkDataProcessor()\n\n# Read data\ncustomers_df, orders_df, products_df, events_df = processor.read_from_sources()\n\n# Join datasets\nenriched_orders = orders_df \\\n    .join(customers_df, \"customer_id\", \"inner\") \\\n    .join(products_df, \"product_id\", \"inner\")\n\n# Apply transformations\ncustomer_analytics = processor.advanced_transformations(enriched_orders)\n\n# Data quality checks\nquality_report = processor.data_quality_checks(customer_analytics, \"customer_analytics\")\nprint(f\"Data Quality Report: {quality_report}\")\n\n# Write optimized output\nprocessor.optimize_and_write(\n    customer_analytics, \n    \"s3a://output-bucket/customer-analytics/\",\n    partition_columns=[\"customer_segment\"]\n)\n\n# Don't forget to stop Spark session\nprocessor.spark.stop()\n```\n\n### Pandas for Medium-Scale Processing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport multiprocessing as mp\nfrom functools import partial\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass PandasDataProcessor:\n    def __init__(self, n_jobs=-1):\n        self.n_jobs = n_jobs if n_jobs > 0 else mp.cpu_count()\n    \n    def chunk_processor(self, chunk, processing_func):\n        \"\"\"Process a chunk of data\"\"\"\n        return processing_func(chunk)\n    \n    def parallel_apply(self, df, processing_func, chunk_size=10000):\n        \"\"\"Apply function in parallel using multiprocessing\"\"\"\n        \n        # Split DataFrame into chunks\n        chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n        \n        # Process chunks in parallel\n        with mp.Pool(processes=self.n_jobs) as pool:\n            func = partial(self.chunk_processor, processing_func=processing_func)\n            results = pool.map(func, chunks)\n        \n        # Combine results\n        return pd.concat(results, ignore_index=True)\n    \n    def memory_efficient_read_csv(self, filepath, chunk_size=50000, processing_func=None):\n        \"\"\"Read large CSV files in chunks\"\"\"\n        chunks = []\n        \n        for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n            if processing_func:\n                chunk = processing_func(chunk)\n            chunks.append(chunk)\n        \n        return pd.concat(chunks, ignore_index=True)\n    \n    def advanced_feature_engineering(self, df):\n        \"\"\"Advanced feature engineering with pandas\"\"\"\n        \n        # Date features\n        if 'timestamp' in df.columns:\n            df['timestamp'] = pd.to_datetime(df['timestamp'])\n            df['hour'] = df['timestamp'].dt.hour\n            df['day_of_week'] = df['timestamp'].dt.dayofweek\n            df['month'] = df['timestamp'].dt.month\n            df['quarter'] = df['timestamp'].dt.quarter\n            df['is_weekend'] = df['day_of_week'].isin([5, 6])\n            df['is_business_hour'] = df['hour'].between(9, 17)\n        \n        # Rolling statistics\n        if 'amount' in df.columns and 'customer_id' in df.columns:\n            df = df.sort_values(['customer_id', 'timestamp'])\n            \n            # Rolling aggregations\n            df['rolling_avg_7d'] = df.groupby('customer_id')['amount'].transform(\n                lambda x: x.rolling(window=7, min_periods=1).mean()\n            )\n            df['rolling_std_7d'] = df.groupby('customer_id')['amount'].transform(\n                lambda x: x.rolling(window=7, min_periods=1).std()\n            )\n            \n            # Lag features\n            df['amount_lag_1'] = df.groupby('customer_id')['amount'].shift(1)\n            df['amount_lag_7'] = df.groupby('customer_id')['amount'].shift(7)\n            \n            # Customer metrics\n            df['customer_total_spent'] = df.groupby('customer_id')['amount'].transform('sum')\n            df['customer_avg_amount'] = df.groupby('customer_id')['amount'].transform('mean')\n            df['customer_transaction_count'] = df.groupby('customer_id')['amount'].transform('count')\n        \n        # Categorical encoding\n        categorical_columns = df.select_dtypes(include=['object']).columns\n        \n        for col in categorical_columns:\n            if col not in ['customer_id', 'timestamp']:  # Skip ID columns\n                # Frequency encoding\n                freq_map = df[col].value_counts().to_dict()\n                df[f'{col}_frequency'] = df[col].map(freq_map)\n                \n                # Target encoding (if target column exists)\n                if 'target' in df.columns:\n                    target_mean = df.groupby(col)['target'].mean()\n                    df[f'{col}_target_encoded'] = df[col].map(target_mean)\n        \n        return df\n    \n    def detect_anomalies(self, df, columns):\n        \"\"\"Detect anomalies using statistical methods\"\"\"\n        anomalies = pd.DataFrame()\n        \n        for col in columns:\n            if df[col].dtype in ['int64', 'float64']:\n                # Z-score method\n                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n                z_anomalies = df[z_scores > 3].copy()\n                z_anomalies['anomaly_type'] = 'z_score'\n                z_anomalies['anomaly_column'] = col\n                z_anomalies['anomaly_score'] = z_scores[z_scores > 3]\n                \n                # IQR method\n                Q1 = df[col].quantile(0.25)\n                Q3 = df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                \n                iqr_anomalies = df[(df[col] < lower_bound) | (df[col] > upper_bound)].copy()\n                iqr_anomalies['anomaly_type'] = 'iqr'\n                iqr_anomalies['anomaly_column'] = col\n                iqr_anomalies['anomaly_score'] = np.where(\n                    iqr_anomalies[col] < lower_bound,\n                    lower_bound - iqr_anomalies[col],\n                    iqr_anomalies[col] - upper_bound\n                )\n                \n                anomalies = pd.concat([anomalies, z_anomalies, iqr_anomalies])\n        \n        return anomalies.drop_duplicates()\n    \n    def create_data_summary(self, df):\n        \"\"\"Create comprehensive data summary\"\"\"\n        summary = {\n            'basic_info': {\n                'rows': len(df),\n                'columns': len(df.columns),\n                'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024),\n                'dtypes': df.dtypes.value_counts().to_dict()\n            },\n            'missing_data': {},\n            'numeric_summary': {},\n            'categorical_summary': {}\n        }\n        \n        # Missing data analysis\n        missing_data = df.isnull().sum()\n        summary['missing_data'] = {\n            col: {\n                'count': int(missing_data[col]),\n                'percentage': round((missing_data[col] / len(df)) * 100, 2)\n            }\n            for col in missing_data[missing_data > 0].index\n        }\n        \n        # Numeric columns summary\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        for col in numeric_cols:\n            summary['numeric_summary'][col] = {\n                'mean': float(df[col].mean()),\n                'std': float(df[col].std()),\n                'min': float(df[col].min()),\n                'max': float(df[col].max()),\n                'median': float(df[col].median()),\n                'q25': float(df[col].quantile(0.25)),\n                'q75': float(df[col].quantile(0.75)),\n                'unique_count': int(df[col].nunique()),\n                'zero_count': int((df[col] == 0).sum())\n            }\n        \n        # Categorical columns summary\n        categorical_cols = df.select_dtypes(include=['object']).columns\n        for col in categorical_cols:\n            value_counts = df[col].value_counts().head(10)\n            summary['categorical_summary'][col] = {\n                'unique_count': int(df[col].nunique()),\n                'most_frequent': str(value_counts.index[0]) if len(value_counts) > 0 else None,\n                'most_frequent_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,\n                'top_values': value_counts.to_dict()\n            }\n        \n        return summary\n\n# Usage example\nprocessor = PandasDataProcessor(n_jobs=4)\n\n# Read large file efficiently\ndef preprocessing_func(chunk):\n    # Clean data\n    chunk = chunk.dropna()\n    chunk['amount'] = pd.to_numeric(chunk['amount'], errors='coerce')\n    return chunk\n\ndf = processor.memory_efficient_read_csv(\n    'large_file.csv', \n    chunk_size=50000,\n    processing_func=preprocessing_func\n)\n\n# Feature engineering\ndf_engineered = processor.advanced_feature_engineering(df)\n\n# Parallel processing for complex operations\ndef complex_processing(chunk):\n    # Simulate complex processing\n    chunk['complex_feature'] = chunk['amount'] * np.log(chunk['customer_transaction_count'] + 1)\n    return chunk\n\ndf_processed = processor.parallel_apply(df_engineered, complex_processing)\n\n# Detect anomalies\nanomalies = processor.detect_anomalies(df_processed, ['amount', 'complex_feature'])\n\n# Generate summary\ndata_summary = processor.create_data_summary(df_processed)\nprint(\"Data Summary:\", data_summary)\n```\n\n---\n\n## ðŸ”„ Data Pipelines & Orchestration {#pipelines}\n\n### Apache Airflow for Workflow Orchestration\n\n**Setting Up Airflow DAGs**\n\n```python\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.postgres_operator import PostgresOperator\nfrom airflow.sensors.s3_key_sensor import S3KeySensor\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.hooks.S3_hook import S3Hook\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport logging\n\n# Default arguments for DAG\ndefault_args = {\n    'owner': 'data-engineering-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'catchup': False\n}\n\n# Create DAG\ndag = DAG(\n    'sales_data_pipeline',\n    default_args=default_args,\n    description='Complete sales data processing pipeline',\n    schedule_interval=timedelta(hours=1),  # Run hourly\n    max_active_runs=1,\n    tags=['sales', 'etl', 'production']\n)\n\ndef extract_from_source_db(**context):\n    \"\"\"Extract data from source database\"\"\"\n    execution_date = context['execution_date']\n    start_time = execution_date - timedelta(hours=1)\n    end_time = execution_date\n    \n    # Connect to source database\n    postgres_hook = PostgresHook(postgres_conn_id='source_db')\n    \n    query = f\"\"\"\n    SELECT \n        order_id,\n        customer_id,\n        product_id,\n        quantity,\n        price,\n        order_date,\n        status\n    FROM orders \n    WHERE order_date BETWEEN '{start_time}' AND '{end_time}'\n    AND status IN ('completed', 'shipped')\n    \"\"\"\n    \n    df = postgres_hook.get_pandas_df(query)\n    \n    # Save to S3 staging area\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    \n    csv_buffer = df.to_csv(index=False)\n    s3_key = f\"staging/orders/{execution_date.strftime('%Y/%m/%d/%H')}/orders.csv\"\n    \n    s3_hook.load_string(\n        string_data=csv_buffer,\n        key=s3_key,\n        bucket_name='data-pipeline-bucket',\n        replace=True\n    )\n    \n    logging.info(f\"Extracted {len(df)} records to s3://data-pipeline-bucket/{s3_key}\")\n    return s3_key\n\ndef validate_data_quality(**context):\n    \"\"\"Validate data quality before processing\"\"\"\n    s3_key = context['task_instance'].xcom_pull(task_ids='extract_data')\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    \n    # Read data from S3\n    csv_content = s3_hook.read_key(s3_key, bucket_name='data-pipeline-bucket')\n    df = pd.read_csv(pd.StringIO(csv_content))\n    \n    # Data quality checks\n    quality_issues = []\n    \n    # Check for nulls in critical columns\n    critical_columns = ['order_id', 'customer_id', 'product_id', 'quantity', 'price']\n    for col in critical_columns:\n        null_count = df[col].isnull().sum()\n        if null_count > 0:\n            quality_issues.append(f\"Found {null_count} null values in {col}\")\n    \n    # Check for duplicates\n    duplicate_count = df.duplicated(subset=['order_id']).sum()\n    if duplicate_count > 0:\n        quality_issues.append(f\"Found {duplicate_count} duplicate order_ids\")\n    \n    # Check data ranges\n    if (df['quantity'] <= 0).any():\n        quality_issues.append(\"Found invalid quantity values (<= 0)\")\n    \n    if (df['price'] <= 0).any():\n        quality_issues.append(\"Found invalid price values (<= 0)\")\n    \n    # Fail if critical issues found\n    if quality_issues:\n        error_msg = \"Data quality issues found: \" + \"; \".join(quality_issues)\n        logging.error(error_msg)\n        raise ValueError(error_msg)\n    \n    logging.info(f\"Data quality validation passed for {len(df)} records\")\n    return True\n\ndef transform_data(**context):\n    \"\"\"Transform and enrich data\"\"\"\n    s3_key = context['task_instance'].xcom_pull(task_ids='extract_data')\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    \n    # Read data\n    csv_content = s3_hook.read_key(s3_key, bucket_name='data-pipeline-bucket')\n    df = pd.read_csv(pd.StringIO(csv_content))\n    \n    # Transformations\n    df['total_amount'] = df['quantity'] * df['price']\n    df['order_date'] = pd.to_datetime(df['order_date'])\n    df['order_year'] = df['order_date'].dt.year\n    df['order_month'] = df['order_date'].dt.month\n    df['order_day'] = df['order_date'].dt.day\n    df['order_hour'] = df['order_date'].dt.hour\n    \n    # Enrichment - add customer and product info\n    postgres_hook = PostgresHook(postgres_conn_id='source_db')\n    \n    # Get customer info\n    customer_ids = df['customer_id'].unique().tolist()\n    customer_query = f\"\"\"\n    SELECT customer_id, customer_name, customer_segment, country\n    FROM customers \n    WHERE customer_id IN ({','.join(map(str, customer_ids))})\n    \"\"\"\n    customers_df = postgres_hook.get_pandas_df(customer_query)\n    \n    # Get product info\n    product_ids = df['product_id'].unique().tolist()\n    product_query = f\"\"\"\n    SELECT product_id, product_name, category, subcategory, brand\n    FROM products \n    WHERE product_id IN ({','.join(map(str, product_ids))})\n    \"\"\"\n    products_df = postgres_hook.get_pandas_df(product_query)\n    \n    # Merge with master data\n    df_enriched = df.merge(customers_df, on='customer_id', how='left')\n    df_enriched = df_enriched.merge(products_df, on='product_id', how='left')\n    \n    # Save transformed data\n    execution_date = context['execution_date']\n    transformed_key = f\"transformed/orders/{execution_date.strftime('%Y/%m/%d/%H')}/orders_transformed.parquet\"\n    \n    parquet_buffer = df_enriched.to_parquet(index=False)\n    s3_hook.load_bytes(\n        bytes_data=parquet_buffer,\n        key=transformed_key,\n        bucket_name='data-pipeline-bucket',\n        replace=True\n    )\n    \n    logging.info(f\"Transformed and saved {len(df_enriched)} records to s3://data-pipeline-bucket/{transformed_key}\")\n    return transformed_key\n\ndef load_to_warehouse(**context):\n    \"\"\"Load data to data warehouse\"\"\"\n    transformed_key = context['task_instance'].xcom_pull(task_ids='transform_data')\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    postgres_hook = PostgresHook(postgres_conn_id='warehouse_db')\n    \n    # Read transformed data\n    parquet_content = s3_hook.read_key(transformed_key, bucket_name='data-pipeline-bucket')\n    df = pd.read_parquet(pd.BytesIO(parquet_content))\n    \n    # Create temporary table\n    execution_date = context['execution_date']\n    temp_table = f\"temp_orders_{execution_date.strftime('%Y%m%d_%H%M%S')}\"\n    \n    # Load to temporary table\n    df.to_sql(\n        temp_table,\n        con=postgres_hook.get_sqlalchemy_engine(),\n        if_exists='replace',\n        index=False,\n        method='multi',\n        chunksize=1000\n    )\n    \n    # Merge into main table (upsert)\n    merge_query = f\"\"\"\n    INSERT INTO fact_orders \n    SELECT * FROM {temp_table}\n    ON CONFLICT (order_id) \n    DO UPDATE SET\n        quantity = EXCLUDED.quantity,\n        price = EXCLUDED.price,\n        total_amount = EXCLUDED.total_amount,\n        status = EXCLUDED.status,\n        updated_at = CURRENT_TIMESTAMP;\n    \"\"\"\n    \n    postgres_hook.run(merge_query)\n    \n    # Drop temporary table\n    postgres_hook.run(f\"DROP TABLE {temp_table}\")\n    \n    logging.info(f\"Loaded {len(df)} records to data warehouse\")\n\ndef update_data_catalog(**context):\n    \"\"\"Update data catalog with pipeline metadata\"\"\"\n    execution_date = context['execution_date']\n    \n    # Update metadata table\n    postgres_hook = PostgresHook(postgres_conn_id='warehouse_db')\n    \n    metadata_query = f\"\"\"\n    INSERT INTO pipeline_runs (\n        pipeline_name,\n        run_date,\n        status,\n        records_processed,\n        execution_time\n    ) VALUES (\n        'sales_data_pipeline',\n        '{execution_date}',\n        'SUCCESS',\n        (SELECT COUNT(*) FROM fact_orders WHERE DATE(created_at) = '{execution_date.date()}'),\n        CURRENT_TIMESTAMP\n    )\n    \"\"\"\n    \n    postgres_hook.run(metadata_query)\n    \n    logging.info(\"Updated data catalog with pipeline metadata\")\n\n# Define tasks\n# 1. Wait for source data\nwait_for_data = S3KeySensor(\n    task_id='wait_for_source_data',\n    bucket_name='source-data-bucket',\n    bucket_key='orders/{{ ds }}/ready.flag',\n    aws_conn_id='aws_default',\n    timeout=60 * 30,  # Wait up to 30 minutes\n    poke_interval=60,  # Check every minute\n    dag=dag\n)\n\n# 2. Extract data\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_from_source_db,\n    dag=dag\n)\n\n# 3. Validate data quality\nvalidate_task = PythonOperator(\n    task_id='validate_data_quality',\n    python_callable=validate_data_quality,\n    dag=dag\n)\n\n# 4. Transform data\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\n# 5. Load to warehouse\nload_task = PythonOperator(\n    task_id='load_to_warehouse',\n    python_callable=load_to_warehouse,\n    dag=dag\n)\n\n# 6. Update data catalog\ncatalog_task = PythonOperator(\n    task_id='update_data_catalog',\n    python_callable=update_data_catalog,\n    dag=dag\n)\n\n# 7. Data quality check post-load\npost_load_check = PostgresOperator(\n    task_id='post_load_quality_check',\n    postgres_conn_id='warehouse_db',\n    sql=\"\"\"\n    SELECT \n        CASE \n            WHEN COUNT(*) > 0 THEN 'PASS'\n            ELSE 'FAIL'\n        END as status\n    FROM fact_orders \n    WHERE DATE(created_at) = '{{ ds }}'\n    AND total_amount > 0;\n    \"\"\",\n    dag=dag\n)\n\n# 8. Send success notification\nsuccess_notification = BashOperator(\n    task_id='send_success_notification',\n    bash_command=\"\"\"\n    echo \"Sales data pipeline completed successfully for {{ ds }}\" | \n    mail -s \"Pipeline Success\" data-team@company.com\n    \"\"\",\n    dag=dag\n)\n\n# Define task dependencies\nwait_for_data >> extract_task >> validate_task >> transform_task >> load_task >> catalog_task >> post_load_check >> success_notification\n\n# Set up failure notifications\ndef task_fail_alert(context):\n    \"\"\"Send alert on task failure\"\"\"\n    task_instance = context.get('task_instance')\n    dag_id = context.get('dag').dag_id\n    \n    subject = f\"Airflow Alert: {dag_id} - {task_instance.task_id} Failed\"\n    html_content = f\"\"\"\n    <h2>Task Failed</h2>\n    <p><strong>DAG:</strong> {dag_id}</p>\n    <p><strong>Task:</strong> {task_instance.task_id}</p>\n    <p><strong>Execution Date:</strong> {context.get('execution_date')}</p>\n    <p><strong>Error:</strong> {context.get('exception')}</p>\n    \"\"\"\n    \n    # Send email alert (configure email settings in airflow.cfg)\n    send_email(['data-team@company.com'], subject, html_content)\n\n# Apply failure callback to all tasks\nfor task in dag.tasks:\n    task.on_failure_callback = task_fail_alert\n```\n\n### Modern Orchestration with Prefect\n\n```python\nfrom prefect import flow, task, get_run_logger\nfrom prefect.deployments import Deployment\nfrom prefect.server.schemas.schedules import CronSchedule\nfrom prefect.filesystems import S3\nimport pandas as pd\nimport boto3\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nimport asyncio\n\n@task(retries=3, retry_delay_seconds=60)\ndef extract_sales_data(start_date: str, end_date: str) -> pd.DataFrame:\n    \"\"\"Extract sales data from source system\"\"\"\n    logger = get_run_logger()\n    \n    # Simulate API call to source system\n    import requests\n    \n    response = requests.get(\n        \"https://api.example.com/sales\",\n        params={\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"limit\": 10000\n        },\n        headers={\"Authorization\": \"Bearer your-token\"}\n    )\n    \n    data = response.json()\n    df = pd.DataFrame(data['results'])\n    \n    logger.info(f\"Extracted {len(df)} sales records\")\n    return df\n\n@task\ndef validate_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Validate data quality\"\"\"\n    logger = get_run_logger()\n    \n    # Check for required columns\n    required_columns = ['order_id', 'customer_id', 'amount', 'order_date']\n    missing_columns = set(required_columns) - set(df.columns)\n    \n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n    \n    # Remove duplicates\n    initial_count = len(df)\n    df = df.drop_duplicates(subset=['order_id'])\n    final_count = len(df)\n    \n    if initial_count != final_count:\n        logger.warning(f\"Removed {initial_count - final_count} duplicate records\")\n    \n    # Check for nulls\n    null_counts = df[required_columns].isnull().sum()\n    if null_counts.sum() > 0:\n        logger.warning(f\"Null values found: {null_counts.to_dict()}\")\n        # Remove rows with nulls in critical columns\n        df = df.dropna(subset=required_columns)\n    \n    logger.info(f\"Validation complete. Final record count: {len(df)}\")\n    return df\n\n@task\nasync def enrich_with_customer_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Enrich sales data with customer information\"\"\"\n    logger = get_run_logger()\n    \n    # Simulate async API calls for customer data\n    async def get_customer_info(customer_id):\n        # Simulate async HTTP request\n        await asyncio.sleep(0.1)  # Simulate network delay\n        return {\n            \"customer_id\": customer_id,\n            \"segment\": \"Premium\" if customer_id % 3 == 0 else \"Standard\",\n            \"country\": \"US\" if customer_id % 2 == 0 else \"UK\"\n        }\n    \n    # Get unique customer IDs\n    customer_ids = df['customer_id'].unique()\n    \n    # Fetch customer data concurrently\n    customer_tasks = [get_customer_info(cid) for cid in customer_ids]\n    customer_data = await asyncio.gather(*customer_tasks)\n    \n    # Create customer DataFrame\n    customer_df = pd.DataFrame(customer_data)\n    \n    # Merge with sales data\n    enriched_df = df.merge(customer_df, on='customer_id', how='left')\n    \n    logger.info(f\"Enriched {len(enriched_df)} records with customer data\")\n    return enriched_df\n\n@task\ndef aggregate_metrics(df: pd.DataFrame) -> Dict:\n    \"\"\"Calculate business metrics\"\"\"\n    logger = get_run_logger()\n    \n    metrics = {\n        \"total_revenue\": float(df['amount'].sum()),\n        \"total_orders\": int(len(df)),\n        \"avg_order_value\": float(df['amount'].mean()),\n        \"unique_customers\": int(df['customer_id'].nunique()),\n        \"revenue_by_segment\": df.groupby('segment')['amount'].sum().to_dict(),\n        \"orders_by_country\": df['country'].value_counts().to_dict(),\n        \"processing_timestamp\": datetime.now().isoformat()\n    }\n    \n    logger.info(f\"Calculated metrics: {metrics}\")\n    return metrics\n\n@task\ndef save_to_warehouse(df: pd.DataFrame, table_name: str = \"fact_sales\") -> str:\n    \"\"\"Save data to data warehouse\"\"\"\n    logger = get_run_logger()\n    \n    # Save to S3 (simulating data warehouse)\n    s3_client = boto3.client('s3')\n    \n    # Convert to parquet\n    parquet_buffer = df.to_parquet(index=False)\n    \n    # Create S3 key with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    s3_key = f\"warehouse/{table_name}/{timestamp}.parquet\"\n    \n    # Upload to S3\n    s3_client.put_object(\n        Bucket='data-warehouse-bucket',\n        Key=s3_key,\n        Body=parquet_buffer\n    )\n    \n    logger.info(f\"Saved {len(df)} records to s3://data-warehouse-bucket/{s3_key}\")\n    return s3_key\n\n@task\ndef update_metrics_dashboard(metrics: Dict) -> None:\n    \"\"\"Update business metrics dashboard\"\"\"\n    logger = get_run_logger()\n    \n    # Simulate updating dashboard/monitoring system\n    import json\n    \n    # Save metrics to monitoring system\n    s3_client = boto3.client('s3')\n    \n    metrics_json = json.dumps(metrics, indent=2)\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    s3_client.put_object(\n        Bucket='metrics-bucket',\n        Key=f\"daily-metrics/{timestamp}.json\",\n        Body=metrics_json,\n        ContentType='application/json'\n    )\n    \n    logger.info(\"Updated metrics dashboard\")\n\n@flow(name=\"sales-data-pipeline\", description=\"Complete sales data processing pipeline\")\nasync def sales_pipeline(start_date: str = None, end_date: str = None):\n    \"\"\"Main pipeline flow\"\"\"\n    logger = get_run_logger()\n    \n    # Set default dates if not provided\n    if not start_date:\n        start_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n    if not end_date:\n        end_date = datetime.now().strftime('%Y-%m-%d')\n    \n    logger.info(f\"Starting sales pipeline for {start_date} to {end_date}\")\n    \n    # Extract data\n    raw_data = extract_sales_data(start_date, end_date)\n    \n    # Validate data\n    clean_data = validate_data(raw_data)\n    \n    # Enrich with customer data (async)\n    enriched_data = await enrich_with_customer_data(clean_data)\n    \n    # Calculate metrics\n    metrics = aggregate_metrics(enriched_data)\n    \n    # Save to warehouse and update dashboard in parallel\n    warehouse_path = save_to_warehouse(enriched_data)\n    update_metrics_dashboard(metrics)\n    \n    logger.info(f\"Pipeline completed successfully. Data saved to {warehouse_path}\")\n    \n    return {\n        \"status\": \"success\",\n        \"records_processed\": len(enriched_data),\n        \"warehouse_path\": warehouse_path,\n        \"metrics\": metrics\n    }\n\n# Create deployment\nif __name__ == \"__main__\":\n    # Create S3 storage for flow code\n    s3_storage = S3(bucket_path=\"prefect-flows/sales-pipeline\")\n    \n    deployment = Deployment.build_from_flow(\n        flow=sales_pipeline,\n        name=\"daily-sales-processing\",\n        storage=s3_storage,\n        schedule=CronSchedule(cron=\"0 2 * * *\"),  # Run daily at 2 AM\n        work_pool_name=\"kubernetes-pool\",\n        tags=[\"sales\", \"production\", \"daily\"]\n    )\n    \n    deployment.apply()\n```\n\n---\n\n---\n\n## ðŸ˜ Big Data Technologies {#bigdata}\n\n### Apache Hadoop Ecosystem\n\n**Hadoop Distributed File System (HDFS)**\n\n```bash\n# HDFS Commands\n# List files\nhdfs dfs -ls /user/data/\n\n# Create directory\nhdfs dfs -mkdir /user/data/sales\n\n# Copy file from local to HDFS\nhdfs dfs -put sales_data.csv /user/data/sales/\n\n# Copy file from HDFS to local\nhdfs dfs -get /user/data/sales/processed_data.csv ./\n\n# Check file size and replication\nhdfs dfs -du -h /user/data/sales/\n\n# View file content\nhdfs dfs -cat /user/data/sales/sales_data.csv | head -10\n\n# Set replication factor\nhdfs dfs -setrep 3 /user/data/sales/important_file.csv\n```\n\n**MapReduce with Python (MRJob)**\n\n```python\nfrom mrjob.job import MRJob\nfrom mrjob.step import MRStep\nimport re\n\nclass SalesAnalysis(MRJob):\n    \n    def configure_args(self):\n        super(SalesAnalysis, self).configure_args()\n        self.add_passthru_arg('--min-amount', type=float, default=0,\n                            help='Minimum sale amount to include')\n    \n    def steps(self):\n        return [\n            MRStep(mapper=self.mapper_extract_sales,\n                  reducer=self.reducer_sum_by_customer),\n            MRStep(mapper=self.mapper_format_output,\n                  reducer=self.reducer_top_customers)\n        ]\n    \n    def mapper_extract_sales(self, _, line):\n        \"\"\"Extract customer ID and sale amount\"\"\"\n        # Parse CSV line: customer_id,product_id,amount,date\n        fields = line.strip().split(',')\n        \n        if len(fields) == 4 and fields[0] != 'customer_id':  # Skip header\n            try:\n                customer_id = fields[0]\n                amount = float(fields[2])\n                \n                if amount >= self.options.min_amount:\n                    yield customer_id, amount\n            except ValueError:\n                pass  # Skip invalid lines\n    \n    def reducer_sum_by_customer(self, customer_id, amounts):\n        \"\"\"Sum total sales per customer\"\"\"\n        total_amount = sum(amounts)\n        yield None, (total_amount, customer_id)  # Use None key for global sorting\n    \n    def mapper_format_output(self, _, customer_data):\n        \"\"\"Prepare data for final sorting\"\"\"\n        total_amount, customer_id = customer_data\n        yield total_amount, customer_id\n    \n    def reducer_top_customers(self, total_amount, customer_ids):\n        \"\"\"Output top customers by total sales\"\"\"\n        for customer_id in customer_ids:\n            yield customer_id, total_amount\n\n# Run locally\n# python sales_analysis.py sales_data.csv --min-amount=100\n\n# Run on Hadoop\n# python sales_analysis.py -r hadoop hdfs:///user/data/sales/*.csv --min-amount=100\n```\n\n### Apache Kafka for Stream Processing\n\n**Kafka Producer and Consumer**\n\n```python\nfrom kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient, NewTopic\nfrom kafka.errors import TopicAlreadyExistsError\nimport json\nimport logging\nfrom datetime import datetime\nimport threading\nimport time\n\nclass KafkaManager:\n    def __init__(self, bootstrap_servers=['localhost:9092']):\n        self.bootstrap_servers = bootstrap_servers\n        self.logger = logging.getLogger(__name__)\n    \n    def create_topic(self, topic_name, num_partitions=3, replication_factor=1):\n        \"\"\"Create Kafka topic\"\"\"\n        admin_client = KafkaAdminClient(bootstrap_servers=self.bootstrap_servers)\n        \n        topic = NewTopic(\n            name=topic_name,\n            num_partitions=num_partitions,\n            replication_factor=replication_factor\n        )\n        \n        try:\n            result = admin_client.create_topics([topic])\n            self.logger.info(f\"Created topic: {topic_name}\")\n        except TopicAlreadyExistsError:\n            self.logger.info(f\"Topic {topic_name} already exists\")\n    \n    def produce_events(self, topic_name, events):\n        \"\"\"Produce events to Kafka topic\"\"\"\n        producer = KafkaProducer(\n            bootstrap_servers=self.bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n            key_serializer=lambda k: str(k).encode('utf-8') if k else None,\n            # Performance settings\n            acks='all',  # Wait for all replicas\n            retries=3,\n            batch_size=16384,\n            linger_ms=10,  # Wait 10ms to batch messages\n            buffer_memory=33554432\n        )\n        \n        try:\n            for event in events:\n                # Use customer_id as partition key for ordering\n                key = event.get('customer_id')\n                future = producer.send(topic_name, key=key, value=event)\n                \n                # Optional: wait for confirmation\n                future.get(timeout=10)\n                \n            producer.flush()  # Ensure all messages are sent\n            self.logger.info(f\"Produced {len(events)} events to {topic_name}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to produce events: {e}\")\n        finally:\n            producer.close()\n    \n    def consume_events(self, topic_name, group_id, auto_offset_reset='latest'):\n        \"\"\"Consume events from Kafka topic\"\"\"\n        consumer = KafkaConsumer(\n            topic_name,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id=group_id,\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            auto_offset_reset=auto_offset_reset,\n            enable_auto_commit=True,\n            auto_commit_interval_ms=1000,\n            # Performance settings\n            fetch_min_bytes=1,\n            fetch_max_wait_ms=500,\n            max_poll_records=500\n        )\n        \n        try:\n            self.logger.info(f\"Starting to consume from {topic_name}\")\n            for message in consumer:\n                yield {\n                    'topic': message.topic,\n                    'partition': message.partition,\n                    'offset': message.offset,\n                    'key': message.key.decode('utf-8') if message.key else None,\n                    'value': message.value,\n                    'timestamp': message.timestamp\n                }\n        except KeyboardInterrupt:\n            self.logger.info(\"Consumer interrupted\")\n        finally:\n            consumer.close()\n\n# Real-time Analytics with Kafka Streams (Python equivalent)\nclass RealTimeAnalytics:\n    def __init__(self, kafka_manager):\n        self.kafka_manager = kafka_manager\n        self.running = False\n        self.metrics = {}\n    \n    def process_sales_stream(self):\n        \"\"\"Process sales events in real-time\"\"\"\n        self.running = True\n        \n        # Consume from sales events topic\n        for message in self.kafka_manager.consume_events('sales-events', 'analytics-group'):\n            if not self.running:\n                break\n            \n            event = message['value']\n            self.update_metrics(event)\n            \n            # Produce aggregated metrics every 100 events\n            if len(self.metrics) % 100 == 0:\n                self.produce_metrics()\n    \n    def update_metrics(self, event):\n        \"\"\"Update real-time metrics\"\"\"\n        customer_id = event.get('customer_id')\n        amount = event.get('amount', 0)\n        product_category = event.get('product_category', 'unknown')\n        \n        # Customer metrics\n        if customer_id not in self.metrics:\n            self.metrics[customer_id] = {\n                'total_spent': 0,\n                'order_count': 0,\n                'categories': set()\n            }\n        \n        self.metrics[customer_id]['total_spent'] += amount\n        self.metrics[customer_id]['order_count'] += 1\n        self.metrics[customer_id]['categories'].add(product_category)\n    \n    def produce_metrics(self):\n        \"\"\"Produce aggregated metrics\"\"\"\n        metrics_events = []\n        \n        for customer_id, metrics in self.metrics.items():\n            metric_event = {\n                'customer_id': customer_id,\n                'total_spent': metrics['total_spent'],\n                'order_count': metrics['order_count'],\n                'unique_categories': len(metrics['categories']),\n                'avg_order_value': metrics['total_spent'] / metrics['order_count'],\n                'timestamp': datetime.now().isoformat()\n            }\n            metrics_events.append(metric_event)\n        \n        self.kafka_manager.produce_events('customer-metrics', metrics_events)\n    \n    def stop(self):\n        self.running = False\n\n# Usage Example\nkafka_mgr = KafkaManager(['localhost:9092'])\n\n# Create topics\nkafka_mgr.create_topic('sales-events', num_partitions=6)\nkafka_mgr.create_topic('customer-metrics', num_partitions=3)\n\n# Sample sales events\nsample_events = [\n    {\n        'customer_id': 'cust_001',\n        'product_id': 'prod_123',\n        'product_category': 'Electronics',\n        'amount': 299.99,\n        'timestamp': datetime.now().isoformat()\n    },\n    {\n        'customer_id': 'cust_002', \n        'product_id': 'prod_456',\n        'product_category': 'Clothing',\n        'amount': 79.99,\n        'timestamp': datetime.now().isoformat()\n    }\n]\n\n# Produce events\nkafka_mgr.produce_events('sales-events', sample_events)\n\n# Start real-time analytics\nanalytics = RealTimeAnalytics(kafka_mgr)\nanalytics_thread = threading.Thread(target=analytics.process_sales_stream)\nanalytics_thread.start()\n\n# Let it run for a while\ntime.sleep(60)\n\n# Stop analytics\nanalytics.stop()\nanalytics_thread.join()\n```\n\n### Apache Spark Advanced\n\n**Spark Streaming with Kafka**\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.streaming import StreamingContext\nimport json\n\n# Create Spark session with Kafka support\nspark = SparkSession.builder \\\n    .appName(\"RealTimeAnalytics\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n    .getOrCreate()\n\n# Define schema for incoming events\nevent_schema = StructType([\n    StructField(\"customer_id\", StringType(), True),\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"product_category\", StringType(), True),\n    StructField(\"amount\", DoubleType(), True),\n    StructField(\"timestamp\", TimestampType(), True)\n])\n\n# Read from Kafka stream\nkafka_df = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"sales-events\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .load()\n\n# Parse JSON data\nparsed_df = kafka_df.select(\n    from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n).select(\"data.*\")\n\n# Real-time aggregations\n# 1. Customer spending by window\ncustomer_spending = parsed_df \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        \"customer_id\",\n        window(col(\"timestamp\"), \"5 minutes\", \"1 minute\")  # 5-minute windows, sliding every minute\n    ) \\\n    .agg(\n        sum(\"amount\").alias(\"total_spent\"),\n        count(\"*\").alias(\"transaction_count\"),\n        avg(\"amount\").alias(\"avg_transaction\"),\n        countDistinct(\"product_category\").alias(\"unique_categories\")\n    )\n\n# 2. Product category trends\ncategory_trends = parsed_df \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        \"product_category\",\n        window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\")\n    ) \\\n    .agg(\n        sum(\"amount\").alias(\"total_revenue\"),\n        count(\"*\").alias(\"transaction_count\"),\n        countDistinct(\"customer_id\").alias(\"unique_customers\")\n    )\n\n# Write customer spending to console (for monitoring)\ncustomer_query = customer_spending.writeStream \\\n    .outputMode(\"update\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", False) \\\n    .start()\n\n# Write category trends to Kafka for downstream processing\ncategory_query = category_trends \\\n    .select(to_json(struct(\"*\")).alias(\"value\")) \\\n    .writeStream \\\n    .outputMode(\"update\") \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"topic\", \"category-trends\") \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoint-trends\") \\\n    .start()\n\n# Write detailed events to data lake (S3/HDFS)\nlake_query = parsed_df \\\n    .withColumn(\"year\", year(col(\"timestamp\"))) \\\n    .withColumn(\"month\", month(col(\"timestamp\"))) \\\n    .withColumn(\"day\", dayofmonth(col(\"timestamp\"))) \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"parquet\") \\\n    .option(\"path\", \"s3a://data-lake/sales-events/\") \\\n    .partitionBy(\"year\", \"month\", \"day\") \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoint-lake\") \\\n    .start()\n\n# Wait for streams to finish\ncustomer_query.awaitTermination()\ncategory_query.awaitTermination()\nlake_query.awaitTermination()\n```\n\n---\n\n## â˜ï¸ Cloud Data Platforms {#cloud}\n\n### AWS Data Engineering Stack\n\n**AWS S3 Data Lake with Lambda Processing**\n\n```python\nimport boto3\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport logging\n\n# Lambda function for S3 event processing\ndef lambda_handler(event, context):\n    \"\"\"Process S3 events for incoming data files\"\"\"\n    \n    s3_client = boto3.client('s3')\n    glue_client = boto3.client('glue')\n    sns_client = boto3.client('sns')\n    \n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    \n    try:\n        for record in event['Records']:\n            bucket = record['s3']['bucket']['name']\n            key = record['s3']['object']['key']\n            \n            logger.info(f\"Processing file: s3://{bucket}/{key}\")\n            \n            # Check if it's a data file\n            if key.endswith('.csv') or key.endswith('.json'):\n                \n                # Trigger Glue job for processing\n                job_name = 'sales-data-processing'\n                \n                response = glue_client.start_job_run(\n                    JobName=job_name,\n                    Arguments={\n                        '--s3_input_path': f's3://{bucket}/{key}',\n                        '--s3_output_path': f's3://{bucket}/processed/',\n                        '--job_bookmark': 'job-bookmark-enable'\n                    }\n                )\n                \n                logger.info(f\"Started Glue job {job_name} with run ID: {response['JobRunId']}\")\n                \n                # Send SNS notification\n                sns_client.publish(\n                    TopicArn='arn:aws:sns:us-east-1:123456789012:data-pipeline-notifications',\n                    Subject=f'Data Processing Started',\n                    Message=f'Started processing file: s3://{bucket}/{key}'\n                )\n        \n        return {\n            'statusCode': 200,\n            'body': json.dumps('Files processed successfully')\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error processing files: {str(e)}\")\n        \n        # Send error notification\n        sns_client.publish(\n            TopicArn='arn:aws:sns:us-east-1:123456789012:data-pipeline-alerts',\n            Subject=f'Data Processing Error',\n            Message=f'Error processing files: {str(e)}'\n        )\n        \n        return {\n            'statusCode': 500,\n            'body': json.dumps(f'Error: {str(e)}')\n        }\n\n# AWS Glue ETL Job\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 's3_input_path', 's3_output_path'])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read data from S3\ninput_dyf = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [args['s3_input_path']],\n        \"recurse\": True\n    },\n    format=\"csv\",\n    format_options={\n        \"withHeader\": True,\n        \"separator\": \",\"\n    }\n)\n\n# Data transformations\n# Convert to Spark DataFrame for complex operations\ndf = input_dyf.toDF()\n\n# Data cleaning and transformation\ndf_cleaned = df.filter(df.amount > 0) \\\n    .filter(df.customer_id.isNotNull()) \\\n    .withColumn(\"processed_date\", current_date()) \\\n    .withColumn(\"year\", year(col(\"order_date\"))) \\\n    .withColumn(\"month\", month(col(\"order_date\"))) \\\n    .withColumn(\"quarter\", quarter(col(\"order_date\")))\n\n# Convert back to DynamicFrame\noutput_dyf = DynamicFrame.fromDF(df_cleaned, glueContext, \"cleaned_data\")\n\n# Write to S3 in Parquet format with partitioning\nglueContext.write_dynamic_frame.from_options(\n    frame=output_dyf,\n    connection_type=\"s3\",\n    connection_options={\n        \"path\": args['s3_output_path'],\n        \"partitionKeys\": [\"year\", \"month\"]\n    },\n    format=\"glueparquet\",\n    format_options={\n        \"compression\": \"snappy\"\n    }\n)\n\njob.commit()\n```\n\n**Amazon Kinesis Data Streams**\n\n```python\nimport boto3\nimport json\nimport time\nfrom datetime import datetime\nimport threading\nimport random\n\nclass KinesisDataProcessor:\n    def __init__(self, stream_name, region='us-east-1'):\n        self.stream_name = stream_name\n        self.kinesis_client = boto3.client('kinesis', region_name=region)\n        self.running = False\n    \n    def create_stream(self, shard_count=1):\n        \"\"\"Create Kinesis stream\"\"\"\n        try:\n            self.kinesis_client.create_stream(\n                StreamName=self.stream_name,\n                ShardCount=shard_count\n            )\n            \n            # Wait for stream to be active\n            waiter = self.kinesis_client.get_waiter('stream_exists')\n            waiter.wait(StreamName=self.stream_name)\n            \n            print(f\"Stream {self.stream_name} created successfully\")\n            \n        except self.kinesis_client.exceptions.ResourceInUseException:\n            print(f\"Stream {self.stream_name} already exists\")\n    \n    def put_record(self, data, partition_key):\n        \"\"\"Put single record to stream\"\"\"\n        response = self.kinesis_client.put_record(\n            StreamName=self.stream_name,\n            Data=json.dumps(data),\n            PartitionKey=str(partition_key)\n        )\n        return response\n    \n    def put_records_batch(self, records):\n        \"\"\"Put multiple records to stream\"\"\"\n        kinesis_records = []\n        \n        for record in records:\n            kinesis_record = {\n                'Data': json.dumps(record['data']),\n                'PartitionKey': str(record['partition_key'])\n            }\n            kinesis_records.append(kinesis_record)\n        \n        # Kinesis put_records supports up to 500 records per request\n        batch_size = 500\n        responses = []\n        \n        for i in range(0, len(kinesis_records), batch_size):\n            batch = kinesis_records[i:i+batch_size]\n            \n            response = self.kinesis_client.put_records(\n                Records=batch,\n                StreamName=self.stream_name\n            )\n            responses.append(response)\n        \n        return responses\n    \n    def consume_records(self, shard_iterator_type='TRIM_HORIZON'):\n        \"\"\"Consume records from stream\"\"\"\n        \n        # Get stream description\n        stream_desc = self.kinesis_client.describe_stream(StreamName=self.stream_name)\n        shard_id = stream_desc['StreamDescription']['Shards'][0]['ShardId']\n        \n        # Get shard iterator\n        shard_iterator_response = self.kinesis_client.get_shard_iterator(\n            StreamName=self.stream_name,\n            ShardId=shard_id,\n            ShardIteratorType=shard_iterator_type\n        )\n        \n        shard_iterator = shard_iterator_response['ShardIterator']\n        \n        while self.running:\n            try:\n                # Get records\n                response = self.kinesis_client.get_records(\n                    ShardIterator=shard_iterator,\n                    Limit=100\n                )\n                \n                records = response['Records']\n                \n                if records:\n                    for record in records:\n                        data = json.loads(record['Data'])\n                        print(f\"Processed record: {data}\")\n                        yield data\n                \n                # Update shard iterator\n                shard_iterator = response['NextShardIterator']\n                \n                # Sleep to avoid hitting rate limits\n                time.sleep(1)\n                \n            except Exception as e:\n                print(f\"Error consuming records: {e}\")\n                break\n    \n    def start_consumer(self):\n        \"\"\"Start consuming records in background thread\"\"\"\n        self.running = True\n        consumer_thread = threading.Thread(target=self._consumer_loop)\n        consumer_thread.start()\n        return consumer_thread\n    \n    def stop_consumer(self):\n        \"\"\"Stop consuming records\"\"\"\n        self.running = False\n    \n    def _consumer_loop(self):\n        \"\"\"Consumer loop for background processing\"\"\"\n        for record in self.consume_records():\n            if not self.running:\n                break\n            # Process record here\n            self._process_record(record)\n    \n    def _process_record(self, record):\n        \"\"\"Process individual record\"\"\"\n        # Implement your record processing logic here\n        print(f\"Processing: {record}\")\n\n# Usage example\nkinesis_processor = KinesisDataProcessor('sales-stream')\n\n# Create stream\nkinesis_processor.create_stream(shard_count=2)\n\n# Generate sample data\ndef generate_sales_events(count=100):\n    events = []\n    for i in range(count):\n        event = {\n            'data': {\n                'event_id': f'evt_{i:06d}',\n                'customer_id': f'cust_{random.randint(1, 1000):04d}',\n                'product_id': f'prod_{random.randint(1, 100):03d}',\n                'amount': round(random.uniform(10, 500), 2),\n                'timestamp': datetime.now().isoformat()\n            },\n            'partition_key': f'cust_{random.randint(1, 1000):04d}'\n        }\n        events.append(event)\n    return events\n\n# Send data to stream\nsample_events = generate_sales_events(1000)\nresponses = kinesis_processor.put_records_batch(sample_events)\nprint(f\"Sent {len(sample_events)} events to stream\")\n\n# Start consuming\nconsumer_thread = kinesis_processor.start_consumer()\n\n# Let it run for a while\ntime.sleep(30)\n\n# Stop consumer\nkinesis_processor.stop_consumer()\nconsumer_thread.join()\n```\n\n### Google Cloud Platform (GCP) Data Engineering\n\n**BigQuery Data Processing**\n\n```python\nfrom google.cloud import bigquery\nfrom google.cloud import storage\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport logging\n\nclass GCPDataPipeline:\n    def __init__(self, project_id, dataset_id):\n        self.project_id = project_id\n        self.dataset_id = dataset_id\n        self.bq_client = bigquery.Client(project=project_id)\n        self.storage_client = storage.Client(project=project_id)\n        self.logger = logging.getLogger(__name__)\n    \n    def create_dataset(self):\n        \"\"\"Create BigQuery dataset\"\"\"\n        dataset_id_full = f\"{self.project_id}.{self.dataset_id}\"\n        dataset = bigquery.Dataset(dataset_id_full)\n        dataset.location = \"US\"\n        dataset.description = \"Sales data analytics dataset\"\n        \n        try:\n            dataset = self.bq_client.create_dataset(dataset)\n            self.logger.info(f\"Created dataset {dataset_id_full}\")\n        except Exception as e:\n            if \"Already Exists\" in str(e):\n                self.logger.info(f\"Dataset {dataset_id_full} already exists\")\n            else:\n                raise e\n    \n    def create_tables(self):\n        \"\"\"Create BigQuery tables with optimized schema\"\"\"\n        \n        # Fact table for sales\n        sales_schema = [\n            bigquery.SchemaField(\"sale_id\", \"STRING\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"customer_id\", \"STRING\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"product_id\", \"STRING\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"quantity\", \"INTEGER\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"unit_price\", \"NUMERIC\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"total_amount\", \"NUMERIC\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"sale_date\", \"DATE\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"sale_timestamp\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n            bigquery.SchemaField(\"product_category\", \"STRING\"),\n            bigquery.SchemaField(\"customer_segment\", \"STRING\"),\n            bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"REQUIRED\")\n        ]\n        \n        # Create partitioned and clustered table\n        table_id = f\"{self.project_id}.{self.dataset_id}.fact_sales\"\n        table = bigquery.Table(table_id, schema=sales_schema)\n        \n        # Partition by sale_date for better query performance\n        table.time_partitioning = bigquery.TimePartitioning(\n            type_=bigquery.TimePartitioningType.DAY,\n            field=\"sale_date\"\n        )\n        \n        # Cluster by customer_segment and product_category\n        table.clustering_fields = [\"customer_segment\", \"product_category\"]\n        \n        try:\n            table = self.bq_client.create_table(table)\n            self.logger.info(f\"Created table {table_id}\")\n        except Exception as e:\n            if \"Already Exists\" in str(e):\n                self.logger.info(f\"Table {table_id} already exists\")\n            else:\n                raise e\n    \n    def load_data_from_gcs(self, gcs_uri, table_name, write_disposition=\"WRITE_APPEND\"):\n        \"\"\"Load data from Google Cloud Storage to BigQuery\"\"\"\n        \n        table_id = f\"{self.project_id}.{self.dataset_id}.{table_name}\"\n        \n        job_config = bigquery.LoadJobConfig(\n            source_format=bigquery.SourceFormat.CSV,\n            skip_leading_rows=1,  # Skip header row\n            autodetect=True,\n            write_disposition=write_disposition\n        )\n        \n        load_job = self.bq_client.load_table_from_uri(\n            gcs_uri, table_id, job_config=job_config\n        )\n        \n        load_job.result()  # Wait for job to complete\n        \n        destination_table = self.bq_client.get_table(table_id)\n        self.logger.info(f\"Loaded {destination_table.num_rows} rows to {table_id}\")\n    \n    def run_analytics_query(self):\n        \"\"\"Run complex analytics query\"\"\"\n        \n        query = f\"\"\"\n        WITH daily_sales AS (\n            SELECT \n                sale_date,\n                customer_segment,\n                product_category,\n                COUNT(*) as transaction_count,\n                SUM(total_amount) as daily_revenue,\n                AVG(total_amount) as avg_transaction_value,\n                COUNT(DISTINCT customer_id) as unique_customers\n            FROM `{self.project_id}.{self.dataset_id}.fact_sales`\n            WHERE sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\n            GROUP BY sale_date, customer_segment, product_category\n        ),\n        segment_performance AS (\n            SELECT \n                customer_segment,\n                SUM(daily_revenue) as total_revenue,\n                AVG(daily_revenue) as avg_daily_revenue,\n                SUM(transaction_count) as total_transactions,\n                SUM(unique_customers) as total_customers,\n                COUNT(DISTINCT sale_date) as active_days\n            FROM daily_sales\n            GROUP BY customer_segment\n        )\n        SELECT \n            customer_segment,\n            total_revenue,\n            avg_daily_revenue,\n            total_transactions,\n            total_customers,\n            active_days,\n            total_revenue / total_customers as revenue_per_customer,\n            total_transactions / total_customers as transactions_per_customer\n        FROM segment_performance\n        ORDER BY total_revenue DESC\n        \"\"\"\n        \n        query_job = self.bq_client.query(query)\n        results = query_job.result()\n        \n        # Convert to pandas DataFrame\n        df = results.to_dataframe()\n        return df\n    \n    def create_scheduled_query(self, query_name, query_sql, schedule=\"every 24 hours\"):\n        \"\"\"Create scheduled query for regular data processing\"\"\"\n        \n        from google.cloud import bigquery_datatransfer\n        \n        transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n        \n        parent = transfer_client.common_project_path(self.project_id)\n        \n        transfer_config = {\n            \"destination_dataset_id\": self.dataset_id,\n            \"display_name\": query_name,\n            \"data_source_id\": \"scheduled_query\",\n            \"schedule\": schedule,\n            \"params\": {\n                \"query\": query_sql,\n                \"write_disposition\": \"WRITE_APPEND\"\n            }\n        }\n        \n        response = transfer_client.create_transfer_config(\n            parent=parent,\n            transfer_config=transfer_config\n        )\n        \n        self.logger.info(f\"Created scheduled query: {response.name}\")\n        return response\n    \n    def export_to_gcs(self, table_name, gcs_bucket, file_prefix):\n        \"\"\"Export BigQuery table to Google Cloud Storage\"\"\"\n        \n        table_id = f\"{self.project_id}.{self.dataset_id}.{table_name}\"\n        destination_uri = f\"gs://{gcs_bucket}/{file_prefix}_*.parquet\"\n        \n        job_config = bigquery.ExtractJobConfig()\n        job_config.destination_format = bigquery.DestinationFormat.PARQUET\n        job_config.compression = bigquery.Compression.SNAPPY\n        \n        extract_job = self.bq_client.extract_table(\n            table_id,\n            destination_uri,\n            job_config=job_config\n        )\n        \n        extract_job.result()  # Wait for job to complete\n        \n        self.logger.info(f\"Exported {table_name} to {destination_uri}\")\n\n# Usage\ngcp_pipeline = GCPDataPipeline('my-project-id', 'sales_analytics')\n\n# Setup\ngcp_pipeline.create_dataset()\ngcp_pipeline.create_tables()\n\n# Load data\ngcp_pipeline.load_data_from_gcs(\n    'gs://my-data-bucket/sales-data/*.csv',\n    'fact_sales'\n)\n\n# Run analytics\nresults_df = gcp_pipeline.run_analytics_query()\nprint(\"Analytics Results:\")\nprint(results_df)\n\n# Export results\ngcp_pipeline.export_to_gcs('fact_sales', 'my-export-bucket', 'sales_export')\n```\n\n### Azure Data Engineering\n\n**Azure Data Factory Pipelines**\n\n```python\nfrom azure.identity import DefaultAzureCredential\nfrom azure.mgmt.datafactory import DataFactoryManagementClient\nfrom azure.mgmt.datafactory.models import *\nimport json\n\nclass AzureDataFactoryManager:\n    def __init__(self, subscription_id, resource_group, factory_name):\n        self.subscription_id = subscription_id\n        self.resource_group = resource_group\n        self.factory_name = factory_name\n        \n        # Authenticate\n        self.credential = DefaultAzureCredential()\n        self.adf_client = DataFactoryManagementClient(\n            self.credential, \n            subscription_id\n        )\n    \n    def create_linked_services(self):\n        \"\"\"Create linked services for data sources\"\"\"\n        \n        # Azure SQL Database linked service\n        sql_db_linked_service = LinkedServiceResource(\n            properties=AzureSqlDatabaseLinkedService(\n                connection_string=SecureString(\n                    value=\"Server=tcp:myserver.database.windows.net,1433;Database=mydb;User ID=myuser;Password=mypassword;Trusted_Connection=False;Encrypt=True;Connection Timeout=30;\"\n                )\n            )\n        )\n        \n        self.adf_client.linked_services.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"AzureSqlDatabase1\",\n            sql_db_linked_service\n        )\n        \n        # Azure Blob Storage linked service\n        blob_storage_linked_service = LinkedServiceResource(\n            properties=AzureBlobStorageLinkedService(\n                connection_string=SecureString(\n                    value=\"DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=mykey;EndpointSuffix=core.windows.net\"\n                )\n            )\n        )\n        \n        self.adf_client.linked_services.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"AzureBlobStorage1\",\n            blob_storage_linked_service\n        )\n    \n    def create_datasets(self):\n        \"\"\"Create datasets for source and sink\"\"\"\n        \n        # Source dataset (SQL Database)\n        source_dataset = DatasetResource(\n            properties=AzureSqlTableDataset(\n                linked_service_name=LinkedServiceReference(\n                    reference_name=\"AzureSqlDatabase1\"\n                ),\n                table_name=\"sales_data\"\n            )\n        )\n        \n        self.adf_client.datasets.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"SourceDataset\",\n            source_dataset\n        )\n        \n        # Sink dataset (Blob Storage)\n        sink_dataset = DatasetResource(\n            properties=DelimitedTextDataset(\n                linked_service_name=LinkedServiceReference(\n                    reference_name=\"AzureBlobStorage1\"\n                ),\n                location=BlobStorageLocation(\n                    container=\"data\",\n                    folder_path=\"processed\"\n                ),\n                column_delimiter=\",\",\n                first_row_as_header=True\n            )\n        )\n        \n        self.adf_client.datasets.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"SinkDataset\",\n            sink_dataset\n        )\n    \n    def create_pipeline(self):\n        \"\"\"Create data processing pipeline\"\"\"\n        \n        # Copy activity\n        copy_activity = CopyActivity(\n            name=\"CopyFromSQLToBlob\",\n            inputs=[DatasetReference(reference_name=\"SourceDataset\")],\n            outputs=[DatasetReference(reference_name=\"SinkDataset\")],\n            source=SqlSource(),\n            sink=DelimitedTextSink()\n        )\n        \n        # Data flow activity for transformations\n        data_flow_activity = ExecuteDataFlowActivity(\n            name=\"TransformData\",\n            data_flow=DataFlowReference(reference_name=\"TransformationDataFlow\"),\n            staging=DataFlowStagingInfo(\n                linked_service=LinkedServiceReference(\n                    reference_name=\"AzureBlobStorage1\"\n                )\n            )\n        )\n        \n        # Create pipeline\n        pipeline = PipelineResource(\n            activities=[copy_activity, data_flow_activity],\n            parameters={\n                \"inputPath\": ParameterSpecification(type=\"String\"),\n                \"outputPath\": ParameterSpecification(type=\"String\")\n            }\n        )\n        \n        self.adf_client.pipelines.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"SalesDataPipeline\",\n            pipeline\n        )\n    \n    def create_trigger(self):\n        \"\"\"Create trigger to schedule pipeline\"\"\"\n        \n        trigger = ScheduleTriggerResource(\n            properties=ScheduleTrigger(\n                recurrence=ScheduleTriggerRecurrence(\n                    frequency=\"Day\",\n                    interval=1,\n                    start_time=\"2024-01-01T00:00:00Z\",\n                    time_zone=\"UTC\",\n                    schedule=RecurrenceSchedule(\n                        hours=[2],  # Run at 2 AM\n                        minutes=[0]\n                    )\n                ),\n                pipelines=[\n                    TriggerPipelineReference(\n                        pipeline_reference=PipelineReference(\n                            reference_name=\"SalesDataPipeline\"\n                        ),\n                        parameters={\n                            \"inputPath\": \"sales/raw\",\n                            \"outputPath\": \"sales/processed\"\n                        }\n                    )\n                ]\n            )\n        )\n        \n        self.adf_client.triggers.create_or_update(\n            self.resource_group,\n            self.factory_name,\n            \"DailyTrigger\",\n            trigger\n        )\n    \n    def run_pipeline(self, pipeline_name, parameters=None):\n        \"\"\"Manually trigger pipeline run\"\"\"\n        \n        run_response = self.adf_client.pipelines.create_run(\n            self.resource_group,\n            self.factory_name,\n            pipeline_name,\n            parameters=parameters or {}\n        )\n        \n        return run_response.run_id\n    \n    def monitor_pipeline_run(self, run_id):\n        \"\"\"Monitor pipeline run status\"\"\"\n        \n        pipeline_run = self.adf_client.pipeline_runs.get(\n            self.resource_group,\n            self.factory_name,\n            run_id\n        )\n        \n        return {\n            'status': pipeline_run.status,\n            'start_time': pipeline_run.run_start,\n            'end_time': pipeline_run.run_end,\n            'duration': pipeline_run.duration_in_ms\n        }\n\n# Usage\nadf_manager = AzureDataFactoryManager(\n    subscription_id=\"your-subscription-id\",\n    resource_group=\"my-resource-group\",\n    factory_name=\"my-data-factory\"\n)\n\n# Setup pipeline components\nadf_manager.create_linked_services()\nadf_manager.create_datasets()\nadf_manager.create_pipeline()\nadf_manager.create_trigger()\n\n# Run pipeline manually\nrun_id = adf_manager.run_pipeline(\"SalesDataPipeline\")\nprint(f\"Started pipeline run: {run_id}\")\n\n# Monitor run\nimport time\nwhile True:\n    status = adf_manager.monitor_pipeline_run(run_id)\n    print(f\"Pipeline status: {status['status']}\")\n    \n    if status['status'] in ['Succeeded', 'Failed', 'Cancelled']:\n        break\n    \n    time.sleep(30)\n```\n\n---\n\n## ðŸŒŠ Stream Processing {#streaming}\n\n### Apache Flink for Complex Event Processing\n\n```python\n# Note: This is pseudo-code as PyFlink has specific setup requirements\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment, EnvironmentSettings\nfrom pyflink.datastream.connectors import FlinkKafkaConsumer\nfrom pyflink.common.serialization import SimpleStringSchema\nfrom pyflink.datastream.functions import MapFunction, FilterFunction, KeyedProcessFunction\nfrom pyflink.datastream.state import ValueStateDescriptor\nfrom pyflink.common.typeinfo import Types\nfrom pyflink.common.time import Time\nimport json\n\nclass EventEnrichmentFunction(MapFunction):\n    \"\"\"Enrich events with additional data\"\"\"\n    \n    def map(self, value):\n        event = json.loads(value)\n        \n        # Add processing metadata\n        event['processed_timestamp'] = time.time()\n        event['processing_version'] = '1.0'\n        \n        # Enrich with business logic\n        if event.get('amount', 0) > 1000:\n            event['is_high_value'] = True\n            event['priority'] = 'HIGH'\n        else:\n            event['is_high_value'] = False\n            event['priority'] = 'NORMAL'\n        \n        return json.dumps(event)\n\nclass FraudDetectionFunction(KeyedProcessFunction):\n    \"\"\"Detect fraudulent patterns using state\"\"\"\n    \n    def __init__(self):\n        self.transaction_count_state = None\n        self.amount_sum_state = None\n    \n    def open(self, runtime_context):\n        # Initialize state\n        self.transaction_count_state = runtime_context.get_state(\n            ValueStateDescriptor(\"transaction_count\", Types.INT())\n        )\n        self.amount_sum_state = runtime_context.get_state(\n            ValueStateDescriptor(\"amount_sum\", Types.DOUBLE())\n        )\n    \n    def process_element(self, value, ctx, out):\n        event = json.loads(value)\n        \n        # Get current state\n        current_count = self.transaction_count_state.value() or 0\n        current_sum = self.amount_sum_state.value() or 0.0\n        \n        # Update state\n        new_count = current_count + 1\n        new_sum = current_sum + event.get('amount', 0)\n        \n        self.transaction_count_state.update(new_count)\n        self.amount_sum_state.update(new_sum)\n        \n        # Fraud detection logic\n        time_window_minutes = 10\n        \n        # Rule 1: Too many transactions in short time\n        if new_count > 20:  # More than 20 transactions\n            event['fraud_score'] = min(100, new_count * 2)\n            event['fraud_reason'] = 'High transaction frequency'\n            event['is_fraud_suspected'] = True\n        \n        # Rule 2: Unusually high spending\n        elif new_sum > 10000:  # More than $10k in window\n            event['fraud_score'] = min(100, new_sum / 100)\n            event['fraud_reason'] = 'High spending amount'\n            event['is_fraud_suspected'] = True\n        \n        else:\n            event['fraud_score'] = 0\n            event['is_fraud_suspected'] = False\n        \n        out.collect(json.dumps(event))\n        \n        # Set timer to clear state after time window\n        ctx.timer_service().register_event_time_timer(\n            ctx.timestamp() + Time.minutes(time_window_minutes).to_milliseconds()\n        )\n    \n    def on_timer(self, timestamp, ctx, out):\n        # Clear state after time window\n        self.transaction_count_state.clear()\n        self.amount_sum_state.clear()\n\ndef create_flink_pipeline():\n    \"\"\"Create Flink streaming pipeline\"\"\"\n    \n    # Create execution environment\n    env = StreamExecutionEnvironment.get_execution_environment()\n    env.set_parallelism(4)\n    env.enable_checkpointing(60000)  # Checkpoint every minute\n    \n    # Kafka consumer properties\n    kafka_props = {\n        'bootstrap.servers': 'localhost:9092',\n        'group.id': 'flink-fraud-detection'\n    }\n    \n    # Create Kafka consumer\n    kafka_consumer = FlinkKafkaConsumer(\n        topics=['financial-transactions'],\n        deserialization_schema=SimpleStringSchema(),\n        properties=kafka_props\n    )\n    \n    # Start from latest offset\n    kafka_consumer.set_start_from_latest()\n    \n    # Create data stream\n    transaction_stream = env.add_source(kafka_consumer)\n    \n    # Data processing pipeline\n    processed_stream = transaction_stream \\\n        .map(EventEnrichmentFunction()) \\\n        .filter(lambda event: json.loads(event).get('amount', 0) > 0) \\\n        .key_by(lambda event: json.loads(event).get('customer_id')) \\\n        .process(FraudDetectionFunction())\n    \n    # Split stream based on fraud detection\n    fraud_stream = processed_stream.filter(\n        lambda event: json.loads(event).get('is_fraud_suspected', False)\n    )\n    \n    normal_stream = processed_stream.filter(\n        lambda event: not json.loads(event).get('is_fraud_suspected', False)\n    )\n    \n    # Send fraud alerts to alert topic\n    fraud_stream.add_sink(\n        FlinkKafkaProducer(\n            topic='fraud-alerts',\n            serialization_schema=SimpleStringSchema(),\n            producer_config=kafka_props\n        )\n    )\n    \n    # Send normal transactions to processed topic\n    normal_stream.add_sink(\n        FlinkKafkaProducer(\n            topic='processed-transactions',\n            serialization_schema=SimpleStringSchema(),\n            producer_config=kafka_props\n        )\n    )\n    \n    # Execute pipeline\n    env.execute(\"Fraud Detection Pipeline\")\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    create_flink_pipeline()\n```\n\n### Real-time Analytics with Apache Druid\n\n```python\nimport requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nclass DruidConnector:\n    def __init__(self, broker_url='http://localhost:8082'):\n        self.broker_url = broker_url\n    \n    def submit_ingestion_task(self, datasource_name, kafka_config):\n        \"\"\"Submit Kafka ingestion task to Druid\"\"\"\n        \n        task_spec = {\n            \"type\": \"kafka\",\n            \"dataSchema\": {\n                \"dataSource\": datasource_name,\n                \"timestampSpec\": {\n                    \"column\": \"timestamp\",\n                    \"format\": \"iso\"\n                },\n                \"dimensionsSpec\": {\n                    \"dimensions\": [\n                        \"customer_id\",\n                        \"product_category\",\n                        \"event_type\",\n                        \"country\"\n                    ]\n                },\n                \"metricsSpec\": [\n                    {\n                        \"name\": \"count\",\n                        \"type\": \"count\"\n                    },\n                    {\n                        \"name\": \"total_amount\",\n                        \"type\": \"doubleSum\",\n                        \"fieldName\": \"amount\"\n                    },\n                    {\n                        \"name\": \"unique_customers\",\n                        \"type\": \"hyperUnique\",\n                        \"fieldName\": \"customer_id\"\n                    }\n                ],\n                \"granularitySpec\": {\n                    \"type\": \"uniform\",\n                    \"segmentGranularity\": \"hour\",\n                    \"queryGranularity\": \"minute\"\n                }\n            },\n            \"ioConfig\": {\n                \"topic\": kafka_config['topic'],\n                \"consumerProperties\": kafka_config['consumer_properties'],\n                \"taskDuration\": \"PT1H\",  # 1 hour\n                \"replicas\": 2,\n                \"taskCount\": 4\n            },\n            \"tuningConfig\": {\n                \"type\": \"kafka\",\n                \"maxRowsPerSegment\": 5000000,\n                \"intermediatePersistPeriod\": \"PT10M\"\n            }\n        }\n        \n        response = requests.post(\n            f\"{self.broker_url}/druid/indexer/v1/task\",\n            json=task_spec,\n            headers={'Content-Type': 'application/json'}\n        )\n        \n        return response.json()\n    \n    def query_timeseries(self, datasource, start_time, end_time, granularity=\"minute\"):\n        \"\"\"Run timeseries query\"\"\"\n        \n        query = {\n            \"queryType\": \"timeseries\",\n            \"dataSource\": datasource,\n            \"granularity\": granularity,\n            \"intervals\": [f\"{start_time}/{end_time}\"],\n            \"aggregations\": [\n                {\n                    \"type\": \"longSum\",\n                    \"name\": \"total_events\",\n                    \"fieldName\": \"count\"\n                },\n                {\n                    \"type\": \"doubleSum\", \n                    \"name\": \"total_revenue\",\n                    \"fieldName\": \"total_amount\"\n                },\n                {\n                    \"type\": \"hyperUniqueCardinality\",\n                    \"name\": \"unique_customers\",\n                    \"fieldName\": \"unique_customers\"\n                }\n            ],\n            \"postAggregations\": [\n                {\n                    \"type\": \"arithmetic\",\n                    \"name\": \"avg_revenue_per_customer\",\n                    \"fn\": \"/\",\n                    \"fields\": [\n                        {\"type\": \"fieldAccess\", \"fieldName\": \"total_revenue\"},\n                        {\"type\": \"fieldAccess\", \"fieldName\": \"unique_customers\"}\n                    ]\n                }\n            ]\n        }\n        \n        response = requests.post(\n            f\"{self.broker_url}/druid/v2/\",\n            json=query,\n            headers={'Content-Type': 'application/json'}\n        )\n        \n        return response.json()\n    \n    def query_top_n(self, datasource, dimension, metric, threshold=10):\n        \"\"\"Run TopN query for top customers, products, etc.\"\"\"\n        \n        query = {\n            \"queryType\": \"topN\",\n            \"dataSource\": datasource,\n            \"dimension\": dimension,\n            \"threshold\": threshold,\n            \"metric\": metric,\n            \"granularity\": \"all\",\n            \"intervals\": [\"2024-01-01/2024-12-31\"],\n            \"aggregations\": [\n                {\n                    \"type\": \"longSum\",\n                    \"name\": \"total_events\",\n                    \"fieldName\": \"count\"\n                },\n                {\n                    \"type\": \"doubleSum\",\n                    \"name\": \"total_amount\",\n                    \"fieldName\": \"total_amount\"\n                }\n            ]\n        }\n        \n        response = requests.post(\n            f\"{self.broker_url}/druid/v2/\",\n            json=query,\n            headers={'Content-Type': 'application/json'}\n        )\n        \n        return response.json()\n    \n    def query_group_by(self, datasource, dimensions, filters=None):\n        \"\"\"Run GroupBy query with multiple dimensions\"\"\"\n        \n        query = {\n            \"queryType\": \"groupBy\",\n            \"dataSource\": datasource,\n            \"dimensions\": dimensions,\n            \"granularity\": \"hour\",\n            \"intervals\": [\"2024-01-01/2024-12-31\"],\n            \"aggregations\": [\n                {\n                    \"type\": \"longSum\",\n                    \"name\": \"event_count\",\n                    \"fieldName\": \"count\"\n                },\n                {\n                    \"type\": \"doubleSum\",\n                    \"name\": \"revenue\",\n                    \"fieldName\": \"total_amount\"\n                },\n                {\n                    \"type\": \"hyperUniqueCardinality\",\n                    \"name\": \"unique_customers\",\n                    \"fieldName\": \"unique_customers\"\n                }\n            ]\n        }\n        \n        if filters:\n            query[\"filter\"] = filters\n        \n        response = requests.post(\n            f\"{self.broker_url}/druid/v2/\",\n            json=query,\n            headers={'Content-Type': 'application/json'}\n        )\n        \n        return response.json()\n\n# Usage\ndruid = DruidConnector()\n\n# Setup Kafka ingestion\nkafka_config = {\n    'topic': 'sales-events',\n    'consumer_properties': {\n        'bootstrap.servers': 'localhost:9092'\n    }\n}\n\n# Start ingestion\ningestion_result = druid.submit_ingestion_task('sales_events', kafka_config)\nprint(f\"Ingestion task submitted: {ingestion_result}\")\n\n# Real-time analytics queries\n# 1. Revenue trend over time\nrevenue_trend = druid.query_timeseries(\n    'sales_events',\n    (datetime.now() - timedelta(hours=24)).isoformat(),\n    datetime.now().isoformat(),\n    granularity=\"PT5M\"  # 5-minute intervals\n)\n\nprint(\"Revenue Trend (Last 24 hours):\")\nfor point in revenue_trend:\n    print(f\"{point['timestamp']}: ${point['result']['total_revenue']:.2f}\")\n\n# 2. Top customers by spending\ntop_customers = druid.query_top_n(\n    'sales_events',\n    'customer_id',\n    'total_amount',\n    threshold=20\n)\n\nprint(\"\\nTop 20 Customers by Revenue:\")\nfor customer in top_customers[0]['result']:\n    print(f\"Customer {customer['customer_id']}: ${customer['total_amount']:.2f}\")\n\n# 3. Sales by product category and country\ncategory_sales = druid.query_group_by(\n    'sales_events',\n    ['product_category', 'country'],\n    filters={\n        \"type\": \"and\",\n        \"fields\": [\n            {\n                \"type\": \"selector\",\n                \"dimension\": \"event_type\",\n                \"value\": \"purchase\"\n            },\n            {\n                \"type\": \"bound\",\n                \"dimension\": \"total_amount\",\n                \"lower\": \"100\"\n            }\n        ]\n    }\n)\n\nprint(\"\\nSales by Category and Country (Orders > $100):\")\nfor result in category_sales:\n    event = result['event']\n    print(f\"{event['product_category']} - {event['country']}: \"\n          f\"${event['revenue']:.2f} ({event['event_count']} orders)\")\n```\n\n---\n\n## âœ… Data Quality & Governance {#quality}\n\n### Data Quality Framework with Great Expectations\n\n```python\nimport great_expectations as ge\nfrom great_expectations.data_context import DataContext\nfrom great_expectations.checkpoint import SimpleCheckpoint\nimport pandas as pd\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Any\n\nclass DataQualityFramework:\n    def __init__(self, context_root_dir=\"./great_expectations\"):\n        self.context = DataContext(context_root_dir=context_root_dir)\n        self.logger = logging.getLogger(__name__)\n    \n    def create_expectation_suite(self, suite_name: str, data_asset_name: str):\n        \"\"\"Create a new expectation suite\"\"\"\n        \n        # Create suite\n        suite = self.context.create_expectation_suite(\n            expectation_suite_name=suite_name,\n            overwrite_existing=True\n        )\n        \n        return suite\n    \n    def define_sales_data_expectations(self, suite_name: str = \"sales_data_quality\"):\n        \"\"\"Define comprehensive expectations for sales data\"\"\"\n        \n        suite = self.create_expectation_suite(suite_name, \"sales_data\")\n        \n        # Create a validator\n        validator = self.context.get_validator(\n            batch_request=RuntimeBatchRequest(\n                datasource_name=\"pandas_datasource\",\n                data_connector_name=\"default_runtime_data_connector_name\",\n                data_asset_name=\"sales_data\",\n                runtime_parameters={\"batch_data\": pd.DataFrame()}  # Empty for now\n            ),\n            expectation_suite_name=suite_name\n        )\n        \n        # Column existence expectations\n        validator.expect_table_columns_to_match_ordered_list(\n            column_list=[\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \n                        \"unit_price\", \"total_amount\", \"order_date\", \"status\"]\n        )\n        \n        # Data type expectations\n        validator.expect_column_values_to_be_of_type(\"order_id\", \"str\")\n        validator.expect_column_values_to_be_of_type(\"customer_id\", \"str\")\n        validator.expect_column_values_to_be_of_type(\"quantity\", \"int\")\n        validator.expect_column_values_to_be_of_type(\"unit_price\", \"float\")\n        validator.expect_column_values_to_be_of_type(\"total_amount\", \"float\")\n        \n        # Null value expectations\n        validator.expect_column_values_to_not_be_null(\"order_id\")\n        validator.expect_column_values_to_not_be_null(\"customer_id\")\n        validator.expect_column_values_to_not_be_null(\"product_id\")\n        validator.expect_column_values_to_not_be_null(\"total_amount\")\n        \n        # Uniqueness expectations\n        validator.expect_column_values_to_be_unique(\"order_id\")\n        \n        # Range expectations\n        validator.expect_column_values_to_be_between(\"quantity\", min_value=1, max_value=1000)\n        validator.expect_column_values_to_be_between(\"unit_price\", min_value=0.01, max_value=10000)\n        validator.expect_column_values_to_be_between(\"total_amount\", min_value=0.01, max_value=100000)\n        \n        # Set expectations\n        validator.expect_column_values_to_be_in_set(\n            \"status\", \n            [\"pending\", \"confirmed\", \"shipped\", \"delivered\", \"cancelled\"]\n        )\n        \n        # Pattern expectations\n        validator.expect_column_values_to_match_regex(\n            \"order_id\",\n            regex=r\"^ORD-\\d{8}-\\d{6}$\"  # Format: ORD-YYYYMMDD-HHMMSS\n        )\n        \n        # Business logic expectations\n        validator.expect_column_pair_values_A_to_be_greater_than_B(\n            column_A=\"total_amount\",\n            column_B=\"unit_price\",\n            or_equal=True\n        )\n        \n        # Date expectations\n        validator.expect_column_values_to_be_between(\n            \"order_date\",\n            min_value=\"2020-01-01\",\n            max_value=datetime.now().strftime(\"%Y-%m-%d\")\n        )\n        \n        # Statistical expectations\n        validator.expect_column_mean_to_be_between(\"total_amount\", min_value=10, max_value=500)\n        validator.expect_column_stdev_to_be_between(\"total_amount\", min_value=1, max_value=200)\n        \n        # Save the suite\n        validator.save_expectation_suite(discard_failed_expectations=False)\n        \n        return validator.get_expectation_suite()\n    \n    def validate_dataframe(self, df: pd.DataFrame, suite_name: str) -> Dict[str, Any]:\n        \"\"\"Validate dataframe against expectation suite\"\"\"\n        \n        # Create batch request\n        batch_request = RuntimeBatchRequest(\n            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"default_runtime_data_connector_name\",\n            data_asset_name=\"sales_data\",\n            runtime_parameters={\"batch_data\": df},\n            batch_identifiers={\"batch_id\": f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"}\n        )\n        \n        # Get validator\n        validator = self.context.get_validator(\n            batch_request=batch_request,\n            expectation_suite_name=suite_name\n        )\n        \n        # Run validation\n        validation_result = validator.validate()\n        \n        # Process results\n        results_summary = {\n            \"success\": validation_result.success,\n            \"statistics\": validation_result.statistics,\n            \"results\": []\n        }\n        \n        for result in validation_result.results:\n            result_dict = {\n                \"expectation_type\": result.expectation_config.expectation_type,\n                \"success\": result.success,\n                \"result\": result.result\n            }\n            \n            if not result.success:\n                result_dict[\"failure_info\"] = {\n                    \"observed_value\": result.result.get(\"observed_value\"),\n                    \"element_count\": result.result.get(\"element_count\"),\n                    \"missing_count\": result.result.get(\"missing_count\"),\n                    \"unexpected_count\": result.result.get(\"unexpected_count\")\n                }\n            \n            results_summary[\"results\"].append(result_dict)\n        \n        return results_summary\n    \n    def create_checkpoint(self, checkpoint_name: str, suite_names: List[str]):\n        \"\"\"Create checkpoint for automated validation\"\"\"\n        \n        checkpoint_config = {\n            \"name\": checkpoint_name,\n            \"config_version\": 1,\n            \"template_name\": None,\n            \"run_name_template\": f\"{checkpoint_name}_%Y%m%d-%H%M%S\",\n            \"expectation_suite_name\": None,\n            \"batch_request\": {},\n            \"action_list\": [\n                {\n                    \"name\": \"store_validation_result\",\n                    \"action\": {\n                        \"class_name\": \"StoreValidationResultAction\"\n                    }\n                },\n                {\n                    \"name\": \"store_evaluation_params\",\n                    \"action\": {\n                        \"class_name\": \"StoreEvaluationParametersAction\"\n                    }\n                },\n                {\n                    \"name\": \"update_data_docs\",\n                    \"action\": {\n                        \"class_name\": \"UpdateDataDocsAction\"\n                    }\n                }\n            ],\n            \"evaluation_parameters\": {},\n            \"runtime_configuration\": {},\n            \"validations\": []\n        }\n        \n        # Add validations for each suite\n        for suite_name in suite_names:\n            validation_config = {\n                \"batch_request\": {\n                    \"datasource_name\": \"pandas_datasource\",\n                    \"data_connector_name\": \"default_runtime_data_connector_name\",\n                    \"data_asset_name\": \"sales_data\"\n                },\n                \"expectation_suite_name\": suite_name\n            }\n            checkpoint_config[\"validations\"].append(validation_config)\n        \n        # Create checkpoint\n        checkpoint = SimpleCheckpoint(\n            f\"{checkpoint_name}_checkpoint\",\n            self.context,\n            **checkpoint_config\n        )\n        \n        return checkpoint\n    \n    def automated_data_profiling(self, df: pd.DataFrame, suite_name: str = \"auto_profiled\"):\n        \"\"\"Automatically profile data and create basic expectations\"\"\"\n        \n        # Create batch request\n        batch_request = RuntimeBatchRequest(\n            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"default_runtime_data_connector_name\",\n            data_asset_name=\"auto_profiled_data\",\n            runtime_parameters={\"batch_data\": df}\n        )\n        \n        # Get validator\n        validator = self.context.get_validator(\n            batch_request=batch_request,\n            expectation_suite_name=suite_name,\n            create_expectation_suite_with_name=suite_name\n        )\n        \n        # Auto-profile the dataset\n        profiler_config = {\n            \"ignored_columns\": [],\n            \"not_null_only\": False,\n            \"primary_or_compound_key\": [],\n            \"semantic_types_dict\": {},\n            \"table_expectations_only\": False,\n            \"value_set_threshold\": \"MANY\"\n        }\n        \n        # This would normally use the UserConfigurableProfiler\n        # For demonstration, we'll add some basic expectations\n        \n        for column in df.columns:\n            # Null expectations\n            null_percentage = df[column].isnull().sum() / len(df) * 100\n            if null_percentage < 5:  # Less than 5% nulls\n                validator.expect_column_values_to_not_be_null(column)\n            \n            # Type expectations\n            if df[column].dtype in ['int64', 'int32']:\n                validator.expect_column_values_to_be_of_type(column, \"int\")\n                # Range expectations\n                min_val, max_val = df[column].min(), df[column].max()\n                validator.expect_column_values_to_be_between(\n                    column, min_value=min_val, max_value=max_val\n                )\n            elif df[column].dtype in ['float64', 'float32']:\n                validator.expect_column_values_to_be_of_type(column, \"float\")\n                min_val, max_val = df[column].min(), df[column].max()\n                validator.expect_column_values_to_be_between(\n                    column, min_value=min_val, max_value=max_val\n                )\n            elif df[column].dtype == 'object':\n                unique_count = df[column].nunique()\n                if unique_count <= 20:  # Categorical with few values\n                    unique_values = df[column].unique().tolist()\n                    validator.expect_column_values_to_be_in_set(column, unique_values)\n        \n        # Save the suite\n        validator.save_expectation_suite(discard_failed_expectations=False)\n        \n        return validator.get_expectation_suite()\n\n# Data lineage tracking\nclass DataLineageTracker:\n    def __init__(self):\n        self.lineage_graph = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def track_data_flow(self, source: str, target: str, transformation: str, metadata: Dict = None):\n        \"\"\"Track data flow between source and target\"\"\"\n        \n        lineage_entry = {\n            \"source\": source,\n            \"target\": target,\n            \"transformation\": transformation,\n            \"timestamp\": datetime.now().isoformat(),\n            \"metadata\": metadata or {}\n        }\n        \n        if source not in self.lineage_graph:\n            self.lineage_graph[source] = []\n        \n        self.lineage_graph[source].append(lineage_entry)\n        \n        self.logger.info(f\"Tracked lineage: {source} -> {target} via {transformation}\")\n    \n    def get_data_lineage(self, data_asset: str) -> List[Dict]:\n        \"\"\"Get lineage for a specific data asset\"\"\"\n        \n        lineage = []\n        \n        # Find as source\n        if data_asset in self.lineage_graph:\n            lineage.extend(self.lineage_graph[data_asset])\n        \n        # Find as target\n        for source, flows in self.lineage_graph.items():\n            for flow in flows:\n                if flow[\"target\"] == data_asset:\n                    lineage.append(flow)\n        \n        return lineage\n    \n    def export_lineage_graph(self, format: str = \"json\") -> str:\n        \"\"\"Export lineage graph in various formats\"\"\"\n        \n        if format == \"json\":\n            return json.dumps(self.lineage_graph, indent=2)\n        elif format == \"dot\":\n            # Generate DOT format for Graphviz\n            dot_content = \"digraph DataLineage {\\n\"\n            for source, flows in self.lineage_graph.items():\n                for flow in flows:\n                    dot_content += f'  \"{source}\" -> \"{flow[\"target\"]}\" [label=\"{flow[\"transformation\"]}\"];\\n'\n            dot_content += \"}\\n\"\n            return dot_content\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Initialize data quality framework\n    dq_framework = DataQualityFramework()\n    \n    # Create sample sales data\n    sample_data = pd.DataFrame({\n        'order_id': ['ORD-20240101-120000', 'ORD-20240101-120001', 'ORD-20240101-120002'],\n        'customer_id': ['CUST001', 'CUST002', 'CUST001'],\n        'product_id': ['PROD001', 'PROD002', 'PROD003'],\n        'quantity': [2, 1, 3],\n        'unit_price': [25.50, 15.99, 30.00],\n        'total_amount': [51.00, 15.99, 90.00],\n        'order_date': ['2024-01-01', '2024-01-01', '2024-01-01'],\n        'status': ['confirmed', 'shipped', 'confirmed']\n    })\n    \n    # Define expectations\n    suite = dq_framework.define_sales_data_expectations()\n    print(f\"Created expectation suite with {len(suite.expectations)} expectations\")\n    \n    # Validate data\n    validation_results = dq_framework.validate_dataframe(sample_data, \"sales_data_quality\")\n    \n    print(f\"\\nValidation Results:\")\n    print(f\"Success: {validation_results['success']}\")\n    print(f\"Total Expectations: {validation_results['statistics']['evaluated_expectations']}\")\n    print(f\"Successful Expectations: {validation_results['statistics']['successful_expectations']}\")\n    print(f\"Failed Expectations: {validation_results['statistics']['unsuccessful_expectations']}\")\n    \n    # Track data lineage\n    lineage_tracker = DataLineageTracker()\n    \n    lineage_tracker.track_data_flow(\n        source=\"raw_orders_table\",\n        target=\"cleaned_orders_table\", \n        transformation=\"data_cleaning_pipeline\",\n        metadata={\"pipeline_version\": \"1.2.3\", \"records_processed\": 1000}\n    )\n    \n    lineage_tracker.track_data_flow(\n        source=\"cleaned_orders_table\",\n        target=\"sales_analytics_table\",\n        transformation=\"aggregation_pipeline\",\n        metadata={\"aggregation_type\": \"daily_summary\"}\n    )\n    \n    # Get lineage\n    lineage = lineage_tracker.get_data_lineage(\"cleaned_orders_table\")\n    print(f\"\\nData Lineage for cleaned_orders_table:\")\n    for entry in lineage:\n        print(f\"  {entry['source']} -> {entry['target']} via {entry['transformation']}\")\n    \n    # Export lineage graph\n    lineage_json = lineage_tracker.export_lineage_graph(\"json\")\n    print(f\"\\nLineage Graph (JSON):\\n{lineage_json}\")\n\n---\n\n## ðŸ“Š Monitoring & Observability {#monitoring}\n\n### Data Pipeline Monitoring\n\n```python\nimport logging\nimport time\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nimport boto3\nimport requests\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass PipelineStatus(Enum):\n    RUNNING = \"RUNNING\"\n    SUCCESS = \"SUCCESS\"\n    FAILED = \"FAILED\"\n    WARNING = \"WARNING\"\n    CANCELLED = \"CANCELLED\"\n\n@dataclass\nclass PipelineMetrics:\n    pipeline_name: str\n    start_time: datetime\n    end_time: Optional[datetime]\n    status: PipelineStatus\n    records_processed: int\n    records_failed: int\n    duration_seconds: float\n    error_message: Optional[str] = None\n    data_quality_score: float = 100.0\n    resource_usage: Dict[str, float] = None\n\nclass DataPipelineMonitor:\n    def __init__(self, metrics_backend=\"cloudwatch\"):\n        self.metrics_backend = metrics_backend\n        self.logger = logging.getLogger(__name__)\n        \n        if metrics_backend == \"cloudwatch\":\n            self.cloudwatch = boto3.client('cloudwatch')\n        elif metrics_backend == \"prometheus\":\n            self.prometheus_gateway = \"http://localhost:9091\"\n        \n        self.active_pipelines = {}\n    \n    def start_pipeline_monitoring(self, pipeline_name: str) -> str:\n        \"\"\"Start monitoring a pipeline execution\"\"\"\n        run_id = f\"{pipeline_name}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}\"\n        \n        metrics = PipelineMetrics(\n            pipeline_name=pipeline_name,\n            start_time=datetime.now(),\n            end_time=None,\n            status=PipelineStatus.RUNNING,\n            records_processed=0,\n            records_failed=0,\n            duration_seconds=0.0\n        )\n        \n        self.active_pipelines[run_id] = metrics\n        \n        # Send start metric\n        self._send_metric(f\"pipeline.started\", 1, {\n            \"pipeline_name\": pipeline_name,\n            \"run_id\": run_id\n        })\n        \n        self.logger.info(f\"Started monitoring pipeline {pipeline_name} with run_id {run_id}\")\n        return run_id\n    \n    def update_pipeline_metrics(self, run_id: str, **kwargs):\n        \"\"\"Update pipeline metrics during execution\"\"\"\n        if run_id not in self.active_pipelines:\n            raise ValueError(f\"Pipeline run_id {run_id} not found\")\n        \n        metrics = self.active_pipelines[run_id]\n        \n        # Update metrics\n        if 'records_processed' in kwargs:\n            metrics.records_processed = kwargs['records_processed']\n            self._send_metric(f\"pipeline.records_processed\", kwargs['records_processed'], {\n                \"pipeline_name\": metrics.pipeline_name,\n                \"run_id\": run_id\n            })\n        \n        if 'records_failed' in kwargs:\n            metrics.records_failed = kwargs['records_failed']\n            self._send_metric(f\"pipeline.records_failed\", kwargs['records_failed'], {\n                \"pipeline_name\": metrics.pipeline_name,\n                \"run_id\": run_id\n            })\n        \n        if 'data_quality_score' in kwargs:\n            metrics.data_quality_score = kwargs['data_quality_score']\n            self._send_metric(f\"pipeline.data_quality_score\", kwargs['data_quality_score'], {\n                \"pipeline_name\": metrics.pipeline_name,\n                \"run_id\": run_id\n            })\n        \n        if 'status' in kwargs:\n            metrics.status = kwargs['status']\n    \n    def finish_pipeline_monitoring(self, run_id: str, status: PipelineStatus, error_message: str = None):\n        \"\"\"Finish monitoring a pipeline execution\"\"\"\n        if run_id not in self.active_pipelines:\n            raise ValueError(f\"Pipeline run_id {run_id} not found\")\n        \n        metrics = self.active_pipelines[run_id]\n        metrics.end_time = datetime.now()\n        metrics.status = status\n        metrics.duration_seconds = (metrics.end_time - metrics.start_time).total_seconds()\n        metrics.error_message = error_message\n        \n        # Send completion metrics\n        self._send_metric(f\"pipeline.completed\", 1, {\n            \"pipeline_name\": metrics.pipeline_name,\n            \"run_id\": run_id,\n            \"status\": status.value\n        })\n        \n        self._send_metric(f\"pipeline.duration_seconds\", metrics.duration_seconds, {\n            \"pipeline_name\": metrics.pipeline_name,\n            \"run_id\": run_id\n        })\n        \n        # Calculate success rate\n        if metrics.records_processed > 0:\n            success_rate = ((metrics.records_processed - metrics.records_failed) / \n                          metrics.records_processed) * 100\n            \n            self._send_metric(f\"pipeline.success_rate\", success_rate, {\n                \"pipeline_name\": metrics.pipeline_name,\n                \"run_id\": run_id\n            })\n        \n        # Log completion\n        if status == PipelineStatus.SUCCESS:\n            self.logger.info(f\"Pipeline {metrics.pipeline_name} completed successfully. \"\n                           f\"Processed {metrics.records_processed} records in \"\n                           f\"{metrics.duration_seconds:.2f} seconds\")\n        else:\n            self.logger.error(f\"Pipeline {metrics.pipeline_name} failed. \"\n                            f\"Error: {error_message}\")\n        \n        # Store for historical analysis\n        self._store_pipeline_metrics(metrics)\n        \n        return metrics\n    \n    def _send_metric(self, metric_name: str, value: float, dimensions: Dict[str, str]):\n        \"\"\"Send metric to monitoring backend\"\"\"\n        \n        if self.metrics_backend == \"cloudwatch\":\n            try:\n                self.cloudwatch.put_metric_data(\n                    Namespace='DataPipeline',\n                    MetricData=[\n                        {\n                            'MetricName': metric_name,\n                            'Value': value,\n                            'Unit': 'Count' if 'count' in metric_name.lower() else 'None',\n                            'Dimensions': [\n                                {\n                                    'Name': key,\n                                    'Value': str(val)\n                                } for key, val in dimensions.items()\n                            ],\n                            'Timestamp': datetime.now()\n                        }\n                    ]\n                )\n            except Exception as e:\n                self.logger.error(f\"Failed to send metric to CloudWatch: {e}\")\n        \n        elif self.metrics_backend == \"prometheus\":\n            # Send to Prometheus Pushgateway\n            try:\n                metric_data = {\n                    'metric_name': metric_name,\n                    'value': value,\n                    'labels': dimensions,\n                    'timestamp': datetime.now().isoformat()\n                }\n                \n                response = requests.post(\n                    f\"{self.prometheus_gateway}/metrics/job/data_pipeline\",\n                    json=metric_data,\n                    timeout=5\n                )\n                response.raise_for_status()\n                \n            except Exception as e:\n                self.logger.error(f\"Failed to send metric to Prometheus: {e}\")\n    \n    def _store_pipeline_metrics(self, metrics: PipelineMetrics):\n        \"\"\"Store pipeline metrics for historical analysis\"\"\"\n        \n        # Store in database or data warehouse\n        metrics_dict = {\n            'pipeline_name': metrics.pipeline_name,\n            'start_time': metrics.start_time.isoformat(),\n            'end_time': metrics.end_time.isoformat() if metrics.end_time else None,\n            'status': metrics.status.value,\n            'records_processed': metrics.records_processed,\n            'records_failed': metrics.records_failed,\n            'duration_seconds': metrics.duration_seconds,\n            'data_quality_score': metrics.data_quality_score,\n            'error_message': metrics.error_message\n        }\n        \n        # This could be sent to a database, S3, etc.\n        self.logger.info(f\"Stored metrics: {json.dumps(metrics_dict, indent=2)}\")\n    \n    def get_pipeline_health_status(self, pipeline_name: str, hours: int = 24) -> Dict[str, Any]:\n        \"\"\"Get health status for a pipeline over the last N hours\"\"\"\n        \n        # This would typically query your metrics backend\n        # For demo purposes, we'll simulate the response\n        \n        health_status = {\n            'pipeline_name': pipeline_name,\n            'time_period_hours': hours,\n            'total_runs': 12,\n            'successful_runs': 10,\n            'failed_runs': 2,\n            'success_rate': 83.33,\n            'avg_duration_seconds': 1850.5,\n            'avg_records_processed': 45000,\n            'avg_data_quality_score': 97.5,\n            'last_run_status': 'SUCCESS',\n            'last_run_time': (datetime.now() - timedelta(hours=2)).isoformat(),\n            'alerts': [\n                {\n                    'severity': 'WARNING',\n                    'message': 'Data quality score dropped below 95%',\n                    'timestamp': (datetime.now() - timedelta(hours=6)).isoformat()\n                }\n            ]\n        }\n        \n        return health_status\n    \n    def setup_alerts(self, pipeline_name: str, alert_config: Dict[str, Any]):\n        \"\"\"Setup alerts for pipeline monitoring\"\"\"\n        \n        alerts = {\n            'pipeline_name': pipeline_name,\n            'alert_rules': [\n                {\n                    'name': 'Pipeline Failure',\n                    'condition': 'status == FAILED',\n                    'severity': 'CRITICAL',\n                    'notification_channels': ['email', 'slack']\n                },\n                {\n                    'name': 'High Failure Rate',\n                    'condition': 'failure_rate > 10%',\n                    'severity': 'WARNING',\n                    'notification_channels': ['slack']\n                },\n                {\n                    'name': 'Low Data Quality',\n                    'condition': 'data_quality_score < 95',\n                    'severity': 'WARNING',\n                    'notification_channels': ['email']\n                },\n                {\n                    'name': 'Long Runtime',\n                    'condition': 'duration > 3600 seconds',\n                    'severity': 'WARNING',\n                    'notification_channels': ['slack']\n                }\n            ]\n        }\n        \n        # Store alert configuration\n        self.logger.info(f\"Configured alerts for {pipeline_name}: {json.dumps(alerts, indent=2)}\")\n        return alerts\n\n# Data freshness monitoring\nclass DataFreshnessMonitor:\n    def __init__(self):\n        self.freshness_checks = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def register_data_source(self, source_name: str, expected_frequency_minutes: int):\n        \"\"\"Register a data source for freshness monitoring\"\"\"\n        self.freshness_checks[source_name] = {\n            'expected_frequency_minutes': expected_frequency_minutes,\n            'last_update': None,\n            'status': 'UNKNOWN'\n        }\n        \n        self.logger.info(f\"Registered data source {source_name} with expected frequency \"\n                        f\"{expected_frequency_minutes} minutes\")\n    \n    def update_data_timestamp(self, source_name: str, timestamp: datetime = None):\n        \"\"\"Update the last seen timestamp for a data source\"\"\"\n        if source_name not in self.freshness_checks:\n            raise ValueError(f\"Data source {source_name} not registered\")\n        \n        timestamp = timestamp or datetime.now()\n        self.freshness_checks[source_name]['last_update'] = timestamp\n        self.freshness_checks[source_name]['status'] = 'FRESH'\n        \n        self.logger.info(f\"Updated timestamp for {source_name}: {timestamp}\")\n    \n    def check_data_freshness(self) -> Dict[str, Any]:\n        \"\"\"Check freshness of all registered data sources\"\"\"\n        current_time = datetime.now()\n        freshness_report = {\n            'check_time': current_time.isoformat(),\n            'sources': {}\n        }\n        \n        for source_name, config in self.freshness_checks.items():\n            if config['last_update'] is None:\n                status = 'NO_DATA'\n                staleness_minutes = float('inf')\n            else:\n                staleness_minutes = (current_time - config['last_update']).total_seconds() / 60\n                \n                if staleness_minutes <= config['expected_frequency_minutes'] * 1.5:  # 50% tolerance\n                    status = 'FRESH'\n                elif staleness_minutes <= config['expected_frequency_minutes'] * 3:\n                    status = 'STALE'\n                else:\n                    status = 'VERY_STALE'\n            \n            freshness_report['sources'][source_name] = {\n                'status': status,\n                'last_update': config['last_update'].isoformat() if config['last_update'] else None,\n                'staleness_minutes': staleness_minutes if staleness_minutes != float('inf') else None,\n                'expected_frequency_minutes': config['expected_frequency_minutes']\n            }\n            \n            # Update status\n            self.freshness_checks[source_name]['status'] = status\n        \n        return freshness_report\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Pipeline monitoring\n    monitor = DataPipelineMonitor(metrics_backend=\"cloudwatch\")\n    \n    # Start monitoring a pipeline\n    run_id = monitor.start_pipeline_monitoring(\"daily_sales_etl\")\n    \n    try:\n        # Simulate pipeline execution\n        for i in range(1, 6):\n            time.sleep(1)  # Simulate work\n            \n            # Update metrics during execution\n            monitor.update_pipeline_metrics(\n                run_id,\n                records_processed=i * 1000,\n                records_failed=i * 2,\n                data_quality_score=99.5 - (i * 0.1)\n            )\n        \n        # Finish successfully\n        monitor.finish_pipeline_monitoring(run_id, PipelineStatus.SUCCESS)\n        \n    except Exception as e:\n        # Finish with failure\n        monitor.finish_pipeline_monitoring(\n            run_id, \n            PipelineStatus.FAILED, \n            error_message=str(e)\n        )\n    \n    # Get health status\n    health = monitor.get_pipeline_health_status(\"daily_sales_etl\")\n    print(f\"Pipeline Health: {json.dumps(health, indent=2)}\")\n    \n    # Setup alerts\n    alert_config = {\n        'email_recipients': ['data-team@company.com'],\n        'slack_channel': '#data-alerts'\n    }\n    \n    monitor.setup_alerts(\"daily_sales_etl\", alert_config)\n    \n    # Data freshness monitoring\n    freshness_monitor = DataFreshnessMonitor()\n    \n    # Register data sources\n    freshness_monitor.register_data_source(\"sales_api\", 60)  # Expected every hour\n    freshness_monitor.register_data_source(\"user_events\", 5)  # Expected every 5 minutes\n    \n    # Simulate data updates\n    freshness_monitor.update_data_timestamp(\"sales_api\", datetime.now() - timedelta(minutes=30))\n    freshness_monitor.update_data_timestamp(\"user_events\", datetime.now() - timedelta(minutes=10))\n    \n    # Check freshness\n    freshness_report = freshness_monitor.check_data_freshness()\n    print(f\"\\nData Freshness Report: {json.dumps(freshness_report, indent=2)}\")\n```\n\n### Dashboard and Alerting\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport requests\nimport json\n\nclass DataPipelineDashboard:\n    def __init__(self):\n        self.metrics_data = []\n        \n    def generate_pipeline_metrics_dashboard(self, pipeline_metrics: List[PipelineMetrics]):\n        \"\"\"Generate comprehensive dashboard for pipeline metrics\"\"\"\n        \n        # Convert metrics to DataFrame\n        df = pd.DataFrame([\n            {\n                'pipeline_name': m.pipeline_name,\n                'start_time': m.start_time,\n                'duration_seconds': m.duration_seconds,\n                'records_processed': m.records_processed,\n                'records_failed': m.records_failed,\n                'success_rate': ((m.records_processed - m.records_failed) / m.records_processed * 100) if m.records_processed > 0 else 0,\n                'data_quality_score': m.data_quality_score,\n                'status': m.status.value\n            }\n            for m in pipeline_metrics\n        ])\n        \n        # Create subplots\n        fig = make_subplots(\n            rows=3, cols=2,\n            subplot_titles=[\n                'Pipeline Success Rate Over Time',\n                'Processing Volume Trends',\n                'Data Quality Score Trends',\n                'Pipeline Duration Analysis',\n                'Pipeline Status Distribution',\n                'Error Rate Trends'\n            ],\n            specs=[\n                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n                [{\"type\": \"scatter\"}, {\"type\": \"box\"}],\n                [{\"type\": \"pie\"}, {\"type\": \"scatter\"}]\n            ]\n        )\n        \n        # 1. Success Rate Over Time\n        fig.add_trace(\n            go.Scatter(\n                x=df['start_time'],\n                y=df['success_rate'],\n                mode='lines+markers',\n                name='Success Rate %',\n                line=dict(color='green')\n            ),\n            row=1, col=1\n        )\n        \n        # 2. Processing Volume\n        volume_by_pipeline = df.groupby('pipeline_name')['records_processed'].sum().reset_index()\n        fig.add_trace(\n            go.Bar(\n                x=volume_by_pipeline['pipeline_name'],\n                y=volume_by_pipeline['records_processed'],\n                name='Records Processed',\n                marker_color='blue'\n            ),\n            row=1, col=2\n        )\n        \n        # 3. Data Quality Score\n        fig.add_trace(\n            go.Scatter(\n                x=df['start_time'],\n                y=df['data_quality_score'],\n                mode='lines+markers',\n                name='Data Quality Score',\n                line=dict(color='orange')\n            ),\n            row=2, col=1\n        )\n        \n        # 4. Duration Analysis\n        for pipeline in df['pipeline_name'].unique():\n            pipeline_data = df[df['pipeline_name'] == pipeline]\n            fig.add_trace(\n                go.Box(\n                    y=pipeline_data['duration_seconds'],\n                    name=pipeline,\n                    boxpoints='all'\n                ),\n                row=2, col=2\n            )\n        \n        # 5. Status Distribution\n        status_counts = df['status'].value_counts()\n        fig.add_trace(\n            go.Pie(\n                labels=status_counts.index,\n                values=status_counts.values,\n                name=\"Status Distribution\"\n            ),\n            row=3, col=1\n        )\n        \n        # 6. Error Rate Trends\n        df['error_rate'] = (df['records_failed'] / df['records_processed'] * 100).fillna(0)\n        fig.add_trace(\n            go.Scatter(\n                x=df['start_time'],\n                y=df['error_rate'],\n                mode='lines+markers',\n                name='Error Rate %',\n                line=dict(color='red')\n            ),\n            row=3, col=2\n        )\n        \n        # Update layout\n        fig.update_layout(\n            height=1200,\n            title=\"Data Pipeline Monitoring Dashboard\",\n            showlegend=False\n        )\n        \n        # Save dashboard\n        fig.write_html(\"pipeline_dashboard.html\")\n        fig.show()\n        \n        return fig\n    \n    def create_sla_report(self, pipeline_metrics: List[PipelineMetrics], sla_config: Dict[str, Any]):\n        \"\"\"Generate SLA compliance report\"\"\"\n        \n        sla_report = {\n            'report_date': datetime.now().isoformat(),\n            'sla_config': sla_config,\n            'pipeline_sla_status': {}\n        }\n        \n        for pipeline_name in set(m.pipeline_name for m in pipeline_metrics):\n            pipeline_data = [m for m in pipeline_metrics if m.pipeline_name == pipeline_name]\n            \n            # Calculate SLA metrics\n            total_runs = len(pipeline_data)\n            successful_runs = len([m for m in pipeline_data if m.status == PipelineStatus.SUCCESS])\n            success_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0\n            \n            avg_duration = np.mean([m.duration_seconds for m in pipeline_data])\n            avg_quality_score = np.mean([m.data_quality_score for m in pipeline_data])\n            \n            # Check SLA compliance\n            sla_config_for_pipeline = sla_config.get(pipeline_name, sla_config.get('default', {}))\n            \n            sla_status = {\n                'success_rate': {\n                    'actual': success_rate,\n                    'target': sla_config_for_pipeline.get('min_success_rate', 95),\n                    'compliant': success_rate >= sla_config_for_pipeline.get('min_success_rate', 95)\n                },\n                'avg_duration': {\n                    'actual': avg_duration,\n                    'target': sla_config_for_pipeline.get('max_duration_seconds', 3600),\n                    'compliant': avg_duration <= sla_config_for_pipeline.get('max_duration_seconds', 3600)\n                },\n                'data_quality': {\n                    'actual': avg_quality_score,\n                    'target': sla_config_for_pipeline.get('min_quality_score', 95),\n                    'compliant': avg_quality_score >= sla_config_for_pipeline.get('min_quality_score', 95)\n                }\n            }\n            \n            # Overall compliance\n            overall_compliant = all(metric['compliant'] for metric in sla_status.values())\n            sla_status['overall_compliant'] = overall_compliant\n            \n            sla_report['pipeline_sla_status'][pipeline_name] = sla_status\n        \n        return sla_report\n\nclass AlertManager:\n    def __init__(self):\n        self.notification_channels = {\n            'email': self._send_email_alert,\n            'slack': self._send_slack_alert,\n            'pagerduty': self._send_pagerduty_alert\n        }\n    \n    def send_alert(self, alert_type: str, severity: str, message: str, channels: List[str]):\n        \"\"\"Send alert to specified channels\"\"\"\n        \n        alert_payload = {\n            'alert_type': alert_type,\n            'severity': severity,\n            'message': message,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        for channel in channels:\n            if channel in self.notification_channels:\n                try:\n                    self.notification_channels[channel](alert_payload)\n                except Exception as e:\n                    print(f\"Failed to send alert to {channel}: {e}\")\n            else:\n                print(f\"Unknown notification channel: {channel}\")\n    \n    def _send_email_alert(self, alert_payload: Dict[str, Any]):\n        \"\"\"Send email alert\"\"\"\n        \n        # Email configuration (use environment variables in production)\n        smtp_server = \"smtp.gmail.com\"\n        smtp_port = 587\n        sender_email = \"alerts@company.com\"\n        sender_password = \"your-app-password\"\n        recipient_emails = [\"data-team@company.com\"]\n        \n        # Create message\n        message = MIMEMultipart()\n        message[\"From\"] = sender_email\n        message[\"To\"] = \", \".join(recipient_emails)\n        message[\"Subject\"] = f\"Data Pipeline Alert - {alert_payload['severity']}\"\n        \n        body = f\"\"\"\n        Alert Type: {alert_payload['alert_type']}\n        Severity: {alert_payload['severity']}\n        Timestamp: {alert_payload['timestamp']}\n        \n        Message:\n        {alert_payload['message']}\n        \"\"\"\n        \n        message.attach(MIMEText(body, \"plain\"))\n        \n        # Send email\n        try:\n            server = smtplib.SMTP(smtp_server, smtp_port)\n            server.starttls()\n            server.login(sender_email, sender_password)\n            text = message.as_string()\n            server.sendmail(sender_email, recipient_emails, text)\n            server.quit()\n            \n            print(f\"Email alert sent successfully\")\n            \n        except Exception as e:\n            print(f\"Failed to send email: {e}\")\n    \n    def _send_slack_alert(self, alert_payload: Dict[str, Any]):\n        \"\"\"Send Slack alert\"\"\"\n        \n        webhook_url = \"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\"\n        \n        # Create Slack message\n        color = {\n            'CRITICAL': '#ff0000',\n            'WARNING': '#ffaa00',\n            'INFO': '#0066cc'\n        }.get(alert_payload['severity'], '#666666')\n        \n        slack_message = {\n            \"attachments\": [\n                {\n                    \"color\": color,\n                    \"title\": f\"Data Pipeline Alert - {alert_payload['severity']}\",\n                    \"fields\": [\n                        {\n                            \"title\": \"Alert Type\",\n                            \"value\": alert_payload['alert_type'],\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Timestamp\",\n                            \"value\": alert_payload['timestamp'],\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Message\",\n                            \"value\": alert_payload['message'],\n                            \"short\": False\n                        }\n                    ]\n                }\n            ]\n        }\n        \n        try:\n            response = requests.post(\n                webhook_url,\n                json=slack_message,\n                headers={'Content-Type': 'application/json'},\n                timeout=10\n            )\n            response.raise_for_status()\n            \n            print(f\"Slack alert sent successfully\")\n            \n        except Exception as e:\n            print(f\"Failed to send Slack alert: {e}\")\n    \n    def _send_pagerduty_alert(self, alert_payload: Dict[str, Any]):\n        \"\"\"Send PagerDuty alert\"\"\"\n        \n        pagerduty_url = \"https://events.pagerduty.com/v2/enqueue\"\n        integration_key = \"your-pagerduty-integration-key\"\n        \n        if alert_payload['severity'] != 'CRITICAL':\n            return  # Only send critical alerts to PagerDuty\n        \n        pagerduty_payload = {\n            \"routing_key\": integration_key,\n            \"event_action\": \"trigger\",\n            \"payload\": {\n                \"summary\": f\"Data Pipeline Alert: {alert_payload['alert_type']}\",\n                \"source\": \"data-pipeline-monitor\",\n                \"severity\": alert_payload['severity'].lower(),\n                \"custom_details\": {\n                    \"message\": alert_payload['message'],\n                    \"timestamp\": alert_payload['timestamp']\n                }\n            }\n        }\n        \n        try:\n            response = requests.post(\n                pagerduty_url,\n                json=pagerduty_payload,\n                headers={'Content-Type': 'application/json'},\n                timeout=10\n            )\n            response.raise_for_status()\n            \n            print(f\"PagerDuty alert sent successfully\")\n            \n        except Exception as e:\n            print(f\"Failed to send PagerDuty alert: {e}\")\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Create sample metrics data\n    sample_metrics = []\n    \n    for i in range(30):  # 30 days of data\n        for pipeline in ['daily_etl', 'hourly_streaming', 'weekly_analytics']:\n            metrics = PipelineMetrics(\n                pipeline_name=pipeline,\n                start_time=datetime.now() - timedelta(days=29-i, hours=np.random.randint(0, 24)),\n                end_time=datetime.now() - timedelta(days=29-i, hours=np.random.randint(0, 24)),\n                status=PipelineStatus.SUCCESS if np.random.random() > 0.1 else PipelineStatus.FAILED,\n                records_processed=np.random.randint(10000, 100000),\n                records_failed=np.random.randint(0, 1000),\n                duration_seconds=np.random.uniform(300, 3600),\n                data_quality_score=np.random.uniform(95, 100)\n            )\n            sample_metrics.append(metrics)\n    \n    # Generate dashboard\n    dashboard = DataPipelineDashboard()\n    fig = dashboard.generate_pipeline_metrics_dashboard(sample_metrics)\n    \n    # Generate SLA report\n    sla_config = {\n        'default': {\n            'min_success_rate': 95,\n            'max_duration_seconds': 3600,\n            'min_quality_score': 95\n        },\n        'daily_etl': {\n            'min_success_rate': 98,\n            'max_duration_seconds': 1800,\n            'min_quality_score': 98\n        }\n    }\n    \n    sla_report = dashboard.create_sla_report(sample_metrics, sla_config)\n    print(f\"SLA Report: {json.dumps(sla_report, indent=2)}\")\n    \n    # Test alerting\n    alert_manager = AlertManager()\n    \n    alert_manager.send_alert(\n        alert_type=\"Pipeline Failure\",\n        severity=\"CRITICAL\",\n        message=\"The daily_etl pipeline failed with database connection error\",\n        channels=['email', 'slack', 'pagerduty']\n    )\n```\n\n---\n\n## ðŸ” Data Security & Compliance {#security}\n\n### Data Encryption and Security\n\n```python\nimport hashlib\nimport hmac\nimport base64\nimport os\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport boto3\nfrom datetime import datetime, timedelta\nimport json\nimport logging\nfrom typing import Dict, List, Any, Optional\n\nclass DataEncryptionManager:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def generate_encryption_key(self, password: str, salt: bytes = None) -> bytes:\n        \"\"\"Generate encryption key from password\"\"\"\n        if salt is None:\n            salt = os.urandom(16)\n        \n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=salt,\n            iterations=100000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))\n        return key\n    \n    def encrypt_data(self, data: str, key: bytes) -> bytes:\n        \"\"\"Encrypt data using Fernet symmetric encryption\"\"\"\n        f = Fernet(key)\n        encrypted_data = f.encrypt(data.encode())\n        return encrypted_data\n    \n    def decrypt_data(self, encrypted_data: bytes, key: bytes) -> str:\n        \"\"\"Decrypt data using Fernet symmetric encryption\"\"\"\n        f = Fernet(key)\n        decrypted_data = f.decrypt(encrypted_data)\n        return decrypted_data.decode()\n    \n    def encrypt_pii_fields(self, data: Dict[str, Any], pii_fields: List[str], key: bytes) -> Dict[str, Any]:\n        \"\"\"Encrypt specified PII fields in a dictionary\"\"\"\n        encrypted_data = data.copy()\n        \n        for field in pii_fields:\n            if field in encrypted_data:\n                original_value = str(encrypted_data[field])\n                encrypted_value = self.encrypt_data(original_value, key)\n                encrypted_data[field] = base64.b64encode(encrypted_value).decode()\n                encrypted_data[f\"{field}_encrypted\"] = True\n        \n        return encrypted_data\n    \n    def decrypt_pii_fields(self, data: Dict[str, Any], pii_fields: List[str], key: bytes) -> Dict[str, Any]:\n        \"\"\"Decrypt specified PII fields in a dictionary\"\"\"\n        decrypted_data = data.copy()\n        \n        for field in pii_fields:\n            if field in decrypted_data and decrypted_data.get(f\"{field}_encrypted\"):\n                encrypted_value = base64.b64decode(decrypted_data[field])\n                original_value = self.decrypt_data(encrypted_value, key)\n                decrypted_data[field] = original_value\n                decrypted_data.pop(f\"{field}_encrypted\", None)\n        \n        return decrypted_data\n\nclass DataMaskingManager:\n    \"\"\"Handle data masking for non-production environments\"\"\"\n    \n    def __init__(self):\n        self.masking_rules = {}\n    \n    def add_masking_rule(self, field_name: str, masking_type: str, **kwargs):\n        \"\"\"Add masking rule for a field\"\"\"\n        self.masking_rules[field_name] = {\n            'type': masking_type,\n            'options': kwargs\n        }\n    \n    def mask_email(self, email: str) -> str:\n        \"\"\"Mask email address\"\"\"\n        if '@' not in email:\n            return email\n        \n        local, domain = email.split('@', 1)\n        if len(local) <= 2:\n            masked_local = local[0] + '*'\n        else:\n            masked_local = local[0] + '*' * (len(local) - 2) + local[-1]\n        \n        return f\"{masked_local}@{domain}\"\n    \n    def mask_phone(self, phone: str) -> str:\n        \"\"\"Mask phone number\"\"\"\n        digits_only = ''.join(filter(str.isdigit, phone))\n        if len(digits_only) >= 4:\n            return phone[:-4] + '****'\n        return '****'\n    \n    def mask_credit_card(self, cc_number: str) -> str:\n        \"\"\"Mask credit card number\"\"\"\n        digits_only = ''.join(filter(str.isdigit, cc_number))\n        if len(digits_only) >= 4:\n            return '*' * (len(digits_only) - 4) + digits_only[-4:]\n        return '****'\n    \n    def mask_ssn(self, ssn: str) -> str:\n        \"\"\"Mask Social Security Number\"\"\"\n        digits_only = ''.join(filter(str.isdigit, ssn))\n        if len(digits_only) == 9:\n            return f\"***-**-{digits_only[-4:]}\"\n        return \"***-**-****\"\n    \n    def randomize_date(self, date_str: str, days_variance: int = 30) -> str:\n        \"\"\"Randomize date within specified variance\"\"\"\n        try:\n            original_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n            random_days = os.urandom(1)[0] % (days_variance * 2 + 1) - days_variance\n            new_date = original_date + timedelta(days=random_days)\n            return new_date.isoformat()\n        except:\n            return date_str\n    \n    def apply_masking(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Apply masking rules to data\"\"\"\n        masked_data = data.copy()\n        \n        for field_name, rule in self.masking_rules.items():\n            if field_name in masked_data:\n                original_value = masked_data[field_name]\n                \n                if rule['type'] == 'email':\n                    masked_data[field_name] = self.mask_email(str(original_value))\n                elif rule['type'] == 'phone':\n                    masked_data[field_name] = self.mask_phone(str(original_value))\n                elif rule['type'] == 'credit_card':\n                    masked_data[field_name] = self.mask_credit_card(str(original_value))\n                elif rule['type'] == 'ssn':\n                    masked_data[field_name] = self.mask_ssn(str(original_value))\n                elif rule['type'] == 'date_randomize':\n                    days_variance = rule['options'].get('days_variance', 30)\n                    masked_data[field_name] = self.randomize_date(str(original_value), days_variance)\n                elif rule['type'] == 'hash':\n                    salt = rule['options'].get('salt', 'default_salt')\n                    masked_data[field_name] = hashlib.sha256(f\"{salt}{original_value}\".encode()).hexdigest()[:10]\n        \n        return masked_data\n\nclass DataAccessManager:\n    \"\"\"Manage data access controls and audit logging\"\"\"\n    \n    def __init__(self):\n        self.access_policies = {}\n        self.access_logs = []\n        self.logger = logging.getLogger(__name__)\n    \n    def create_access_policy(self, policy_name: str, resources: List[str], \n                           actions: List[str], users: List[str], \n                           conditions: Dict[str, Any] = None):\n        \"\"\"Create data access policy\"\"\"\n        policy = {\n            'policy_name': policy_name,\n            'resources': resources,  # Tables, datasets, etc.\n            'actions': actions,      # read, write, delete, etc.\n            'users': users,          # List of authorized users\n            'conditions': conditions or {},  # Time-based, IP-based, etc.\n            'created_at': datetime.now().isoformat()\n        }\n        \n        self.access_policies[policy_name] = policy\n        self.logger.info(f\"Created access policy: {policy_name}\")\n        \n        return policy\n    \n    def check_access(self, user: str, resource: str, action: str, \n                    context: Dict[str, Any] = None) -> bool:\n        \"\"\"Check if user has access to perform action on resource\"\"\"\n        \n        context = context or {}\n        \n        for policy_name, policy in self.access_policies.items():\n            # Check if resource matches\n            resource_match = any(\n                resource.startswith(res) or res == '*' \n                for res in policy['resources']\n            )\n            \n            # Check if action is allowed\n            action_match = action in policy['actions'] or '*' in policy['actions']\n            \n            # Check if user is authorized\n            user_match = user in policy['users'] or '*' in policy['users']\n            \n            if resource_match and action_match and user_match:\n                # Check conditions\n                if self._check_conditions(policy.get('conditions', {}), context):\n                    self._log_access(user, resource, action, 'GRANTED', policy_name, context)\n                    return True\n        \n        self._log_access(user, resource, action, 'DENIED', None, context)\n        return False\n    \n    def _check_conditions(self, conditions: Dict[str, Any], context: Dict[str, Any]) -> bool:\n        \"\"\"Check if access conditions are met\"\"\"\n        \n        # Time-based conditions\n        if 'time_range' in conditions:\n            current_hour = datetime.now().hour\n            start_hour, end_hour = conditions['time_range']\n            if not (start_hour <= current_hour <= end_hour):\n                return False\n        \n        # IP-based conditions\n        if 'allowed_ips' in conditions:\n            user_ip = context.get('ip_address')\n            if user_ip not in conditions['allowed_ips']:\n                return False\n        \n        # Role-based conditions\n        if 'required_roles' in conditions:\n            user_roles = context.get('user_roles', [])\n            if not any(role in user_roles for role in conditions['required_roles']):\n                return False\n        \n        return True\n    \n    def _log_access(self, user: str, resource: str, action: str, \n                   result: str, policy: str, context: Dict[str, Any]):\n        \"\"\"Log access attempt\"\"\"\n        \n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'user': user,\n            'resource': resource,\n            'action': action,\n            'result': result,\n            'policy': policy,\n            'context': context\n        }\n        \n        self.access_logs.append(log_entry)\n        \n        # In production, send to centralized logging system\n        self.logger.info(f\"Access {result}: {user} -> {action} on {resource}\")\n    \n    def get_access_report(self, start_date: str = None, end_date: str = None) -> Dict[str, Any]:\n        \"\"\"Generate access report\"\"\"\n        \n        if start_date:\n            start_dt = datetime.fromisoformat(start_date)\n            filtered_logs = [\n                log for log in self.access_logs \n                if datetime.fromisoformat(log['timestamp']) >= start_dt\n            ]\n        else:\n            filtered_logs = self.access_logs\n        \n        if end_date:\n            end_dt = datetime.fromisoformat(end_date)\n            filtered_logs = [\n                log for log in filtered_logs \n                if datetime.fromisoformat(log['timestamp']) <= end_dt\n            ]\n        \n        report = {\n            'report_period': {\n                'start_date': start_date,\n                'end_date': end_date\n            },\n            'total_access_attempts': len(filtered_logs),\n            'granted_access': len([log for log in filtered_logs if log['result'] == 'GRANTED']),\n            'denied_access': len([log for log in filtered_logs if log['result'] == 'DENIED']),\n            'unique_users': len(set(log['user'] for log in filtered_logs)),\n            'most_accessed_resources': {},\n            'user_activity': {},\n            'access_patterns': []\n        }\n        \n        # Most accessed resources\n        resource_counts = {}\n        for log in filtered_logs:\n            resource = log['resource']\n            resource_counts[resource] = resource_counts.get(resource, 0) + 1\n        \n        report['most_accessed_resources'] = dict(\n            sorted(resource_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n        )\n        \n        # User activity\n        user_activity = {}\n        for log in filtered_logs:\n            user = log['user']\n            if user not in user_activity:\n                user_activity[user] = {'granted': 0, 'denied': 0}\n            user_activity[user][log['result'].lower()] += 1\n        \n        report['user_activity'] = user_activity\n        \n        return report\n\nclass GDPRComplianceManager:\n    \"\"\"Manage GDPR compliance requirements\"\"\"\n    \n    def __init__(self):\n        self.data_subjects = {}  # Track data subjects and their data\n        self.processing_activities = []\n        self.consent_records = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def register_data_subject(self, subject_id: str, personal_data: Dict[str, Any]):\n        \"\"\"Register a data subject and their personal data\"\"\"\n        self.data_subjects[subject_id] = {\n            'personal_data': personal_data,\n            'registered_at': datetime.now().isoformat(),\n            'consent_given': False,\n            'data_locations': []  # Track where their data is stored\n        }\n        \n        self.logger.info(f\"Registered data subject: {subject_id}\")\n    \n    def record_consent(self, subject_id: str, purpose: str, consent_given: bool, \n                      consent_method: str = None):\n        \"\"\"Record consent for data processing\"\"\"\n        if subject_id not in self.consent_records:\n            self.consent_records[subject_id] = []\n        \n        consent_record = {\n            'purpose': purpose,\n            'consent_given': consent_given,\n            'consent_method': consent_method,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        self.consent_records[subject_id].append(consent_record)\n        \n        if subject_id in self.data_subjects:\n            self.data_subjects[subject_id]['consent_given'] = consent_given\n        \n        self.logger.info(f\"Recorded consent for {subject_id}: {purpose} = {consent_given}\")\n    \n    def handle_data_subject_request(self, subject_id: str, request_type: str) -> Dict[str, Any]:\n        \"\"\"Handle data subject rights requests (GDPR Article 15-22)\"\"\"\n        \n        if subject_id not in self.data_subjects:\n            return {'error': 'Data subject not found', 'status': 'failed'}\n        \n        response = {\n            'subject_id': subject_id,\n            'request_type': request_type,\n            'processed_at': datetime.now().isoformat(),\n            'status': 'completed'\n        }\n        \n        if request_type == 'access':  # Right to access (Article 15)\n            response['personal_data'] = self.data_subjects[subject_id]['personal_data']\n            response['consent_records'] = self.consent_records.get(subject_id, [])\n            response['data_locations'] = self.data_subjects[subject_id]['data_locations']\n            \n        elif request_type == 'rectification':  # Right to rectification (Article 16)\n            response['message'] = 'Data rectification process initiated'\n            # In practice, you would update the data here\n            \n        elif request_type == 'erasure':  # Right to erasure (Article 17)\n            # Remove personal data\n            del self.data_subjects[subject_id]\n            if subject_id in self.consent_records:\n                del self.consent_records[subject_id]\n            \n            response['message'] = 'Personal data erased successfully'\n            # In practice, you would delete from all systems\n            \n        elif request_type == 'portability':  # Right to data portability (Article 20)\n            response['portable_data'] = self.data_subjects[subject_id]['personal_data']\n            response['format'] = 'JSON'\n            \n        elif request_type == 'object':  # Right to object (Article 21)\n            response['message'] = 'Processing objection recorded'\n            # Stop processing for marketing, etc.\n            \n        elif request_type == 'restrict':  # Right to restrict processing (Article 18)\n            response['message'] = 'Processing restriction applied'\n            # Implement processing restrictions\n        \n        self.logger.info(f\"Processed {request_type} request for {subject_id}\")\n        return response\n    \n    def generate_compliance_report(self) -> Dict[str, Any]:\n        \"\"\"Generate GDPR compliance report\"\"\"\n        \n        total_subjects = len(self.data_subjects)\n        consented_subjects = len([\n            s for s in self.data_subjects.values() \n            if s['consent_given']\n        ])\n        \n        report = {\n            'report_date': datetime.now().isoformat(),\n            'total_data_subjects': total_subjects,\n            'consented_subjects': consented_subjects,\n            'consent_rate': (consented_subjects / total_subjects * 100) if total_subjects > 0 else 0,\n            'total_consent_records': sum(len(records) for records in self.consent_records.values()),\n            'data_retention_status': 'compliant',  # Would check actual retention\n            'processing_lawfulness': 'consent',\n            'recommendations': []\n        }\n        \n        # Add recommendations based on analysis\n        if report['consent_rate'] < 100:\n            report['recommendations'].append(\n                \"Review consent collection process for unconsented subjects\"\n            )\n        \n        return report\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Data encryption\n    encryption_manager = DataEncryptionManager()\n    \n    # Generate encryption key\n    password = \"secure_password_123\"\n    key = encryption_manager.generate_encryption_key(password)\n    \n    # Sample customer data with PII\n    customer_data = {\n        'customer_id': 'CUST001',\n        'name': 'John Doe',\n        'email': 'john.doe@example.com',\n        'phone': '+1-555-0123',\n        'ssn': '123-45-6789',\n        'address': '123 Main St, Anytown, USA',\n        'purchase_amount': 299.99\n    }\n    \n    # Encrypt PII fields\n    pii_fields = ['name', 'email', 'phone', 'ssn', 'address']\n    encrypted_data = encryption_manager.encrypt_pii_fields(customer_data, pii_fields, key)\n    print(f\"Encrypted data: {json.dumps(encrypted_data, indent=2)}\")\n    \n    # Data masking for non-production\n    masking_manager = DataMaskingManager()\n    masking_manager.add_masking_rule('email', 'email')\n    masking_manager.add_masking_rule('phone', 'phone')\n    masking_manager.add_masking_rule('ssn', 'ssn')\n    \n    masked_data = masking_manager.apply_masking(customer_data)\n    print(f\"\\nMasked data: {json.dumps(masked_data, indent=2)}\")\n    \n    # Access control\n    access_manager = DataAccessManager()\n    \n    # Create access policies\n    access_manager.create_access_policy(\n        'data_analysts_policy',\n        resources=['customer_data', 'sales_data'],\n        actions=['read'],\n        users=['analyst1@company.com', 'analyst2@company.com'],\n        conditions={\n            'time_range': (9, 17),  # 9 AM to 5 PM\n            'required_roles': ['analyst', 'data_scientist']\n        }\n    )\n    \n    # Check access\n    context = {\n        'ip_address': '192.168.1.100',\n        'user_roles': ['analyst']\n    }\n    \n    access_granted = access_manager.check_access(\n        'analyst1@company.com',\n        'customer_data',\n        'read',\n        context\n    )\n    print(f\"\\nAccess granted: {access_granted}\")\n    \n    # Generate access report\n    access_report = access_manager.get_access_report()\n    print(f\"\\nAccess report: {json.dumps(access_report, indent=2)}\")\n    \n    # GDPR compliance\n    gdpr_manager = GDPRComplianceManager()\n    \n    # Register data subject\n    gdpr_manager.register_data_subject('john.doe@example.com', customer_data)\n    \n    # Record consent\n    gdpr_manager.record_consent(\n        'john.doe@example.com',\n        'marketing_communications',\n        True,\n        'web_form'\n    )\n    \n    # Handle data subject request\n    access_request = gdpr_manager.handle_data_subject_request(\n        'john.doe@example.com',\n        'access'\n    )\n    print(f\"\\nGDPR access request response: {json.dumps(access_request, indent=2)}\")\n    \n    # Generate compliance report\n    compliance_report = gdpr_manager.generate_compliance_report()\n    print(f\"\\nGDPR compliance report: {json.dumps(compliance_report, indent=2)}\")\n```\n\n---\n\n## ðŸ’¡ Best Practices & Patterns {#best-practices}\n\n### Data Engineering Design Patterns\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Callable\nimport pandas as pd\nimport logging\nfrom datetime import datetime\nimport asyncio\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\n\n# 1. Strategy Pattern for Data Processing\nclass ProcessingStrategy(ABC):\n    @abstractmethod\n    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n        pass\n\nclass BatchProcessingStrategy(ProcessingStrategy):\n    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process data in batches\"\"\"\n        # Apply batch transformations\n        data['processed_at'] = datetime.now()\n        data['processing_type'] = 'batch'\n        return data.groupby('customer_id').agg({\n            'amount': 'sum',\n            'quantity': 'sum',\n            'processed_at': 'first',\n            'processing_type': 'first'\n        }).reset_index()\n\nclass StreamProcessingStrategy(ProcessingStrategy):\n    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process data as stream\"\"\"\n        # Apply stream transformations\n        data['processed_at'] = datetime.now()\n        data['processing_type'] = 'stream'\n        # Real-time aggregations\n        return data\n\nclass RealTimeProcessingStrategy(ProcessingStrategy):\n    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process data in real-time\"\"\"\n        data['processed_at'] = datetime.now()\n        data['processing_type'] = 'realtime'\n        # Apply real-time transformations\n        data['alert_triggered'] = data['amount'] > 1000\n        return data\n\nclass DataProcessor:\n    def __init__(self, strategy: ProcessingStrategy):\n        self._strategy = strategy\n    \n    def set_strategy(self, strategy: ProcessingStrategy):\n        self._strategy = strategy\n    \n    def process_data(self, data: pd.DataFrame) -> pd.DataFrame:\n        return self._strategy.process(data)\n\n# 2. Factory Pattern for Data Sources\nclass DataSourceType(Enum):\n    DATABASE = \"database\"\n    API = \"api\"\n    FILE = \"file\"\n    STREAM = \"stream\"\n\nclass DataSource(ABC):\n    @abstractmethod\n    def extract(self) -> pd.DataFrame:\n        pass\n    \n    @abstractmethod\n    def validate_connection(self) -> bool:\n        pass\n\nclass DatabaseSource(DataSource):\n    def __init__(self, connection_string: str, query: str):\n        self.connection_string = connection_string\n        self.query = query\n    \n    def validate_connection(self) -> bool:\n        # Validate database connection\n        return True\n    \n    def extract(self) -> pd.DataFrame:\n        # Extract data from database\n        return pd.DataFrame({'id': [1, 2, 3], 'value': [10, 20, 30]})\n\nclass APISource(DataSource):\n    def __init__(self, endpoint: str, headers: Dict[str, str]):\n        self.endpoint = endpoint\n        self.headers = headers\n    \n    def validate_connection(self) -> bool:\n        # Validate API endpoint\n        return True\n    \n    def extract(self) -> pd.DataFrame:\n        # Extract data from API\n        return pd.DataFrame({'api_data': ['a', 'b', 'c']})\n\nclass FileSource(DataSource):\n    def __init__(self, file_path: str, file_format: str):\n        self.file_path = file_path\n        self.file_format = file_format\n    \n    def validate_connection(self) -> bool:\n        # Validate file exists\n        return True\n    \n    def extract(self) -> pd.DataFrame:\n        # Extract data from file\n        return pd.DataFrame({'file_data': [1, 2, 3]})\n\nclass DataSourceFactory:\n    @staticmethod\n    def create_source(source_type: DataSourceType, **kwargs) -> DataSource:\n        if source_type == DataSourceType.DATABASE:\n            return DatabaseSource(\n                kwargs.get('connection_string'),\n                kwargs.get('query')\n            )\n        elif source_type == DataSourceType.API:\n            return APISource(\n                kwargs.get('endpoint'),\n                kwargs.get('headers', {})\n            )\n        elif source_type == DataSourceType.FILE:\n            return FileSource(\n                kwargs.get('file_path'),\n                kwargs.get('file_format', 'csv')\n            )\n        else:\n            raise ValueError(f\"Unknown source type: {source_type}\")\n\n# 3. Observer Pattern for Data Quality Monitoring\nclass DataQualityObserver(ABC):\n    @abstractmethod\n    def notify(self, data: pd.DataFrame, quality_metrics: Dict[str, Any]):\n        pass\n\nclass EmailAlertObserver(DataQualityObserver):\n    def notify(self, data: pd.DataFrame, quality_metrics: Dict[str, Any]):\n        if quality_metrics['quality_score'] < 95:\n            print(f\"EMAIL ALERT: Data quality score is {quality_metrics['quality_score']}\")\n\nclass MetricsLogger(DataQualityObserver):\n    def notify(self, data: pd.DataFrame, quality_metrics: Dict[str, Any]):\n        logging.info(f\"Data Quality Metrics: {quality_metrics}\")\n\nclass DataQualitySubject:\n    def __init__(self):\n        self._observers: List[DataQualityObserver] = []\n    \n    def attach(self, observer: DataQualityObserver):\n        self._observers.append(observer)\n    \n    def detach(self, observer: DataQualityObserver):\n        self._observers.remove(observer)\n    \n    def notify_observers(self, data: pd.DataFrame, quality_metrics: Dict[str, Any]):\n        for observer in self._observers:\n            observer.notify(data, quality_metrics)\n    \n    def check_quality(self, data: pd.DataFrame):\n        # Calculate quality metrics\n        quality_metrics = {\n            'total_records': len(data),\n            'null_count': data.isnull().sum().sum(),\n            'duplicate_count': data.duplicated().sum(),\n            'quality_score': 100 - (data.isnull().sum().sum() / len(data) * 100)\n        }\n        \n        self.notify_observers(data, quality_metrics)\n        return quality_metrics\n\n# 4. Command Pattern for Pipeline Operations\nclass PipelineCommand(ABC):\n    @abstractmethod\n    def execute(self) -> Any:\n        pass\n    \n    @abstractmethod\n    def undo(self) -> Any:\n        pass\n\nclass ExtractCommand(PipelineCommand):\n    def __init__(self, source: DataSource):\n        self.source = source\n        self.extracted_data = None\n    \n    def execute(self) -> pd.DataFrame:\n        self.extracted_data = self.source.extract()\n        print(f\"Extracted {len(self.extracted_data)} records\")\n        return self.extracted_data\n    \n    def undo(self):\n        self.extracted_data = None\n        print(\"Extract operation undone\")\n\nclass TransformCommand(PipelineCommand):\n    def __init__(self, data: pd.DataFrame, transformation_func: Callable):\n        self.data = data\n        self.transformation_func = transformation_func\n        self.original_data = data.copy()\n        self.transformed_data = None\n    \n    def execute(self) -> pd.DataFrame:\n        self.transformed_data = self.transformation_func(self.data)\n        print(f\"Transformed data: {len(self.transformed_data)} records\")\n        return self.transformed_data\n    \n    def undo(self) -> pd.DataFrame:\n        self.transformed_data = None\n        print(\"Transform operation undone\")\n        return self.original_data\n\nclass LoadCommand(PipelineCommand):\n    def __init__(self, data: pd.DataFrame, destination: str):\n        self.data = data\n        self.destination = destination\n        self.loaded = False\n    \n    def execute(self):\n        # Simulate loading data\n        print(f\"Loaded {len(self.data)} records to {self.destination}\")\n        self.loaded = True\n    \n    def undo(self):\n        if self.loaded:\n            print(f\"Removed data from {self.destination}\")\n            self.loaded = False\n\nclass PipelineInvoker:\n    def __init__(self):\n        self.commands: List[PipelineCommand] = []\n        self.executed_commands: List[PipelineCommand] = []\n    \n    def add_command(self, command: PipelineCommand):\n        self.commands.append(command)\n    \n    def execute_pipeline(self):\n        results = []\n        for command in self.commands:\n            try:\n                result = command.execute()\n                self.executed_commands.append(command)\n                results.append(result)\n            except Exception as e:\n                print(f\"Pipeline execution failed at {command.__class__.__name__}: {e}\")\n                self.rollback()\n                raise\n        return results\n    \n    def rollback(self):\n        print(\"Rolling back pipeline...\")\n        for command in reversed(self.executed_commands):\n            command.undo()\n        self.executed_commands.clear()\n\n# 5. Builder Pattern for Complex Pipelines\nclass PipelineBuilder:\n    def __init__(self):\n        self.source = None\n        self.transformations = []\n        self.destination = None\n        self.quality_checks = []\n        self.error_handling = None\n    \n    def with_source(self, source: DataSource):\n        self.source = source\n        return self\n    \n    def add_transformation(self, transformation: Callable):\n        self.transformations.append(transformation)\n        return self\n    \n    def with_destination(self, destination: str):\n        self.destination = destination\n        return self\n    \n    def add_quality_check(self, check: Callable):\n        self.quality_checks.append(check)\n        return self\n    \n    def with_error_handling(self, handler: Callable):\n        self.error_handling = handler\n        return self\n    \n    def build(self):\n        return DataPipeline(\n            source=self.source,\n            transformations=self.transformations,\n            destination=self.destination,\n            quality_checks=self.quality_checks,\n            error_handler=self.error_handling\n        )\n\nclass DataPipeline:\n    def __init__(self, source: DataSource, transformations: List[Callable],\n                 destination: str, quality_checks: List[Callable],\n                 error_handler: Callable = None):\n        self.source = source\n        self.transformations = transformations\n        self.destination = destination\n        self.quality_checks = quality_checks\n        self.error_handler = error_handler\n    \n    def run(self):\n        try:\n            # Extract\n            data = self.source.extract()\n            \n            # Transform\n            for transformation in self.transformations:\n                data = transformation(data)\n            \n            # Quality checks\n            for check in self.quality_checks:\n                if not check(data):\n                    raise ValueError(\"Data quality check failed\")\n            \n            # Load\n            print(f\"Loading {len(data)} records to {self.destination}\")\n            \n            return data\n            \n        except Exception as e:\n            if self.error_handler:\n                self.error_handler(e)\n            else:\n                raise\n\n# 6. Singleton Pattern for Configuration Management\nclass DataPipelineConfig:\n    _instance = None\n    _config = {}\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n    \n    def set_config(self, key: str, value: Any):\n        self._config[key] = value\n    \n    def get_config(self, key: str, default: Any = None) -> Any:\n        return self._config.get(key, default)\n    \n    def load_from_file(self, config_file: str):\n        # Load configuration from file\n        self._config.update({\n            'database_url': 'postgresql://localhost:5432/data',\n            'max_batch_size': 10000,\n            'retry_attempts': 3,\n            'notification_email': 'data-team@company.com'\n        })\n```\n\n### Performance Optimization Patterns\n\n```python\nimport time\nimport functools\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nimport threading\nimport queue\nimport asyncio\nfrom typing import Callable, Any, List\nimport pandas as pd\nimport numpy as np\n\n# 1. Caching Pattern\nclass DataCache:\n    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n        self.cache = {}\n        self.access_times = {}\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.lock = threading.Lock()\n    \n    def get(self, key: str) -> Any:\n        with self.lock:\n            if key in self.cache:\n                # Check TTL\n                if time.time() - self.access_times[key] < self.ttl_seconds:\n                    self.access_times[key] = time.time()  # Update access time\n                    return self.cache[key]\n                else:\n                    # Expired\n                    del self.cache[key]\n                    del self.access_times[key]\n            return None\n    \n    def put(self, key: str, value: Any):\n        with self.lock:\n            # Evict oldest if cache is full\n            if len(self.cache) >= self.max_size:\n                oldest_key = min(self.access_times, key=self.access_times.get)\n                del self.cache[oldest_key]\n                del self.access_times[oldest_key]\n            \n            self.cache[key] = value\n            self.access_times[key] = time.time()\n\ndef cached(cache: DataCache):\n    \"\"\"Decorator for caching function results\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = f\"{func.__name__}_{hash(str(args) + str(kwargs))}\"\n            \n            # Try to get from cache\n            result = cache.get(key)\n            if result is not None:\n                print(f\"Cache hit for {func.__name__}\")\n                return result\n            \n            # Compute result\n            print(f\"Computing {func.__name__}\")\n            result = func(*args, **kwargs)\n            \n            # Store in cache\n            cache.put(key, result)\n            \n            return result\n        return wrapper\n    return decorator\n\n# 2. Connection Pooling Pattern\nclass ConnectionPool:\n    def __init__(self, connection_factory: Callable, pool_size: int = 10):\n        self.connection_factory = connection_factory\n        self.pool = queue.Queue(maxsize=pool_size)\n        self.active_connections = set()\n        \n        # Pre-populate pool\n        for _ in range(pool_size):\n            conn = self.connection_factory()\n            self.pool.put(conn)\n    \n    def get_connection(self):\n        \"\"\"Get connection from pool\"\"\"\n        try:\n            conn = self.pool.get(timeout=30)  # 30 second timeout\n            self.active_connections.add(conn)\n            return conn\n        except queue.Empty:\n            raise Exception(\"Connection pool exhausted\")\n    \n    def return_connection(self, conn):\n        \"\"\"Return connection to pool\"\"\"\n        if conn in self.active_connections:\n            self.active_connections.remove(conn)\n            self.pool.put(conn)\n    \n    def close_all(self):\n        \"\"\"Close all connections\"\"\"\n        while not self.pool.empty():\n            conn = self.pool.get()\n            if hasattr(conn, 'close'):\n                conn.close()\n\n# 3. Parallel Processing Pattern\nclass ParallelProcessor:\n    def __init__(self, n_workers: int = None):\n        self.n_workers = n_workers or mp.cpu_count()\n    \n    def process_chunks_multiprocessing(self, data: pd.DataFrame, \n                                     process_func: Callable, \n                                     chunk_size: int = 10000) -> pd.DataFrame:\n        \"\"\"Process data chunks using multiprocessing\"\"\"\n        \n        # Split data into chunks\n        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n        \n        # Process chunks in parallel\n        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:\n            results = list(executor.map(process_func, chunks))\n        \n        # Combine results\n        return pd.concat(results, ignore_index=True)\n    \n    def process_chunks_threading(self, data: List[Any], \n                               process_func: Callable) -> List[Any]:\n        \"\"\"Process data using threading (I/O bound tasks)\"\"\"\n        \n        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n            results = list(executor.map(process_func, data))\n        \n        return results\n    \n    async def process_chunks_async(self, data: List[Any], \n                                 async_process_func: Callable) -> List[Any]:\n        \"\"\"Process data using async/await\"\"\"\n        \n        semaphore = asyncio.Semaphore(self.n_workers)\n        \n        async def process_with_semaphore(item):\n            async with semaphore:\n                return await async_process_func(item)\n        \n        tasks = [process_with_semaphore(item) for item in data]\n        results = await asyncio.gather(*tasks)\n        \n        return results\n\n# 4. Circuit Breaker Pattern for Resilience\nclass CircuitBreakerState:\n    CLOSED = \"CLOSED\"\n    OPEN = \"OPEN\"\n    HALF_OPEN = \"HALF_OPEN\"\n\nclass CircuitBreaker:\n    def __init__(self, failure_threshold: int = 5, \n                 recovery_timeout: int = 60,\n                 expected_exception: type = Exception):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exception = expected_exception\n        \n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitBreakerState.CLOSED\n    \n    def call(self, func: Callable, *args, **kwargs) -> Any:\n        if self.state == CircuitBreakerState.OPEN:\n            if time.time() - self.last_failure_time > self.recovery_timeout:\n                self.state = CircuitBreakerState.HALF_OPEN\n            else:\n                raise Exception(\"Circuit breaker is OPEN\")\n        \n        try:\n            result = func(*args, **kwargs)\n            self.on_success()\n            return result\n        except self.expected_exception as e:\n            self.on_failure()\n            raise e\n    \n    def on_success(self):\n        self.failure_count = 0\n        self.state = CircuitBreakerState.CLOSED\n    \n    def on_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.failure_count >= self.failure_threshold:\n            self.state = CircuitBreakerState.OPEN\n\n# 5. Batch Processing Pattern\nclass BatchProcessor:\n    def __init__(self, batch_size: int = 1000, max_wait_time: int = 30):\n        self.batch_size = batch_size\n        self.max_wait_time = max_wait_time\n        self.batch = []\n        self.last_batch_time = time.time()\n    \n    def add_item(self, item: Any, process_func: Callable = None):\n        self.batch.append(item)\n        \n        # Process if batch is full or max wait time exceeded\n        if (len(self.batch) >= self.batch_size or \n            time.time() - self.last_batch_time > self.max_wait_time):\n            self.process_batch(process_func)\n    \n    def process_batch(self, process_func: Callable = None):\n        if self.batch:\n            if process_func:\n                process_func(self.batch)\n            else:\n                print(f\"Processing batch of {len(self.batch)} items\")\n            \n            self.batch.clear()\n            self.last_batch_time = time.time()\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Example usage of design patterns\n    \n    # 1. Strategy Pattern\n    print(\"1. Strategy Pattern Example:\")\n    sample_data = pd.DataFrame({\n        'customer_id': [1, 2, 3, 1, 2],\n        'amount': [100, 200, 300, 150, 250],\n        'quantity': [1, 2, 3, 1, 2]\n    })\n    \n    processor = DataProcessor(BatchProcessingStrategy())\n    batch_result = processor.process_data(sample_data)\n    print(f\"Batch processing result:\\n{batch_result}\")\n    \n    processor.set_strategy(StreamProcessingStrategy())\n    stream_result = processor.process_data(sample_data)\n    print(f\"Stream processing result:\\n{stream_result}\")\n    \n    # 2. Factory Pattern\n    print(\"\\n2. Factory Pattern Example:\")\n    db_source = DataSourceFactory.create_source(\n        DataSourceType.DATABASE,\n        connection_string=\"postgresql://localhost/db\",\n        query=\"SELECT * FROM users\"\n    )\n    \n    api_source = DataSourceFactory.create_source(\n        DataSourceType.API,\n        endpoint=\"https://api.example.com/data\",\n        headers={\"Authorization\": \"Bearer token\"}\n    )\n    \n    print(f\"Created database source: {type(db_source).__name__}\")\n    print(f\"Created API source: {type(api_source).__name__}\")\n    \n    # 3. Observer Pattern\n    print(\"\\n3. Observer Pattern Example:\")\n    quality_subject = DataQualitySubject()\n    quality_subject.attach(EmailAlertObserver())\n    quality_subject.attach(MetricsLogger())\n    \n    # Add some null values to trigger quality alerts\n    sample_data_with_nulls = sample_data.copy()\n    sample_data_with_nulls.loc[0, 'amount'] = None\n    sample_data_with_nulls.loc[1, 'customer_id'] = None\n    \n    quality_metrics = quality_subject.check_quality(sample_data_with_nulls)\n    \n    # 4. Command Pattern\n    print(\"\\n4. Command Pattern Example:\")\n    invoker = PipelineInvoker()\n    \n    # Add commands\n    extract_cmd = ExtractCommand(db_source)\n    transform_cmd = TransformCommand(\n        sample_data, \n        lambda df: df.groupby('customer_id').sum().reset_index()\n    )\n    load_cmd = LoadCommand(sample_data, \"warehouse_table\")\n    \n    invoker.add_command(extract_cmd)\n    invoker.add_command(transform_cmd)\n    invoker.add_command(load_cmd)\n    \n    # Execute pipeline\n    try:\n        results = invoker.execute_pipeline()\n        print(\"Pipeline executed successfully\")\n    except Exception as e:\n        print(f\"Pipeline failed: {e}\")\n    \n    # 5. Builder Pattern\n    print(\"\\n5. Builder Pattern Example:\")\n    \n    def sample_transformation(df):\n        df['transformed'] = df['amount'] * 2\n        return df\n    \n    def quality_check(df):\n        return len(df) > 0 and df['amount'].notna().all()\n    \n    pipeline = (PipelineBuilder()\n                .with_source(db_source)\n                .add_transformation(sample_transformation)\n                .add_quality_check(quality_check)\n                .with_destination(\"output_table\")\n                .with_error_handling(lambda e: print(f\"Error handled: {e}\"))\n                .build())\n    \n    try:\n        result = pipeline.run()\n        print(\"Builder pattern pipeline executed successfully\")\n    except Exception as e:\n        print(f\"Builder pattern pipeline failed: {e}\")\n    \n    # 6. Performance Patterns\n    print(\"\\n6. Performance Patterns Example:\")\n    \n    # Caching\n    cache = DataCache(max_size=100, ttl_seconds=60)\n    \n    @cached(cache)\n    def expensive_calculation(n):\n        time.sleep(1)  # Simulate expensive operation\n        return n * n\n    \n    # First call - computed\n    result1 = expensive_calculation(5)\n    \n    # Second call - cached\n    result2 = expensive_calculation(5)\n    \n    # Parallel processing\n    def process_chunk(chunk):\n        return chunk ** 2\n    \n    large_array = np.random.randint(0, 100, 10000)\n    chunks = np.array_split(large_array, 4)\n    \n    processor = ParallelProcessor(n_workers=4)\n    \n    # Process with threading (simulated I/O bound)\n    results = processor.process_chunks_threading(\n        chunks, \n        lambda chunk: np.sum(chunk ** 2)\n    )\n    \n    print(f\"Parallel processing results: {results}\")\n    \n    # Circuit breaker\n    def unreliable_service():\n        if np.random.random() < 0.7:  # 70% failure rate\n            raise Exception(\"Service temporarily unavailable\")\n        return \"Success!\"\n    \n    circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=5)\n    \n    for i in range(10):\n        try:\n            result = circuit_breaker.call(unreliable_service)\n            print(f\"Call {i}: {result}\")\n        except Exception as e:\n            print(f\"Call {i}: Failed - {e}\")\n        time.sleep(1)\n\n---\n\n## ðŸŽ¯ Real-World Projects {#projects}\n\n### Project 1: E-commerce Analytics Platform\n\n**Overview**: Build a complete data pipeline for e-commerce analytics with real-time dashboards.\n\n**Architecture**:\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Website   â”‚â”€â”€â”€â–¶â”‚   Kafka     â”‚â”€â”€â”€â–¶â”‚   Spark     â”‚â”€â”€â”€â–¶â”‚ Data Lake   â”‚\nâ”‚   Events    â”‚    â”‚  Streams    â”‚    â”‚ Processing  â”‚    â”‚   (S3)      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                                 â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\nâ”‚  Dashboard  â”‚â—„â”€â”€â”€â”‚ Redshift    â”‚â—„â”€â”€â”€â”‚   Airflow   â”‚    â”‚  Raw Data   â”‚\nâ”‚  (Tableau)  â”‚    â”‚Data Warehouseâ”‚    â”‚ Scheduler   â”‚    â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Implementation**:\n\n```python\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nimport boto3\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport requests\nimport json\n\n# Define the DAG\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'ecommerce_analytics_pipeline',\n    default_args=default_args,\n    description='Complete e-commerce analytics pipeline',\n    schedule_interval=timedelta(hours=1),\n    catchup=False,\n    tags=['ecommerce', 'analytics', 'production']\n)\n\ndef extract_user_events(**context):\n    \"\"\"Extract user events from various sources\"\"\"\n    s3_client = boto3.client('s3')\n    \n    # Extract from web analytics API\n    events_data = []\n    \n    # Simulate extracting from Google Analytics API\n    ga_events = extract_google_analytics_data()\n    events_data.extend(ga_events)\n    \n    # Extract from application database\n    db_events = extract_database_events()\n    events_data.extend(db_events)\n    \n    # Extract from mobile app events\n    mobile_events = extract_mobile_events()\n    events_data.extend(mobile_events)\n    \n    # Save to S3\n    df = pd.DataFrame(events_data)\n    \n    execution_date = context['execution_date']\n    s3_key = f\"raw-events/date={execution_date.date()}/events.parquet\"\n    \n    # Upload to S3\n    parquet_buffer = df.to_parquet(index=False)\n    s3_client.put_object(\n        Bucket='ecommerce-data-lake',\n        Key=s3_key,\n        Body=parquet_buffer\n    )\n    \n    return s3_key\n\ndef extract_google_analytics_data():\n    \"\"\"Extract data from Google Analytics API\"\"\"\n    # Simulate Google Analytics API call\n    return [\n        {\n            'event_type': 'page_view',\n            'user_id': f'user_{i}',\n            'page_url': f'/product/{i % 100}',\n            'timestamp': datetime.now() - timedelta(hours=i),\n            'session_id': f'session_{i // 10}',\n            'source': 'google_analytics'\n        }\n        for i in range(1000)\n    ]\n\ndef extract_database_events():\n    \"\"\"Extract events from application database\"\"\"\n    engine = create_engine('postgresql://user:pass@localhost:5432/ecommerce')\n    \n    query = \"\"\"\n    SELECT \n        event_id,\n        user_id,\n        event_type,\n        product_id,\n        amount,\n        timestamp,\n        'database' as source\n    FROM user_events \n    WHERE timestamp >= NOW() - INTERVAL '1 hour'\n    \"\"\"\n    \n    df = pd.read_sql(query, engine)\n    return df.to_dict('records')\n\ndef extract_mobile_events():\n    \"\"\"Extract events from mobile analytics\"\"\"\n    # Simulate mobile analytics API\n    return [\n        {\n            'event_type': 'app_open',\n            'user_id': f'mobile_user_{i}',\n            'device_type': 'mobile',\n            'timestamp': datetime.now() - timedelta(minutes=i),\n            'source': 'mobile_app'\n        }\n        for i in range(500)\n    ]\n\ndef transform_and_enrich_data(**context):\n    \"\"\"Transform and enrich the raw event data\"\"\"\n    s3_client = boto3.client('s3')\n    \n    # Get the S3 key from previous task\n    s3_key = context['task_instance'].xcom_pull(task_ids='extract_events')\n    \n    # Read data from S3\n    response = s3_client.get_object(Bucket='ecommerce-data-lake', Key=s3_key)\n    df = pd.read_parquet(response['Body'])\n    \n    # Data transformations\n    df = transform_events_data(df)\n    \n    # Enrich with user data\n    df = enrich_with_user_data(df)\n    \n    # Enrich with product data  \n    df = enrich_with_product_data(df)\n    \n    # Calculate derived metrics\n    df = calculate_derived_metrics(df)\n    \n    # Save transformed data\n    execution_date = context['execution_date']\n    transformed_key = f\"transformed-events/date={execution_date.date()}/enriched_events.parquet\"\n    \n    parquet_buffer = df.to_parquet(index=False)\n    s3_client.put_object(\n        Bucket='ecommerce-data-lake',\n        Key=transformed_key,\n        Body=parquet_buffer\n    )\n    \n    return transformed_key\n\ndef transform_events_data(df):\n    \"\"\"Apply data transformations\"\"\"\n    # Convert timestamp to datetime\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    \n    # Extract time components\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5, 6])\n    \n    # Clean and standardize data\n    df['event_type'] = df['event_type'].str.lower().str.strip()\n    df['user_id'] = df['user_id'].str.strip()\n    \n    # Remove duplicates\n    df = df.drop_duplicates(subset=['user_id', 'timestamp', 'event_type'])\n    \n    return df\n\ndef enrich_with_user_data(df):\n    \"\"\"Enrich events with user demographic data\"\"\"\n    # Simulate user data lookup\n    user_data = {\n        f'user_{i}': {\n            'age_group': '25-34' if i % 3 == 0 else '35-44',\n            'country': 'US' if i % 2 == 0 else 'UK',\n            'customer_segment': 'Premium' if i % 4 == 0 else 'Standard'\n        }\n        for i in range(1000)\n    }\n    \n    # Merge with events\n    df['age_group'] = df['user_id'].map(lambda x: user_data.get(x, {}).get('age_group', 'Unknown'))\n    df['country'] = df['user_id'].map(lambda x: user_data.get(x, {}).get('country', 'Unknown'))\n    df['customer_segment'] = df['user_id'].map(lambda x: user_data.get(x, {}).get('customer_segment', 'Standard'))\n    \n    return df\n\ndef enrich_with_product_data(df):\n    \"\"\"Enrich events with product information\"\"\"\n    # For events that have product_id, add product details\n    product_data = {\n        f'product_{i}': {\n            'category': 'Electronics' if i % 3 == 0 else 'Clothing',\n            'price': round(20 + (i % 100) * 5, 2),\n            'brand': f'Brand_{i % 10}'\n        }\n        for i in range(100)\n    }\n    \n    df['product_category'] = df['product_id'].map(lambda x: product_data.get(str(x), {}).get('category') if pd.notna(x) else None)\n    df['product_price'] = df['product_id'].map(lambda x: product_data.get(str(x), {}).get('price') if pd.notna(x) else None)\n    df['product_brand'] = df['product_id'].map(lambda x: product_data.get(str(x), {}).get('brand') if pd.notna(x) else None)\n    \n    return df\n\ndef calculate_derived_metrics(df):\n    \"\"\"Calculate business metrics\"\"\"\n    # Session-level metrics\n    session_metrics = df.groupby('session_id').agg({\n        'timestamp': ['min', 'max', 'count'],\n        'event_type': lambda x: x.nunique()\n    }).reset_index()\n    \n    session_metrics.columns = ['session_id', 'session_start', 'session_end', 'event_count', 'unique_event_types']\n    session_metrics['session_duration_minutes'] = (\n        (session_metrics['session_end'] - session_metrics['session_start']).dt.total_seconds() / 60\n    )\n    \n    # Merge back to events\n    df = df.merge(session_metrics, on='session_id', how='left')\n    \n    # User journey metrics\n    df = df.sort_values(['user_id', 'timestamp'])\n    df['event_sequence'] = df.groupby('user_id').cumcount() + 1\n    df['time_since_last_event'] = df.groupby('user_id')['timestamp'].diff().dt.total_seconds()\n    \n    return df\n\ndef load_to_data_warehouse(**context):\n    \"\"\"Load transformed data to Redshift data warehouse\"\"\"\n    s3_client = boto3.client('s3')\n    \n    # Get transformed data key\n    transformed_key = context['task_instance'].xcom_pull(task_ids='transform_data')\n    \n    # Read transformed data\n    response = s3_client.get_object(Bucket='ecommerce-data-lake', Key=transformed_key)\n    df = pd.read_parquet(response['Body'])\n    \n    # Load to Redshift\n    engine = create_engine('redshift+psycopg2://user:pass@cluster.redshift.amazonaws.com:5439/analytics')\n    \n    # Create fact table if not exists\n    create_fact_table_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS fact_user_events (\n        event_id VARCHAR(50),\n        user_id VARCHAR(50),\n        session_id VARCHAR(50),\n        event_type VARCHAR(50),\n        product_id VARCHAR(50),\n        timestamp TIMESTAMP,\n        hour INTEGER,\n        day_of_week INTEGER,\n        is_weekend BOOLEAN,\n        age_group VARCHAR(20),\n        country VARCHAR(10),\n        customer_segment VARCHAR(20),\n        product_category VARCHAR(50),\n        product_price DECIMAL(10,2),\n        session_duration_minutes DECIMAL(10,2),\n        event_sequence INTEGER\n    )\n    DISTSTYLE KEY\n    DISTKEY (user_id)\n    SORTKEY (timestamp);\n    \"\"\"\n    \n    with engine.connect() as conn:\n        conn.execute(create_fact_table_sql)\n    \n    # Insert data\n    df.to_sql(\n        'fact_user_events',\n        engine,\n        if_exists='append',\n        index=False,\n        method='multi',\n        chunksize=1000\n    )\n    \n    print(f\"Loaded {len(df)} events to data warehouse\")\n\ndef create_aggregated_tables(**context):\n    \"\"\"Create aggregated tables for faster analytics\"\"\"\n    engine = create_engine('redshift+psycopg2://user:pass@cluster.redshift.amazonaws.com:5439/analytics')\n    \n    # Daily aggregations\n    daily_aggregation_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS daily_user_metrics AS\n    SELECT \n        DATE(timestamp) as event_date,\n        user_id,\n        customer_segment,\n        country,\n        COUNT(*) as total_events,\n        COUNT(DISTINCT session_id) as sessions,\n        SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchases,\n        AVG(session_duration_minutes) as avg_session_duration\n    FROM fact_user_events\n    WHERE DATE(timestamp) = CURRENT_DATE - 1\n    GROUP BY 1, 2, 3, 4;\n    \"\"\"\n    \n    # Product performance aggregation\n    product_performance_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS daily_product_metrics AS\n    SELECT \n        DATE(timestamp) as event_date,\n        product_category,\n        product_brand,\n        COUNT(*) as total_interactions,\n        COUNT(DISTINCT user_id) as unique_users,\n        SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchases,\n        AVG(product_price) as avg_price\n    FROM fact_user_events\n    WHERE product_id IS NOT NULL \n    AND DATE(timestamp) = CURRENT_DATE - 1\n    GROUP BY 1, 2, 3;\n    \"\"\"\n    \n    with engine.connect() as conn:\n        conn.execute(daily_aggregation_sql)\n        conn.execute(product_performance_sql)\n    \n    print(\"Created aggregated tables\")\n\n# Define tasks\nextract_task = PythonOperator(\n    task_id='extract_events',\n    python_callable=extract_user_events,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_and_enrich_data,\n    dag=dag\n)\n\nload_task = PythonOperator(\n    task_id='load_to_warehouse',\n    python_callable=load_to_data_warehouse,\n    dag=dag\n)\n\naggregate_task = PythonOperator(\n    task_id='create_aggregations',\n    python_callable=create_aggregated_tables,\n    dag=dag\n)\n\n# Data quality check\nquality_check_sql = \"\"\"\nSELECT \n    COUNT(*) as total_events,\n    COUNT(DISTINCT user_id) as unique_users,\n    COUNT(DISTINCT session_id) as unique_sessions,\n    MIN(timestamp) as earliest_event,\n    MAX(timestamp) as latest_event\nFROM fact_user_events\nWHERE DATE(timestamp) = CURRENT_DATE - 1;\n\"\"\"\n\nquality_check_task = BashOperator(\n    task_id='data_quality_check',\n    bash_command=f'psql -h cluster.redshift.amazonaws.com -d analytics -c \"{quality_check_sql}\"',\n    dag=dag\n)\n\n# Define dependencies\nextract_task >> transform_task >> load_task >> aggregate_task >> quality_check_task\n```\n\n### Project 2: Real-Time Fraud Detection System\n\n**Overview**: Build a real-time fraud detection system using streaming data.\n\n**Architecture**:\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Transaction â”‚â”€â”€â”€â–¶â”‚   Kafka     â”‚â”€â”€â”€â–¶â”‚   Flink     â”‚â”€â”€â”€â–¶â”‚  Alerts     â”‚\nâ”‚   Events    â”‚    â”‚   Topic     â”‚    â”‚  Processor  â”‚    â”‚ Service     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                             â”‚\n                                             â–¼\n                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                    â”‚  Features   â”‚\n                                    â”‚   Store     â”‚\n                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Implementation**:\n\n```python\nfrom kafka import KafkaProducer, KafkaConsumer\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any\nimport redis\nimport logging\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nclass FraudDetectionEngine:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n        self.producer = KafkaProducer(\n            bootstrap_servers=['localhost:9092'],\n            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n        )\n        \n        # Load pre-trained model\n        self.fraud_model = joblib.load('fraud_detection_model.pkl')\n        self.scaler = joblib.load('feature_scaler.pkl')\n        \n    def extract_features(self, transaction: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Extract features for fraud detection\"\"\"\n        user_id = transaction['user_id']\n        amount = transaction['amount']\n        merchant_id = transaction['merchant_id']\n        timestamp = datetime.fromisoformat(transaction['timestamp'])\n        \n        features = {}\n        \n        # Transaction amount features\n        features['amount'] = amount\n        features['amount_log'] = np.log1p(amount)\n        \n        # Time-based features\n        features['hour'] = timestamp.hour\n        features['day_of_week'] = timestamp.weekday()\n        features['is_weekend'] = timestamp.weekday() >= 5\n        features['is_night'] = timestamp.hour < 6 or timestamp.hour > 22\n        \n        # User historical features (from Redis cache)\n        user_features = self.get_user_features(user_id, timestamp)\n        features.update(user_features)\n        \n        # Merchant features\n        merchant_features = self.get_merchant_features(merchant_id, timestamp)\n        features.update(merchant_features)\n        \n        # Velocity features\n        velocity_features = self.calculate_velocity_features(user_id, timestamp, amount)\n        features.update(velocity_features)\n        \n        return features\n    \n    def get_user_features(self, user_id: str, timestamp: datetime) -> Dict[str, float]:\n        \"\"\"Get user historical features from cache\"\"\"\n        features = {}\n        \n        # Get user transaction history from Redis\n        user_key = f\"user:{user_id}:transactions\"\n        recent_transactions = self.redis_client.lrange(user_key, 0, 99)  # Last 100 transactions\n        \n        if recent_transactions:\n            amounts = []\n            merchants = set()\n            \n            for txn_json in recent_transactions:\n                txn = json.loads(txn_json)\n                txn_time = datetime.fromisoformat(txn['timestamp'])\n                \n                # Only consider recent transactions\n                if (timestamp - txn_time).days <= 30:\n                    amounts.append(txn['amount'])\n                    merchants.add(txn['merchant_id'])\n            \n            if amounts:\n                features['user_avg_amount'] = np.mean(amounts)\n                features['user_std_amount'] = np.std(amounts)\n                features['user_max_amount'] = np.max(amounts)\n                features['user_transaction_count'] = len(amounts)\n                features['user_unique_merchants'] = len(merchants)\n            else:\n                features.update({\n                    'user_avg_amount': 0,\n                    'user_std_amount': 0,\n                    'user_max_amount': 0,\n                    'user_transaction_count': 0,\n                    'user_unique_merchants': 0\n                })\n        else:\n            # New user\n            features.update({\n                'user_avg_amount': 0,\n                'user_std_amount': 0,\n                'user_max_amount': 0,\n                'user_transaction_count': 0,\n                'user_unique_merchants': 0,\n                'is_new_user': 1\n            })\n        \n        return features\n    \n    def get_merchant_features(self, merchant_id: str, timestamp: datetime) -> Dict[str, float]:\n        \"\"\"Get merchant features\"\"\"\n        features = {}\n        \n        merchant_key = f\"merchant:{merchant_id}:stats\"\n        merchant_stats = self.redis_client.hgetall(merchant_key)\n        \n        if merchant_stats:\n            features['merchant_avg_amount'] = float(merchant_stats.get(b'avg_amount', 0))\n            features['merchant_fraud_rate'] = float(merchant_stats.get(b'fraud_rate', 0))\n            features['merchant_transaction_count'] = int(merchant_stats.get(b'transaction_count', 0))\n        else:\n            features.update({\n                'merchant_avg_amount': 0,\n                'merchant_fraud_rate': 0,\n                'merchant_transaction_count': 0,\n                'is_new_merchant': 1\n            })\n        \n        return features\n    \n    def calculate_velocity_features(self, user_id: str, timestamp: datetime, amount: float) -> Dict[str, float]:\n        \"\"\"Calculate velocity-based features\"\"\"\n        features = {}\n        \n        # Check transactions in different time windows\n        windows = [\n            ('1h', timedelta(hours=1)),\n            ('24h', timedelta(hours=24)),\n            ('7d', timedelta(days=7))\n        ]\n        \n        for window_name, window_duration in windows:\n            window_key = f\"user:{user_id}:velocity:{window_name}\"\n            window_start = timestamp - window_duration\n            \n            # Get transactions in this window\n            transactions = []\n            for txn_json in self.redis_client.lrange(f\"user:{user_id}:transactions\", 0, -1):\n                txn = json.loads(txn_json)\n                txn_time = datetime.fromisoformat(txn['timestamp'])\n                \n                if window_start <= txn_time <= timestamp:\n                    transactions.append(txn)\n            \n            # Calculate velocity features\n            features[f'velocity_count_{window_name}'] = len(transactions)\n            features[f'velocity_amount_{window_name}'] = sum(txn['amount'] for txn in transactions)\n            \n            if transactions:\n                features[f'velocity_avg_amount_{window_name}'] = features[f'velocity_amount_{window_name}'] / len(transactions)\n            else:\n                features[f'velocity_avg_amount_{window_name}'] = 0\n        \n        return features\n    \n    def predict_fraud(self, features: Dict[str, float]) -> Dict[str, Any]:\n        \"\"\"Predict if transaction is fraudulent\"\"\"\n        \n        # Prepare feature vector\n        feature_names = [\n            'amount', 'amount_log', 'hour', 'day_of_week', 'is_weekend', 'is_night',\n            'user_avg_amount', 'user_std_amount', 'user_max_amount', 'user_transaction_count',\n            'user_unique_merchants', 'merchant_avg_amount', 'merchant_fraud_rate',\n            'velocity_count_1h', 'velocity_amount_1h', 'velocity_count_24h', 'velocity_amount_24h'\n        ]\n        \n        feature_vector = [features.get(name, 0) for name in feature_names]\n        feature_vector = np.array(feature_vector).reshape(1, -1)\n        \n        # Scale features\n        feature_vector_scaled = self.scaler.transform(feature_vector)\n        \n        # Predict\n        fraud_score = self.fraud_model.decision_function(feature_vector_scaled)[0]\n        is_fraud = self.fraud_model.predict(feature_vector_scaled)[0] == -1\n        \n        # Convert score to probability-like value (0-100)\n        fraud_probability = max(0, min(100, (1 - fraud_score) * 50))\n        \n        return {\n            'is_fraud': bool(is_fraud),\n            'fraud_score': float(fraud_score),\n            'fraud_probability': float(fraud_probability),\n            'risk_level': self.categorize_risk(fraud_probability)\n        }\n    \n    def categorize_risk(self, probability: float) -> str:\n        \"\"\"Categorize risk level based on probability\"\"\"\n        if probability >= 80:\n            return 'HIGH'\n        elif probability >= 60:\n            return 'MEDIUM'\n        elif probability >= 40:\n            return 'LOW'\n        else:\n            return 'VERY_LOW'\n    \n    def update_user_history(self, transaction: Dict[str, Any]):\n        \"\"\"Update user transaction history in Redis\"\"\"\n        user_id = transaction['user_id']\n        user_key = f\"user:{user_id}:transactions\"\n        \n        # Add new transaction to history\n        self.redis_client.lpush(user_key, json.dumps(transaction))\n        \n        # Keep only last 100 transactions\n        self.redis_client.ltrim(user_key, 0, 99)\n        \n        # Set expiration (30 days)\n        self.redis_client.expire(user_key, 30 * 24 * 3600)\n    \n    def send_alert(self, transaction: Dict[str, Any], prediction: Dict[str, Any]):\n        \"\"\"Send fraud alert\"\"\"\n        alert = {\n            'alert_type': 'fraud_detection',\n            'transaction_id': transaction['transaction_id'],\n            'user_id': transaction['user_id'],\n            'amount': transaction['amount'],\n            'merchant_id': transaction['merchant_id'],\n            'prediction': prediction,\n            'timestamp': datetime.now().isoformat(),\n            'severity': prediction['risk_level']\n        }\n        \n        # Send to alerts topic\n        self.producer.send('fraud-alerts', value=alert)\n        \n        self.logger.warning(f\"Fraud alert sent for transaction {transaction['transaction_id']}: \"\n                          f\"Risk {prediction['risk_level']} ({prediction['fraud_probability']:.1f}%)\")\n    \n    def process_transaction(self, transaction: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process a single transaction for fraud detection\"\"\"\n        \n        # Extract features\n        features = self.extract_features(transaction)\n        \n        # Predict fraud\n        prediction = self.predict_fraud(features)\n        \n        # Update user history\n        self.update_user_history(transaction)\n        \n        # Send alert if high risk\n        if prediction['risk_level'] in ['HIGH', 'MEDIUM']:\n            self.send_alert(transaction, prediction)\n        \n        # Return result\n        result = {\n            'transaction_id': transaction['transaction_id'],\n            'processed_at': datetime.now().isoformat(),\n            'prediction': prediction,\n            'features': features\n        }\n        \n        return result\n\nclass FraudDetectionConsumer:\n    def __init__(self):\n        self.consumer = KafkaConsumer(\n            'transactions',\n            bootstrap_servers=['localhost:9092'],\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            group_id='fraud-detection-group'\n        )\n        self.fraud_engine = FraudDetectionEngine()\n        self.logger = logging.getLogger(__name__)\n    \n    def run(self):\n        \"\"\"Run the fraud detection consumer\"\"\"\n        self.logger.info(\"Starting fraud detection consumer...\")\n        \n        try:\n            for message in self.consumer:\n                transaction = message.value\n                \n                try:\n                    result = self.fraud_engine.process_transaction(transaction)\n                    \n                    # Log result\n                    self.logger.info(f\"Processed transaction {result['transaction_id']}: \"\n                                   f\"Fraud probability {result['prediction']['fraud_probability']:.1f}%\")\n                    \n                except Exception as e:\n                    self.logger.error(f\"Error processing transaction {transaction.get('transaction_id', 'unknown')}: {e}\")\n                    \n        except KeyboardInterrupt:\n            self.logger.info(\"Shutting down fraud detection consumer...\")\n        finally:\n            self.consumer.close()\n\n# Model training script\ndef train_fraud_model():\n    \"\"\"Train the fraud detection model\"\"\"\n    \n    # Load historical transaction data\n    # This would typically come from your data warehouse\n    df = pd.read_csv('historical_transactions.csv')\n    \n    # Feature engineering\n    features = []\n    labels = []\n    \n    for _, row in df.iterrows():\n        # Extract features (same as in real-time processing)\n        feature_dict = extract_historical_features(row)\n        feature_vector = [feature_dict.get(name, 0) for name in feature_names]\n        \n        features.append(feature_vector)\n        labels.append(row['is_fraud'])\n    \n    X = np.array(features)\n    y = np.array(labels)\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Train model (using Isolation Forest for anomaly detection)\n    model = IsolationForest(\n        contamination=0.1,  # Assume 10% fraud rate\n        random_state=42,\n        n_estimators=100\n    )\n    \n    model.fit(X_scaled[y == 0])  # Train on non-fraud transactions only\n    \n    # Save model and scaler\n    joblib.dump(model, 'fraud_detection_model.pkl')\n    joblib.dump(scaler, 'feature_scaler.pkl')\n    \n    print(\"Fraud detection model trained and saved\")\n\nif __name__ == \"__main__\":\n    # Run fraud detection consumer\n    consumer = FraudDetectionConsumer()\n    consumer.run()\n```\n\n### Project 3: Data Lake Implementation on AWS\n\n**Overview**: Build a serverless data lake using AWS services.\n\n**Architecture**:\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Data       â”‚â”€â”€â”€â–¶â”‚     S3      â”‚â”€â”€â”€â–¶â”‚   Lambda    â”‚â”€â”€â”€â–¶â”‚    Glue     â”‚\nâ”‚ Sources     â”‚    â”‚ Data Lake   â”‚    â”‚  Trigger    â”‚    â”‚  Catalog    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                                 â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\nâ”‚   Athena    â”‚â—„â”€â”€â”€â”‚  QuickSight â”‚â—„â”€â”€â”€â”‚   Redshift  â”‚â—„â”€â”€â”€â”‚    ETL      â”‚\nâ”‚  Queries    â”‚    â”‚ Dashboard   â”‚    â”‚ Spectrum    â”‚    â”‚   Jobs      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Implementation**:\n\n```python\nimport boto3\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport logging\nfrom typing import Dict, List, Any\nimport os\n\nclass AWSDataLake:\n    def __init__(self, region='us-east-1'):\n        self.region = region\n        self.s3_client = boto3.client('s3', region_name=region)\n        self.glue_client = boto3.client('glue', region_name=region)\n        self.athena_client = boto3.client('athena', region_name=region)\n        self.lambda_client = boto3.client('lambda', region_name=region)\n        \n        self.logger = logging.getLogger(__name__)\n        \n        # Configuration\n        self.data_lake_bucket = 'my-company-data-lake'\n        self.glue_database = 'data_lake_db'\n        self.athena_output_bucket = 'my-company-athena-results'\n    \n    def setup_data_lake_infrastructure(self):\n        \"\"\"Set up the complete data lake infrastructure\"\"\"\n        \n        # Create S3 buckets\n        self.create_s3_buckets()\n        \n        # Create Glue database\n        self.create_glue_database()\n        \n        # Deploy Lambda functions\n        self.deploy_lambda_functions()\n        \n        # Create Glue jobs\n        self.create_glue_jobs()\n        \n        # Set up Athena workgroup\n        self.create_athena_workgroup()\n        \n        self.logger.info(\"Data lake infrastructure setup complete\")\n    \n    def create_s3_buckets(self):\n        \"\"\"Create S3 buckets for data lake\"\"\"\n        buckets = [\n            self.data_lake_bucket,\n            self.athena_output_bucket,\n            f'{self.data_lake_bucket}-logs'\n        ]\n        \n        for bucket in buckets:\n            try:\n                self.s3_client.create_bucket(Bucket=bucket)\n                \n                # Enable versioning\n                self.s3_client.put_bucket_versioning(\n                    Bucket=bucket,\n                    VersioningConfiguration={'Status': 'Enabled'}\n                )\n                \n                # Set lifecycle policy\n                lifecycle_config = {\n                    'Rules': [\n                        {\n                            'ID': 'transition-to-ia',\n                            'Status': 'Enabled',\n                            'Filter': {'Prefix': 'raw-data/'},\n                            'Transitions': [\n                                {\n                                    'Days': 30,\n                                    'StorageClass': 'STANDARD_IA'\n                                },\n                                {\n                                    'Days': 90,\n                                    'StorageClass': 'GLACIER'\n                                },\n                                {\n                                    'Days': 365,\n                                    'StorageClass': 'DEEP_ARCHIVE'\n                                }\n                            ]\n                        }\n                    ]\n                }\n                \n                self.s3_client.put_bucket_lifecycle_configuration(\n                    Bucket=bucket,\n                    LifecycleConfiguration=lifecycle_config\n                )\n                \n                self.logger.info(f\"Created S3 bucket: {bucket}\")\n                \n            except Exception as e:\n                if 'BucketAlreadyExists' not in str(e):\n                    self.logger.error(f\"Error creating bucket {bucket}: {e}\")\n    \n    def create_glue_database(self):\n        \"\"\"Create Glue database for data catalog\"\"\"\n        try:\n            self.glue_client.create_database(\n                DatabaseInput={\n                    'Name': self.glue_database,\n                    'Description': 'Data Lake catalog database'\n                }\n            )\n            self.logger.info(f\"Created Glue database: {self.glue_database}\")\n        except Exception as e:\n            if 'AlreadyExistsException' not in str(e):\n                self.logger.error(f\"Error creating Glue database: {e}\")\n    \n    def deploy_lambda_functions(self):\n        \"\"\"Deploy Lambda functions for data processing\"\"\"\n        \n        # S3 event processor Lambda\n        lambda_code = self.create_s3_processor_lambda_code()\n        \n        function_name = 'data-lake-s3-processor'\n        \n        try:\n            self.lambda_client.create_function(\n                FunctionName=function_name,\n                Runtime='python3.9',\n                Role='arn:aws:iam::ACCOUNT:role/lambda-execution-role',  # Replace with actual role ARN\n                Handler='lambda_function.lambda_handler',\n                Code={'ZipFile': lambda_code},\n                Description='Process S3 events for data lake',\n                Timeout=300,\n                MemorySize=512,\n                Environment={\n                    'Variables': {\n                        'GLUE_DATABASE': self.glue_database,\n                        'DATA_LAKE_BUCKET': self.data_lake_bucket\n                    }\n                }\n            )\n            \n            # Add S3 trigger\n            self.s3_client.put_bucket_notification_configuration(\n                Bucket=self.data_lake_bucket,\n                NotificationConfiguration={\n                    'LambdaConfigurations': [\n                        {\n                            'Id': 'data-lake-processor',\n                            'LambdaFunctionArn': f'arn:aws:lambda:{self.region}:ACCOUNT:function:{function_name}',\n                            'Events': ['s3:ObjectCreated:*'],\n                            'Filter': {\n                                'Key': {\n                                    'FilterRules': [\n                                        {\n                                            'Name': 'prefix',\n                                            'Value': 'raw-data/'\n                                        }\n                                    ]\n                                }\n                            }\n                        }\n                    ]\n                }\n            )\n            \n            self.logger.info(f\"Created Lambda function: {function_name}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating Lambda function: {e}\")\n    \n    def create_s3_processor_lambda_code(self) -> bytes:\n        \"\"\"Create Lambda function code for S3 event processing\"\"\"\n        code = '''\nimport boto3\nimport json\nimport urllib.parse\nimport os\n\ndef lambda_handler(event, context):\n    s3_client = boto3.client('s3')\n    glue_client = boto3.client('glue')\n    \n    glue_database = os.environ['GLUE_DATABASE']\n    \n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = urllib.parse.unquote_plus(record['s3']['object']['key'])\n        \n        print(f\"Processing file: s3://{bucket}/{key}\")\n        \n        # Trigger Glue crawler based on file type\n        if key.endswith('.csv'):\n            table_name = key.split('/')[-2]  # Use directory name as table name\n            \n            # Create Glue table\n            create_glue_table(glue_client, glue_database, table_name, bucket, key)\n            \n        # Trigger ETL job if needed\n        if 'raw-data' in key:\n            trigger_etl_job(glue_client, key)\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps('Successfully processed S3 events')\n    }\n\ndef create_glue_table(glue_client, database, table_name, bucket, key):\n    try:\n        glue_client.create_table(\n            DatabaseName=database,\n            TableInput={\n                'Name': table_name,\n                'StorageDescriptor': {\n                    'Columns': [\n                        {'Name': 'col1', 'Type': 'string'},\n                        {'Name': 'col2', 'Type': 'string'}\n                    ],\n                    'Location': f's3://{bucket}/{\"/\".join(key.split(\"/\")[:-1])}/',\n                    'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n                    'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n                    'SerdeInfo': {\n                        'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n                        'Parameters': {\n                            'field.delim': ',',\n                            'skip.header.line.count': '1'\n                        }\n                    }\n                }\n            }\n        )\n    except Exception as e:\n        print(f\"Error creating Glue table: {e}\")\n\ndef trigger_etl_job(glue_client, s3_key):\n    try:\n        glue_client.start_job_run(\n            JobName='data-lake-etl-job',\n            Arguments={\n                '--input_path': s3_key\n            }\n        )\n    except Exception as e:\n        print(f\"Error triggering ETL job: {e}\")\n'''\n        return code.encode('utf-8')\n    \n    def create_glue_jobs(self):\n        \"\"\"Create Glue ETL jobs\"\"\"\n        \n        job_script = '''\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path'])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read data from S3\ndf = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [args['input_path']]\n    },\n    format=\"csv\",\n    format_options={\n        \"withHeader\": True\n    }\n)\n\n# Transform data\n# Add your transformation logic here\ntransformed_df = df.apply_mapping([\n    (\"old_column\", \"string\", \"new_column\", \"string\")\n])\n\n# Write to S3 in Parquet format\nglueContext.write_dynamic_frame.from_options(\n    frame=transformed_df,\n    connection_type=\"s3\",\n    connection_options={\n        \"path\": \"s3://my-company-data-lake/processed-data/\"\n    },\n    format=\"glueparquet\"\n)\n\njob.commit()\n'''\n        \n        try:\n            self.glue_client.create_job(\n                Name='data-lake-etl-job',\n                Role='arn:aws:iam::ACCOUNT:role/glue-execution-role',  # Replace with actual role ARN\n                Command={\n                    'Name': 'glueetl',\n                    'ScriptLocation': f's3://{self.data_lake_bucket}/scripts/etl_job.py',\n                    'PythonVersion': '3'\n                },\n                DefaultArguments={\n                    '--job-bookmark-option': 'job-bookmark-enable'\n                },\n                MaxRetries=1,\n                Timeout=60,\n                GlueVersion='3.0'\n            )\n            \n            # Upload script to S3\n            self.s3_client.put_object(\n                Bucket=self.data_lake_bucket,\n                Key='scripts/etl_job.py',\n                Body=job_script\n            )\n            \n            self.logger.info(\"Created Glue ETL job\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating Glue job: {e}\")\n    \n    def create_athena_workgroup(self):\n        \"\"\"Create Athena workgroup for queries\"\"\"\n        try:\n            self.athena_client.create_work_group(\n                Name='data-lake-workgroup',\n                Description='Workgroup for data lake queries',\n                Configuration={\n                    'ResultConfigurationUpdates': {\n                        'OutputLocation': f's3://{self.athena_output_bucket}/query-results/',\n                        'EncryptionConfiguration': {\n                            'EncryptionOption': 'SSE_S3'\n                        }\n                    },\n                    'EnforceWorkGroupConfiguration': True\n                }\n            )\n            self.logger.info(\"Created Athena workgroup\")\n        except Exception as e:\n            if 'WorkGroupAlreadyExistsException' not in str(e):\n                self.logger.error(f\"Error creating Athena workgroup: {e}\")\n    \n    def query_data_lake(self, query: str) -> List[Dict[str, Any]]:\n        \"\"\"Query data lake using Athena\"\"\"\n        \n        # Start query execution\n        response = self.athena_client.start_query_execution(\n            QueryString=query,\n            QueryExecutionContext={\n                'Database': self.glue_database\n            },\n            ResultConfiguration={\n                'OutputLocation': f's3://{self.athena_output_bucket}/query-results/'\n            },\n            WorkGroup='data-lake-workgroup'\n        )\n        \n        query_execution_id = response['QueryExecutionId']\n        \n        # Wait for query to complete\n        while True:\n            result = self.athena_client.get_query_execution(\n                QueryExecutionId=query_execution_id\n            )\n            \n            status = result['QueryExecution']['Status']['State']\n            \n            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                break\n            \n            time.sleep(1)\n        \n        if status != 'SUCCEEDED':\n            raise Exception(f\"Query failed with status: {status}\")\n        \n        # Get query results\n        results = self.athena_client.get_query_results(\n            QueryExecutionId=query_execution_id\n        )\n        \n        # Parse results\n        rows = []\n        columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n        \n        for row in results['ResultSet']['Rows'][1:]:  # Skip header\n            values = [col.get('VarCharValue', '') for col in row['Data']]\n            rows.append(dict(zip(columns, values)))\n        \n        return rows\n    \n    def upload_data_to_lake(self, data: pd.DataFrame, table_name: str, partition_columns: List[str] = None):\n        \"\"\"Upload data to data lake with partitioning\"\"\"\n        \n        if partition_columns:\n            # Create partitioned structure\n            for partition_values, group_df in data.groupby(partition_columns):\n                if not isinstance(partition_values, tuple):\n                    partition_values = (partition_values,)\n                \n                # Create partition path\n                partition_path = '/'.join([\n                    f\"{col}={val}\" for col, val in zip(partition_columns, partition_values)\n                ])\n                \n                # Save to S3\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n                s3_key = f\"raw-data/{table_name}/{partition_path}/data_{timestamp}.parquet\"\n                \n                parquet_buffer = group_df.drop(columns=partition_columns).to_parquet(index=False)\n                \n                self.s3_client.put_object(\n                    Bucket=self.data_lake_bucket,\n                    Key=s3_key,\n                    Body=parquet_buffer\n                )\n        else:\n            # Save without partitioning\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n            s3_key = f\"raw-data/{table_name}/data_{timestamp}.parquet\"\n            \n            parquet_buffer = data.to_parquet(index=False)\n            \n            self.s3_client.put_object(\n                Bucket=self.data_lake_bucket,\n                Key=s3_key,\n                Body=parquet_buffer\n            )\n        \n        self.logger.info(f\"Uploaded data to data lake: {table_name}\")\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize data lake\n    data_lake = AWSDataLake()\n    \n    # Set up infrastructure\n    data_lake.setup_data_lake_infrastructure()\n    \n    # Upload sample data\n    sample_data = pd.DataFrame({\n        'customer_id': [1, 2, 3, 4, 5],\n        'product_id': [101, 102, 103, 104, 105],\n        'amount': [100.0, 200.0, 150.0, 300.0, 75.0],\n        'order_date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],\n        'region': ['US', 'US', 'EU', 'EU', 'US']\n    })\n    \n    # Upload with partitioning\n    data_lake.upload_data_to_lake(\n        sample_data, \n        'sales_data', \n        partition_columns=['order_date', 'region']\n    )\n    \n    # Query data\n    query = \"\"\"\n    SELECT region, SUM(amount) as total_sales\n    FROM sales_data\n    WHERE order_date >= '2024-01-01'\n    GROUP BY region\n    ORDER BY total_sales DESC\n    \"\"\"\n    \n    results = data_lake.query_data_lake(query)\n    print(f\"Query results: {results}\")\n```\n\n---\n\n## ðŸŽ“ Career Path & Skills {#career}\n\n### Data Engineer Career Roadmap\n\n**Entry Level (0-2 years)**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 Junior Data Engineer                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Skills to Develop:                                      â”‚\nâ”‚ â€¢ SQL (Advanced)                                        â”‚\nâ”‚ â€¢ Python/Java basics                                    â”‚\nâ”‚ â€¢ ETL concepts                                          â”‚\nâ”‚ â€¢ Database fundamentals                                 â”‚\nâ”‚ â€¢ Basic cloud knowledge (AWS/GCP/Azure)               â”‚\nâ”‚ â€¢ Git version control                                   â”‚\nâ”‚ â€¢ Basic data modeling                                   â”‚\nâ”‚                                                         â”‚\nâ”‚ Typical Responsibilities:                               â”‚\nâ”‚ â€¢ Data extraction and basic transformations            â”‚\nâ”‚ â€¢ Writing SQL queries                                   â”‚\nâ”‚ â€¢ Data quality checks                                   â”‚\nâ”‚ â€¢ Documentation                                         â”‚\nâ”‚ â€¢ Supporting senior engineers                           â”‚\nâ”‚                                                         â”‚\nâ”‚ Salary Range: $60,000 - $85,000                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Mid Level (2-5 years)**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   Data Engineer                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Skills to Master:                                       â”‚\nâ”‚ â€¢ Advanced Python/Scala/Java                          â”‚\nâ”‚ â€¢ Apache Spark                                          â”‚\nâ”‚ â€¢ Data warehouse design                                 â”‚\nâ”‚ â€¢ Cloud platforms (1-2 specializations)               â”‚\nâ”‚ â€¢ Workflow orchestration (Airflow)                     â”‚\nâ”‚ â€¢ Big data technologies (Kafka, Hadoop)               â”‚\nâ”‚ â€¢ Data security and governance                          â”‚\nâ”‚ â€¢ Performance optimization                              â”‚\nâ”‚                                                         â”‚\nâ”‚ Typical Responsibilities:                               â”‚\nâ”‚ â€¢ Design and build data pipelines                      â”‚\nâ”‚ â€¢ Data architecture decisions                           â”‚\nâ”‚ â€¢ Performance tuning                                    â”‚\nâ”‚ â€¢ Mentoring junior engineers                           â”‚\nâ”‚ â€¢ Cross-functional collaboration                        â”‚\nâ”‚                                                         â”‚\nâ”‚ Salary Range: $85,000 - $130,000                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Senior Level (5-8 years)**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              Senior Data Engineer                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Skills to Excel:                                        â”‚\nâ”‚ â€¢ System architecture design                            â”‚\nâ”‚ â€¢ Multiple cloud platforms                              â”‚\nâ”‚ â€¢ Advanced distributed systems                          â”‚\nâ”‚ â€¢ DataOps and MLOps                                     â”‚\nâ”‚ â€¢ Team leadership                                       â”‚\nâ”‚ â€¢ Business acumen                                       â”‚\nâ”‚ â€¢ Advanced security and compliance                      â”‚\nâ”‚ â€¢ Cost optimization                                     â”‚\nâ”‚                                                         â”‚\nâ”‚ Typical Responsibilities:                               â”‚\nâ”‚ â€¢ Lead complex data projects                           â”‚\nâ”‚ â€¢ Architecture design and reviews                       â”‚\nâ”‚ â€¢ Technical strategy development                        â”‚\nâ”‚ â€¢ Team coordination                                     â”‚\nâ”‚ â€¢ Stakeholder management                                â”‚\nâ”‚                                                         â”‚\nâ”‚ Salary Range: $130,000 - $180,000                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Expert Level (8+ years)**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         Principal Data Engineer / Architect             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Skills to Lead:                                         â”‚\nâ”‚ â€¢ Enterprise architecture                               â”‚\nâ”‚ â€¢ Strategic technology planning                         â”‚\nâ”‚ â€¢ Advanced team management                              â”‚\nâ”‚ â€¢ Innovation and R&D                                    â”‚\nâ”‚ â€¢ Industry expertise                                    â”‚\nâ”‚ â€¢ Thought leadership                                    â”‚\nâ”‚ â€¢ Budget and resource management                        â”‚\nâ”‚ â€¢ Vendor relationships                                  â”‚\nâ”‚                                                         â”‚\nâ”‚ Typical Responsibilities:                               â”‚\nâ”‚ â€¢ Company-wide data strategy                           â”‚\nâ”‚ â€¢ Technology evaluation and selection                   â”‚\nâ”‚ â€¢ Team building and hiring                             â”‚\nâ”‚ â€¢ External representation (conferences, etc.)          â”‚\nâ”‚ â€¢ Innovation projects                                   â”‚\nâ”‚                                                         â”‚\nâ”‚ Salary Range: $180,000 - $300,000+                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Essential Skills Matrix\n\n**Technical Skills**:\n\n| Skill Category | Beginner | Intermediate | Advanced | Expert |\n|----------------|----------|--------------|----------|--------|\n| **SQL** | Basic queries, joins | Window functions, CTEs | Query optimization, complex analytics | Database tuning, advanced analytics |\n| **Python** | Basic syntax, pandas | APIs, data processing | Advanced libraries, optimization | Framework development |\n| **Cloud Platforms** | Basic services | Core data services | Multi-service architectures | Enterprise solutions |\n| **Big Data** | Concepts understanding | Spark basics | Distributed processing | Architecture design |\n| **Data Modeling** | Basic concepts | Star/snowflake schema | Advanced modeling | Enterprise data architecture |\n| **DevOps** | Git basics | CI/CD pipelines | Infrastructure as code | Advanced automation |\n\n**Soft Skills**:\n- **Communication**: Explain technical concepts to non-technical stakeholders\n- **Problem Solving**: Debug complex data issues and design solutions\n- **Project Management**: Plan and execute data projects\n- **Business Acumen**: Understand business requirements and translate to technical solutions\n- **Collaboration**: Work effectively with data scientists, analysts, and engineers\n- **Continuous Learning**: Stay updated with rapidly evolving technology landscape\n\n### Certification Paths\n\n**AWS Data Engineering**:\n1. AWS Cloud Practitioner (foundational)\n2. AWS Solutions Architect Associate\n3. AWS Data Analytics Specialty\n4. AWS Machine Learning Specialty\n\n**Google Cloud Platform**:\n1. Google Cloud Associate Cloud Engineer\n2. Professional Data Engineer\n3. Professional Cloud Architect\n4. Professional Machine Learning Engineer\n\n**Microsoft Azure**:\n1. Azure Fundamentals\n2. Azure Data Engineer Associate\n3. Azure Solutions Architect Expert\n4. Azure Data Scientist Associate\n\n**Technology-Specific**:\n- Databricks Certified Data Engineer\n- Confluent Certified Developer for Apache Kafka\n- Snowflake SnowPro Core Certification\n- Cloudera Data Platform Generalist\n\n### Building a Portfolio\n\n**Essential Projects to Showcase**:\n\n1. **End-to-End ETL Pipeline**\n   - Data extraction from multiple sources\n   - Transformation and cleansing\n   - Loading to data warehouse\n   - Monitoring and alerting\n\n2. **Real-Time Data Processing**\n   - Streaming data ingestion\n   - Real-time analytics\n   - Dashboard integration\n\n3. **Data Lake Implementation**\n   - Raw data storage\n   - Data cataloging\n   - Query optimization\n   - Cost management\n\n4. **ML Pipeline for Analytics**\n   - Feature engineering pipeline\n   - Model training automation\n   - Model deployment\n   - Performance monitoring\n\n5. **Data Quality Framework**\n   - Automated data validation\n   - Quality metrics tracking\n   - Issue detection and alerting\n   - Data lineage tracking\n\n### Interview Preparation\n\n**Technical Interview Topics**:\n\n1. **System Design Questions**:\n   - Design a data pipeline for processing millions of events per day\n   - Design a data warehouse for a retail company\n   - Design a real-time recommendation system\n   - Design a data lake architecture\n\n2. **Coding Questions**:\n   - SQL optimization problems\n   - Python data processing challenges\n   - Algorithm and data structure problems\n   - Distributed systems concepts\n\n3. **Scenario-Based Questions**:\n   - How would you handle data quality issues?\n   - How would you optimize a slow-running pipeline?\n   - How would you ensure data security and compliance?\n   - How would you design for scalability?\n\n**Sample Interview Questions**:\n\n**Beginner Level**:\n- Explain the difference between OLTP and OLAP systems\n- How would you handle duplicate records in a dataset?\n- What is the difference between a data lake and data warehouse?\n- Write a SQL query to find the second-highest salary\n\n**Intermediate Level**:\n- Design a data pipeline to process customer clickstream data\n- How would you implement slowly changing dimensions (SCD)?\n- Explain the CAP theorem and its implications for data systems\n- How would you optimize a Spark job that's running slowly?\n\n**Advanced Level**:\n- Design a multi-petabyte data lake architecture\n- How would you implement GDPR compliance in a data pipeline?\n- Explain different consistency models in distributed systems\n- Design a system to handle both batch and stream processing\n\n### Networking and Community\n\n**Professional Communities**:\n- **Data Engineering Weekly** - Newsletter and community\n- **dbt Community** - Modern data stack discussions\n- **Apache Foundation** - Open source big data projects\n- **Local Meetups** - Data engineering and analytics groups\n- **Conferences** - Strata, DataEngConf, Modern Data Stack Conference\n\n**Online Learning Resources**:\n- **Courses**: DataCamp, Coursera, Udacity, Pluralsight\n- **Books**: \n  - \"Designing Data-Intensive Applications\" by Martin Kleppmann\n  - \"The Data Warehouse Toolkit\" by Ralph Kimball\n  - \"Stream Processing with Apache Kafka\" by Guido Schmutz\n- **YouTube Channels**: Data Engineering channels, conference talks\n- **Blogs**: Engineering blogs from Netflix, Uber, Airbnb, Spotify\n\n---\n\n## ðŸ† Conclusion & Next Steps\n\nCongratulations! You've completed the comprehensive Data Engineering Zero to Hero guide. You now have the knowledge and tools to:\n\n### âœ… What You've Learned\n\n**Fundamentals**:\n- âœ… Data engineering lifecycle and core concepts\n- âœ… Programming languages (Python, SQL, Bash)\n- âœ… Data collection and ingestion strategies\n- âœ… Storage solutions (databases, data lakes, warehouses)\n\n**Advanced Technologies**:\n- âœ… Big data processing with Spark and Hadoop\n- âœ… Stream processing with Kafka and Flink\n- âœ… Cloud platforms (AWS, GCP, Azure)\n- âœ… Workflow orchestration with Airflow\n\n**Production Excellence**:\n- âœ… Data quality and governance frameworks\n- âœ… Security and compliance (GDPR, encryption)\n- âœ… Monitoring and observability\n- âœ… Performance optimization patterns\n\n**Real-World Application**:\n- âœ… Three complete production-ready projects\n- âœ… Best practices and design patterns\n- âœ… Career roadmap and skill development\n- âœ… Interview preparation strategies\n\n### ðŸŽ¯ Your Next Steps\n\n**Immediate Actions (Next 30 Days)**:\n1. **Choose Your Specialization**: Pick one cloud platform (AWS/GCP/Azure) to master first\n2. **Build Your First Project**: Start with the e-commerce analytics project\n3. **Set Up Your Environment**: Install tools and create accounts\n4. **Join Communities**: Connect with data engineering professionals\n5. **Create Your Portfolio**: Document your learning journey\n\n**Short-Term Goals (3-6 Months)**:\n1. **Complete 2-3 Real Projects**: Build end-to-end data pipelines\n2. **Get Certified**: Pursue relevant cloud/technology certifications\n3. **Contribute to Open Source**: Participate in data engineering projects\n4. **Network Actively**: Attend meetups and conferences\n5. **Apply for Positions**: Target data engineering roles\n\n**Long-Term Vision (1-2 Years)**:\n1. **Specialize Further**: Deep dive into specific domains (ML, real-time, etc.)\n2. **Lead Projects**: Take ownership of critical data infrastructure\n3. **Mentor Others**: Share knowledge and help junior engineers\n4. **Stay Current**: Keep up with emerging technologies and trends\n5. **Build Expertise**: Become known for specific skills or domains\n\n### ðŸŒŸ The Data Engineering Landscape\n\nThe field of data engineering is rapidly evolving with exciting opportunities:\n\n**Emerging Trends**:\n- **DataOps and MLOps**: Operational excellence for data and ML pipelines\n- **Real-time Everything**: Increasing demand for real-time data processing\n- **Serverless Data Processing**: Cloud-native, serverless data architectures\n- **Data Mesh**: Decentralized data architecture paradigm\n- **AI-Assisted Development**: AI tools helping with code generation and optimization\n\n**High-Demand Skills**:\n- Stream processing and real-time analytics\n- Multi-cloud and hybrid cloud architectures\n- Data governance and privacy compliance\n- Performance optimization and cost management\n- Leadership and cross-functional collaboration\n\n**Industry Opportunities**:\n- **Technology Companies**: Building scalable data platforms\n- **Financial Services**: Risk management and regulatory compliance\n- **Healthcare**: Patient data analytics and research\n- **E-commerce**: Recommendation systems and personalization\n- **Consulting**: Helping organizations modernize data architecture\n\n### ðŸ’ª Stay Motivated\n\nRemember, becoming an expert data engineer is a journey, not a destination. The field is constantly evolving, which makes it exciting but also means continuous learning is essential.\n\n**Keys to Success**:\n- **Practice Consistently**: Build something every week\n- **Learn in Public**: Share your projects and learnings\n- **Embrace Challenges**: Tackle problems slightly beyond your comfort zone\n- **Focus on Fundamentals**: Strong basics enable rapid learning of new tools\n- **Think Like an Engineer**: Always consider scalability, reliability, and maintainability\n\n### ðŸš€ Final Words\n\nData engineering is one of the most rewarding and high-impact fields in technology. As a data engineer, you'll:\n\n- **Enable Data-Driven Decisions**: Your work directly impacts business outcomes\n- **Solve Complex Problems**: Tackle interesting technical challenges at scale\n- **Work with Cutting-Edge Technology**: Use the latest tools and platforms\n- **Collaborate Widely**: Work with diverse teams across the organization\n- **Make a Real Impact**: See your data solutions drive business success\n\nThe journey from zero to hero in data engineering is challenging but incredibly rewarding. You have all the knowledge you need to get started. Now it's time to build, create, and innovate!\n\n**Remember**: Every expert was once a beginner. Your journey to becoming a data engineering hero starts with the first line of code you write today.\n\nKeep learning, keep building, and most importantly, keep pushing the boundaries of what's possible with data! ðŸŒŸ\n\n---\n\n## ðŸ“š Additional Resources\n\n**Essential Reading**:\n- [Designing Data-Intensive Applications](https://dataintensive.net/) by Martin Kleppmann\n- [The Data Warehouse Toolkit](https://www.kimballgroup.com/) by Ralph Kimball\n- [Building Analytics Teams](https://www.oreilly.com/library/view/building-analytics-teams/9781492024415/) by John K. Thompson\n\n**Online Communities**:\n- [r/dataengineering](https://www.reddit.com/r/dataengineering/) - Reddit community\n- [Data Engineering Weekly](https://www.dataengineeringweekly.com/) - Newsletter\n- [Modern Data Stack](https://www.moderndatastack.xyz/) - Community and resources\n\n**Conferences & Events**:\n- DataEngConf - Annual data engineering conference\n- Strata Data Conference - O'Reilly's data conference  \n- Data Council - Data engineering and science conference\n- Local meetups and user groups\n\n**YouTube Channels**:\n- Seattle Data Guy\n- Data Engineering\n- Databricks\n- Snowflake Inc.\n\n**Podcasts**:\n- The Analytics Engineering Podcast\n- Data Engineering Podcast\n- DataFramed\n- Machine Learning Guide\n\nThe journey begins now. Go build something amazing! ðŸš€ðŸŽ¯\n\n---\n\n*\"The best time to plant a tree was 20 years ago. The second best time is now.\"* - Start your data engineering journey today!"}