{"id":"performance-optimization","title":"âš¡ Performance Optimization Complete Guide","content":"# Performance Optimization: Complete Guide\n## Frontend, Backend, Database & Infrastructure Performance Mastery\n\n---\n\n## ğŸ“š Table of Contents\n\n1. [Introduction to Performance Optimization](#introduction)\n2. [Performance Fundamentals](#fundamentals)\n3. [Frontend Performance Optimization](#frontend)\n4. [Backend Performance Optimization](#backend)\n5. [Database Performance Tuning](#database)\n6. [Caching Strategies](#caching)\n7. [Load Testing & Monitoring](#testing)\n8. [Network & Infrastructure Optimization](#network)\n9. [Code-Level Optimizations](#code-level)\n10. [Memory Management & Optimization](#memory)\n11. [Security vs Performance Trade-offs](#security)\n12. [Real-World Case Studies](#case-studies)\n13. [Performance Best Practices](#best-practices)\n\n---\n\n## ğŸ¯ Introduction to Performance Optimization {#introduction}\n\n### What is Performance Optimization?\n\n**Performance Optimization** is the process of making systems faster, more efficient, and capable of handling larger loads while maintaining reliability and user experience.\n\n```\nBefore Optimization\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Slow Response Times (3-5 seconds)  â”‚\nâ”‚  High Resource Usage (80%+ CPU)     â”‚\nâ”‚  Poor User Experience              â”‚\nâ”‚  Limited Concurrent Users          â”‚\nâ”‚  Frequent Timeouts                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nAfter Optimization  \nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Fast Response Times (<200ms)       â”‚\nâ”‚  Optimized Resource Usage (40% CPU) â”‚\nâ”‚  Excellent User Experience         â”‚\nâ”‚  High Concurrent Users             â”‚\nâ”‚  Reliable Performance              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Performance Metrics That Matter\n\n**Frontend Metrics**:\n- **First Contentful Paint (FCP)**: < 1.8 seconds\n- **Largest Contentful Paint (LCP)**: < 2.5 seconds  \n- **First Input Delay (FID)**: < 100 milliseconds\n- **Cumulative Layout Shift (CLS)**: < 0.1\n- **Time to Interactive (TTI)**: < 3.8 seconds\n\n**Backend Metrics**:\n- **Response Time**: < 200ms for API calls\n- **Throughput**: Requests handled per second\n- **Error Rate**: < 0.1% for critical paths\n- **CPU Usage**: < 70% under normal load\n- **Memory Usage**: < 80% of available memory\n\n**Database Metrics**:\n- **Query Response Time**: < 100ms for simple queries\n- **Connection Pool Usage**: < 80% of pool size\n- **Cache Hit Ratio**: > 90% for frequently accessed data\n- **Index Usage**: > 95% of queries use indexes\n- **Lock Wait Time**: < 10ms average\n\n### Performance Budget Framework\n\n```javascript\n// Performance budgets for different page types\nconst performanceBudgets = {\n  'landing-page': {\n    totalSize: '1.5MB',\n    javascript: '300KB',\n    css: '100KB', \n    images: '1MB',\n    fonts: '100KB',\n    firstLoad: '2s',\n    repeatVisit: '1s'\n  },\n  'dashboard': {\n    totalSize: '2MB',\n    javascript: '500KB',\n    css: '150KB',\n    images: '1.2MB',\n    fonts: '150KB',\n    firstLoad: '3s',\n    repeatVisit: '1.5s'\n  },\n  'mobile': {\n    totalSize: '1MB',\n    javascript: '200KB',\n    css: '80KB',\n    images: '600KB',\n    fonts: '80KB',\n    firstLoad: '1.5s',\n    repeatVisit: '0.8s'\n  }\n}\n\n// Budget monitoring function\nfunction checkPerformanceBudget(pageType, actualMetrics) {\n  const budget = performanceBudgets[pageType]\n  const violations = []\n  \n  Object.keys(budget).forEach(metric => {\n    const budgetValue = parseSize(budget[metric])\n    const actualValue = actualMetrics[metric]\n    \n    if (actualValue > budgetValue) {\n      violations.push({\n        metric,\n        budget: budget[metric],\n        actual: actualValue,\n        overBudget: actualValue - budgetValue\n      })\n    }\n  })\n  \n  return violations\n}\n\nfunction parseSize(sizeString) {\n  const units = { 'KB': 1024, 'MB': 1024 * 1024, 's': 1 }\n  const match = sizeString.match(/^([\\d.]+)([A-Za-z]+)$/)\n  \n  if (match) {\n    const [, number, unit] = match\n    return parseFloat(number) * (units[unit] || 1)\n  }\n  \n  return parseFloat(sizeString)\n}\n```\n\n---\n\n## âš¡ Performance Fundamentals {#fundamentals}\n\n### The Performance Optimization Process\n\n```\n1. Measure & Baseline\n   â”œâ”€ Set up monitoring\n   â”œâ”€ Collect baseline metrics\n   â””â”€ Identify bottlenecks\n\n2. Analyze & Prioritize\n   â”œâ”€ Profile application\n   â”œâ”€ Find performance hotspots\n   â””â”€ Prioritize by impact\n\n3. Optimize & Test\n   â”œâ”€ Implement optimizations\n   â”œâ”€ Measure improvements\n   â””â”€ Validate no regressions\n\n4. Monitor & Iterate\n   â”œâ”€ Continuous monitoring\n   â”œâ”€ Performance alerts\n   â””â”€ Regular optimization cycles\n```\n\n### Performance Profiling Tools\n\n```python\nimport time\nimport functools\nimport psutil\nimport tracemalloc\nfrom memory_profiler import profile\nimport cProfile\nimport pstats\nimport io\nfrom typing import Callable, Any, Dict\nimport logging\nfrom contextlib import contextmanager\n\nclass PerformanceProfiler:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.metrics = {}\n    \n    def timing_decorator(self, func_name: str = None):\n        \"\"\"Decorator to measure function execution time\"\"\"\n        def decorator(func: Callable) -> Callable:\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                name = func_name or func.__name__\n                start_time = time.perf_counter()\n                \n                try:\n                    result = func(*args, **kwargs)\n                    return result\n                finally:\n                    end_time = time.perf_counter()\n                    execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n                    \n                    # Track timing metrics\n                    if name not in self.metrics:\n                        self.metrics[name] = []\n                    self.metrics[name].append(execution_time)\n                    \n                    self.logger.info(f\"Function {name} took {execution_time:.2f}ms\")\n            \n            return wrapper\n        return decorator\n    \n    @contextmanager\n    def measure_block(self, block_name: str):\n        \"\"\"Context manager to measure code block performance\"\"\"\n        start_time = time.perf_counter()\n        start_memory = psutil.Process().memory_info().rss\n        \n        try:\n            yield\n        finally:\n            end_time = time.perf_counter()\n            end_memory = psutil.Process().memory_info().rss\n            \n            execution_time = (end_time - start_time) * 1000\n            memory_delta = end_memory - start_memory\n            \n            self.logger.info(f\"Block {block_name}: {execution_time:.2f}ms, \"\n                           f\"Memory delta: {memory_delta / 1024 / 1024:.2f}MB\")\n    \n    def profile_function(self, func: Callable, *args, **kwargs):\n        \"\"\"Profile function with cProfile\"\"\"\n        pr = cProfile.Profile()\n        pr.enable()\n        \n        try:\n            result = func(*args, **kwargs)\n        finally:\n            pr.disable()\n        \n        # Get profile stats\n        s = io.StringIO()\n        ps = pstats.Stats(pr, stream=s).sort_stats('tottime')\n        ps.print_stats(20)  # Top 20 functions\n        \n        profile_output = s.getvalue()\n        self.logger.info(f\"Profile for {func.__name__}:\\n{profile_output}\")\n        \n        return result\n    \n    def memory_profile_function(self, func: Callable, *args, **kwargs):\n        \"\"\"Profile function memory usage\"\"\"\n        tracemalloc.start()\n        \n        try:\n            result = func(*args, **kwargs)\n        finally:\n            current, peak = tracemalloc.get_traced_memory()\n            tracemalloc.stop()\n            \n            self.logger.info(f\"Memory usage for {func.__name__}: \"\n                           f\"Current: {current / 1024 / 1024:.2f}MB, \"\n                           f\"Peak: {peak / 1024 / 1024:.2f}MB\")\n        \n        return result\n    \n    def get_timing_statistics(self, func_name: str) -> Dict[str, float]:\n        \"\"\"Get timing statistics for a function\"\"\"\n        if func_name not in self.metrics:\n            return {}\n        \n        timings = self.metrics[func_name]\n        \n        return {\n            'count': len(timings),\n            'mean': sum(timings) / len(timings),\n            'min': min(timings),\n            'max': max(timings),\n            'p50': sorted(timings)[len(timings) // 2],\n            'p95': sorted(timings)[int(len(timings) * 0.95)],\n            'p99': sorted(timings)[int(len(timings) * 0.99)]\n        }\n\n# System Resource Monitor\nclass SystemResourceMonitor:\n    def __init__(self, interval_seconds: int = 1):\n        self.interval_seconds = interval_seconds\n        self.monitoring = False\n        self.metrics_history = []\n    \n    def start_monitoring(self):\n        \"\"\"Start system resource monitoring\"\"\"\n        self.monitoring = True\n        \n        import threading\n        \n        def monitor_loop():\n            while self.monitoring:\n                metrics = self.collect_system_metrics()\n                self.metrics_history.append(metrics)\n                time.sleep(self.interval_seconds)\n        \n        monitor_thread = threading.Thread(target=monitor_loop)\n        monitor_thread.daemon = True\n        monitor_thread.start()\n    \n    def stop_monitoring(self):\n        \"\"\"Stop system resource monitoring\"\"\"\n        self.monitoring = False\n    \n    def collect_system_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect current system metrics\"\"\"\n        process = psutil.Process()\n        \n        metrics = {\n            'timestamp': datetime.now().isoformat(),\n            'cpu': {\n                'process_percent': process.cpu_percent(),\n                'system_percent': psutil.cpu_percent(interval=None),\n                'core_count': psutil.cpu_count()\n            },\n            'memory': {\n                'process_mb': process.memory_info().rss / 1024 / 1024,\n                'process_percent': process.memory_percent(),\n                'system_available_mb': psutil.virtual_memory().available / 1024 / 1024,\n                'system_used_percent': psutil.virtual_memory().percent\n            },\n            'disk': {\n                'io_read_mb': psutil.disk_io_counters().read_bytes / 1024 / 1024 if psutil.disk_io_counters() else 0,\n                'io_write_mb': psutil.disk_io_counters().write_bytes / 1024 / 1024 if psutil.disk_io_counters() else 0,\n                'usage_percent': psutil.disk_usage('/').percent\n            },\n            'network': {\n                'bytes_sent': psutil.net_io_counters().bytes_sent if psutil.net_io_counters() else 0,\n                'bytes_recv': psutil.net_io_counters().bytes_recv if psutil.net_io_counters() else 0,\n                'packets_sent': psutil.net_io_counters().packets_sent if psutil.net_io_counters() else 0,\n                'packets_recv': psutil.net_io_counters().packets_recv if psutil.net_io_counters() else 0\n            }\n        }\n        \n        return metrics\n    \n    def get_performance_summary(self, duration_minutes: int = 5) -> Dict[str, Any]:\n        \"\"\"Get performance summary for specified duration\"\"\"\n        cutoff_time = datetime.now() - timedelta(minutes=duration_minutes)\n        \n        recent_metrics = [\n            m for m in self.metrics_history \n            if datetime.fromisoformat(m['timestamp']) > cutoff_time\n        ]\n        \n        if not recent_metrics:\n            return {}\n        \n        # Calculate averages\n        summary = {\n            'period_minutes': duration_minutes,\n            'sample_count': len(recent_metrics),\n            'cpu': {\n                'avg_process_percent': sum(m['cpu']['process_percent'] for m in recent_metrics) / len(recent_metrics),\n                'max_process_percent': max(m['cpu']['process_percent'] for m in recent_metrics),\n                'avg_system_percent': sum(m['cpu']['system_percent'] for m in recent_metrics) / len(recent_metrics)\n            },\n            'memory': {\n                'avg_process_mb': sum(m['memory']['process_mb'] for m in recent_metrics) / len(recent_metrics),\n                'max_process_mb': max(m['memory']['process_mb'] for m in recent_metrics),\n                'avg_system_used_percent': sum(m['memory']['system_used_percent'] for m in recent_metrics) / len(recent_metrics)\n            }\n        }\n        \n        return summary\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Initialize profiler\n    profiler = PerformanceProfiler()\n    \n    # Example function to profile\n    @profiler.timing_decorator(\"slow_function\")\n    def slow_function(n):\n        \"\"\"Simulate slow function\"\"\"\n        total = 0\n        for i in range(n):\n            total += i ** 2\n        time.sleep(0.1)  # Simulate I/O\n        return total\n    \n    # Run function multiple times\n    for i in range(5):\n        result = slow_function(10000)\n    \n    # Get timing statistics\n    stats = profiler.get_timing_statistics(\"slow_function\")\n    print(f\"Timing stats: {stats}\")\n    \n    # Profile with context manager\n    with profiler.measure_block(\"data_processing\"):\n        # Simulate data processing\n        import pandas as pd\n        df = pd.DataFrame({'x': range(10000), 'y': range(10000)})\n        result = df.groupby('x').sum()\n    \n    # Start system monitoring\n    monitor = SystemResourceMonitor(interval_seconds=1)\n    monitor.start_monitoring()\n    \n    # Simulate workload\n    time.sleep(5)\n    \n    # Stop monitoring and get summary\n    monitor.stop_monitoring()\n    summary = monitor.get_performance_summary(duration_minutes=1)\n    print(f\"System performance summary: {summary}\")\n```\n\n---\n\n## ğŸ¨ Frontend Performance Optimization {#frontend}\n\n### Critical Rendering Path Optimization\n\n```html\n<!-- Optimized HTML Structure -->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <!-- Preconnect to external domains -->\n    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n    <link rel=\"preconnect\" href=\"https://api.example.com\">\n    \n    <!-- DNS prefetch for third-party resources -->\n    <link rel=\"dns-prefetch\" href=\"https://analytics.google.com\">\n    \n    <!-- Inline critical CSS -->\n    <style>\n        /* Critical above-the-fold CSS */\n        body { font-family: Arial, sans-serif; margin: 0; }\n        .header { background: #333; color: white; padding: 1rem; }\n        .main-content { min-height: 400px; }\n    </style>\n    \n    <!-- Preload critical resources -->\n    <link rel=\"preload\" href=\"/fonts/main.woff2\" as=\"font\" type=\"font/woff2\" crossorigin>\n    <link rel=\"preload\" href=\"/js/critical.js\" as=\"script\">\n    \n    <!-- Load non-critical CSS asynchronously -->\n    <link rel=\"preload\" href=\"/css/non-critical.css\" as=\"style\" onload=\"this.onload=null;this.rel='stylesheet'\">\n    <noscript><link rel=\"stylesheet\" href=\"/css/non-critical.css\"></noscript>\n    \n    <title>Performance Optimized Page</title>\n</head>\n<body>\n    <!-- Critical above-the-fold content -->\n    <header class=\"header\">\n        <h1>Fast Loading Site</h1>\n    </header>\n    \n    <main class=\"main-content\">\n        <!-- Hero section - visible immediately -->\n        <section class=\"hero\">\n            <h2>Welcome to our fast site!</h2>\n            <!-- Optimized image with modern formats -->\n            <picture>\n                <source srcset=\"/images/hero.webp\" type=\"image/webp\">\n                <source srcset=\"/images/hero.avif\" type=\"image/avif\">\n                <img src=\"/images/hero.jpg\" \n                     alt=\"Hero image\" \n                     loading=\"eager\"\n                     width=\"800\" \n                     height=\"400\"\n                     decoding=\"sync\">\n            </picture>\n        </section>\n        \n        <!-- Below-the-fold content with lazy loading -->\n        <section class=\"products\">\n            <h2>Our Products</h2>\n            <div class=\"product-grid\" id=\"product-grid\">\n                <!-- Products loaded dynamically -->\n            </div>\n        </section>\n    </main>\n    \n    <!-- Critical JavaScript inline -->\n    <script>\n        // Critical functionality\n        (function() {\n            // Track performance\n            window.performance.mark('contentLoaded');\n            \n            // Lazy load non-critical components\n            const observer = new IntersectionObserver((entries) => {\n                entries.forEach(entry => {\n                    if (entry.isIntersecting) {\n                        loadProductGrid();\n                        observer.disconnect();\n                    }\n                });\n            });\n            \n            observer.observe(document.getElementById('product-grid'));\n        })();\n    </script>\n    \n    <!-- Non-critical JavaScript loaded asynchronously -->\n    <script src=\"/js/analytics.js\" async></script>\n    <script src=\"/js/non-critical.js\" defer></script>\n</body>\n</html>\n```\n\n### JavaScript Performance Optimization\n\n```javascript\n// 1. Efficient DOM Manipulation\nclass DOMOptimizer {\n    constructor() {\n        this.batchedUpdates = [];\n        this.updateScheduled = false;\n    }\n    \n    // Batch DOM updates to avoid layout thrashing\n    batchUpdate(element, property, value) {\n        this.batchedUpdates.push({ element, property, value });\n        \n        if (!this.updateScheduled) {\n            this.updateScheduled = true;\n            requestAnimationFrame(() => this.flushUpdates());\n        }\n    }\n    \n    flushUpdates() {\n        // Use document fragment for multiple DOM insertions\n        const fragment = document.createDocumentFragment();\n        \n        this.batchedUpdates.forEach(({ element, property, value }) => {\n            if (property === 'appendChild') {\n                fragment.appendChild(value);\n            } else {\n                element[property] = value;\n            }\n        });\n        \n        // Single DOM update\n        if (fragment.children.length > 0) {\n            document.body.appendChild(fragment);\n        }\n        \n        this.batchedUpdates = [];\n        this.updateScheduled = false;\n    }\n    \n    // Virtual scrolling for large lists\n    createVirtualList(container, items, itemHeight, visibleCount) {\n        const totalHeight = items.length * itemHeight;\n        const viewport = document.createElement('div');\n        viewport.style.height = `${visibleCount * itemHeight}px`;\n        viewport.style.overflow = 'auto';\n        \n        const content = document.createElement('div');\n        content.style.height = `${totalHeight}px`;\n        content.style.position = 'relative';\n        \n        viewport.appendChild(content);\n        container.appendChild(viewport);\n        \n        let startIndex = 0;\n        \n        const updateVisibleItems = () => {\n            const scrollTop = viewport.scrollTop;\n            const newStartIndex = Math.floor(scrollTop / itemHeight);\n            \n            if (newStartIndex !== startIndex) {\n                startIndex = newStartIndex;\n                const endIndex = Math.min(startIndex + visibleCount, items.length);\n                \n                // Clear existing items\n                content.innerHTML = '';\n                \n                // Render visible items\n                for (let i = startIndex; i < endIndex; i++) {\n                    const itemElement = document.createElement('div');\n                    itemElement.style.position = 'absolute';\n                    itemElement.style.top = `${i * itemHeight}px`;\n                    itemElement.style.height = `${itemHeight}px`;\n                    itemElement.textContent = items[i];\n                    content.appendChild(itemElement);\n                }\n            }\n        };\n        \n        viewport.addEventListener('scroll', updateVisibleItems);\n        updateVisibleItems(); // Initial render\n    }\n}\n\n// 2. Efficient Event Handling\nclass EventOptimizer {\n    constructor() {\n        this.throttleTimers = new Map();\n        this.debounceTimers = new Map();\n    }\n    \n    // Throttle function execution\n    throttle(func, delay, key = 'default') {\n        if (!this.throttleTimers.has(key)) {\n            this.throttleTimers.set(key, null);\n        }\n        \n        if (!this.throttleTimers.get(key)) {\n            func();\n            this.throttleTimers.set(key, setTimeout(() => {\n                this.throttleTimers.set(key, null);\n            }, delay));\n        }\n    }\n    \n    // Debounce function execution\n    debounce(func, delay, key = 'default') {\n        if (this.debounceTimers.has(key)) {\n            clearTimeout(this.debounceTimers.get(key));\n        }\n        \n        this.debounceTimers.set(key, setTimeout(func, delay));\n    }\n    \n    // Optimized scroll handling\n    optimizeScrollEvents() {\n        let ticking = false;\n        \n        function updateScrollPosition() {\n            // Expensive scroll operations here\n            console.log('Scroll position updated');\n            ticking = false;\n        }\n        \n        window.addEventListener('scroll', () => {\n            if (!ticking) {\n                requestAnimationFrame(updateScrollPosition);\n                ticking = true;\n            }\n        });\n    }\n    \n    // Event delegation for dynamic content\n    setupEventDelegation(container, selector, eventType, handler) {\n        container.addEventListener(eventType, (event) => {\n            if (event.target.matches(selector)) {\n                handler(event);\n            }\n        });\n    }\n}\n\n// 3. Memory Management\nclass MemoryManager {\n    constructor() {\n        this.cache = new Map();\n        this.maxCacheSize = 1000;\n        this.observers = [];\n    }\n    \n    // Object pooling for frequently created objects\n    createObjectPool(createFn, resetFn, initialSize = 10) {\n        const pool = [];\n        \n        // Pre-populate pool\n        for (let i = 0; i < initialSize; i++) {\n            pool.push(createFn());\n        }\n        \n        return {\n            acquire: () => {\n                if (pool.length > 0) {\n                    return pool.pop();\n                } else {\n                    return createFn();\n                }\n            },\n            \n            release: (obj) => {\n                resetFn(obj);\n                if (pool.length < initialSize * 2) {\n                    pool.push(obj);\n                }\n            }\n        };\n    }\n    \n    // Efficient caching with LRU eviction\n    set(key, value) {\n        // Remove if exists to update position\n        if (this.cache.has(key)) {\n            this.cache.delete(key);\n        }\n        \n        // Evict oldest if cache is full\n        if (this.cache.size >= this.maxCacheSize) {\n            const firstKey = this.cache.keys().next().value;\n            this.cache.delete(firstKey);\n        }\n        \n        this.cache.set(key, value);\n    }\n    \n    get(key) {\n        if (this.cache.has(key)) {\n            // Move to end (most recently used)\n            const value = this.cache.get(key);\n            this.cache.delete(key);\n            this.cache.set(key, value);\n            return value;\n        }\n        return null;\n    }\n    \n    // Cleanup unused observers\n    cleanupObservers() {\n        this.observers = this.observers.filter(observer => {\n            if (observer.element && !document.contains(observer.element)) {\n                // Element no longer in DOM, cleanup observer\n                if (observer.disconnect) {\n                    observer.disconnect();\n                }\n                return false;\n            }\n            return true;\n        });\n    }\n}\n\n// 4. Image Optimization\nclass ImageOptimizer {\n    constructor() {\n        this.loadedImages = new Set();\n        this.imageCache = new Map();\n    }\n    \n    // Lazy loading with Intersection Observer\n    setupLazyLoading() {\n        const imageObserver = new IntersectionObserver((entries) => {\n            entries.forEach(entry => {\n                if (entry.isIntersecting) {\n                    const img = entry.target;\n                    this.loadImage(img);\n                    imageObserver.unobserve(img);\n                }\n            });\n        }, {\n            rootMargin: '50px'  // Start loading 50px before entering viewport\n        });\n        \n        // Observe all images with data-src\n        document.querySelectorAll('img[data-src]').forEach(img => {\n            imageObserver.observe(img);\n        });\n    }\n    \n    loadImage(img) {\n        const src = img.dataset.src;\n        \n        if (this.loadedImages.has(src)) {\n            img.src = src;\n            return;\n        }\n        \n        // Show loading placeholder\n        img.classList.add('loading');\n        \n        const imageLoader = new Image();\n        imageLoader.onload = () => {\n            img.src = src;\n            img.classList.remove('loading');\n            img.classList.add('loaded');\n            this.loadedImages.add(src);\n        };\n        \n        imageLoader.onerror = () => {\n            img.classList.remove('loading');\n            img.classList.add('error');\n            img.src = '/images/placeholder.jpg'; // Fallback image\n        };\n        \n        imageLoader.src = src;\n    }\n    \n    // Responsive image loading\n    loadResponsiveImage(img) {\n        const sizes = [480, 768, 1024, 1200, 1600];\n        const devicePixelRatio = window.devicePixelRatio || 1;\n        const screenWidth = window.screen.width * devicePixelRatio;\n        \n        // Find appropriate size\n        const targetSize = sizes.find(size => size >= screenWidth) || sizes[sizes.length - 1];\n        \n        const baseUrl = img.dataset.baseUrl;\n        const format = this.supportsWebP() ? 'webp' : 'jpg';\n        const optimizedSrc = `${baseUrl}?w=${targetSize}&f=${format}&q=80`;\n        \n        img.src = optimizedSrc;\n    }\n    \n    supportsWebP() {\n        // Check WebP support\n        return document.createElement('canvas')\n            .toDataURL('image/webp')\n            .indexOf('data:image/webp') === 0;\n    }\n    \n    // Preload critical images\n    preloadCriticalImages(imageUrls) {\n        imageUrls.forEach(url => {\n            if (!this.loadedImages.has(url)) {\n                const link = document.createElement('link');\n                link.rel = 'preload';\n                link.as = 'image';\n                link.href = url;\n                document.head.appendChild(link);\n                \n                this.loadedImages.add(url);\n            }\n        });\n    }\n}\n\n// 5. JavaScript Bundle Optimization\nclass BundleOptimizer {\n    // Code splitting with dynamic imports\n    async loadComponent(componentName) {\n        try {\n            const module = await import(`./components/${componentName}.js`);\n            return module.default;\n        } catch (error) {\n            console.error(`Failed to load component ${componentName}:`, error);\n            return null;\n        }\n    }\n    \n    // Service worker for aggressive caching\n    registerServiceWorker() {\n        if ('serviceWorker' in navigator) {\n            navigator.serviceWorker.register('/sw.js')\n                .then(registration => {\n                    console.log('SW registered:', registration);\n                })\n                .catch(error => {\n                    console.log('SW registration failed:', error);\n                });\n        }\n    }\n    \n    // Resource prioritization\n    prioritizeResources() {\n        // Critical resources\n        const criticalScripts = ['/js/critical.js', '/js/framework.js'];\n        const nonCriticalScripts = ['/js/analytics.js', '/js/social.js'];\n        \n        // Load critical scripts first\n        criticalScripts.forEach(script => {\n            const scriptElement = document.createElement('script');\n            scriptElement.src = script;\n            scriptElement.defer = true;\n            document.head.appendChild(scriptElement);\n        });\n        \n        // Load non-critical scripts after page load\n        window.addEventListener('load', () => {\n            setTimeout(() => {\n                nonCriticalScripts.forEach(script => {\n                    const scriptElement = document.createElement('script');\n                    scriptElement.src = script;\n                    scriptElement.async = true;\n                    document.head.appendChild(scriptElement);\n                });\n            }, 1000);\n        });\n    }\n}\n\n// Service Worker for Advanced Caching\n// sw.js\nconst CACHE_NAME = 'app-cache-v2';\nconst STATIC_CACHE = 'static-cache-v2';\nconst DYNAMIC_CACHE = 'dynamic-cache-v1';\n\n// Cache strategies\nconst CACHE_STRATEGIES = {\n    'cache-first': ['images', 'fonts', 'css'],\n    'network-first': ['api', 'html'],\n    'stale-while-revalidate': ['js']\n};\n\nself.addEventListener('install', (event) => {\n    event.waitUntil(\n        Promise.all([\n            caches.open(STATIC_CACHE).then(cache => {\n                return cache.addAll([\n                    '/',\n                    '/css/critical.css',\n                    '/js/critical.js',\n                    '/fonts/main.woff2',\n                    '/images/logo.svg'\n                ]);\n            })\n        ])\n    );\n});\n\nself.addEventListener('fetch', (event) => {\n    const { request } = event;\n    const url = new URL(request.url);\n    \n    // Skip non-GET requests\n    if (request.method !== 'GET') {\n        return;\n    }\n    \n    // Handle different resource types\n    if (url.pathname.startsWith('/api/')) {\n        event.respondWith(networkFirst(request));\n    } else if (url.pathname.match(/\\.(js|css)$/)) {\n        event.respondWith(staleWhileRevalidate(request));\n    } else if (url.pathname.match(/\\.(png|jpg|jpeg|gif|svg|webp)$/)) {\n        event.respondWith(cacheFirst(request));\n    } else {\n        event.respondWith(networkFirst(request));\n    }\n});\n\n// Cache strategies\nasync function cacheFirst(request) {\n    const cached = await caches.match(request);\n    \n    if (cached) {\n        return cached;\n    }\n    \n    try {\n        const response = await fetch(request);\n        \n        if (response.ok) {\n            const cache = await caches.open(DYNAMIC_CACHE);\n            cache.put(request, response.clone());\n        }\n        \n        return response;\n    } catch (error) {\n        // Return fallback for images\n        if (request.url.match(/\\.(png|jpg|jpeg|gif|svg|webp)$/)) {\n            return caches.match('/images/fallback.jpg');\n        }\n        throw error;\n    }\n}\n\nasync function networkFirst(request) {\n    try {\n        const response = await fetch(request);\n        \n        if (response.ok) {\n            const cache = await caches.open(DYNAMIC_CACHE);\n            cache.put(request, response.clone());\n        }\n        \n        return response;\n    } catch (error) {\n        const cached = await caches.match(request);\n        if (cached) {\n            return cached;\n        }\n        throw error;\n    }\n}\n\nasync function staleWhileRevalidate(request) {\n    const cached = await caches.match(request);\n    \n    // Fetch fresh version in background\n    const fetchPromise = fetch(request).then(response => {\n        if (response.ok) {\n            const cache = caches.open(DYNAMIC_CACHE);\n            cache.then(c => c.put(request, response.clone()));\n        }\n        return response;\n    });\n    \n    // Return cached version immediately if available\n    return cached || fetchPromise;\n}\n```\n\n### React Performance Optimization\n\n```jsx\nimport React, { \n    memo, \n    useMemo, \n    useCallback, \n    useState, \n    useEffect,\n    Suspense,\n    lazy\n} from 'react';\n\n// 1. Component Memoization\nconst ExpensiveComponent = memo(({ data, onUpdate }) => {\n    // This component only re-renders when data or onUpdate changes\n    const processedData = useMemo(() => {\n        // Expensive calculation\n        return data.map(item => ({\n            ...item,\n            calculated: item.value * 2.5,\n            formatted: `$${item.value.toFixed(2)}`\n        }));\n    }, [data]);\n    \n    // Memoize event handlers\n    const handleClick = useCallback((id) => {\n        onUpdate(id);\n    }, [onUpdate]);\n    \n    return (\n        <div className=\"expensive-component\">\n            {processedData.map(item => (\n                <div key={item.id} onClick={() => handleClick(item.id)}>\n                    {item.formatted}\n                </div>\n            ))}\n        </div>\n    );\n});\n\n// 2. Code Splitting and Lazy Loading\nconst LazyDashboard = lazy(() => import('./Dashboard'));\nconst LazyReports = lazy(() => import('./Reports'));\nconst LazySettings = lazy(() => import('./Settings'));\n\nconst App = () => {\n    const [currentView, setCurrentView] = useState('dashboard');\n    \n    const renderView = () => {\n        switch (currentView) {\n            case 'dashboard':\n                return <LazyDashboard />;\n            case 'reports':\n                return <LazyReports />;\n            case 'settings':\n                return <LazySettings />;\n            default:\n                return <div>Select a view</div>;\n        }\n    };\n    \n    return (\n        <div className=\"app\">\n            <nav>\n                <button onClick={() => setCurrentView('dashboard')}>Dashboard</button>\n                <button onClick={() => setCurrentView('reports')}>Reports</button>\n                <button onClick={() => setCurrentView('settings')}>Settings</button>\n            </nav>\n            \n            <main>\n                <Suspense fallback={<div className=\"loading\">Loading...</div>}>\n                    {renderView()}\n                </Suspense>\n            </main>\n        </div>\n    );\n};\n\n// 3. Virtualized Lists for Large Datasets\nimport { FixedSizeList as List } from 'react-window';\n\nconst VirtualizedList = ({ items }) => {\n    const Row = memo(({ index, style }) => (\n        <div style={style} className=\"list-item\">\n            <div>{items[index].name}</div>\n            <div>{items[index].value}</div>\n        </div>\n    ));\n    \n    return (\n        <List\n            height={600}        // Container height\n            itemCount={items.length}\n            itemSize={60}       // Height of each item\n            width=\"100%\"\n        >\n            {Row}\n        </List>\n    );\n};\n\n// 4. Optimized State Management\nimport { createStore } from 'zustand';\nimport { subscribeWithSelector } from 'zustand/middleware';\n\n// Efficient store with selectors\nconst useAppStore = createStore(\n    subscribeWithSelector((set, get) => ({\n        users: [],\n        products: [],\n        orders: [],\n        \n        // Optimized actions\n        addUser: (user) => set(state => ({\n            users: [...state.users, user]\n        })),\n        \n        updateUser: (id, updates) => set(state => ({\n            users: state.users.map(user => \n                user.id === id ? { ...user, ...updates } : user\n            )\n        })),\n        \n        // Memoized selectors\n        getUserById: (id) => get().users.find(user => user.id === id),\n        \n        getActiveUsers: () => get().users.filter(user => user.active),\n        \n        getUserCount: () => get().users.length\n    }))\n);\n\n// Component with optimized subscriptions\nconst UserList = () => {\n    // Subscribe only to users array, not entire store\n    const users = useAppStore(state => state.users);\n    const activeUsers = useAppStore(state => state.getActiveUsers());\n    \n    // Only re-render when user count changes\n    const userCount = useAppStore(\n        state => state.users.length,\n        // Custom equality function\n        (oldCount, newCount) => oldCount === newCount\n    );\n    \n    return (\n        <div>\n            <h2>Users ({userCount})</h2>\n            {activeUsers.map(user => (\n                <UserCard key={user.id} user={user} />\n            ))}\n        </div>\n    );\n};\n\nconst UserCard = memo(({ user }) => {\n    return (\n        <div className=\"user-card\">\n            <h3>{user.name}</h3>\n            <p>{user.email}</p>\n        </div>\n    );\n});\n```\n\n### CSS Performance Optimization\n\n```css\n/* 1. Efficient CSS Selectors */\n/* âŒ Inefficient - descendant selector */\n.container .header .nav .menu .item {\n    color: blue;\n}\n\n/* âœ… Efficient - class selector */\n.nav-menu-item {\n    color: blue;\n}\n\n/* 2. Hardware Acceleration */\n.accelerated-element {\n    /* Trigger hardware acceleration */\n    transform: translateZ(0);\n    /* or */\n    will-change: transform, opacity;\n}\n\n.smooth-animation {\n    /* Use transform instead of changing layout properties */\n    transition: transform 0.3s ease;\n}\n\n.smooth-animation:hover {\n    /* âœ… Good - only affects composite layer */\n    transform: scale(1.05);\n    \n    /* âŒ Bad - triggers layout recalculation */\n    /* width: 120%; height: 120%; */\n}\n\n/* 3. Critical CSS Inlining */\n/* Above-the-fold styles - inline in HTML */\n.hero-section {\n    background: linear-gradient(45deg, #1e3c72, #2a5298);\n    height: 100vh;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    color: white;\n}\n\n.hero-title {\n    font-size: 3rem;\n    font-weight: bold;\n    text-align: center;\n    margin-bottom: 1rem;\n}\n\n/* 4. CSS Grid and Flexbox Performance */\n.performance-grid {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n    gap: 1rem;\n    /* More efficient than float-based layouts */\n}\n\n.performance-flex {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 1rem;\n    /* More efficient than table-based layouts */\n}\n\n/* 5. Contain Property for Performance */\n.isolated-component {\n    /* Isolate layout, style, and paint operations */\n    contain: layout style paint;\n}\n\n.size-contained {\n    /* Hint that size won't change */\n    contain: size;\n}\n\n/* 6. Optimized Animations */\n@keyframes optimized-fade-in {\n    from {\n        opacity: 0;\n        transform: translateY(20px);\n    }\n    to {\n        opacity: 1;\n        transform: translateY(0);\n    }\n}\n\n.fade-in-element {\n    animation: optimized-fade-in 0.3s ease-out;\n    /* Hint browser about changes */\n    will-change: opacity, transform;\n}\n\n/* Remove will-change after animation */\n.fade-in-element.animation-done {\n    will-change: auto;\n}\n\n/* 7. Font Performance */\n@font-face {\n    font-family: 'OptimizedFont';\n    src: url('/fonts/font.woff2') format('woff2'),\n         url('/fonts/font.woff') format('woff');\n    font-display: swap; /* Show fallback font immediately */\n    font-weight: 300 700; /* Variable font range */\n}\n\n/* 8. Media Queries for Performance */\n/* Mobile-first approach */\n.responsive-layout {\n    /* Base styles for mobile */\n    display: block;\n}\n\n/* Only load desktop styles on larger screens */\n@media (min-width: 768px) {\n    .responsive-layout {\n        display: grid;\n        grid-template-columns: 1fr 3fr;\n    }\n}\n\n/* Reduce animations on low-end devices */\n@media (prefers-reduced-motion: reduce) {\n    .animated-element {\n        animation: none;\n        transition: none;\n    }\n}\n```\n\n---\n\n## ğŸ”§ Backend Performance Optimization {#backend}\n\n### API Performance Optimization\n\n```python\nimport asyncio\nimport time\nfrom functools import wraps\nfrom typing import Dict, Any, Optional\nimport redis\nimport json\nimport hashlib\nfrom contextlib import asynccontextmanager\n\nclass APIPerformanceOptimizer:\n    def __init__(self):\n        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n        self.metrics = {}\n    \n    def cache_response(self, ttl: int = 3600):\n        \"\"\"Decorator to cache API responses\"\"\"\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                # Create cache key from function name and arguments\n                cache_key = self._generate_cache_key(func.__name__, args, kwargs)\n                \n                # Try to get from cache\n                cached_result = self.redis_client.get(cache_key)\n                if cached_result:\n                    return json.loads(cached_result)\n                \n                # Execute function and cache result\n                result = await func(*args, **kwargs)\n                self.redis_client.setex(cache_key, ttl, json.dumps(result))\n                \n                return result\n            return wrapper\n        return decorator\n    \n    def rate_limit(self, requests_per_minute: int = 60):\n        \"\"\"Decorator for API rate limiting\"\"\"\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                # Extract user/client identifier\n                client_id = kwargs.get('client_id', 'anonymous')\n                \n                # Check rate limit\n                key = f\"rate_limit:{func.__name__}:{client_id}\"\n                current_count = self.redis_client.get(key)\n                \n                if current_count and int(current_count) >= requests_per_minute:\n                    raise Exception(f\"Rate limit exceeded: {requests_per_minute} requests per minute\")\n                \n                # Increment counter\n                pipe = self.redis_client.pipeline()\n                pipe.incr(key)\n                pipe.expire(key, 60)  # Reset every minute\n                pipe.execute()\n                \n                return await func(*args, **kwargs)\n            return wrapper\n        return decorator\n    \n    def _generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:\n        \"\"\"Generate deterministic cache key\"\"\"\n        key_data = f\"{func_name}:{str(args)}:{str(sorted(kwargs.items()))}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n# Database Connection Pooling\nimport asyncpg\nimport asyncio\nfrom asyncio import Queue\n\nclass DatabasePool:\n    def __init__(self, dsn: str, min_size: int = 10, max_size: int = 50):\n        self.dsn = dsn\n        self.min_size = min_size\n        self.max_size = max_size\n        self.pool: Optional[asyncpg.Pool] = None\n        self.connection_count = 0\n    \n    async def initialize(self):\n        \"\"\"Initialize connection pool\"\"\"\n        self.pool = await asyncpg.create_pool(\n            self.dsn,\n            min_size=self.min_size,\n            max_size=self.max_size,\n            command_timeout=30,\n            server_settings={\n                'application_name': 'high_performance_app',\n                'jit': 'off'  # Disable JIT for predictable performance\n            }\n        )\n    \n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Get connection from pool with proper cleanup\"\"\"\n        async with self.pool.acquire() as connection:\n            try:\n                yield connection\n            except Exception:\n                # Reset connection on error\n                await connection.reset()\n                raise\n    \n    async def execute_query(self, query: str, *args) -> list:\n        \"\"\"Execute query with connection from pool\"\"\"\n        async with self.get_connection() as conn:\n            return await conn.fetch(query, *args)\n    \n    async def execute_transaction(self, queries: list) -> bool:\n        \"\"\"Execute multiple queries in a transaction\"\"\"\n        async with self.get_connection() as conn:\n            async with conn.transaction():\n                for query, args in queries:\n                    await conn.execute(query, *args)\n                return True\n\n# Response Compression\nfrom gzip import compress\nimport json\n\nclass ResponseCompressor:\n    @staticmethod\n    def compress_json_response(data: Dict[str, Any]) -> bytes:\n        \"\"\"Compress JSON response with gzip\"\"\"\n        json_str = json.dumps(data, separators=(',', ':'))\n        return compress(json_str.encode('utf-8'))\n    \n    @staticmethod\n    def should_compress(content_length: int, content_type: str) -> bool:\n        \"\"\"Determine if response should be compressed\"\"\"\n        # Only compress text-based content over 1KB\n        compressible_types = ['application/json', 'text/html', 'text/css', 'application/javascript']\n        return content_length > 1024 and any(ct in content_type for ct in compressible_types)\n\n# Async Processing with Background Tasks\nimport asyncio\nfrom celery import Celery\n\n# Celery for heavy background tasks\ncelery_app = Celery('performance_app', broker='redis://localhost:6379')\n\n@celery_app.task\ndef process_large_dataset(dataset_id: str):\n    \"\"\"Process large dataset in background\"\"\"\n    # Simulate heavy processing\n    import pandas as pd\n    import time\n    \n    # Load dataset\n    df = pd.read_csv(f\"datasets/{dataset_id}.csv\")\n    \n    # Heavy processing\n    processed_df = df.groupby('category').agg({\n        'value': ['sum', 'mean', 'std'],\n        'quantity': 'sum'\n    })\n    \n    # Save results\n    processed_df.to_csv(f\"results/{dataset_id}_processed.csv\")\n    \n    return f\"Processed {len(df)} records\"\n\n# FastAPI with performance optimizations\nfrom fastapi import FastAPI, BackgroundTasks, Depends\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\nimport uvicorn\n\napp = FastAPI(title=\"High Performance API\")\n\n# Add compression middleware\napp.add_middleware(GZipMiddleware, minimum_size=1000)\n\n# Security middleware\napp.add_middleware(TrustedHostMiddleware, allowed_hosts=[\"*.example.com\", \"localhost\"])\n\n# Dependency for database pool\nasync def get_db_pool() -> DatabasePool:\n    return db_pool\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    global db_pool\n    db_pool = DatabasePool(\"postgresql://user:pass@localhost/db\")\n    await db_pool.initialize()\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int, db: DatabasePool = Depends(get_db_pool)):\n    \"\"\"Optimized user endpoint with caching\"\"\"\n    \n    @cache_response(ttl=300)  # Cache for 5 minutes\n    async def fetch_user_data(uid: int):\n        query = \"\"\"\n        SELECT u.id, u.name, u.email, \n               COUNT(o.id) as order_count,\n               SUM(o.total) as total_spent\n        FROM users u\n        LEFT JOIN orders o ON u.id = o.user_id\n        WHERE u.id = $1\n        GROUP BY u.id, u.name, u.email\n        \"\"\"\n        \n        result = await db.execute_query(query, uid)\n        \n        if result:\n            return dict(result[0])\n        return None\n    \n    user_data = await fetch_user_data(user_id)\n    \n    if not user_data:\n        return {\"error\": \"User not found\"}\n    \n    return user_data\n\n@app.post(\"/process-dataset\")\nasync def process_dataset(dataset_id: str, background_tasks: BackgroundTasks):\n    \"\"\"Endpoint with background processing\"\"\"\n    \n    # Add task to background processing\n    background_tasks.add_task(process_large_dataset.delay, dataset_id)\n    \n    return {\"message\": f\"Processing started for dataset {dataset_id}\"}\n```\n\n---\n\n## ğŸ—ƒï¸ Database Performance Tuning {#database}\n\n### PostgreSQL Optimization\n\n```sql\n-- 1. Index Optimization\n-- Efficient compound index\nCREATE INDEX CONCURRENTLY idx_orders_customer_date \nON orders(customer_id, order_date DESC) \nWHERE status = 'completed';\n\n-- Partial index for common queries\nCREATE INDEX CONCURRENTLY idx_active_users \nON users(last_login_date) \nWHERE active = true;\n\n-- Expression index for computed values\nCREATE INDEX CONCURRENTLY idx_orders_total_rounded \nON orders(ROUND(total_amount));\n\n-- 2. Query Optimization\n-- âŒ Slow - SELECT *\nSELECT * FROM orders WHERE customer_id = 123;\n\n-- âœ… Fast - Specific columns\nSELECT id, total_amount, order_date \nFROM orders \nWHERE customer_id = 123\nORDER BY order_date DESC\nLIMIT 10;\n\n-- 3. Window Functions for Analytics\nSELECT \n    customer_id,\n    order_date,\n    total_amount,\n    -- Running total\n    SUM(total_amount) OVER (\n        PARTITION BY customer_id \n        ORDER BY order_date \n        ROWS UNBOUNDED PRECEDING\n    ) as running_total,\n    -- Rank within customer\n    ROW_NUMBER() OVER (\n        PARTITION BY customer_id \n        ORDER BY total_amount DESC\n    ) as amount_rank\nFROM orders;\n\n-- 4. Efficient Pagination\n-- âŒ Slow - OFFSET for large datasets\nSELECT * FROM products \nORDER BY created_at DESC \nLIMIT 20 OFFSET 10000;\n\n-- âœ… Fast - Cursor-based pagination\nSELECT * FROM products \nWHERE created_at < '2024-01-15 10:00:00'\nORDER BY created_at DESC \nLIMIT 20;\n\n-- 5. Bulk Operations\n-- âŒ Slow - Individual inserts\nINSERT INTO products (name, price) VALUES ('Product 1', 29.99);\nINSERT INTO products (name, price) VALUES ('Product 2', 39.99);\n\n-- âœ… Fast - Bulk insert\nINSERT INTO products (name, price) \nVALUES \n    ('Product 1', 29.99),\n    ('Product 2', 39.99),\n    ('Product 3', 49.99);\n\n-- Even faster - COPY from CSV\nCOPY products(name, price) \nFROM '/path/to/products.csv' \nWITH (FORMAT csv, HEADER true);\n```\n\n### Database Connection Optimization\n\n```python\nimport asyncpg\nimport asyncio\nfrom typing import Dict, Any, List\nimport logging\nimport time\nfrom contextlib import asynccontextmanager\n\nclass OptimizedDatabaseManager:\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self.pool = None\n        self.query_cache = {}\n        self.logger = logging.getLogger(__name__)\n    \n    async def initialize_pool(self):\n        \"\"\"Initialize optimized connection pool\"\"\"\n        self.pool = await asyncpg.create_pool(\n            self.dsn,\n            min_size=10,\n            max_size=50,\n            max_queries=50000,  # Queries per connection\n            max_inactive_connection_lifetime=3600,  # 1 hour\n            command_timeout=30,\n            server_settings={\n                'application_name': 'optimized_app',\n                'shared_preload_libraries': 'pg_stat_statements',\n                'max_connections': '200',\n                'shared_buffers': '256MB',\n                'effective_cache_size': '1GB',\n                'work_mem': '4MB',\n                'maintenance_work_mem': '64MB'\n            }\n        )\n        \n        self.logger.info(\"Database pool initialized\")\n    \n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Get connection with automatic cleanup\"\"\"\n        start_time = time.time()\n        \n        async with self.pool.acquire() as connection:\n            try:\n                yield connection\n            finally:\n                duration = time.time() - start_time\n                if duration > 1.0:  # Log slow connections\n                    self.logger.warning(f\"Slow connection usage: {duration:.2f}s\")\n    \n    async def execute_prepared_statement(self, query: str, *args) -> List[Dict[str, Any]]:\n        \"\"\"Execute prepared statement for better performance\"\"\"\n        \n        # Cache prepared statements\n        if query not in self.query_cache:\n            async with self.get_connection() as conn:\n                stmt = await conn.prepare(query)\n                self.query_cache[query] = stmt\n                \n                # Analyze query performance\n                explain_query = f\"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}\"\n                plan = await conn.fetchval(explain_query, *args)\n                \n                execution_time = plan[0]['Execution Time']\n                if execution_time > 100:  # Log slow queries\n                    self.logger.warning(f\"Slow query detected: {execution_time:.2f}ms\")\n        \n        async with self.get_connection() as conn:\n            stmt = self.query_cache[query]\n            return await stmt.fetch(*args)\n    \n    async def bulk_upsert(self, table: str, data: List[Dict[str, Any]], \n                         conflict_column: str) -> int:\n        \"\"\"Optimized bulk upsert operation\"\"\"\n        if not data:\n            return 0\n        \n        columns = list(data[0].keys())\n        placeholders = ', '.join([f'${i+1}' for i in range(len(columns))])\n        \n        # Build upsert query\n        query = f\"\"\"\n        INSERT INTO {table} ({', '.join(columns)})\n        VALUES ({placeholders})\n        ON CONFLICT ({conflict_column})\n        DO UPDATE SET {', '.join([f'{col} = EXCLUDED.{col}' for col in columns if col != conflict_column])}\n        \"\"\"\n        \n        async with self.get_connection() as conn:\n            async with conn.transaction():\n                # Use executemany for batch processing\n                await conn.executemany(query, [tuple(row[col] for col in columns) for row in data])\n        \n        return len(data)\n\n# Advanced Caching Strategies\nclass MultiLevelCache:\n    def __init__(self):\n        # L1: In-memory cache (fastest)\n        self.l1_cache = {}\n        self.l1_max_size = 1000\n        \n        # L2: Redis cache (fast, shared)\n        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n        \n        # L3: Database query cache\n        self.query_cache = {}\n    \n    async def get(self, key: str) -> Any:\n        \"\"\"Multi-level cache get\"\"\"\n        # Try L1 cache first\n        if key in self.l1_cache:\n            return self.l1_cache[key]\n        \n        # Try L2 cache (Redis)\n        redis_value = self.redis_client.get(key)\n        if redis_value:\n            value = json.loads(redis_value)\n            # Promote to L1 cache\n            self._set_l1(key, value)\n            return value\n        \n        return None\n    \n    async def set(self, key: str, value: Any, ttl: int = 3600):\n        \"\"\"Multi-level cache set\"\"\"\n        # Set in L1\n        self._set_l1(key, value)\n        \n        # Set in L2 (Redis)\n        self.redis_client.setex(key, ttl, json.dumps(value))\n    \n    def _set_l1(self, key: str, value: Any):\n        \"\"\"Set value in L1 cache with LRU eviction\"\"\"\n        if len(self.l1_cache) >= self.l1_max_size:\n            # Remove oldest entry (simple FIFO, could improve with LRU)\n            oldest_key = next(iter(self.l1_cache))\n            del self.l1_cache[oldest_key]\n        \n        self.l1_cache[key] = value\n\n# Async Request Processing\nimport asyncio\nimport aiohttp\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncRequestProcessor:\n    def __init__(self, max_concurrent_requests: int = 100):\n        self.semaphore = asyncio.Semaphore(max_concurrent_requests)\n        self.session: Optional[aiohttp.ClientSession] = None\n        \n    async def __aenter__(self):\n        self.session = aiohttp.ClientSession(\n            timeout=aiohttp.ClientTimeout(total=30),\n            connector=aiohttp.TCPConnector(\n                limit=100,  # Total connection limit\n                limit_per_host=30,  # Per host limit\n                keepalive_timeout=30\n            )\n        )\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            await self.session.close()\n    \n    async def make_concurrent_requests(self, urls: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Make multiple HTTP requests concurrently\"\"\"\n        \n        async def fetch_url(url: str) -> Dict[str, Any]:\n            async with self.semaphore:  # Limit concurrency\n                try:\n                    async with self.session.get(url) as response:\n                        return {\n                            'url': url,\n                            'status': response.status,\n                            'data': await response.json(),\n                            'headers': dict(response.headers)\n                        }\n                except Exception as e:\n                    return {\n                        'url': url,\n                        'error': str(e),\n                        'status': 0\n                    }\n        \n        tasks = [fetch_url(url) for url in urls]\n        return await asyncio.gather(*tasks)\n\n# CPU-bound task optimization\nclass CPUOptimizer:\n    def __init__(self, max_workers: int = None):\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n    \n    async def parallel_cpu_task(self, data_chunks: List[Any], process_func: callable) -> List[Any]:\n        \"\"\"Process CPU-intensive tasks in parallel\"\"\"\n        loop = asyncio.get_event_loop()\n        \n        tasks = [\n            loop.run_in_executor(self.executor, process_func, chunk)\n            for chunk in data_chunks\n        ]\n        \n        return await asyncio.gather(*tasks)\n```\n\n---\n\n## ğŸ—„ï¸ Database Performance Tuning {#database}\n\n### Advanced Query Optimization\n\n```python\nimport asyncpg\nimport pandas as pd\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\nfrom dataclasses import dataclass\nimport logging\n\n@dataclass\nclass QueryPerformanceMetrics:\n    query_text: str\n    execution_time_ms: float\n    rows_examined: int\n    rows_returned: int\n    buffer_usage_mb: float\n    index_usage: bool\n    suggestions: List[str]\n\nclass DatabasePerformanceTuner:\n    def __init__(self, pool: asyncpg.Pool):\n        self.pool = pool\n        self.logger = logging.getLogger(__name__)\n        self.slow_query_threshold_ms = 100\n    \n    async def analyze_query_performance(self, query: str, *args) -> QueryPerformanceMetrics:\n        \"\"\"Analyze query performance with EXPLAIN\"\"\"\n        \n        async with self.pool.acquire() as conn:\n            # Execute with analysis\n            explain_query = f\"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}\"\n            plan = await conn.fetchval(explain_query, *args)\n            \n            # Execute actual query for timing\n            start_time = time.time()\n            result = await conn.fetch(query, *args)\n            execution_time = (time.time() - start_time) * 1000\n            \n            # Parse execution plan\n            plan_data = plan[0]\n            \n            suggestions = []\n            \n            # Check for sequential scans\n            if 'Seq Scan' in str(plan_data):\n                suggestions.append(\"Consider adding an index - sequential scan detected\")\n            \n            # Check for large sort operations\n            if plan_data.get('Sort', {}).get('Sort Method') == 'external merge':\n                suggestions.append(\"Increase work_mem - external sort detected\")\n            \n            # Check buffer usage\n            buffer_usage = plan_data.get('Shared Hit Blocks', 0) + plan_data.get('Shared Read Blocks', 0)\n            buffer_usage_mb = buffer_usage * 8 / 1024  # 8KB per block\n            \n            if buffer_usage_mb > 100:\n                suggestions.append(\"High buffer usage - consider query optimization\")\n            \n            return QueryPerformanceMetrics(\n                query_text=query,\n                execution_time_ms=execution_time,\n                rows_examined=plan_data.get('Actual Rows', 0),\n                rows_returned=len(result),\n                buffer_usage_mb=buffer_usage_mb,\n                index_usage='Index' in str(plan_data),\n                suggestions=suggestions\n            )\n    \n    async def optimize_table_statistics(self, table_name: str):\n        \"\"\"Update table statistics for better query planning\"\"\"\n        async with self.pool.acquire() as conn:\n            await conn.execute(f\"ANALYZE {table_name}\")\n            self.logger.info(f\"Updated statistics for table: {table_name}\")\n    \n    async def create_missing_indexes(self, table_name: str):\n        \"\"\"Suggest and create missing indexes based on query patterns\"\"\"\n        \n        # Find most common WHERE clauses\n        query = \"\"\"\n        SELECT query, calls, mean_exec_time\n        FROM pg_stat_statements \n        WHERE query LIKE '%' || $1 || '%'\n        ORDER BY calls DESC\n        LIMIT 10\n        \"\"\"\n        \n        async with self.pool.acquire() as conn:\n            common_queries = await conn.fetch(query, table_name)\n            \n            for row in common_queries:\n                query_text = row['query']\n                \n                # Simple heuristic for index suggestions\n                if 'WHERE' in query_text.upper():\n                    # Extract WHERE conditions (simplified)\n                    where_part = query_text.upper().split('WHERE')[1].split('ORDER BY')[0]\n                    \n                    if 'AND' in where_part:\n                        self.logger.info(f\"Consider compound index for: {where_part.strip()}\")\n                    else:\n                        self.logger.info(f\"Consider single column index for: {where_part.strip()}\")\n\n# MongoDB Performance Optimization\nimport pymongo\nfrom pymongo import MongoClient\nimport time\n\nclass MongoPerformanceTuner:\n    def __init__(self, connection_string: str):\n        self.client = MongoClient(\n            connection_string,\n            maxPoolSize=50,\n            minPoolSize=10,\n            maxIdleTimeMS=30000,\n            waitQueueMultiple=10\n        )\n        self.db = self.client.get_default_database()\n    \n    def create_optimal_indexes(self, collection_name: str, query_patterns: List[Dict[str, Any]]):\n        \"\"\"Create indexes based on query patterns\"\"\"\n        collection = self.db[collection_name]\n        \n        for pattern in query_patterns:\n            query_fields = list(pattern.keys())\n            sort_fields = pattern.get('sort', {})\n            \n            # Create compound index\n            index_spec = []\n            \n            # Add query fields first\n            for field in query_fields:\n                if field != 'sort':\n                    index_spec.append((field, 1))\n            \n            # Add sort fields\n            for field, direction in sort_fields.items():\n                if (field, direction) not in index_spec:\n                    index_spec.append((field, direction))\n            \n            # Create index\n            try:\n                index_name = collection.create_index(\n                    index_spec, \n                    background=True,\n                    name=f\"idx_{'_'.join(f[0] for f in index_spec)}\"\n                )\n                print(f\"Created index: {index_name}\")\n            except Exception as e:\n                print(f\"Index creation failed: {e}\")\n    \n    def optimize_aggregation_pipeline(self, collection_name: str, pipeline: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Optimize aggregation pipeline\"\"\"\n        optimized_pipeline = []\n        \n        # Move $match stages to beginning\n        match_stages = [stage for stage in pipeline if '$match' in stage]\n        other_stages = [stage for stage in pipeline if '$match' not in stage]\n        \n        # Add match stages first for better performance\n        optimized_pipeline.extend(match_stages)\n        \n        # Add $project early if possible to reduce data size\n        project_stages = [stage for stage in other_stages if '$project' in stage]\n        if project_stages:\n            optimized_pipeline.append(project_stages[0])\n            other_stages = [stage for stage in other_stages if '$project' not in stage]\n        \n        # Add remaining stages\n        optimized_pipeline.extend(other_stages)\n        \n        return optimized_pipeline\n    \n    def analyze_slow_operations(self, threshold_ms: int = 100):\n        \"\"\"Analyze slow operations using profiler\"\"\"\n        # Enable profiler\n        self.db.set_profiling_level(2, slow_ms=threshold_ms)\n        \n        # Get slow operations\n        slow_ops = list(self.db.system.profile.find().sort('ts', -1).limit(50))\n        \n        for op in slow_ops:\n            duration = op.get('durationMillis', 0)\n            command = op.get('command', {})\n            \n            print(f\"Slow operation: {duration}ms\")\n            print(f\"Command: {command}\")\n            print(f\"Collection: {op.get('ns', 'unknown')}\")\n            print(\"---\")\n```\n\n---\n\n## ğŸ’¾ Caching Strategies {#caching}\n\n### Redis Caching Patterns\n\n```python\nimport redis\nimport json\nimport pickle\nimport hashlib\nimport time\nimport asyncio\nfrom typing import Any, Optional, Union, List, Dict\nfrom functools import wraps\nfrom datetime import datetime, timedelta\n\nclass RedisCacheManager:\n    def __init__(self, host='localhost', port=6379, db=0, max_connections=50):\n        self.redis_pool = redis.ConnectionPool(\n            host=host, \n            port=port, \n            db=db,\n            max_connections=max_connections,\n            decode_responses=False  # Handle binary data\n        )\n        self.redis_client = redis.Redis(connection_pool=self.redis_pool)\n    \n    # 1. Simple Key-Value Caching\n    async def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get value from cache\"\"\"\n        try:\n            value = self.redis_client.get(key)\n            if value:\n                return pickle.loads(value)\n            return None\n        except Exception as e:\n            print(f\"Cache get error: {e}\")\n            return None\n    \n    async def set(self, key: str, value: Any, ttl: int = 3600):\n        \"\"\"Set value in cache with TTL\"\"\"\n        try:\n            serialized = pickle.dumps(value)\n            self.redis_client.setex(key, ttl, serialized)\n        except Exception as e:\n            print(f\"Cache set error: {e}\")\n    \n    # 2. Cache-aside Pattern\n    async def cache_aside(self, key: str, fetch_func: callable, ttl: int = 3600) -> Any:\n        \"\"\"Implement cache-aside pattern\"\"\"\n        # Try cache first\n        cached_value = await self.get(key)\n        if cached_value is not None:\n            return cached_value\n        \n        # Fetch from source\n        value = await fetch_func()\n        \n        # Store in cache\n        await self.set(key, value, ttl)\n        \n        return value\n    \n    # 3. Write-through Cache\n    async def write_through(self, key: str, value: Any, update_func: callable, ttl: int = 3600):\n        \"\"\"Write-through cache pattern\"\"\"\n        # Update source first\n        await update_func(value)\n        \n        # Then update cache\n        await self.set(key, value, ttl)\n    \n    # 4. Write-behind (Write-back) Cache\n    class WriteBehindCache:\n        def __init__(self, cache_manager, flush_interval: int = 60):\n            self.cache_manager = cache_manager\n            self.dirty_keys = set()\n            self.flush_interval = flush_interval\n            self.pending_writes = {}\n            self._start_flush_worker()\n        \n        async def set(self, key: str, value: Any):\n            \"\"\"Set value and mark as dirty\"\"\"\n            await self.cache_manager.set(key, value)\n            self.dirty_keys.add(key)\n            self.pending_writes[key] = value\n        \n        def _start_flush_worker(self):\n            \"\"\"Start background worker to flush dirty data\"\"\"\n            async def flush_worker():\n                while True:\n                    if self.dirty_keys:\n                        await self._flush_dirty_keys()\n                    await asyncio.sleep(self.flush_interval)\n            \n            asyncio.create_task(flush_worker())\n        \n        async def _flush_dirty_keys(self):\n            \"\"\"Flush dirty keys to persistent storage\"\"\"\n            keys_to_flush = list(self.dirty_keys)\n            self.dirty_keys.clear()\n            \n            # Batch write to database\n            batch_data = [(key, self.pending_writes[key]) for key in keys_to_flush]\n            \n            try:\n                await self._batch_write_to_db(batch_data)\n                \n                # Remove from pending writes\n                for key in keys_to_flush:\n                    self.pending_writes.pop(key, None)\n                    \n            except Exception as e:\n                # On failure, mark as dirty again\n                self.dirty_keys.update(keys_to_flush)\n                print(f\"Write-behind flush failed: {e}\")\n    \n    # 5. Distributed Cache with Consistent Hashing\n    class DistributedCache:\n        def __init__(self, nodes: List[str]):\n            self.nodes = [redis.Redis.from_url(node) for node in nodes]\n            self.ring = self._build_hash_ring()\n        \n        def _build_hash_ring(self):\n            \"\"\"Build consistent hash ring\"\"\"\n            ring = {}\n            for i, node in enumerate(self.nodes):\n                for j in range(160):  # Virtual nodes for better distribution\n                    key = hashlib.md5(f\"{i}:{j}\".encode()).hexdigest()\n                    ring[key] = node\n            return dict(sorted(ring.items()))\n        \n        def _get_node(self, key: str) -> redis.Redis:\n            \"\"\"Get node for key using consistent hashing\"\"\"\n            if not self.ring:\n                return self.nodes[0]\n            \n            key_hash = hashlib.md5(key.encode()).hexdigest()\n            \n            for ring_key in self.ring:\n                if key_hash <= ring_key:\n                    return self.ring[ring_key]\n            \n            # Wrap around to first node\n            return list(self.ring.values())[0]\n        \n        async def get(self, key: str) -> Optional[Any]:\n            \"\"\"Get from distributed cache\"\"\"\n            node = self._get_node(key)\n            try:\n                value = node.get(key)\n                if value:\n                    return pickle.loads(value)\n            except:\n                pass\n            return None\n        \n        async def set(self, key: str, value: Any, ttl: int = 3600):\n            \"\"\"Set in distributed cache\"\"\"\n            node = self._get_node(key)\n            try:\n                serialized = pickle.dumps(value)\n                node.setex(key, ttl, serialized)\n            except Exception as e:\n                print(f\"Distributed cache set error: {e}\")\n\n# Application-level caching decorator\ndef cached(ttl: int = 3600, key_prefix: str = \"\"):\n    \"\"\"Decorator for caching function results\"\"\"\n    def decorator(func):\n        cache_manager = RedisCacheManager()\n        \n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Generate cache key\n            key_data = f\"{key_prefix}:{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}\"\n            cache_key = hashlib.md5(key_data.encode()).hexdigest()\n            \n            # Try cache first\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                return cached_result\n            \n            # Execute function\n            result = await func(*args, **kwargs)\n            \n            # Cache result\n            await cache_manager.set(cache_key, result, ttl)\n            \n            return result\n        \n        return wrapper\n    return decorator\n\n# Usage examples\n@cached(ttl=1800, key_prefix=\"user_data\")\nasync def get_user_profile(user_id: int) -> Dict[str, Any]:\n    \"\"\"Get user profile with caching\"\"\"\n    # Simulate database query\n    await asyncio.sleep(0.1)\n    return {\n        \"user_id\": user_id,\n        \"name\": f\"User {user_id}\",\n        \"email\": f\"user{user_id}@example.com\"\n    }\n```\n\n---\n\n## ğŸ”„ Load Testing & Performance Monitoring {#testing}\n\n### Comprehensive Load Testing\n\n```python\nimport asyncio\nimport aiohttp\nimport time\nimport statistics\nfrom typing import Dict, List, Any\nimport json\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass LoadTestMetrics:\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    avg_response_time_ms: float\n    p50_response_time_ms: float\n    p95_response_time_ms: float\n    p99_response_time_ms: float\n    max_response_time_ms: float\n    min_response_time_ms: float\n    requests_per_second: float\n    error_rate_percent: float\n    test_duration_seconds: float\n\nclass LoadTester:\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        self.results = []\n        \n    async def run_load_test(self, \n                           endpoint: str,\n                           concurrent_users: int = 10,\n                           duration_seconds: int = 60,\n                           ramp_up_seconds: int = 10) -> LoadTestMetrics:\n        \"\"\"Run comprehensive load test\"\"\"\n        \n        print(f\"Starting load test: {concurrent_users} users for {duration_seconds}s\")\n        \n        # Create semaphore for concurrency control\n        semaphore = asyncio.Semaphore(concurrent_users)\n        \n        # Track metrics\n        response_times = []\n        error_count = 0\n        success_count = 0\n        \n        start_time = time.time()\n        end_time = start_time + duration_seconds\n        \n        async def make_request(session):\n            \"\"\"Make single request\"\"\"\n            async with semaphore:\n                request_start = time.time()\n                try:\n                    async with session.get(f\"{self.base_url}{endpoint}\") as response:\n                        await response.text()  # Consume response body\n                        \n                        request_time = (time.time() - request_start) * 1000\n                        response_times.append(request_time)\n                        \n                        if response.status == 200:\n                            return 'success'\n                        else:\n                            return 'error'\n                except:\n                    return 'error'\n        \n        # Create session with optimized settings\n        timeout = aiohttp.ClientTimeout(total=30)\n        connector = aiohttp.TCPConnector(\n            limit=concurrent_users + 10,\n            keepalive_timeout=30,\n            enable_cleanup_closed=True\n        )\n        \n        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:\n            tasks = []\n            \n            # Ramp up users gradually\n            users_added = 0\n            ramp_interval = ramp_up_seconds / concurrent_users\n            \n            while time.time() < end_time:\n                # Add users during ramp-up period\n                if users_added < concurrent_users and len(tasks) < concurrent_users:\n                    task = asyncio.create_task(make_request(session))\n                    tasks.append(task)\n                    users_added += 1\n                    \n                    if users_added < concurrent_users:\n                        await asyncio.sleep(ramp_interval)\n                \n                # Wait for some requests to complete\n                if len(tasks) >= concurrent_users:\n                    done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n                    \n                    # Process completed requests\n                    for task in done:\n                        result = await task\n                        if result == 'success':\n                            success_count += 1\n                        else:\n                            error_count += 1\n                        \n                        tasks.remove(task)\n                    \n                    # Add new request to maintain load\n                    if time.time() < end_time:\n                        new_task = asyncio.create_task(make_request(session))\n                        tasks.append(new_task)\n            \n            # Wait for remaining tasks\n            if tasks:\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for result in results:\n                    if result == 'success':\n                        success_count += 1\n                    else:\n                        error_count += 1\n        \n        # Calculate metrics\n        total_requests = success_count + error_count\n        test_duration = time.time() - start_time\n        \n        if response_times:\n            response_times.sort()\n            metrics = LoadTestMetrics(\n                total_requests=total_requests,\n                successful_requests=success_count,\n                failed_requests=error_count,\n                avg_response_time_ms=statistics.mean(response_times),\n                p50_response_time_ms=response_times[len(response_times) // 2],\n                p95_response_time_ms=response_times[int(len(response_times) * 0.95)],\n                p99_response_time_ms=response_times[int(len(response_times) * 0.99)],\n                max_response_time_ms=max(response_times),\n                min_response_time_ms=min(response_times),\n                requests_per_second=total_requests / test_duration,\n                error_rate_percent=(error_count / total_requests * 100) if total_requests > 0 else 0,\n                test_duration_seconds=test_duration\n            )\n        else:\n            metrics = LoadTestMetrics(\n                total_requests=0, successful_requests=0, failed_requests=0,\n                avg_response_time_ms=0, p50_response_time_ms=0, p95_response_time_ms=0,\n                p99_response_time_ms=0, max_response_time_ms=0, min_response_time_ms=0,\n                requests_per_second=0, error_rate_percent=100, test_duration_seconds=test_duration\n            )\n        \n        return metrics\n    \n    def generate_report(self, metrics: LoadTestMetrics):\n        \"\"\"Generate load test report\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"LOAD TEST RESULTS\")\n        print(\"=\"*50)\n        print(f\"Total Requests: {metrics.total_requests}\")\n        print(f\"Successful: {metrics.successful_requests}\")\n        print(f\"Failed: {metrics.failed_requests}\")\n        print(f\"Success Rate: {100 - metrics.error_rate_percent:.2f}%\")\n        print(f\"Requests/Second: {metrics.requests_per_second:.2f}\")\n        print(f\"\")\n        print(\"Response Times:\")\n        print(f\"  Average: {metrics.avg_response_time_ms:.2f}ms\")\n        print(f\"  50th percentile: {metrics.p50_response_time_ms:.2f}ms\")\n        print(f\"  95th percentile: {metrics.p95_response_time_ms:.2f}ms\")\n        print(f\"  99th percentile: {metrics.p99_response_time_ms:.2f}ms\")\n        print(f\"  Min: {metrics.min_response_time_ms:.2f}ms\")\n        print(f\"  Max: {metrics.max_response_time_ms:.2f}ms\")\n        print(f\"\")\n        print(f\"Test Duration: {metrics.test_duration_seconds:.2f}s\")\n        print(\"=\"*50)\n\n# Usage\nasync def run_performance_tests():\n    tester = LoadTester(\"http://localhost:3000\")\n    \n    # Test different endpoints\n    endpoints = [\n        \"/api/users\",\n        \"/api/products\", \n        \"/api/orders\"\n    ]\n    \n    for endpoint in endpoints:\n        print(f\"\\nTesting {endpoint}...\")\n        metrics = await tester.run_load_test(\n            endpoint=endpoint,\n            concurrent_users=50,\n            duration_seconds=30\n        )\n        \n        tester.generate_report(metrics)\n\nif __name__ == \"__main__\":\n    asyncio.run(run_performance_tests())\n```\n\n---\n\n## ğŸ† Real-World Case Studies {#case-studies}\n\n### Case Study: E-commerce Performance Optimization\n\n**Problem**: E-commerce site loading in 8+ seconds, 60% bounce rate\n\n**Solution Implementation**:\n\n```python\n# 1. Database Query Optimization\nclass EcommerceOptimizer:\n    async def optimize_product_queries(self):\n        \"\"\"Optimize product listing queries\"\"\"\n        \n        # Before: N+1 query problem\n        # products = await get_products()\n        # for product in products:\n        #     product.reviews = await get_product_reviews(product.id)\n        \n        # After: Single optimized query with JOIN\n        query = \"\"\"\n        SELECT \n            p.id, p.name, p.price, p.image_url,\n            COALESCE(r.avg_rating, 0) as avg_rating,\n            COALESCE(r.review_count, 0) as review_count\n        FROM products p\n        LEFT JOIN (\n            SELECT \n                product_id,\n                AVG(rating) as avg_rating,\n                COUNT(*) as review_count\n            FROM reviews \n            GROUP BY product_id\n        ) r ON p.id = r.product_id\n        WHERE p.active = true\n        ORDER BY p.featured DESC, p.name\n        LIMIT $1 OFFSET $2\n        \"\"\"\n        \n        # Result: Query time reduced from 2.5s to 45ms\n        \n    async def implement_search_optimization(self):\n        \"\"\"Implement Elasticsearch for product search\"\"\"\n        \n        from elasticsearch import AsyncElasticsearch\n        \n        es_client = AsyncElasticsearch(['http://localhost:9200'])\n        \n        # Optimized search with filters and aggregations\n        search_body = {\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"match\": {\"name\": \"laptop\"}},\n                        {\"range\": {\"price\": {\"gte\": 500, \"lte\": 2000}}}\n                    ],\n                    \"filter\": [\n                        {\"term\": {\"active\": True}},\n                        {\"terms\": {\"category\": [\"electronics\", \"computers\"]}}\n                    ]\n                }\n            },\n            \"aggs\": {\n                \"price_ranges\": {\n                    \"range\": {\n                        \"field\": \"price\",\n                        \"ranges\": [\n                            {\"to\": 500},\n                            {\"from\": 500, \"to\": 1000},\n                            {\"from\": 1000, \"to\": 2000},\n                            {\"from\": 2000}\n                        ]\n                    }\n                },\n                \"brands\": {\n                    \"terms\": {\"field\": \"brand\", \"size\": 10}\n                }\n            },\n            \"size\": 20,\n            \"from\": 0\n        }\n        \n        response = await es_client.search(index=\"products\", body=search_body)\n        \n        # Result: Search time reduced from 3.2s to 120ms\n        return response\n```\n\n**Results**:\n- âš¡ Page load time: 8s â†’ 1.2s (85% improvement)\n- ğŸ“ˆ Conversion rate: 2.1% â†’ 4.8% (130% increase)  \n- ğŸ‘¥ Bounce rate: 60% â†’ 25% (58% reduction)\n- ğŸš€ Revenue increase: 180% over 6 months\n\n### Case Study: API Performance at Scale\n\n**Problem**: API serving 10k requests/min with 500ms average response time\n\n**Solution**:\n\n```python\n# Before: Synchronous processing\ndef slow_api_endpoint(request):\n    user = database.query(f\"SELECT * FROM users WHERE id = {request.user_id}\")\n    orders = database.query(f\"SELECT * FROM orders WHERE user_id = {user.id}\")\n    recommendations = ml_service.get_recommendations(user.id)  # 300ms external call\n    \n    return {\n        \"user\": user,\n        \"recent_orders\": orders[-5:],\n        \"recommendations\": recommendations\n    }\n\n# After: Async + caching + optimization\nclass OptimizedAPI:\n    def __init__(self):\n        self.cache = RedisCacheManager()\n        self.db_pool = DatabasePool(\"postgresql://...\")\n    \n    @cached(ttl=300)  # 5 minute cache\n    async def get_user_recommendations(self, user_id: int):\n        \"\"\"Cached recommendation service call\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"http://ml-service/recommendations/{user_id}\") as resp:\n                return await resp.json()\n    \n    async def optimized_api_endpoint(self, request):\n        \"\"\"Optimized endpoint with parallel processing\"\"\"\n        \n        # Execute queries in parallel\n        user_task = self.get_user_data(request.user_id)\n        orders_task = self.get_recent_orders(request.user_id)\n        recommendations_task = self.get_user_recommendations(request.user_id)\n        \n        # Wait for all to complete\n        user, orders, recommendations = await asyncio.gather(\n            user_task, orders_task, recommendations_task\n        )\n        \n        return {\n            \"user\": user,\n            \"recent_orders\": orders,\n            \"recommendations\": recommendations\n        }\n\n# Results:\n# - Response time: 500ms â†’ 85ms (83% improvement)\n# - Throughput: 10k req/min â†’ 45k req/min (350% increase)\n# - CPU usage: 80% â†’ 35% (56% reduction)\n```\n\n---\n\n## âœ… Performance Best Practices Summary {#best-practices}\n\n### Frontend Checklist\n- âœ… Implement lazy loading for images and components\n- âœ… Use modern image formats (WebP, AVIF)\n- âœ… Enable compression (Gzip/Brotli)\n- âœ… Minimize and bundle JavaScript/CSS\n- âœ… Implement service worker for caching\n- âœ… Use CDN for static assets\n- âœ… Optimize critical rendering path\n- âœ… Implement virtual scrolling for large lists\n\n### Backend Checklist  \n- âœ… Use connection pooling for databases\n- âœ… Implement response caching (Redis)\n- âœ… Use async processing for I/O operations\n- âœ… Background processing for heavy tasks\n- âœ… API rate limiting and throttling\n- âœ… Database query optimization\n- âœ… Horizontal scaling with load balancers\n- âœ… Monitor and profile regularly\n\n### Database Checklist\n- âœ… Create proper indexes on query columns\n- âœ… Use EXPLAIN to analyze query performance  \n- âœ… Implement read replicas for scaling reads\n- âœ… Partition large tables by date/range\n- âœ… Regular VACUUM and ANALYZE (PostgreSQL)\n- âœ… Optimize memory settings (shared_buffers, work_mem)\n- âœ… Use materialized views for complex aggregations\n- âœ… Monitor slow query logs\n\n### Infrastructure Checklist\n- âœ… Use CDN for global content delivery\n- âœ… Implement auto-scaling groups\n- âœ… Monitor resource utilization\n- âœ… Use SSD storage for databases\n- âœ… Optimize network configuration\n- âœ… Implement health checks and alerting\n- âœ… Use containerization for consistent environments\n- âœ… Regular performance testing in CI/CD\n\n---\n\n## ğŸ¯ Performance Optimization Roadmap\n\n### Beginner (0-6 months)\n1. **Learn Fundamentals**: Understand performance concepts and metrics\n2. **Master Profiling Tools**: Chrome DevTools, browser performance tabs\n3. **Basic Optimizations**: Image compression, minification, caching\n4. **Database Basics**: Learn SQL optimization and indexing\n5. **Monitoring Setup**: Implement basic performance monitoring\n\n### Intermediate (6-18 months)\n1. **Advanced Frontend**: Code splitting, lazy loading, service workers\n2. **Backend Optimization**: Connection pooling, async processing\n3. **Caching Strategies**: Redis, CDN, application-level caching\n4. **Load Testing**: Implement comprehensive load testing\n5. **Database Tuning**: Advanced query optimization, partitioning\n\n### Advanced (18+ months)\n1. **Infrastructure Scaling**: Auto-scaling, load balancing, CDNs\n2. **Microservices Performance**: Distributed system optimization\n3. **Real-time Systems**: WebSocket optimization, event streaming\n4. **Security vs Performance**: Balance security and performance\n5. **Enterprise Systems**: Large-scale performance architecture\n\n---\n\n## ğŸš€ Conclusion\n\nPerformance optimization is a continuous journey that directly impacts user experience and business success. With the techniques in this guide, you can:\n\n- âœ… Build lightning-fast user interfaces\n- âœ… Scale backend systems to handle millions of requests\n- âœ… Optimize databases for complex queries\n- âœ… Implement effective caching strategies\n- âœ… Monitor and maintain performance in production\n\nRemember: **Measure first, optimize second, validate third!**\n\nKeep optimizing and building amazing high-performance systems! âš¡ğŸš€"}