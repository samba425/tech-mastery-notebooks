{"id":"ml-cheatsheet","title":"ğŸ“‹ ML/DS Quick Reference","content":"# ğŸš€ ML/DS Quick Reference Cheat Sheet\n## Everything You Need at a Glance\n\n---\n\n## ğŸ“Š Algorithm Selection Flowchart\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ What type of problem do you have?      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚               â”‚\n    SUPERVISED    UNSUPERVISED\n       â”‚               â”‚\n   â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”\n   â”‚       â”‚       â”‚       â”‚\nREGRESS CLASSIFY CLUSTER REDUCE\n   â”‚       â”‚       â”‚       â”‚\n   â–¼       â–¼       â–¼       â–¼\n\nREGRESSION:           CLASSIFICATION:        CLUSTERING:         DIM REDUCTION:\nâ”œâ”€ Linear             â”œâ”€ Logistic            â”œâ”€ K-Means          â”œâ”€ PCA\nâ”œâ”€ Ridge/Lasso        â”œâ”€ Decision Tree       â”œâ”€ DBSCAN           â”œâ”€ t-SNE\nâ”œâ”€ Decision Tree      â”œâ”€ Random Forest       â”œâ”€ Hierarchical     â”œâ”€ LDA\nâ”œâ”€ Random Forest      â”œâ”€ SVM                 â””â”€ GMM              â””â”€ Autoencoder\nâ”œâ”€ XGBoost            â”œâ”€ Neural Network\nâ”œâ”€ Neural Network     â”œâ”€ Naive Bayes\nâ””â”€ SVR                â””â”€ KNN\n```\n\n---\n\n## ğŸ¯ Quick Decision Matrix\n\n| Scenario | Best Algorithm | Why? |\n|----------|---------------|------|\n| Small dataset (<1K samples) | Logistic Regression, Naive Bayes | Simple, less overfitting |\n| Large dataset (>100K samples) | Neural Networks, XGBoost | Can learn complex patterns |\n| Need interpretability | Decision Tree, Linear models | Easy to explain |\n| High dimensional data | Regularized models, PCA first | Handles many features |\n| Imbalanced classes | XGBoost, SMOTE + RF | Handle class imbalance well |\n| Text data | Naive Bayes, LSTM, Transformers | Designed for sequences |\n| Image data | CNN, Vision Transformers | Spatial patterns |\n| Time series | ARIMA, LSTM, Prophet | Temporal dependencies |\n| Real-time prediction | Simple models, XGBoost | Low latency |\n| Non-linear relationships | SVM, Random Forest, Neural Networks | Capture non-linearity |\n\n---\n\n## ğŸ“ Essential Formulas\n\n### **Linear Regression**\n```\nÅ· = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b\nLoss (MSE) = (1/n) Î£(y - Å·)Â²\nGradient: dL/dw = (2/n) X^T(Å· - y)\nUpdate: w = w - Î±Â·dL/dw\n```\n\n### **Logistic Regression**\n```\nz = wx + b\nÅ· = Ïƒ(z) = 1/(1 + e^(-z))\nLoss (BCE) = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]\n```\n\n### **Entropy & Information Gain**\n```\nEntropy: H(S) = -Î£ páµ¢Â·logâ‚‚(páµ¢)\nInformation Gain: IG = H(parent) - Î£(wáµ¢Â·H(childáµ¢))\n```\n\n### **Backpropagation**\n```\nForward: a = Ïƒ(wÂ·x + b)\nLoss: L = (y - a)Â²\nBackward: dL/dw = dL/da Â· da/dz Â· dz/dw\n         = 2(a-y) Â· Ïƒ'(z) Â· x\n```\n\n### **Common Metrics**\n```\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nF1 = 2 Â· (Precision Â· Recall) / (Precision + Recall)\n```\n\n---\n\n## ğŸ”§ Feature Engineering Cheat Sheet\n\n### **Handling Missing Values**\n\n| Method | When to Use | Formula |\n|--------|-------------|---------|\n| **Drop** | <5% missing | `df.dropna()` |\n| **Mean** | Normal distribution | `df.fillna(df.mean())` |\n| **Median** | Skewed/outliers | `df.fillna(df.median())` |\n| **Mode** | Categorical | `df.fillna(df.mode()[0])` |\n| **Forward Fill** | Time series | `df.fillna(method='ffill')` |\n| **KNN** | Complex patterns | `KNNImputer(n_neighbors=5)` |\n\n### **Handling Outliers**\n\n| Method | Formula | When to Use |\n|--------|---------|-------------|\n| **IQR** | Q1-1.5Ã—IQR, Q3+1.5Ã—IQR | Default choice |\n| **Z-score** | \\|z\\| > 3 | Normal distribution |\n| **Remove** | Delete rows | <1% outliers |\n| **Cap** | Clip to bounds | Valid but extreme |\n| **Transform** | log, sqrt | Reduce impact |\n\n### **Feature Scaling**\n\n| Method | Formula | Use Case |\n|--------|---------|----------|\n| **Standardization** | (x - Î¼) / Ïƒ | Normal dist, Most ML |\n| **Min-Max** | (x - min) / (max - min) | Neural networks, [0,1] needed |\n| **Robust** | (x - median) / IQR | Has outliers |\n| **Log** | log(x + 1) | Skewed data |\n\n### **Encoding Categorical**\n\n| Cardinality | Method | Example |\n|-------------|--------|---------|\n| **Binary (2)** | Label Encoding | Male=0, Female=1 |\n| **Low (3-10)** | One-Hot Encoding | Color â†’ [is_red, is_blue, is_green] |\n| **Medium (11-50)** | Target Encoding | Category â†’ mean(target) for that category |\n| **High (>50)** | Frequency/Hash | Category â†’ frequency or hash value |\n\n---\n\n## ğŸ§  Neural Network Quick Guide\n\n### **Activation Functions**\n\n| Function | Formula | Range | Use Case |\n|----------|---------|-------|----------|\n| **Sigmoid** | 1/(1+e^(-x)) | (0, 1) | Output layer (binary) |\n| **Tanh** | (e^x - e^(-x))/(e^x + e^(-x)) | (-1, 1) | Hidden layers |\n| **ReLU** | max(0, x) | [0, âˆ) | Hidden layers (default) |\n| **Leaky ReLU** | max(0.01x, x) | (-âˆ, âˆ) | Fix dying ReLU |\n| **Softmax** | e^xi / Î£e^xj | (0, 1), sum=1 | Output (multi-class) |\n\n### **Loss Functions**\n\n| Problem Type | Loss Function | Formula |\n|--------------|---------------|---------|\n| **Regression** | MSE | (1/n)Î£(y-Å·)Â² |\n| **Regression (robust)** | MAE | (1/n)Î£\\|y-Å·\\| |\n| **Binary Classification** | Binary Cross-Entropy | -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)] |\n| **Multi-class** | Categorical Cross-Entropy | -Î£yÂ·log(Å·) |\n| **SVM** | Hinge Loss | max(0, 1 - yÂ·Å·) |\n\n### **Optimizers Comparison**\n\n| Optimizer | Learning Rate | Speed | Memory | Use Case |\n|-----------|---------------|-------|--------|----------|\n| **SGD** | 0.01-0.1 | Fast | Low | Simple problems |\n| **SGD + Momentum** | 0.01-0.1 | Medium | Low | Faster than SGD |\n| **AdaGrad** | 0.01 | Medium | Medium | Sparse data |\n| **RMSprop** | 0.001 | Medium | Medium | RNNs |\n| **Adam** | 0.001 | Fast | High | Default choice! |\n\n### **Network Architecture Rules**\n\n```\nInput Layer:  Number of features\nHidden Layers: \n  - Start with 1-2 layers\n  - Each layer: 2x to input_size\n  - More layers for complex problems\n  - Use dropout (0.2-0.5) between layers\nOutput Layer:\n  - Regression: 1 neuron, linear activation\n  - Binary: 1 neuron, sigmoid activation  \n  - Multi-class: n neurons, softmax activation\n```\n\n---\n\n## ğŸ¨ Deep Learning Architectures\n\n### **CNN Layers**\n\n| Layer | Purpose | Typical Values |\n|-------|---------|----------------|\n| **Conv2D** | Feature extraction | filters=32/64/128, kernel=3Ã—3 |\n| **MaxPool2D** | Downsampling | pool_size=2Ã—2 |\n| **Dropout** | Regularization | rate=0.25-0.5 |\n| **Dense** | Classification | units=128/256/512 |\n| **BatchNorm** | Stabilize training | After Conv/Dense |\n\n**Typical CNN Architecture:**\n```\nInput (224Ã—224Ã—3)\n  â†“\nConv2D(32, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)\n  â†“\nConv2D(64, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)\n  â†“\nConv2D(128, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)\n  â†“\nFlatten\n  â†“\nDense(256) â†’ ReLU â†’ Dropout(0.5)\n  â†“\nDense(num_classes) â†’ Softmax\n```\n\n### **RNN/LSTM Use Cases**\n\n| Architecture | Use Case | Why? |\n|--------------|----------|------|\n| **Vanilla RNN** | Short sequences (<10 steps) | Simple, fast |\n| **LSTM** | Long sequences, text, time series | Handles long dependencies |\n| **GRU** | Similar to LSTM but faster | Good tradeoff |\n| **Bidirectional** | Context from both directions | Better understanding |\n| **Seq2Seq** | Translation, summarization | Input seq â†’ output seq |\n\n### **Transformer Concepts**\n\n```\nSelf-Attention:\n  Q = Query (what I'm looking for)\n  K = Key (what I contain)\n  V = Value (what I output)\n  \n  Attention(Q,K,V) = softmax(QK^T/âˆšd)V\n\nUse: NLP, Vision, Multi-modal\nAdvantages: Parallel, long-range, interpretable\nDisadvantages: Memory intensive, needs more data\n```\n\n---\n\n## ğŸ“Š Evaluation Metrics Guide\n\n### **Classification Metrics**\n\n```\nConfusion Matrix:\n                Predicted\n              Pos    Neg\nActual  Pos   TP     FN\n        Neg   FP     TN\n\nAccuracy = (TP+TN) / Total        [Overall correctness]\nPrecision = TP / (TP+FP)          [Of predicted positive, how many correct?]\nRecall = TP / (TP+FN)             [Of actual positive, how many found?]\nF1 = 2Â·PÂ·R / (P+R)                [Harmonic mean of P and R]\n```\n\n**When to Use What:**\n- **Accuracy**: Balanced classes\n- **Precision**: Cost of false positive high (spam detection)\n- **Recall**: Cost of false negative high (disease detection)\n- **F1**: Imbalanced classes\n- **ROC-AUC**: Overall model quality\n\n### **Regression Metrics**\n\n| Metric | Formula | Interpretation |\n|--------|---------|----------------|\n| **MAE** | (1/n)Î£\\|y-Å·\\| | Average error (same units as y) |\n| **MSE** | (1/n)Î£(y-Å·)Â² | Penalizes large errors more |\n| **RMSE** | âˆšMSE | Same units as y, popular |\n| **RÂ²** | 1 - (SS_res/SS_tot) | % variance explained (0-1) |\n| **MAPE** | (100/n)Î£\\|y-Å·\\|/\\|y\\| | % error, easy to interpret |\n\n---\n\n## ğŸ” Debugging ML Models\n\n### **Common Problems & Solutions**\n\n| Problem | Symptoms | Solutions |\n|---------|----------|-----------|\n| **Underfitting** | Low train & test accuracy | â€¢ More features<br>â€¢ More complex model<br>â€¢ Less regularization<br>â€¢ Train longer |\n| **Overfitting** | High train, low test accuracy | â€¢ More data<br>â€¢ Regularization (L1/L2)<br>â€¢ Dropout<br>â€¢ Simpler model<br>â€¢ Early stopping |\n| **Vanishing Gradient** | NN not learning | â€¢ ReLU activation<br>â€¢ Batch normalization<br>â€¢ Skip connections<br>â€¢ Better initialization |\n| **Exploding Gradient** | NaN/Inf losses | â€¢ Gradient clipping<br>â€¢ Lower learning rate<br>â€¢ Batch normalization |\n| **Slow Training** | Takes too long | â€¢ Smaller model<br>â€¢ Larger batch size<br>â€¢ Better optimizer (Adam)<br>â€¢ GPU/TPU |\n| **Poor Convergence** | Loss oscillates | â€¢ Lower learning rate<br>â€¢ LR schedule<br>â€¢ Better optimizer<br>â€¢ Normalize inputs |\n\n### **Training Checklist**\n\n```\nBefore Training:\nâ˜‘ Normalize/scale features\nâ˜‘ Check for data leakage\nâ˜‘ Split data properly (train/val/test)\nâ˜‘ Handle class imbalance\nâ˜‘ Set random seed for reproducibility\n\nDuring Training:\nâ˜‘ Monitor train & validation loss\nâ˜‘ Check for overfitting\nâ˜‘ Visualize predictions\nâ˜‘ Log metrics\nâ˜‘ Save checkpoints\n\nAfter Training:\nâ˜‘ Evaluate on test set\nâ˜‘ Analyze errors\nâ˜‘ Check feature importance\nâ˜‘ Try ensembling\nâ˜‘ Document everything\n```\n\n---\n\n## ğŸ’» Code Snippets\n\n### **Quick Data Preprocessing**\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Handle missing values\ndf['numeric_col'].fillna(df['numeric_col'].median(), inplace=True)\ndf['categorical_col'].fillna(df['categorical_col'].mode()[0], inplace=True)\n\n# Remove outliers (IQR method)\nQ1 = df['numeric_col'].quantile(0.25)\nQ3 = df['numeric_col'].quantile(0.75)\nIQR = Q3 - Q1\ndf = df[(df['numeric_col'] >= Q1 - 1.5*IQR) & (df['numeric_col'] <= Q3 + 1.5*IQR)]\n\n# Encode categorical\nle = LabelEncoder()\ndf['categorical_col'] = le.fit_transform(df['categorical_col'])\n\n# Split data\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\n### **Quick Model Training**\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Feature importance\nimportance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\nprint(importance)\n```\n\n### **Quick Neural Network (Keras)**\n\n```python\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Build model\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train\nhistory = model.fit(\n    X_train_scaled, y_train,\n    validation_split=0.2,\n    epochs=50,\n    batch_size=32,\n    verbose=1\n)\n\n# Evaluate\nloss, accuracy = model.evaluate(X_test_scaled, y_test)\nprint(f'Test Accuracy: {accuracy:.4f}')\n```\n\n---\n\n## ğŸ¯ Hyperparameter Tuning Guide\n\n### **Common Hyperparameters**\n\n**Random Forest:**\n```python\n{\n    'n_estimators': [100, 200, 500],           # Number of trees\n    'max_depth': [10, 20, None],               # Tree depth\n    'min_samples_split': [2, 5, 10],           # Min samples to split\n    'min_samples_leaf': [1, 2, 4],             # Min samples in leaf\n    'max_features': ['sqrt', 'log2', None]     # Features per split\n}\n```\n\n**XGBoost:**\n```python\n{\n    'learning_rate': [0.01, 0.1, 0.3],         # Shrinkage\n    'n_estimators': [100, 200, 500],           # Boosting rounds\n    'max_depth': [3, 5, 7],                    # Tree depth\n    'subsample': [0.8, 0.9, 1.0],              # Sample ratio\n    'colsample_bytree': [0.8, 0.9, 1.0]        # Feature ratio\n}\n```\n\n**Neural Network:**\n```python\n{\n    'hidden_layers': [[128, 64], [256, 128, 64]],  # Architecture\n    'dropout_rate': [0.2, 0.3, 0.5],                # Dropout\n    'learning_rate': [0.001, 0.0001],               # LR\n    'batch_size': [32, 64, 128],                    # Batch size\n    'optimizer': ['adam', 'rmsprop']                # Optimizer\n}\n```\n\n### **Grid Search vs Random Search**\n\n```python\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Grid Search (exhaustive)\ngrid_search = GridSearchCV(\n    model, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n)\ngrid_search.fit(X_train, y_train)\n\n# Random Search (faster)\nrandom_search = RandomizedSearchCV(\n    model, param_distributions, n_iter=50, cv=5, random_state=42\n)\nrandom_search.fit(X_train, y_train)\n\n# Best parameters\nprint(f\"Best params: {grid_search.best_params_}\")\nprint(f\"Best score: {grid_search.best_score_:.4f}\")\n```\n\n---\n\n## ğŸš€ Production Deployment\n\n### **Model Serving Options**\n\n| Method | Latency | Scalability | Complexity | Use Case |\n|--------|---------|-------------|------------|----------|\n| **Flask API** | Medium | Low | Low | Small apps |\n| **FastAPI** | Low | Medium | Low | Modern APIs |\n| **TF Serving** | Very Low | High | High | Production TF |\n| **AWS Lambda** | Medium | Very High | Medium | Serverless |\n| **Docker + K8s** | Low | Very High | High | Enterprise |\n\n### **Simple API Example**\n\n```python\nfrom fastapi import FastAPI\nimport joblib\nimport numpy as np\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\nscaler = joblib.load('scaler.pkl')\n\n@app.post(\"/predict\")\ndef predict(features: list):\n    # Preprocess\n    X = np.array(features).reshape(1, -1)\n    X_scaled = scaler.transform(X)\n    \n    # Predict\n    prediction = model.predict(X_scaled)[0]\n    probability = model.predict_proba(X_scaled)[0].max()\n    \n    return {\n        \"prediction\": int(prediction),\n        \"confidence\": float(probability)\n    }\n\n# Run: uvicorn app:app --reload\n```\n\n---\n\n## ğŸ“š Interview Cheat Sheet\n\n### **Top 10 ML Interview Questions**\n\n1. **Explain bias-variance tradeoff**\n   - Bias: Error from wrong assumptions (underfitting)\n   - Variance: Error from sensitivity to training data (overfitting)\n   - Tradeoff: Can't minimize both simultaneously\n\n2. **Difference between L1 and L2 regularization**\n   - L1 (Lasso): Creates sparse solutions, feature selection\n   - L2 (Ridge): Shrinks all weights, no feature selection\n   - Formula: L1 = Î»Î£|w|, L2 = Î»Î£wÂ²\n\n3. **How does Random Forest work?**\n   - Ensemble of decision trees\n   - Bootstrap sampling + random features\n   - Majority voting for classification\n   - Reduces variance, prevents overfitting\n\n4. **Explain cross-validation**\n   - Split data into K folds\n   - Train on K-1, validate on 1\n   - Repeat K times, average results\n   - Better estimate of model performance\n\n5. **How to handle imbalanced data?**\n   - SMOTE (oversampling minority)\n   - Undersampling majority\n   - Class weights\n   - Ensemble methods\n   - Proper metrics (F1, not accuracy)\n\n6. **Gradient descent variants**\n   - Batch: All data (slow, stable)\n   - Stochastic: One sample (fast, noisy)\n   - Mini-batch: Batch of samples (best tradeoff)\n\n7. **Activation functions comparison**\n   - Sigmoid: (0,1), output layer\n   - ReLU: Fast, most used in hidden layers\n   - Tanh: (-1,1), zero-centered\n   - Softmax: Multi-class output\n\n8. **Overfitting prevention**\n   - More data\n   - Regularization (L1/L2/Dropout)\n   - Simpler model\n   - Cross-validation\n   - Early stopping\n\n9. **Feature scaling necessity**\n   - Required: KNN, SVM, Neural Networks, PCA\n   - Not required: Tree-based (Decision Tree, RF, XGBoost)\n   - Reason: Distance/gradient-based need same scale\n\n10. **Precision vs Recall**\n    - Precision: Of predicted positive, how many correct?\n    - Recall: Of actual positive, how many found?\n    - Precision for spam (avoid false positives)\n    - Recall for disease (find all positives)\n\n---\n\n## ğŸ”¥ Hot Tips & Tricks\n\n### **Pandas Power Moves**\n```python\n# Quick EDA\ndf.info()                           # Overview\ndf.describe()                       # Statistics\ndf.isnull().sum()                   # Missing values\ndf.corr()                           # Correlations\n\n# Fast operations\ndf.query('age > 30')                # Filter\ndf.groupby('category').agg(['mean', 'sum'])  # Group\ndf.pipe(func1).pipe(func2)          # Chain functions\n```\n\n### **Numpy Tricks**\n```python\n# Efficient operations\nnp.where(condition, x, y)           # Conditional\nnp.clip(array, min, max)            # Limit values\nnp.percentile(array, [25, 75])      # Quartiles\nnp.vectorize(func)(array)           # Apply function\n```\n\n### **Plotting Quick**\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# One-liners\nsns.pairplot(df)                    # All relationships\nsns.heatmap(df.corr(), annot=True)  # Correlation matrix\ndf.hist(figsize=(12,10))            # All histograms\n```\n\n---\n\n## ğŸ“ Learning Resources Priority\n\n### **Must-Read (Do First)**\n1. âœ… Feature-Engineering-Complete-Guide.md\n2. âœ… Build-ML-Models-From-Scratch-Complete-Guide.md\n3. âœ… MASTER-ML-DS-COMPLETE-ROADMAP.md\n4. âœ… This cheat sheet!\n\n### **Books (Pick 2-3)**\n- \"Hands-On Machine Learning\" - AurÃ©lien GÃ©ron â­â­â­\n- \"Deep Learning\" - Goodfellow â­â­\n- \"Introduction to Statistical Learning\" - James et al. â­â­\n\n### **Online (Free)**\n- Fast.ai - Practical DL â­â­â­\n- Stanford CS229 â­â­\n- Kaggle Learn â­â­\n\n---\n\n## âš¡ Command Line Quick Reference\n\n```bash\n# Python environment\npython -m venv env\nsource env/bin/activate\npip install -r requirements.txt\n\n# Jupyter\njupyter notebook\njupyter lab\n\n# Git (save your work!)\ngit add .\ngit commit -m \"descriptive message\"\ngit push origin main\n\n# Common installations\npip install numpy pandas matplotlib seaborn scikit-learn\npip install tensorflow torch torchvision\npip install xgboost lightgbm catboost\npip install jupyterlab notebook\n\n# Check versions\npython --version\npip list\n```\n\n---\n\n## ğŸ¯ Today's Action Items\n\n```\nâ–¡ Save this cheat sheet\nâ–¡ Read one section from main guides\nâ–¡ Code one algorithm from scratch\nâ–¡ Work on one project (30 min)\nâ–¡ Push code to GitHub\nâ–¡ Review one concept before bed\n\nRemember: Small daily progress > Large irregular bursts!\n```\n\n---\n\n**ğŸŒŸ You've got everything you need. Now GO CODE! ğŸ’»**\n\n*Print this. Pin it. Reference it daily. Master it weekly.*\n"}