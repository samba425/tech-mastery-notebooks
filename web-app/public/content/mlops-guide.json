{"id":"mlops-guide","title":"ğŸš€ MLOps Production Complete Guide","content":"# ğŸš€ MLOps & Production ML - Complete Guide\n\n> **Deploy, Monitor, and Scale Machine Learning Systems**\n\n---\n\n## ğŸ¯ What is MLOps?\n\nMLOps = Machine Learning + DevOps + Data Engineering\n\n**Why Critical?**\n- 87% of ML projects never make it to production\n- Companies need ML engineers who can DEPLOY, not just train\n- **Highest salary premium:** +30-50% over data scientists\n\n---\n\n## ğŸ“š Complete MLOps Stack\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          MLOps Complete Stack               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Development                                â”‚\nâ”‚  â”œâ”€â”€ Jupyter, VSCode                       â”‚\nâ”‚  â”œâ”€â”€ Git, DVC (Data Version Control)       â”‚\nâ”‚  â””â”€â”€ Virtual Environments                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Experiment Tracking                        â”‚\nâ”‚  â”œâ”€â”€ MLflow                                â”‚\nâ”‚  â”œâ”€â”€ Weights & Biases                      â”‚\nâ”‚  â””â”€â”€ TensorBoard                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Model Training                             â”‚\nâ”‚  â”œâ”€â”€ PyTorch, TensorFlow                  â”‚\nâ”‚  â”œâ”€â”€ scikit-learn                          â”‚\nâ”‚  â””â”€â”€ XGBoost, LightGBM                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Model Serving                              â”‚\nâ”‚  â”œâ”€â”€ FastAPI, Flask                        â”‚\nâ”‚  â”œâ”€â”€ TorchServe, TF Serving               â”‚\nâ”‚  â””â”€â”€ ONNX Runtime                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Containerization                           â”‚\nâ”‚  â”œâ”€â”€ Docker                                â”‚\nâ”‚  â””â”€â”€ Docker Compose                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Orchestration                              â”‚\nâ”‚  â”œâ”€â”€ Kubernetes                            â”‚\nâ”‚  â”œâ”€â”€ Kubeflow                              â”‚\nâ”‚  â””â”€â”€ Airflow                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Monitoring                                 â”‚\nâ”‚  â”œâ”€â”€ Prometheus                            â”‚\nâ”‚  â”œâ”€â”€ Grafana                               â”‚\nâ”‚  â””â”€â”€ ELK Stack                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Cloud Platforms                            â”‚\nâ”‚  â”œâ”€â”€ AWS (SageMaker, Lambda, ECS)         â”‚\nâ”‚  â”œâ”€â”€ GCP (Vertex AI, Cloud Run)           â”‚\nâ”‚  â””â”€â”€ Azure (ML Studio)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## Part 1: Experiment Tracking\n\n### **MLflow Complete Setup:**\n\n```python\nimport mlflow\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# Start MLflow tracking\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"my-ml-experiment\")\n\ndef train_model(n_estimators, max_depth, data):\n    with mlflow.start_run():\n        # Log parameters\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\", max_depth)\n        \n        # Train model\n        model = RandomForestClassifier(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            random_state=42\n        )\n        model.fit(data['X_train'], data['y_train'])\n        \n        # Predict\n        predictions = model.predict(data['X_test'])\n        \n        # Log metrics\n        accuracy = accuracy_score(data['y_test'], predictions)\n        f1 = f1_score(data['y_test'], predictions, average='weighted')\n        \n        mlflow.log_metric(\"accuracy\", accuracy)\n        mlflow.log_metric(\"f1_score\", f1)\n        \n        # Log model\n        mlflow.sklearn.log_model(model, \"model\")\n        \n        # Log artifacts\n        import matplotlib.pyplot as plt\n        from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n        \n        cm = confusion_matrix(data['y_test'], predictions)\n        disp = ConfusionMatrixDisplay(cm)\n        disp.plot()\n        plt.savefig(\"confusion_matrix.png\")\n        mlflow.log_artifact(\"confusion_matrix.png\")\n        \n        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n        return model\n\n# Hyperparameter tuning with MLflow\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15]\n}\n\nfor n_est in param_grid['n_estimators']:\n    for depth in param_grid['max_depth']:\n        train_model(n_est, depth, data)\n\n# Load best model\nbest_run = mlflow.search_runs(order_by=['metrics.accuracy DESC']).iloc[0]\nmodel_uri = f\"runs:/{best_run.run_id}/model\"\nloaded_model = mlflow.sklearn.load_model(model_uri)\n```\n\n### **Start MLflow UI:**\n\n```bash\nmlflow ui --backend-store-uri sqlite:///mlflow.db\n# Access at http://localhost:5000\n```\n\n---\n\n## Part 2: Model Serving\n\n### **FastAPI Production Server:**\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport mlflow.sklearn\nimport numpy as np\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(title=\"ML Model API\", version=\"1.0\")\n\n# Load model at startup\n@app.on_event(\"startup\")\nasync def load_model():\n    global model\n    try:\n        model = mlflow.sklearn.load_model(\"models:/production-model/latest\")\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\nclass PredictionRequest(BaseModel):\n    features: list[float]\n\nclass PredictionResponse(BaseModel):\n    prediction: int\n    probability: float\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(request: PredictionRequest):\n    \"\"\"Make prediction\"\"\"\n    try:\n        # Validate input\n        features = np.array(request.features).reshape(1, -1)\n        \n        # Predict\n        prediction = model.predict(features)[0]\n        probability = model.predict_proba(features).max()\n        \n        logger.info(f\"Prediction: {prediction}, Probability: {probability}\")\n        \n        return PredictionResponse(\n            prediction=int(prediction),\n            probability=float(probability)\n        )\n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/predict/batch\")\nasync def predict_batch(requests: list[PredictionRequest]):\n    \"\"\"Batch predictions\"\"\"\n    features = np.array([req.features for req in requests])\n    predictions = model.predict(features)\n    probabilities = model.predict_proba(features).max(axis=1)\n    \n    return [\n        {\"prediction\": int(pred), \"probability\": float(prob)}\n        for pred, prob in zip(predictions, probabilities)\n    ]\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": model is not None,\n        \"version\": \"1.0\"\n    }\n\n@app.get(\"/metrics\")\nasync def get_metrics():\n    \"\"\"Get model metrics\"\"\"\n    # In production, track actual metrics\n    return {\n        \"requests_total\": 1000,\n        \"predictions_total\": 950,\n        \"errors_total\": 50,\n        \"avg_latency_ms\": 45.2\n    }\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n---\n\n## Part 3: Docker Containerization\n\n### **Dockerfile (Production-Ready):**\n\n```dockerfile\nFROM python:3.9-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Create non-root user\nRUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\nUSER appuser\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Run application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n```\n\n### **docker-compose.yml:**\n\n```yaml\nversion: '3.8'\n\nservices:\n  ml-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_PATH=/models\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./models:/models\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n    restart: unless-stopped\n  \n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n    restart: unless-stopped\n  \n  prometheus:\n    image: prom/prometheus\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus-data:/prometheus\n    restart: unless-stopped\n  \n  grafana:\n    image: grafana/grafana\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana-data:/var/lib/grafana\n    restart: unless-stopped\n\nvolumes:\n  redis-data:\n  prometheus-data:\n  grafana-data:\n```\n\n### **Build & Run:**\n\n```bash\n# Build image\ndocker build -t ml-model:latest .\n\n# Run container\ndocker run -d -p 8000:8000 --name ml-api ml-model:latest\n\n# Run with compose\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f ml-api\n\n# Scale service\ndocker-compose up -d --scale ml-api=3\n```\n\n---\n\n## Part 4: Kubernetes Deployment\n\n### **deployment.yaml:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-model-deployment\n  labels:\n    app: ml-model\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ml-model\n  template:\n    metadata:\n      labels:\n        app: ml-model\n    spec:\n      containers:\n      - name: ml-model\n        image: ml-model:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MODEL_PATH\n          value: \"/models\"\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ml-model-service\nspec:\n  selector:\n    app: ml-model\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ml-model-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ml-model-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n```\n\n### **Deploy to Kubernetes:**\n\n```bash\n# Apply deployment\nkubectl apply -f deployment.yaml\n\n# Check status\nkubectl get deployments\nkubectl get pods\nkubectl get services\n\n# View logs\nkubectl logs -f deployment/ml-model-deployment\n\n# Scale manually\nkubectl scale deployment ml-model-deployment --replicas=5\n\n# Update deployment\nkubectl set image deployment/ml-model-deployment ml-model=ml-model:v2\n\n# Rollback\nkubectl rollout undo deployment/ml-model-deployment\n```\n\n---\n\n## Part 5: Monitoring & Observability\n\n### **Prometheus Metrics:**\n\n```python\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nfrom functools import wraps\nimport time\n\n# Define metrics\nREQUEST_COUNT = Counter(\n    'ml_requests_total',\n    'Total ML prediction requests',\n    ['endpoint', 'http_status']\n)\n\nREQUEST_LATENCY = Histogram(\n    'ml_request_latency_seconds',\n    'Request latency in seconds',\n    ['endpoint']\n)\n\nPREDICTIONS_COUNT = Counter(\n    'ml_predictions_total',\n    'Total predictions made',\n    ['model_version']\n)\n\nMODEL_CONFIDENCE = Histogram(\n    'ml_prediction_confidence',\n    'Prediction confidence scores',\n    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]\n)\n\nACTIVE_MODELS = Gauge(\n    'ml_active_models',\n    'Number of active models loaded'\n)\n\n# Decorator for monitoring\ndef monitor_predictions(func):\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        start_time = time.time()\n        \n        try:\n            result = await func(*args, **kwargs)\n            REQUEST_COUNT.labels(endpoint=func.__name__, http_status=200).inc()\n            return result\n        except Exception as e:\n            REQUEST_COUNT.labels(endpoint=func.__name__, http_status=500).inc()\n            raise\n        finally:\n            REQUEST_LATENCY.labels(endpoint=func.__name__).observe(\n                time.time() - start_time\n            )\n    \n    return wrapper\n\n# Use in FastAPI\n@app.post(\"/predict\")\n@monitor_predictions\nasync def predict(request: PredictionRequest):\n    prediction = model.predict(...)\n    confidence = model.predict_proba(...).max()\n    \n    PREDICTIONS_COUNT.labels(model_version=\"v1\").inc()\n    MODEL_CONFIDENCE.observe(confidence)\n    \n    return {\"prediction\": prediction, \"confidence\": confidence}\n\n# Start Prometheus metrics server\nstart_http_server(9091)\n```\n\n### **Grafana Dashboard (JSON):**\n\n```json\n{\n  \"dashboard\": {\n    \"title\": \"ML Model Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(ml_requests_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"P95 Latency\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(ml_request_latency_seconds_bucket[5m]))\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Prediction Confidence\",\n        \"targets\": [\n          {\n            \"expr\": \"ml_prediction_confidence\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(ml_requests_total{http_status=\\\"500\\\"}[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Part 6: CI/CD for ML\n\n### **GitHub Actions Workflow:**\n\n```yaml\nname: ML Model CI/CD\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov\n    \n    - name: Run tests\n      run: |\n        pytest tests/ --cov=src --cov-report=xml\n    \n    - name: Check model performance\n      run: |\n        python scripts/evaluate_model.py --threshold 0.85\n  \n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Build Docker image\n      run: |\n        docker build -t ml-model:${{ github.sha }} .\n    \n    - name: Run security scan\n      run: |\n        docker scan ml-model:${{ github.sha }}\n    \n    - name: Push to registry\n      run: |\n        echo \"${{ secrets.DOCKER_PASSWORD }}\" | docker login -u \"${{ secrets.DOCKER_USERNAME }}\" --password-stdin\n        docker push ml-model:${{ github.sha }}\n  \n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    \n    steps:\n    - name: Deploy to Kubernetes\n      run: |\n        kubectl set image deployment/ml-model ml-model=ml-model:${{ github.sha }}\n        kubectl rollout status deployment/ml-model\n```\n\n---\n\n## Part 7: Model Versioning with DVC\n\n### **Setup DVC:**\n\n```bash\n# Initialize DVC\ndvc init\n\n# Add remote storage (S3)\ndvc remote add -d myremote s3://my-bucket/dvc-storage\n\n# Track data\ndvc add data/train.csv\ndvc add models/model.pkl\n\n# Commit to Git\ngit add data/train.csv.dvc models/model.pkl.dvc .dvc/config\ngit commit -m \"Track data and model with DVC\"\n\n# Push data to remote\ndvc push\n\n# Pull data\ndvc pull\n```\n\n### **DVC Pipeline (dvc.yaml):**\n\n```yaml\nstages:\n  prepare:\n    cmd: python src/prepare.py\n    deps:\n      - data/raw\n    outs:\n      - data/processed\n    params:\n      - prepare.test_size\n  \n  train:\n    cmd: python src/train.py\n    deps:\n      - data/processed\n      - src/train.py\n    outs:\n      - models/model.pkl\n    metrics:\n      - metrics.json:\n          cache: false\n    params:\n      - train.n_estimators\n      - train.max_depth\n  \n  evaluate:\n    cmd: python src/evaluate.py\n    deps:\n      - models/model.pkl\n      - data/processed\n    metrics:\n      - eval_metrics.json:\n          cache: false\n```\n\n### **Run Pipeline:**\n\n```bash\n# Run entire pipeline\ndvc repro\n\n# Run specific stage\ndvc repro train\n\n# Compare experiments\ndvc metrics show\ndvc metrics diff\n\n# Visualize pipeline\ndvc dag\n```\n\n---\n\n## Part 8: AWS Deployment\n\n### **Deploy to AWS Lambda:**\n\n```python\n# handler.py\nimport json\nimport boto3\nimport pickle\n\ns3 = boto3.client('s3')\n\ndef load_model():\n    \"\"\"Load model from S3\"\"\"\n    obj = s3.get_object(Bucket='my-models', Key='model.pkl')\n    model = pickle.loads(obj['Body'].read())\n    return model\n\nmodel = load_model()\n\ndef lambda_handler(event, context):\n    \"\"\"Lambda function handler\"\"\"\n    try:\n        # Parse input\n        body = json.loads(event['body'])\n        features = body['features']\n        \n        # Predict\n        prediction = model.predict([features])[0]\n        probability = model.predict_proba([features]).max()\n        \n        return {\n            'statusCode': 200,\n            'body': json.dumps({\n                'prediction': int(prediction),\n                'probability': float(probability)\n            })\n        }\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n```\n\n### **Deploy with Serverless Framework:**\n\n```yaml\n# serverless.yml\nservice: ml-model-api\n\nprovider:\n  name: aws\n  runtime: python3.9\n  region: us-east-1\n  memorySize: 1024\n  timeout: 30\n  \nfunctions:\n  predict:\n    handler: handler.lambda_handler\n    events:\n      - http:\n          path: predict\n          method: post\n    environment:\n      MODEL_BUCKET: my-models\n      MODEL_KEY: model.pkl\n\nplugins:\n  - serverless-python-requirements\n```\n\n```bash\n# Deploy\nserverless deploy\n\n# Invoke\nserverless invoke -f predict -d '{\"features\": [1,2,3,4]}'\n```\n\n---\n\n## ğŸ“š Complete MLOps Checklist\n\n### **Pre-Production:**\n- [ ] Code in version control (Git)\n- [ ] Data versioned (DVC)\n- [ ] Experiments tracked (MLflow)\n- [ ] Models tested (pytest)\n- [ ] API documented (FastAPI/Swagger)\n- [ ] Docker image built\n- [ ] Security scan passed\n\n### **Production:**\n- [ ] Deployed to cloud/K8s\n- [ ] Auto-scaling configured\n- [ ] Monitoring setup (Prometheus)\n- [ ] Logging configured (ELK)\n- [ ] Alerts configured\n- [ ] CI/CD pipeline working\n- [ ] Backup strategy in place\n\n### **Post-Production:**\n- [ ] Model performance monitored\n- [ ] Data drift detected\n- [ ] Retraining automated\n- [ ] A/B testing setup\n- [ ] Cost optimized\n\n---\n\n## ğŸ’¼ Career Impact\n\n**Without MLOps Skills:**\n- Data Scientist: $90K-$120K\n- ML Engineer: $100K-$140K\n\n**With MLOps Skills:**\n- MLOps Engineer: $130K-$180K\n- ML Platform Engineer: $150K-$200K\n- Principal ML Engineer: $200K-$300K+\n\n**Skills Multiplier: +40-60% salary** ğŸš€\n\n---\n\n## ğŸ¯ 8-Week Learning Path\n\n```\nWeek 1-2: Experiment Tracking & Model Serving\nWeek 3-4: Docker & Kubernetes\nWeek 5-6: Monitoring & CI/CD\nWeek 7-8: Cloud Deployment & Production\n\nProject: Deploy end-to-end ML system\n```\n\n---\n\n*MLOps Complete Guide*\n*From Development to Production*\n*Career game-changer skills* ğŸ’°\n\n"}