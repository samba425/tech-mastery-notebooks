{"id":"part4-advanced","title":"üöÄ Part 4: Advanced Backend (Hours 31-40)","content":"# Part 4: Advanced Backend Development\n## Hours 31-40 (Performance, Microservices, Production)\n\n**Duration:** 10 hours total  \n**Target:** Students who completed Frontend development  \n**Format:** 1 hour per session  \n**Style:** Read and teach directly, copy-paste examples\n\n---\n\n## üìö TABLE OF CONTENTS\n\n- [Hour 31: Asynchronous Python (asyncio & FastAPI)](#hour-31)\n- [Hour 32: FastAPI Deep Dive](#hour-32)\n- [Hour 33: Caching & Performance](#hour-33)\n- [Hour 34: Message Queues & Background Processing](#hour-34)\n- [Hour 35: File Uploads, Media Handling & S3](#hour-35)\n- [Hour 36: Security Best Practices](#hour-36)\n- [Hour 37: GraphQL Introduction](#hour-37)\n- [Hour 38: Microservices & API Gateway Concepts](#hour-38)\n- [Hour 39: Observability: Logging, Metrics & Tracing](#hour-39)\n- [Hour 40: Testing & CI for Backend](#hour-40)\n\n---\n\n<a name=\"hour-31\"></a>\n## üìÖ Hour 31: Asynchronous Python (asyncio & FastAPI)\n\n### üéØ Learning Objectives\n- Understand async/await and when to use asynchronous programming\n- Learn the difference between sync and async operations\n- Build your first FastAPI application\n- Convert Flask endpoints to FastAPI async endpoints\n- Benchmark performance differences\n\n### üìñ What to Teach\n\n**\"Today we learn asynchronous programming - the secret to building high-performance web applications!\"**\n\n---\n\n### 1Ô∏è‚É£ What is Asynchronous Programming? (10 minutes)\n\n**Real-Life Analogy:**\n\n```\nSynchronous (Blocking):\n‚îú‚îÄ‚îÄ Chef cooks one dish at a time\n‚îú‚îÄ‚îÄ Waits for each dish to complete\n‚îî‚îÄ‚îÄ Restaurant serves 10 customers/hour\n\nAsynchronous (Non-blocking):\n‚îú‚îÄ‚îÄ Chef starts multiple dishes simultaneously\n‚îú‚îÄ‚îÄ Works on other tasks while food cooks\n‚îî‚îÄ‚îÄ Restaurant serves 50 customers/hour\n```\n\n**Synchronous vs Asynchronous Code:**\n\n```python\nimport time\nimport requests\nimport asyncio\nimport aiohttp\n\n# SYNCHRONOUS (BLOCKING)\ndef fetch_data_sync():\n    urls = [\n        'https://jsonplaceholder.typicode.com/posts/1',\n        'https://jsonplaceholder.typicode.com/posts/2',\n        'https://jsonplaceholder.typicode.com/posts/3'\n    ]\n    \n    results = []\n    start_time = time.time()\n    \n    for url in urls:\n        response = requests.get(url)  # Blocks until response\n        results.append(response.json())\n        print(f\"Fetched {url}\")\n    \n    end_time = time.time()\n    print(f\"Sync took: {end_time - start_time:.2f} seconds\")\n    return results\n\n# ASYNCHRONOUS (NON-BLOCKING)\nasync def fetch_data_async():\n    urls = [\n        'https://jsonplaceholder.typicode.com/posts/1',\n        'https://jsonplaceholder.typicode.com/posts/2',\n        'https://jsonplaceholder.typicode.com/posts/3'\n    ]\n    \n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        # Start all requests simultaneously\n        tasks = [fetch_single(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    \n    end_time = time.time()\n    print(f\"Async took: {end_time - start_time:.2f} seconds\")\n    return results\n\nasync def fetch_single(session, url):\n    async with session.get(url) as response:\n        data = await response.json()\n        print(f\"Fetched {url}\")\n        return data\n\n# Run the comparison\nif __name__ == \"__main__\":\n    # Sync version\n    sync_results = fetch_data_sync()\n    \n    # Async version\n    async_results = asyncio.run(fetch_data_async())\n```\n\n**When to Use Async:**\n- ‚úÖ **I/O Heavy Operations**: Database queries, API calls, file operations\n- ‚úÖ **High Concurrency**: Many simultaneous users\n- ‚úÖ **Real-time Applications**: WebSockets, streaming data\n- ‚ùå **CPU-intensive Tasks**: Mathematical calculations, image processing\n\n---\n\n### 2Ô∏è‚É£ FastAPI Basics (15 minutes)\n\n**Install FastAPI:**\n\n```bash\npip install fastapi uvicorn[standard] aiohttp aiosqlite\n```\n\n**Your First FastAPI App:**\n\n```python\n# main.py\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport asyncio\nimport aiohttp\nimport uvicorn\n\napp = FastAPI(\n    title=\"Async Todo API\",\n    description=\"A high-performance async Todo API built with FastAPI\",\n    version=\"1.0.0\"\n)\n\n# Pydantic models for request/response validation\nclass TodoCreate(BaseModel):\n    title: str\n    description: Optional[str] = None\n    priority: str = \"medium\"\n\nclass TodoResponse(BaseModel):\n    id: int\n    title: str\n    description: Optional[str]\n    completed: bool\n    priority: str\n\nclass TodoUpdate(BaseModel):\n    title: Optional[str] = None\n    description: Optional[str] = None\n    completed: Optional[bool] = None\n    priority: Optional[str] = None\n\n# In-memory storage (replace with database in production)\ntodos_db = []\ntodo_counter = 1\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Welcome endpoint with async example\"\"\"\n    # Simulate async operation\n    await asyncio.sleep(0.1)\n    return {\"message\": \"Welcome to Async Todo API!\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\", \"async\": True}\n\n@app.post(\"/todos\", response_model=TodoResponse)\nasync def create_todo(todo: TodoCreate):\n    \"\"\"Create a new todo item\"\"\"\n    global todo_counter\n    \n    # Simulate async database operation\n    await asyncio.sleep(0.05)\n    \n    new_todo = {\n        \"id\": todo_counter,\n        \"title\": todo.title,\n        \"description\": todo.description,\n        \"completed\": False,\n        \"priority\": todo.priority\n    }\n    \n    todos_db.append(new_todo)\n    todo_counter += 1\n    \n    return new_todo\n\n@app.get(\"/todos\", response_model=List[TodoResponse])\nasync def get_todos(skip: int = 0, limit: int = 10):\n    \"\"\"Get all todos with pagination\"\"\"\n    # Simulate async database query\n    await asyncio.sleep(0.02)\n    \n    return todos_db[skip:skip + limit]\n\n@app.get(\"/todos/{todo_id}\", response_model=TodoResponse)\nasync def get_todo(todo_id: int):\n    \"\"\"Get a specific todo by ID\"\"\"\n    # Simulate async database lookup\n    await asyncio.sleep(0.02)\n    \n    for todo in todos_db:\n        if todo[\"id\"] == todo_id:\n            return todo\n    \n    raise HTTPException(status_code=404, detail=\"Todo not found\")\n\n@app.put(\"/todos/{todo_id}\", response_model=TodoResponse)\nasync def update_todo(todo_id: int, todo_update: TodoUpdate):\n    \"\"\"Update a specific todo\"\"\"\n    # Simulate async database update\n    await asyncio.sleep(0.03)\n    \n    for todo in todos_db:\n        if todo[\"id\"] == todo_id:\n            if todo_update.title is not None:\n                todo[\"title\"] = todo_update.title\n            if todo_update.description is not None:\n                todo[\"description\"] = todo_update.description\n            if todo_update.completed is not None:\n                todo[\"completed\"] = todo_update.completed\n            if todo_update.priority is not None:\n                todo[\"priority\"] = todo_update.priority\n            \n            return todo\n    \n    raise HTTPException(status_code=404, detail=\"Todo not found\")\n\n@app.delete(\"/todos/{todo_id}\")\nasync def delete_todo(todo_id: int):\n    \"\"\"Delete a specific todo\"\"\"\n    # Simulate async database deletion\n    await asyncio.sleep(0.03)\n    \n    for i, todo in enumerate(todos_db):\n        if todo[\"id\"] == todo_id:\n            deleted_todo = todos_db.pop(i)\n            return {\"message\": \"Todo deleted successfully\", \"deleted_todo\": deleted_todo}\n    \n    raise HTTPException(status_code=404, detail=\"Todo not found\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", host=\"127.0.0.1\", port=8000, reload=True)\n```\n\n**Run FastAPI App:**\n\n```bash\n# Method 1: Using uvicorn directly\nuvicorn main:app --reload\n\n# Method 2: Using python\npython main.py\n\n# Access your API:\n# http://127.0.0.1:8000 - API root\n# http://127.0.0.1:8000/docs - Interactive API docs (Swagger)\n# http://127.0.0.1:8000/redoc - Alternative docs\n```\n\n---\n\n### 3Ô∏è‚É£ Converting Flask to FastAPI (15 minutes)\n\n**Flask Version (Synchronous):**\n\n```python\n# flask_app.py\nfrom flask import Flask, request, jsonify\nimport time\nimport requests\n\napp = Flask(__name__)\n\n@app.route('/external-data', methods=['GET'])\ndef get_external_data():\n    \"\"\"Fetch data from multiple external APIs - SYNC version\"\"\"\n    start_time = time.time()\n    \n    urls = [\n        'https://jsonplaceholder.typicode.com/posts/1',\n        'https://jsonplaceholder.typicode.com/users/1',\n        'https://jsonplaceholder.typicode.com/albums/1'\n    ]\n    \n    results = []\n    for url in urls:\n        response = requests.get(url)  # BLOCKING operation\n        results.append(response.json())\n    \n    end_time = time.time()\n    \n    return jsonify({\n        \"data\": results,\n        \"processing_time\": f\"{end_time - start_time:.2f}s\",\n        \"type\": \"synchronous\"\n    })\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)\n```\n\n**FastAPI Version (Asynchronous):**\n\n```python\n# fastapi_app.py\nfrom fastapi import FastAPI\nimport time\nimport asyncio\nimport aiohttp\nfrom typing import List, Dict, Any\n\napp = FastAPI()\n\n@app.get(\"/external-data\")\nasync def get_external_data() -> Dict[str, Any]:\n    \"\"\"Fetch data from multiple external APIs - ASYNC version\"\"\"\n    start_time = time.time()\n    \n    urls = [\n        'https://jsonplaceholder.typicode.com/posts/1',\n        'https://jsonplaceholder.typicode.com/users/1',\n        'https://jsonplaceholder.typicode.com/albums/1'\n    ]\n    \n    # Fetch all URLs concurrently\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    \n    end_time = time.time()\n    \n    return {\n        \"data\": results,\n        \"processing_time\": f\"{end_time - start_time:.2f}s\",\n        \"type\": \"asynchronous\"\n    }\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> Dict[str, Any]:\n    \"\"\"Helper function to fetch a single URL\"\"\"\n    async with session.get(url) as response:\n        return await response.json()\n\n# Additional async endpoint for database simulation\n@app.get(\"/slow-operation\")\nasync def slow_operation():\n    \"\"\"Simulate a slow database operation\"\"\"\n    # This could be a database query, file I/O, etc.\n    await asyncio.sleep(2)  # Non-blocking wait\n    \n    return {\n        \"message\": \"Operation completed\",\n        \"note\": \"This was async - server could handle other requests during the wait\"\n    }\n\n# Concurrent requests handler\n@app.get(\"/concurrent-test/{num_requests}\")\nasync def concurrent_test(num_requests: int):\n    \"\"\"Test concurrent request handling\"\"\"\n    start_time = time.time()\n    \n    # Simulate multiple async operations\n    tasks = [simulate_work(i) for i in range(num_requests)]\n    results = await asyncio.gather(*tasks)\n    \n    end_time = time.time()\n    \n    return {\n        \"results\": results,\n        \"total_requests\": num_requests,\n        \"total_time\": f\"{end_time - start_time:.2f}s\",\n        \"average_time_per_request\": f\"{(end_time - start_time) / num_requests:.2f}s\"\n    }\n\nasync def simulate_work(task_id: int) -> Dict[str, Any]:\n    \"\"\"Simulate some async work\"\"\"\n    # Random delay between 0.1 and 0.5 seconds\n    delay = 0.1 + (task_id % 5) * 0.1\n    await asyncio.sleep(delay)\n    \n    return {\n        \"task_id\": task_id,\n        \"delay\": delay,\n        \"status\": \"completed\"\n    }\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"fastapi_app:app\", host=\"127.0.0.1\", port=8000, reload=True)\n```\n\n---\n\n### 4Ô∏è‚É£ Database Operations with AsyncIO (15 minutes)\n\n**Async Database Setup:**\n\n```python\n# database.py\nimport aiosqlite\nfrom typing import List, Dict, Any, Optional\nimport asyncio\nimport json\n\nclass AsyncTodoDatabase:\n    def __init__(self, db_path: str = \"async_todos.db\"):\n        self.db_path = db_path\n    \n    async def init_database(self):\n        \"\"\"Initialize the database with tables\"\"\"\n        async with aiosqlite.connect(self.db_path) as db:\n            await db.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS todos (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    title TEXT NOT NULL,\n                    description TEXT,\n                    completed BOOLEAN DEFAULT FALSE,\n                    priority TEXT DEFAULT 'medium',\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            await db.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS users (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    username TEXT UNIQUE NOT NULL,\n                    email TEXT UNIQUE NOT NULL,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            await db.commit()\n    \n    async def create_todo(self, title: str, description: str = None, priority: str = \"medium\") -> Dict[str, Any]:\n        \"\"\"Create a new todo item\"\"\"\n        async with aiosqlite.connect(self.db_path) as db:\n            cursor = await db.execute(\n                \"\"\"\n                INSERT INTO todos (title, description, priority)\n                VALUES (?, ?, ?)\n                \"\"\",\n                (title, description, priority)\n            )\n            await db.commit()\n            \n            todo_id = cursor.lastrowid\n            \n            # Fetch the created todo\n            cursor = await db.execute(\n                \"SELECT * FROM todos WHERE id = ?\", (todo_id,)\n            )\n            row = await cursor.fetchone()\n            \n            return {\n                \"id\": row[0],\n                \"title\": row[1],\n                \"description\": row[2],\n                \"completed\": bool(row[3]),\n                \"priority\": row[4],\n                \"created_at\": row[5],\n                \"updated_at\": row[6]\n            }\n    \n    async def get_todos(self, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get todos with pagination\"\"\"\n        async with aiosqlite.connect(self.db_path) as db:\n            cursor = await db.execute(\n                \"\"\"\n                SELECT * FROM todos\n                ORDER BY created_at DESC\n                LIMIT ? OFFSET ?\n                \"\"\",\n                (limit, skip)\n            )\n            rows = await cursor.fetchall()\n            \n            return [\n                {\n                    \"id\": row[0],\n                    \"title\": row[1],\n                    \"description\": row[2],\n                    \"completed\": bool(row[3]),\n                    \"priority\": row[4],\n                    \"created_at\": row[5],\n                    \"updated_at\": row[6]\n                }\n                for row in rows\n            ]\n    \n    async def get_todo_by_id(self, todo_id: int) -> Optional[Dict[str, Any]]:\n        \"\"\"Get a specific todo by ID\"\"\"\n        async with aiosqlite.connect(self.db_path) as db:\n            cursor = await db.execute(\n                \"SELECT * FROM todos WHERE id = ?\", (todo_id,)\n            )\n            row = await cursor.fetchone()\n            \n            if row:\n                return {\n                    \"id\": row[0],\n                    \"title\": row[1],\n                    \"description\": row[2],\n                    \"completed\": bool(row[3]),\n                    \"priority\": row[4],\n                    \"created_at\": row[5],\n                    \"updated_at\": row[6]\n                }\n            return None\n    \n    async def update_todo(self, todo_id: int, **kwargs) -> Optional[Dict[str, Any]]:\n        \"\"\"Update a todo item\"\"\"\n        # Build dynamic update query\n        update_fields = []\n        values = []\n        \n        for field, value in kwargs.items():\n            if value is not None and field in ['title', 'description', 'completed', 'priority']:\n                update_fields.append(f\"{field} = ?\")\n                values.append(value)\n        \n        if not update_fields:\n            return await self.get_todo_by_id(todo_id)\n        \n        # Add updated_at field\n        update_fields.append(\"updated_at = CURRENT_TIMESTAMP\")\n        values.append(todo_id)\n        \n        query = f\"UPDATE todos SET {', '.join(update_fields)} WHERE id = ?\"\n        \n        async with aiosqlite.connect(self.db_path) as db:\n            await db.execute(query, values)\n            await db.commit()\n            \n            return await self.get_todo_by_id(todo_id)\n    \n    async def delete_todo(self, todo_id: int) -> bool:\n        \"\"\"Delete a todo item\"\"\"\n        async with aiosqlite.connect(self.db_path) as db:\n            cursor = await db.execute(\n                \"DELETE FROM todos WHERE id = ?\", (todo_id,)\n            )\n            await db.commit()\n            \n            return cursor.rowcount > 0\n\n# Usage in FastAPI\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom database import AsyncTodoDatabase\n\napp = FastAPI()\ndb = AsyncTodoDatabase()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize database on startup\"\"\"\n    await db.init_database()\n\n@app.post(\"/todos\")\nasync def create_todo_endpoint(title: str, description: str = None, priority: str = \"medium\"):\n    \"\"\"Create todo with async database\"\"\"\n    todo = await db.create_todo(title, description, priority)\n    return todo\n\n@app.get(\"/todos\")\nasync def get_todos_endpoint(skip: int = 0, limit: int = 10):\n    \"\"\"Get todos with async database\"\"\"\n    todos = await db.get_todos(skip, limit)\n    return todos\n\n@app.get(\"/todos/{todo_id}\")\nasync def get_todo_endpoint(todo_id: int):\n    \"\"\"Get single todo with async database\"\"\"\n    todo = await db.get_todo_by_id(todo_id)\n    if not todo:\n        raise HTTPException(status_code=404, detail=\"Todo not found\")\n    return todo\n```\n\n---\n\n### 5Ô∏è‚É£ Performance Benchmarking (5 minutes)\n\n**Load Testing Script:**\n\n```python\n# benchmark.py\nimport asyncio\nimport aiohttp\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\nimport statistics\n\nasync def benchmark_async_endpoint(url: str, num_requests: int = 100):\n    \"\"\"Benchmark async endpoint\"\"\"\n    start_time = time.time()\n    \n    async with aiohttp.ClientSession() as session:\n        tasks = [make_async_request(session, url) for _ in range(num_requests)]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n    \n    end_time = time.time()\n    \n    successful_requests = [r for r in results if not isinstance(r, Exception)]\n    \n    return {\n        \"total_time\": end_time - start_time,\n        \"requests_per_second\": len(successful_requests) / (end_time - start_time),\n        \"successful_requests\": len(successful_requests),\n        \"failed_requests\": num_requests - len(successful_requests)\n    }\n\nasync def make_async_request(session: aiohttp.ClientSession, url: str):\n    \"\"\"Make a single async request\"\"\"\n    try:\n        async with session.get(url) as response:\n            return await response.json()\n    except Exception as e:\n        return e\n\ndef benchmark_sync_endpoint(url: str, num_requests: int = 100, max_workers: int = 10):\n    \"\"\"Benchmark sync endpoint with ThreadPoolExecutor\"\"\"\n    start_time = time.time()\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(make_sync_request, url) for _ in range(num_requests)]\n        results = [future.result() for future in futures]\n    \n    end_time = time.time()\n    \n    successful_requests = [r for r in results if not isinstance(r, Exception)]\n    \n    return {\n        \"total_time\": end_time - start_time,\n        \"requests_per_second\": len(successful_requests) / (end_time - start_time),\n        \"successful_requests\": len(successful_requests),\n        \"failed_requests\": num_requests - len(successful_requests)\n    }\n\ndef make_sync_request(url: str):\n    \"\"\"Make a single sync request\"\"\"\n    try:\n        response = requests.get(url)\n        return response.json()\n    except Exception as e:\n        return e\n\nasync def run_benchmark():\n    \"\"\"Run the benchmark comparison\"\"\"\n    fastapi_url = \"http://127.0.0.1:8000/external-data\"\n    flask_url = \"http://127.0.0.1:5000/external-data\"\n    \n    print(\"üöÄ Starting Performance Benchmark...\")\n    print(\"=\" * 50)\n    \n    # Benchmark FastAPI (async)\n    print(\"Testing FastAPI (Async)...\")\n    async_results = await benchmark_async_endpoint(fastapi_url, 50)\n    \n    # Benchmark Flask (sync with threads)\n    print(\"Testing Flask (Sync with threads)...\")\n    sync_results = benchmark_sync_endpoint(flask_url, 50)\n    \n    # Display results\n    print(\"\\nüìä BENCHMARK RESULTS\")\n    print(\"=\" * 50)\n    print(f\"FastAPI (Async):\")\n    print(f\"  Total time: {async_results['total_time']:.2f}s\")\n    print(f\"  Requests/sec: {async_results['requests_per_second']:.2f}\")\n    print(f\"  Success rate: {async_results['successful_requests']}/50\")\n    \n    print(f\"\\nFlask (Sync + Threads):\")\n    print(f\"  Total time: {sync_results['total_time']:.2f}s\")\n    print(f\"  Requests/sec: {sync_results['requests_per_second']:.2f}\")\n    print(f\"  Success rate: {sync_results['successful_requests']}/50\")\n    \n    # Performance comparison\n    improvement = async_results['requests_per_second'] / sync_results['requests_per_second']\n    print(f\"\\nüèÜ FastAPI is {improvement:.1f}x faster!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_benchmark())\n```\n\n**Run Benchmark:**\n\n```bash\n# Terminal 1: Start FastAPI\nuvicorn fastapi_app:app --port 8000\n\n# Terminal 2: Start Flask  \npython flask_app.py\n\n# Terminal 3: Run benchmark\npython benchmark.py\n```\n\n---\n\n### üè† Homework: Benchmark Sync vs Async\n\n**Task:** Compare performance of synchronous vs asynchronous endpoints\n\n```python\n# Create two endpoints:\n# 1. /sync-heavy - Makes 5 sequential API calls\n# 2. /async-heavy - Makes 5 concurrent API calls\n\n# Benchmark with different loads:\n# - 10 requests\n# - 50 requests  \n# - 100 requests\n\n# Measure and record:\n# - Total response time\n# - Requests per second\n# - Memory usage\n# - CPU usage\n\n# Create a report with your findings\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Async = Non-blocking I/O operations\n‚úÖ FastAPI = Modern async Python web framework\n‚úÖ asyncio = Python's async programming library\n‚úÖ Performance = Significant improvement for I/O heavy apps\n‚úÖ When to use = High concurrency, external API calls, database queries\n\n---\n\n<a name=\"hour-32\"></a>\n## üìÖ Hour 32: FastAPI Deep Dive\n\n### üéØ Learning Objectives\n- Master dependency injection in FastAPI\n- Create robust Pydantic models with validation\n- Implement background tasks and async workers\n- Use auto-generated API documentation\n- Build a complete CRUD system with advanced features\n\n### üìñ What to Teach\n\n**\"Today we master FastAPI's advanced features that make it a production-ready framework!\"**\n\n---\n\n### 1Ô∏è‚É£ Dependency Injection (15 minutes)\n\n**What is Dependency Injection?**\n\n```python\n# Without DI - Hard to test and maintain\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    db = Database()  # Hardcoded dependency\n    user = db.get_user(user_id)\n    return user\n\n# With DI - Flexible and testable\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int, db: Database = Depends(get_database)):\n    user = await db.get_user(user_id)\n    return user\n```\n\n**Database Dependency:**\n\n```python\n# dependencies.py\nimport aiosqlite\nfrom typing import AsyncGenerator\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nimport jwt\nfrom datetime import datetime, timedelta\n\n# Database dependency\nasync def get_database() -> AsyncGenerator[aiosqlite.Connection, None]:\n    \"\"\"Provide database connection\"\"\"\n    async with aiosqlite.connect(\"blog.db\") as db:\n        yield db\n\n# Authentication dependency\nsecurity = HTTPBearer()\n\nasync def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Get current authenticated user\"\"\"\n    try:\n        payload = jwt.decode(credentials.credentials, \"secret_key\", algorithms=[\"HS256\"])\n        user_id: int = payload.get(\"user_id\")\n        if user_id is None:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Could not validate credentials\"\n            )\n        return {\"user_id\": user_id, \"username\": payload.get(\"username\")}\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Could not validate credentials\"\n        )\n\n# Optional authentication (user can be None)\nasync def get_current_user_optional(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Get current user if authenticated, None otherwise\"\"\"\n    try:\n        return await get_current_user(credentials)\n    except HTTPException:\n        return None\n\n# Pagination dependency\nclass PaginationParams:\n    def __init__(self, page: int = 1, size: int = 10):\n        self.page = max(1, page)\n        self.size = min(100, max(1, size))  # Limit max size\n        self.offset = (self.page - 1) * self.size\n\ndef get_pagination_params(page: int = 1, size: int = 10) -> PaginationParams:\n    \"\"\"Get pagination parameters\"\"\"\n    return PaginationParams(page, size)\n\n# Admin role dependency\nasync def require_admin_user(current_user: dict = Depends(get_current_user)):\n    \"\"\"Require admin role\"\"\"\n    if not current_user.get(\"is_admin\", False):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin access required\"\n        )\n    return current_user\n```\n\n**Using Dependencies in Endpoints:**\n\n```python\n# blog_api.py\nfrom fastapi import FastAPI, Depends, HTTPException, status, BackgroundTasks\nfrom dependencies import get_database, get_current_user, get_pagination_params\nfrom models import PostCreate, PostResponse, PostUpdate\nimport aiosqlite\nfrom typing import List\n\napp = FastAPI(\n    title=\"Blog API\",\n    description=\"Advanced FastAPI blog with dependency injection\",\n    version=\"2.0.0\"\n)\n\n@app.post(\"/posts\", response_model=PostResponse)\nasync def create_post(\n    post: PostCreate,\n    current_user: dict = Depends(get_current_user),\n    db: aiosqlite.Connection = Depends(get_database),\n    background_tasks: BackgroundTasks = BackgroundTasks()\n):\n    \"\"\"Create a new blog post\"\"\"\n    # Insert post into database\n    cursor = await db.execute(\n        \"\"\"\n        INSERT INTO posts (title, content, author_id, status)\n        VALUES (?, ?, ?, ?)\n        \"\"\",\n        (post.title, post.content, current_user[\"user_id\"], \"published\")\n    )\n    await db.commit()\n    \n    post_id = cursor.lastrowid\n    \n    # Add background task to send notifications\n    background_tasks.add_task(notify_subscribers, post_id, post.title)\n    \n    # Fetch and return the created post\n    cursor = await db.execute(\n        \"\"\"\n        SELECT p.*, u.username as author_name \n        FROM posts p \n        JOIN users u ON p.author_id = u.id \n        WHERE p.id = ?\n        \"\"\",\n        (post_id,)\n    )\n    row = await cursor.fetchone()\n    \n    return PostResponse(\n        id=row[0],\n        title=row[1],\n        content=row[2],\n        author_id=row[3],\n        author_name=row[6],\n        status=row[4],\n        created_at=row[5]\n    )\n\n@app.get(\"/posts\", response_model=List[PostResponse])\nasync def get_posts(\n    pagination: PaginationParams = Depends(get_pagination_params),\n    db: aiosqlite.Connection = Depends(get_database),\n    tag: str = None,\n    author: str = None\n):\n    \"\"\"Get posts with filtering and pagination\"\"\"\n    query = \"\"\"\n        SELECT p.*, u.username as author_name \n        FROM posts p \n        JOIN users u ON p.author_id = u.id \n        WHERE p.status = 'published'\n    \"\"\"\n    params = []\n    \n    # Add filters\n    if tag:\n        query += \" AND p.tags LIKE ?\"\n        params.append(f\"%{tag}%\")\n    \n    if author:\n        query += \" AND u.username = ?\"\n        params.append(author)\n    \n    # Add ordering and pagination\n    query += \" ORDER BY p.created_at DESC LIMIT ? OFFSET ?\"\n    params.extend([pagination.size, pagination.offset])\n    \n    cursor = await db.execute(query, params)\n    rows = await cursor.fetchall()\n    \n    return [\n        PostResponse(\n            id=row[0],\n            title=row[1],\n            content=row[2],\n            author_id=row[3],\n            author_name=row[6],\n            status=row[4],\n            created_at=row[5]\n        )\n        for row in rows\n    ]\n\n# Background task function\nasync def notify_subscribers(post_id: int, post_title: str):\n    \"\"\"Send notification to subscribers (background task)\"\"\"\n    import asyncio\n    # Simulate sending notifications\n    print(f\"üìß Sending notifications for post: {post_title}\")\n    await asyncio.sleep(2)  # Simulate email sending\n    print(f\"‚úÖ Notifications sent for post {post_id}\")\n```\n\n---\n\n### 2Ô∏è‚É£ Advanced Pydantic Models (15 minutes)\n\n**Comprehensive Model Definitions:**\n\n```python\n# models.py\nfrom pydantic import BaseModel, Field, validator, EmailStr\nfrom typing import Optional, List, Union\nfrom datetime import datetime\nfrom enum import Enum\n\nclass PostStatus(str, Enum):\n    DRAFT = \"draft\"\n    PUBLISHED = \"published\"\n    ARCHIVED = \"archived\"\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\nclass UserRole(str, Enum):\n    USER = \"user\"\n    AUTHOR = \"author\"\n    ADMIN = \"admin\"\n\n# Base models\nclass UserBase(BaseModel):\n    username: str = Field(..., min_length=3, max_length=50, regex=\"^[a-zA-Z0-9_]+$\")\n    email: EmailStr\n    full_name: Optional[str] = Field(None, max_length=100)\n    bio: Optional[str] = Field(None, max_length=500)\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"username\": \"johndoe\",\n                \"email\": \"john@example.com\",\n                \"full_name\": \"John Doe\",\n                \"bio\": \"Software developer and tech enthusiast\"\n            }\n        }\n\nclass UserCreate(UserBase):\n    password: str = Field(..., min_length=8, max_length=100)\n    \n    @validator('password')\n    def validate_password(cls, v):\n        if not any(c.isupper() for c in v):\n            raise ValueError('Password must contain at least one uppercase letter')\n        if not any(c.islower() for c in v):\n            raise ValueError('Password must contain at least one lowercase letter')  \n        if not any(c.isdigit() for c in v):\n            raise ValueError('Password must contain at least one digit')\n        return v\n\nclass UserResponse(UserBase):\n    id: int\n    role: UserRole\n    is_active: bool\n    created_at: datetime\n    posts_count: int = 0\n    \n    class Config:\n        orm_mode = True\n\nclass UserUpdate(BaseModel):\n    email: Optional[EmailStr] = None\n    full_name: Optional[str] = Field(None, max_length=100)\n    bio: Optional[str] = Field(None, max_length=500)\n    is_active: Optional[bool] = None\n\n# Post models\nclass PostBase(BaseModel):\n    title: str = Field(..., min_length=5, max_length=200)\n    content: str = Field(..., min_length=10)\n    summary: Optional[str] = Field(None, max_length=500)\n    tags: List[str] = Field(default_factory=list, max_items=10)\n    status: PostStatus = PostStatus.DRAFT\n    priority: Priority = Priority.MEDIUM\n    \n    @validator('tags')\n    def validate_tags(cls, v):\n        # Remove duplicates and clean tags\n        cleaned_tags = []\n        for tag in v:\n            clean_tag = tag.strip().lower()\n            if clean_tag and clean_tag not in cleaned_tags:\n                cleaned_tags.append(clean_tag)\n        return cleaned_tags[:10]  # Limit to 10 tags\n\nclass PostCreate(PostBase):\n    pass\n\nclass PostUpdate(BaseModel):\n    title: Optional[str] = Field(None, min_length=5, max_length=200)\n    content: Optional[str] = Field(None, min_length=10)\n    summary: Optional[str] = Field(None, max_length=500)\n    tags: Optional[List[str]] = Field(None, max_items=10)\n    status: Optional[PostStatus] = None\n    priority: Optional[Priority] = None\n\nclass PostResponse(PostBase):\n    id: int\n    author_id: int\n    author_name: str\n    slug: str\n    views_count: int = 0\n    likes_count: int = 0\n    created_at: datetime\n    updated_at: Optional[datetime] = None\n    \n    class Config:\n        orm_mode = True\n\n# Comment models\nclass CommentBase(BaseModel):\n    content: str = Field(..., min_length=1, max_length=1000)\n    parent_id: Optional[int] = None  # For nested comments\n\nclass CommentCreate(CommentBase):\n    pass\n\nclass CommentResponse(CommentBase):\n    id: int\n    post_id: int\n    author_id: int\n    author_name: str\n    created_at: datetime\n    replies: List['CommentResponse'] = []\n    \n    class Config:\n        orm_mode = True\n\n# Update forward reference\nCommentResponse.model_rebuild()\n\n# Pagination models\nclass PaginationResponse(BaseModel):\n    page: int\n    size: int\n    total: int\n    total_pages: int\n    has_next: bool\n    has_prev: bool\n\nclass PostListResponse(BaseModel):\n    posts: List[PostResponse]\n    pagination: PaginationResponse\n\n# Search models\nclass SearchQuery(BaseModel):\n    q: str = Field(..., min_length=1, max_length=100)\n    category: Optional[str] = None\n    author: Optional[str] = None\n    status: Optional[PostStatus] = None\n    date_from: Optional[datetime] = None\n    date_to: Optional[datetime] = None\n\nclass SearchResponse(BaseModel):\n    query: str\n    results: List[PostResponse]\n    total_results: int\n    search_time_ms: float\n\n# Analytics models\nclass PostAnalytics(BaseModel):\n    post_id: int\n    views_today: int\n    views_week: int\n    views_month: int\n    likes_count: int\n    comments_count: int\n    shares_count: int\n\nclass UserAnalytics(BaseModel):\n    user_id: int\n    posts_published: int\n    total_views: int\n    total_likes: int\n    avg_engagement: float\n    top_posts: List[PostResponse]\n```\n\n**Model Validation Examples:**\n\n```python\n# validation_examples.py\nfrom models import PostCreate, UserCreate\nfrom pydantic import ValidationError\nimport json\n\n# Test valid data\nvalid_post = {\n    \"title\": \"Getting Started with FastAPI\",\n    \"content\": \"FastAPI is a modern, fast web framework for building APIs with Python...\",\n    \"tags\": [\"python\", \"fastapi\", \"web\", \"api\"],\n    \"status\": \"published\",\n    \"priority\": \"high\"\n}\n\ntry:\n    post = PostCreate(**valid_post)\n    print(\"‚úÖ Valid post created:\", post.dict())\nexcept ValidationError as e:\n    print(\"‚ùå Validation error:\", e.json())\n\n# Test invalid data\ninvalid_post = {\n    \"title\": \"Hi\",  # Too short\n    \"content\": \"Short\",  # Too short\n    \"tags\": [\"python\"] * 15,  # Too many tags\n    \"status\": \"invalid_status\",  # Invalid enum\n    \"priority\": \"urgent\"  # Invalid enum\n}\n\ntry:\n    post = PostCreate(**invalid_post)\n    print(\"‚úÖ Post created:\", post.dict())\nexcept ValidationError as e:\n    print(\"‚ùå Validation errors:\")\n    for error in e.errors():\n        print(f\"  - {error['loc']}: {error['msg']}\")\n```\n\n---\n\n### 3Ô∏è‚É£ Background Tasks and Workers (15 minutes)\n\n**Background Tasks in FastAPI:**\n\n```python\n# background_tasks.py\nfrom fastapi import FastAPI, BackgroundTasks\nimport asyncio\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport aiofiles\nimport json\nfrom datetime import datetime\nimport aiohttp\n\napp = FastAPI()\n\n# Simple background task\nasync def send_notification_email(user_email: str, subject: str, message: str):\n    \"\"\"Send notification email (simulated)\"\"\"\n    print(f\"üìß Sending email to {user_email}\")\n    print(f\"Subject: {subject}\")\n    print(f\"Message: {message}\")\n    \n    # Simulate email sending delay\n    await asyncio.sleep(2)\n    \n    # Log the email\n    log_data = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"to\": user_email,\n        \"subject\": subject,\n        \"status\": \"sent\"\n    }\n    \n    async with aiofiles.open(\"email_log.json\", \"a\") as f:\n        await f.write(json.dumps(log_data) + \"\\n\")\n    \n    print(f\"‚úÖ Email sent to {user_email}\")\n\n# Background task for analytics\nasync def update_post_analytics(post_id: int, action: str):\n    \"\"\"Update post analytics in background\"\"\"\n    print(f\"üìä Updating analytics for post {post_id}: {action}\")\n    \n    # Simulate analytics update\n    await asyncio.sleep(1)\n    \n    analytics_data = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"post_id\": post_id,\n        \"action\": action,\n        \"ip_address\": \"127.0.0.1\",  # In real app, get from request\n        \"user_agent\": \"FastAPI Client\"\n    }\n    \n    async with aiofiles.open(\"analytics.json\", \"a\") as f:\n        await f.write(json.dumps(analytics_data) + \"\\n\")\n    \n    print(f\"‚úÖ Analytics updated for post {post_id}\")\n\n# Background task for external API calls\nasync def sync_with_external_service(post_data: dict):\n    \"\"\"Sync post with external service\"\"\"\n    print(f\"üîÑ Syncing post with external service...\")\n    \n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                \"https://api.example.com/posts\",\n                json=post_data,\n                timeout=30\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    print(f\"‚úÖ Successfully synced post: {result}\")\n                else:\n                    print(f\"‚ùå Sync failed with status: {response.status}\")\n    except Exception as e:\n        print(f\"‚ùå Sync error: {str(e)}\")\n\n# Endpoints using background tasks\n@app.post(\"/posts/{post_id}/view\")\nasync def view_post(post_id: int, background_tasks: BackgroundTasks):\n    \"\"\"Record post view with background analytics\"\"\"\n    # Immediate response\n    response = {\"message\": \"Post viewed\", \"post_id\": post_id}\n    \n    # Background analytics update\n    background_tasks.add_task(update_post_analytics, post_id, \"view\")\n    \n    return response\n\n@app.post(\"/posts\")\nasync def create_post_with_notifications(\n    post: PostCreate,\n    background_tasks: BackgroundTasks,\n    current_user: dict = Depends(get_current_user)\n):\n    \"\"\"Create post with background notifications\"\"\"\n    # Create post (immediate)\n    # ... post creation logic ...\n    \n    post_data = {\n        \"id\": 123,  # Would be actual post ID\n        \"title\": post.title,\n        \"author\": current_user[\"username\"]\n    }\n    \n    # Add background tasks\n    if post.status == \"published\":\n        # Notify subscribers\n        background_tasks.add_task(\n            send_notification_email,\n            \"subscribers@blog.com\",\n            f\"New Post: {post.title}\",\n            f\"A new post '{post.title}' has been published!\"\n        )\n        \n        # Sync with external service\n        background_tasks.add_task(sync_with_external_service, post_data)\n        \n        # Update search index\n        background_tasks.add_task(update_search_index, post_data)\n    \n    return {\"message\": \"Post created\", \"id\": 123}\n\nasync def update_search_index(post_data: dict):\n    \"\"\"Update search index for the post\"\"\"\n    print(f\"üîç Updating search index for post: {post_data['title']}\")\n    await asyncio.sleep(3)  # Simulate indexing time\n    print(f\"‚úÖ Search index updated\")\n\n# Advanced: Periodic background tasks\nimport asyncio\nfrom contextlib import asynccontextmanager\n\nasync def cleanup_old_logs():\n    \"\"\"Periodic cleanup task\"\"\"\n    while True:\n        print(\"üßπ Running cleanup task...\")\n        # Cleanup old log files, temporary data, etc.\n        await asyncio.sleep(3600)  # Run every hour\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Start background tasks\n    cleanup_task = asyncio.create_task(cleanup_old_logs())\n    \n    yield\n    \n    # Cleanup\n    cleanup_task.cancel()\n    try:\n        await cleanup_task\n    except asyncio.CancelledError:\n        pass\n\napp = FastAPI(lifespan=lifespan)\n```\n\n**Advanced Worker with Celery (Optional):**\n\n```python\n# celery_worker.py (for production use)\nfrom celery import Celery\nimport smtplib\nfrom email.mime.text import MIMEText\nimport requests\nimport time\n\n# Configure Celery\ncelery_app = Celery(\n    \"blog_worker\",\n    broker=\"redis://localhost:6379/0\",\n    backend=\"redis://localhost:6379/0\"\n)\n\n@celery_app.task\ndef send_email_task(to_email: str, subject: str, body: str):\n    \"\"\"Send email via Celery worker\"\"\"\n    print(f\"üìß Celery task: Sending email to {to_email}\")\n    \n    # Email sending logic here\n    time.sleep(5)  # Simulate email sending\n    \n    return f\"Email sent to {to_email}\"\n\n@celery_app.task\ndef generate_report_task(user_id: int):\n    \"\"\"Generate user analytics report\"\"\"\n    print(f\"üìä Generating report for user {user_id}\")\n    \n    # Complex report generation\n    time.sleep(30)  # Simulate long processing\n    \n    return f\"Report generated for user {user_id}\"\n\n@celery_app.task(retry_kwargs={'max_retries': 3, 'countdown': 60})\ndef sync_external_api_task(data: dict):\n    \"\"\"Sync with external API with retry logic\"\"\"\n    try:\n        response = requests.post(\"https://api.example.com/sync\", json=data)\n        response.raise_for_status()\n        return \"Sync successful\"\n    except Exception as e:\n        print(f\"Sync failed: {e}\")\n        # This will automatically retry\n        sync_external_api_task.retry(exc=e)\n\n# FastAPI integration with Celery\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.post(\"/send-newsletter\")\nasync def send_newsletter(emails: List[str], subject: str, content: str):\n    \"\"\"Send newsletter to multiple emails\"\"\"\n    # Queue tasks for each email\n    task_ids = []\n    for email in emails:\n        task = send_email_task.delay(email, subject, content)\n        task_ids.append(task.id)\n    \n    return {\n        \"message\": f\"Newsletter queued for {len(emails)} recipients\",\n        \"task_ids\": task_ids\n    }\n\n@app.get(\"/task/{task_id}\")\nasync def get_task_status(task_id: str):\n    \"\"\"Check task status\"\"\"\n    task = celery_app.AsyncResult(task_id)\n    \n    return {\n        \"task_id\": task_id,\n        \"status\": task.status,\n        \"result\": task.result if task.ready() else None\n    }\n```\n\n---\n\n### 4Ô∏è‚É£ API Documentation Features (10 minutes)\n\n**Enhanced Documentation:**\n\n```python\n# documented_api.py\nfrom fastapi import FastAPI, Query, Path, Body, Header\nfrom fastapi.openapi.docs import get_swagger_ui_html\nfrom fastapi.openapi.utils import get_openapi\nfrom typing import List, Optional\nimport json\n\napp = FastAPI(\n    title=\"Advanced Blog API\",\n    description=\"\"\"\n    ## Blog Management API\n    \n    This API provides comprehensive blog management features including:\n    \n    * **Posts** - Create, read, update, and delete blog posts\n    * **Users** - User management and authentication\n    * **Comments** - Nested comment system\n    * **Analytics** - Real-time analytics and reporting\n    \n    ### Authentication\n    \n    Most endpoints require authentication using JWT tokens in the Authorization header:\n    ```\n    Authorization: Bearer <your_jwt_token>\n    ```\n    \n    ### Rate Limiting\n    \n    All endpoints are rate limited to prevent abuse:\n    - **Authenticated users**: 1000 requests/hour\n    - **Anonymous users**: 100 requests/hour\n    \"\"\",\n    version=\"2.0.0\",\n    contact={\n        \"name\": \"Blog API Support\",\n        \"url\": \"https://blog-api.example.com/contact\",\n        \"email\": \"support@blog-api.com\"\n    },\n    license_info={\n        \"name\": \"MIT License\",\n        \"url\": \"https://opensource.org/licenses/MIT\"\n    },\n    openapi_tags=[\n        {\n            \"name\": \"posts\",\n            \"description\": \"Blog post operations\",\n        },\n        {\n            \"name\": \"users\", \n            \"description\": \"User management operations\",\n        },\n        {\n            \"name\": \"analytics\",\n            \"description\": \"Analytics and reporting endpoints\",\n        }\n    ]\n)\n\n@app.get(\n    \"/posts/{post_id}\",\n    response_model=PostResponse,\n    tags=[\"posts\"],\n    summary=\"Get a specific blog post\",\n    description=\"Retrieve a blog post by its ID. Returns detailed post information including author details and engagement metrics.\",\n    responses={\n        200: {\n            \"description\": \"Post retrieved successfully\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"id\": 1,\n                        \"title\": \"Getting Started with FastAPI\",\n                        \"content\": \"FastAPI is a modern web framework...\",\n                        \"author_name\": \"John Doe\",\n                        \"status\": \"published\",\n                        \"views_count\": 150,\n                        \"likes_count\": 25\n                    }\n                }\n            }\n        },\n        404: {\n            \"description\": \"Post not found\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\"detail\": \"Post not found\"}\n                }\n            }\n        }\n    }\n)\nasync def get_post(\n    post_id: int = Path(..., description=\"The unique identifier for the blog post\", gt=0),\n    include_analytics: bool = Query(False, description=\"Include detailed analytics data\"),\n    x_client_version: Optional[str] = Header(None, description=\"Client application version\")\n):\n    \"\"\"\n    Retrieve a specific blog post by ID.\n    \n    This endpoint returns detailed information about a blog post including:\n    - Post content and metadata\n    - Author information\n    - Engagement metrics (views, likes, comments)\n    - Optional analytics data\n    \n    **Example Usage:**\n    ```python\n    import requests\n    \n    response = requests.get(\n        \"http://localhost:8000/posts/1\",\n        params={\"include_analytics\": True},\n        headers={\"X-Client-Version\": \"1.2.0\"}\n    )\n    ```\n    \"\"\"\n    # Implementation here\n    pass\n\n@app.post(\n    \"/posts\",\n    response_model=PostResponse,\n    status_code=201,\n    tags=[\"posts\"],\n    summary=\"Create a new blog post\",\n    description=\"Create a new blog post with title, content, and optional metadata.\"\n)\nasync def create_post(\n    post: PostCreate = Body(\n        ...,\n        example={\n            \"title\": \"My Amazing Blog Post\",\n            \"content\": \"This is the content of my blog post with **markdown** support.\",\n            \"tags\": [\"python\", \"fastapi\", \"web\"],\n            \"status\": \"published\",\n            \"priority\": \"high\"\n        }\n    ),\n    x_request_id: Optional[str] = Header(None, description=\"Unique request identifier for tracing\")\n):\n    \"\"\"\n    Create a new blog post.\n    \n    **Request Body:**\n    - **title**: Post title (5-200 characters)\n    - **content**: Post content (minimum 10 characters) \n    - **tags**: List of tags (max 10)\n    - **status**: Post status (draft/published/archived)\n    - **priority**: Post priority (low/medium/high)\n    \n    **Background Processing:**\n    - Sends notification emails to subscribers\n    - Updates search index\n    - Syncs with external services\n    \"\"\"\n    # Implementation here\n    pass\n\n# Custom OpenAPI schema\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    \n    openapi_schema = get_openapi(\n        title=\"Advanced Blog API\",\n        version=\"2.0.0\",\n        description=\"Comprehensive blog management API with advanced features\",\n        routes=app.routes,\n    )\n    \n    # Add custom security scheme\n    openapi_schema[\"components\"][\"securitySchemes\"] = {\n        \"BearerAuth\": {\n            \"type\": \"http\",\n            \"scheme\": \"bearer\",\n            \"bearerFormat\": \"JWT\"\n        }\n    }\n    \n    # Add server information\n    openapi_schema[\"servers\"] = [\n        {\"url\": \"http://localhost:8000\", \"description\": \"Development server\"},\n        {\"url\": \"https://api.blog.com\", \"description\": \"Production server\"},\n    ]\n    \n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\napp.openapi = custom_openapi\n```\n\n---\n\n### 5Ô∏è‚É£ CRUD with Pagination & Filtering (5 minutes)\n\n**Advanced CRUD Implementation:**\n\n```python\n# advanced_crud.py\nfrom fastapi import FastAPI, Depends, Query, HTTPException\nfrom sqlalchemy import and_, or_, desc, asc\nfrom typing import Optional, List\nfrom enum import Enum\n\nclass SortOrder(str, Enum):\n    ASC = \"asc\"\n    DESC = \"desc\"\n\nclass PostSortBy(str, Enum):\n    CREATED_AT = \"created_at\"\n    UPDATED_AT = \"updated_at\" \n    TITLE = \"title\"\n    VIEWS = \"views_count\"\n    LIKES = \"likes_count\"\n\n@app.get(\"/posts\", response_model=PostListResponse)\nasync def get_posts_advanced(\n    # Pagination\n    page: int = Query(1, ge=1, description=\"Page number\"),\n    size: int = Query(10, ge=1, le=100, description=\"Items per page\"),\n    \n    # Sorting\n    sort_by: PostSortBy = Query(PostSortBy.CREATED_AT, description=\"Sort field\"),\n    sort_order: SortOrder = Query(SortOrder.DESC, description=\"Sort order\"),\n    \n    # Filtering\n    search: Optional[str] = Query(None, description=\"Search in title and content\"),\n    author: Optional[str] = Query(None, description=\"Filter by author username\"),\n    tags: Optional[List[str]] = Query(None, description=\"Filter by tags\"),\n    status: Optional[PostStatus] = Query(None, description=\"Filter by status\"),\n    \n    # Date filtering\n    created_after: Optional[datetime] = Query(None, description=\"Posts created after date\"),\n    created_before: Optional[datetime] = Query(None, description=\"Posts created before date\"),\n    \n    # Dependencies\n    db: Session = Depends(get_database)\n):\n    \"\"\"\n    Get posts with advanced filtering, sorting, and pagination.\n    \n    Supports multiple filter combinations and full-text search.\n    \"\"\"\n    \n    # Build query\n    query = db.query(Post).join(User)\n    \n    # Apply filters\n    if search:\n        query = query.filter(\n            or_(\n                Post.title.ilike(f\"%{search}%\"),\n                Post.content.ilike(f\"%{search}%\")\n            )\n        )\n    \n    if author:\n        query = query.filter(User.username == author)\n    \n    if tags:\n        for tag in tags:\n            query = query.filter(Post.tags.contains([tag]))\n    \n    if status:\n        query = query.filter(Post.status == status)\n    \n    if created_after:\n        query = query.filter(Post.created_at >= created_after)\n    \n    if created_before:\n        query = query.filter(Post.created_at <= created_before)\n    \n    # Apply sorting\n    sort_field = getattr(Post, sort_by.value)\n    if sort_order == SortOrder.DESC:\n        query = query.order_by(desc(sort_field))\n    else:\n        query = query.order_by(asc(sort_field))\n    \n    # Get total count\n    total = query.count()\n    \n    # Apply pagination\n    offset = (page - 1) * size\n    posts = query.offset(offset).limit(size).all()\n    \n    # Calculate pagination metadata\n    total_pages = (total + size - 1) // size\n    has_next = page < total_pages\n    has_prev = page > 1\n    \n    return PostListResponse(\n        posts=[PostResponse.from_orm(post) for post in posts],\n        pagination=PaginationResponse(\n            page=page,\n            size=size,\n            total=total,\n            total_pages=total_pages,\n            has_next=has_next,\n            has_prev=has_prev\n        )\n    )\n```\n\n---\n\n### üè† Homework: Add Pagination & Filtering\n\n**Task:** Enhance your blog API with advanced features\n\n```python\n# Create these enhancements:\n# 1. Add search functionality across title and content\n# 2. Implement tag-based filtering\n# 3. Add date range filtering\n# 4. Create sorting by multiple fields\n# 5. Add response caching for expensive queries\n# 6. Implement user-specific post filtering\n# 7. Add analytics endpoints with background processing\n\n# Example endpoints to create:\n# GET /posts/search?q=fastapi&tags=python,web&sort=views&order=desc\n# GET /posts/analytics/{post_id}\n# POST /posts/{post_id}/like (with background analytics update)\n# GET /users/{user_id}/posts?status=published&limit=5\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Dependency Injection = Flexible, testable code architecture\n‚úÖ Pydantic Models = Robust data validation and documentation\n‚úÖ Background Tasks = Non-blocking operations for better UX  \n‚úÖ Auto Documentation = Interactive API docs with examples\n‚úÖ Advanced CRUD = Filtering, sorting, pagination out of the box\n\n---\n\n<a name=\"hour-33\"></a>\n## üìÖ Hour 33: Caching & Performance\n\n### üéØ Learning Objectives\n- Understand different caching strategies and when to use them\n- Implement Redis caching for expensive operations\n- Optimize database queries and identify bottlenecks\n- Use profiling tools to measure performance\n- Build a high-performance caching layer\n\n### üìñ What to Teach\n\n**\"Today we make our APIs lightning fast with caching and performance optimization!\"**\n\n---\n\n### 1Ô∏è‚É£ Caching Strategies (10 minutes)\n\n**Types of Caching:**\n\n```python\n# 1. IN-MEMORY CACHING (Simple, fast, limited)\nfrom functools import lru_cache\nimport time\nfrom typing import Dict, Any\n\n# Simple in-memory cache with LRU\n@lru_cache(maxsize=128)\ndef get_expensive_calculation(number: int) -> int:\n    \"\"\"Expensive calculation with caching\"\"\"\n    print(f\"üî¢ Computing factorial of {number}...\")\n    time.sleep(2)  # Simulate expensive operation\n    \n    result = 1\n    for i in range(1, number + 1):\n        result *= i\n    return result\n\n# Manual in-memory cache\nclass InMemoryCache:\n    def __init__(self, ttl: int = 300):  # 5 minutes TTL\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.ttl = ttl\n    \n    def get(self, key: str) -> Any:\n        if key in self.cache:\n            data = self.cache[key]\n            if time.time() - data['timestamp'] < self.ttl:\n                print(f\"üíæ Cache HIT: {key}\")\n                return data['value']\n            else:\n                print(f\"‚è∞ Cache EXPIRED: {key}\")\n                del self.cache[key]\n        \n        print(f\"‚ùå Cache MISS: {key}\")\n        return None\n    \n    def set(self, key: str, value: Any) -> None:\n        self.cache[key] = {\n            'value': value,\n            'timestamp': time.time()\n        }\n        print(f\"üíæ Cache SET: {key}\")\n    \n    def delete(self, key: str) -> None:\n        if key in self.cache:\n            del self.cache[key]\n            print(f\"üóëÔ∏è Cache DELETE: {key}\")\n\n# Usage example\ncache = InMemoryCache(ttl=60)  # 1 minute TTL\n\ndef get_user_profile(user_id: int) -> Dict[str, Any]:\n    cache_key = f\"user_profile:{user_id}\"\n    \n    # Try cache first\n    cached_result = cache.get(cache_key)\n    if cached_result:\n        return cached_result\n    \n    # Simulate database query\n    print(f\"üóÑÔ∏è Querying database for user {user_id}\")\n    time.sleep(1)  # Simulate DB delay\n    \n    user_data = {\n        \"id\": user_id,\n        \"name\": f\"User {user_id}\",\n        \"email\": f\"user{user_id}@example.com\",\n        \"posts_count\": user_id * 10\n    }\n    \n    # Cache the result\n    cache.set(cache_key, user_data)\n    return user_data\n```\n\n**Caching Patterns:**\n\n```python\n# 2. CACHE-ASIDE PATTERN (Most common)\nasync def get_post_with_cache_aside(post_id: int, db: Session, cache: RedisCache):\n    \"\"\"Cache-aside pattern implementation\"\"\"\n    cache_key = f\"post:{post_id}\"\n    \n    # 1. Check cache first\n    cached_post = await cache.get(cache_key)\n    if cached_post:\n        return json.loads(cached_post)\n    \n    # 2. Cache miss - query database\n    post = db.query(Post).filter(Post.id == post_id).first()\n    if not post:\n        raise HTTPException(status_code=404, detail=\"Post not found\")\n    \n    # 3. Store in cache for next time\n    post_data = {\n        \"id\": post.id,\n        \"title\": post.title,\n        \"content\": post.content,\n        \"author\": post.author.username\n    }\n    \n    await cache.set(cache_key, json.dumps(post_data), ttl=300)\n    return post_data\n\n# 3. WRITE-THROUGH PATTERN\nasync def update_post_write_through(post_id: int, update_data: dict, db: Session, cache: RedisCache):\n    \"\"\"Write-through pattern - update DB and cache simultaneously\"\"\"\n    # 1. Update database\n    post = db.query(Post).filter(Post.id == post_id).first()\n    for key, value in update_data.items():\n        setattr(post, key, value)\n    db.commit()\n    \n    # 2. Update cache immediately\n    cache_key = f\"post:{post_id}\"\n    post_data = {\n        \"id\": post.id,\n        \"title\": post.title,\n        \"content\": post.content,\n        \"author\": post.author.username\n    }\n    \n    await cache.set(cache_key, json.dumps(post_data), ttl=300)\n    return post_data\n\n# 4. WRITE-BEHIND PATTERN (Advanced)\nwrite_queue = []\n\nasync def update_post_write_behind(post_id: int, update_data: dict, cache: RedisCache):\n    \"\"\"Write-behind pattern - update cache immediately, DB later\"\"\"\n    # 1. Update cache immediately\n    cache_key = f\"post:{post_id}\"\n    cached_post = await cache.get(cache_key)\n    \n    if cached_post:\n        post_data = json.loads(cached_post)\n        post_data.update(update_data)\n        await cache.set(cache_key, json.dumps(post_data), ttl=300)\n    \n    # 2. Queue database update for later\n    write_queue.append({\n        \"post_id\": post_id,\n        \"update_data\": update_data,\n        \"timestamp\": time.time()\n    })\n    \n    return {\"message\": \"Update queued\"}\n```\n\n---\n\n### 2Ô∏è‚É£ Redis Integration (15 minutes)\n\n**Redis Setup and Configuration:**\n\n```python\n# redis_client.py\nimport redis.asyncio as redis\nimport json\nimport pickle\nfrom typing import Any, Optional, Union\nfrom datetime import timedelta\nimport asyncio\n\nclass RedisCache:\n    def __init__(self, url: str = \"redis://localhost:6379/0\"):\n        self.redis = redis.from_url(url, decode_responses=True)\n    \n    async def get(self, key: str) -> Optional[str]:\n        \"\"\"Get value from cache\"\"\"\n        try:\n            value = await self.redis.get(key)\n            if value:\n                print(f\"üíæ Redis HIT: {key}\")\n                return value\n            print(f\"‚ùå Redis MISS: {key}\")\n            return None\n        except Exception as e:\n            print(f\"üö´ Redis GET error: {e}\")\n            return None\n    \n    async def set(self, key: str, value: str, ttl: int = 300) -> bool:\n        \"\"\"Set value in cache with TTL\"\"\"\n        try:\n            await self.redis.setex(key, ttl, value)\n            print(f\"üíæ Redis SET: {key} (TTL: {ttl}s)\")\n            return True\n        except Exception as e:\n            print(f\"üö´ Redis SET error: {e}\")\n            return False\n    \n    async def delete(self, key: str) -> bool:\n        \"\"\"Delete key from cache\"\"\"\n        try:\n            result = await self.redis.delete(key)\n            print(f\"üóëÔ∏è Redis DELETE: {key}\")\n            return bool(result)\n        except Exception as e:\n            print(f\"üö´ Redis DELETE error: {e}\")\n            return False\n    \n    async def get_json(self, key: str) -> Optional[dict]:\n        \"\"\"Get and deserialize JSON value\"\"\"\n        value = await self.get(key)\n        if value:\n            try:\n                return json.loads(value)\n            except json.JSONDecodeError:\n                print(f\"üö´ Invalid JSON in cache: {key}\")\n                await self.delete(key)\n        return None\n    \n    async def set_json(self, key: str, value: dict, ttl: int = 300) -> bool:\n        \"\"\"Serialize and set JSON value\"\"\"\n        try:\n            json_str = json.dumps(value)\n            return await self.set(key, json_str, ttl)\n        except (TypeError, ValueError) as e:\n            print(f\"üö´ JSON serialization error: {e}\")\n            return False\n    \n    async def increment(self, key: str, amount: int = 1) -> int:\n        \"\"\"Increment a counter\"\"\"\n        try:\n            result = await self.redis.incrby(key, amount)\n            print(f\"‚ûï Redis INCREMENT: {key} by {amount} = {result}\")\n            return result\n        except Exception as e:\n            print(f\"üö´ Redis INCREMENT error: {e}\")\n            return 0\n    \n    async def expire(self, key: str, ttl: int) -> bool:\n        \"\"\"Set expiration for existing key\"\"\"\n        try:\n            result = await self.redis.expire(key, ttl)\n            return bool(result)\n        except Exception as e:\n            print(f\"üö´ Redis EXPIRE error: {e}\")\n            return False\n    \n    async def exists(self, key: str) -> bool:\n        \"\"\"Check if key exists\"\"\"\n        try:\n            result = await self.redis.exists(key)\n            return bool(result)\n        except Exception as e:\n            print(f\"üö´ Redis EXISTS error: {e}\")\n            return False\n    \n    async def flush_pattern(self, pattern: str) -> int:\n        \"\"\"Delete all keys matching pattern\"\"\"\n        try:\n            keys = await self.redis.keys(pattern)\n            if keys:\n                result = await self.redis.delete(*keys)\n                print(f\"üóëÔ∏è Redis FLUSH: {len(keys)} keys matching '{pattern}'\")\n                return result\n            return 0\n        except Exception as e:\n            print(f\"üö´ Redis FLUSH error: {e}\")\n            return 0\n\n# Initialize Redis cache\ncache = RedisCache()\n```\n\n**FastAPI Integration with Redis:**\n\n```python\n# cached_api.py\nfrom fastapi import FastAPI, Depends, HTTPException, Query\nfrom redis_client import RedisCache\nimport json\nimport hashlib\nfrom typing import List, Optional\nimport time\n\napp = FastAPI(title=\"Cached Blog API\")\n\n# Cache dependency\nasync def get_cache() -> RedisCache:\n    return RedisCache()\n\n# Cache decorator\ndef cache_response(ttl: int = 300):\n    def decorator(func):\n        async def wrapper(*args, **kwargs):\n            # Generate cache key from function name and arguments\n            cache_data = {\n                \"func\": func.__name__,\n                \"args\": args,\n                \"kwargs\": {k: v for k, v in kwargs.items() if k != 'cache'}\n            }\n            cache_key = f\"api:{hashlib.md5(json.dumps(cache_data, sort_keys=True).encode()).hexdigest()}\"\n            \n            # Try cache first\n            cache = await get_cache()\n            cached_result = await cache.get_json(cache_key)\n            if cached_result:\n                return cached_result\n            \n            # Execute function and cache result\n            result = await func(*args, **kwargs)\n            await cache.set_json(cache_key, result, ttl)\n            \n            return result\n        return wrapper\n    return decorator\n\n@app.get(\"/posts/{post_id}\")\n@cache_response(ttl=300)  # 5 minutes\nasync def get_post_cached(post_id: int, cache: RedisCache = Depends(get_cache)):\n    \"\"\"Get post with caching\"\"\"\n    # Simulate database query\n    await asyncio.sleep(1)  # Simulate slow DB\n    \n    post_data = {\n        \"id\": post_id,\n        \"title\": f\"Post {post_id}\",\n        \"content\": f\"This is the content of post {post_id}\",\n        \"views\": post_id * 100,\n        \"cached_at\": time.time()\n    }\n    \n    return post_data\n\n@app.get(\"/posts\")\nasync def get_posts_with_cache(\n    page: int = Query(1, ge=1),\n    size: int = Query(10, ge=1, le=100),\n    tag: Optional[str] = None,\n    cache: RedisCache = Depends(get_cache)\n):\n    \"\"\"Get posts with intelligent caching\"\"\"\n    \n    # Create cache key based on parameters\n    cache_params = {\"page\": page, \"size\": size, \"tag\": tag}\n    cache_key = f\"posts:list:{hashlib.md5(json.dumps(cache_params, sort_keys=True).encode()).hexdigest()}\"\n    \n    # Try cache first\n    cached_posts = await cache.get_json(cache_key)\n    if cached_posts:\n        cached_posts[\"from_cache\"] = True\n        return cached_posts\n    \n    # Simulate database query\n    print(f\"üóÑÔ∏è Querying database: page={page}, size={size}, tag={tag}\")\n    await asyncio.sleep(2)  # Simulate slow query\n    \n    # Generate fake data\n    start_id = (page - 1) * size + 1\n    posts = []\n    \n    for i in range(start_id, start_id + size):\n        posts.append({\n            \"id\": i,\n            \"title\": f\"Post {i}\",\n            \"content\": f\"Content for post {i}\",\n            \"tag\": tag if tag else f\"tag{i % 5}\",\n            \"views\": i * 50\n        })\n    \n    result = {\n        \"posts\": posts,\n        \"page\": page,\n        \"size\": size,\n        \"total\": 1000,  # Fake total\n        \"from_cache\": False\n    }\n    \n    # Cache for 2 minutes (shorter for list endpoints)\n    await cache.set_json(cache_key, result, ttl=120)\n    \n    return result\n\n@app.post(\"/posts/{post_id}/view\")\nasync def increment_post_views(post_id: int, cache: RedisCache = Depends(get_cache)):\n    \"\"\"Increment post views with Redis counter\"\"\"\n    \n    # Increment view counter\n    view_key = f\"post:views:{post_id}\"\n    new_count = await cache.increment(view_key, 1)\n    \n    # Set expiration if this is a new key\n    if new_count == 1:\n        await cache.expire(view_key, 86400)  # Expire in 24 hours\n    \n    # Invalidate cached post data\n    await cache.flush_pattern(f\"api:*post_id*{post_id}*\")\n    \n    return {\n        \"post_id\": post_id,\n        \"views\": new_count,\n        \"message\": \"View count updated\"\n    }\n\n@app.delete(\"/cache/{pattern}\")\nasync def clear_cache(pattern: str, cache: RedisCache = Depends(get_cache)):\n    \"\"\"Clear cache by pattern (admin endpoint)\"\"\"\n    deleted_count = await cache.flush_pattern(pattern)\n    \n    return {\n        \"message\": f\"Cleared {deleted_count} cache entries\",\n        \"pattern\": pattern\n    }\n\n# Advanced caching with cache warming\n@app.post(\"/cache/warm\")\nasync def warm_cache(cache: RedisCache = Depends(get_cache)):\n    \"\"\"Warm up cache with popular content\"\"\"\n    print(\"üî• Warming up cache...\")\n    \n    # Pre-load popular posts\n    popular_post_ids = [1, 2, 3, 4, 5]\n    \n    for post_id in popular_post_ids:\n        # Simulate loading post data\n        post_data = {\n            \"id\": post_id,\n            \"title\": f\"Popular Post {post_id}\",\n            \"content\": f\"This is a popular post {post_id}\",\n            \"views\": post_id * 1000\n        }\n        \n        cache_key = f\"post:{post_id}\"\n        await cache.set_json(cache_key, post_data, ttl=600)  # 10 minutes\n    \n    # Pre-load first page of posts\n    first_page_data = {\n        \"posts\": [{\"id\": i, \"title\": f\"Post {i}\"} for i in range(1, 11)],\n        \"page\": 1,\n        \"size\": 10,\n        \"total\": 1000\n    }\n    \n    await cache.set_json(\"posts:list:page_1\", first_page_data, ttl=300)\n    \n    return {\"message\": f\"Cache warmed with {len(popular_post_ids) + 1} items\"}\n```\n\n---\n\n### 3Ô∏è‚É£ Database Query Optimization (15 minutes)\n\n**Query Performance Analysis:**\n\n```python\n# query_optimization.py\nimport time\nimport asyncio\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.orm import sessionmaker\nimport logging\n\n# Enable SQL logging\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\nclass QueryProfiler:\n    def __init__(self):\n        self.queries = []\n    \n    def profile_query(self, description: str = \"\"):\n        def decorator(func):\n            async def wrapper(*args, **kwargs):\n                start_time = time.time()\n                result = await func(*args, **kwargs)\n                end_time = time.time()\n                \n                execution_time = end_time - start_time\n                self.queries.append({\n                    \"description\": description or func.__name__,\n                    \"execution_time\": execution_time,\n                    \"timestamp\": start_time\n                })\n                \n                if execution_time > 1.0:  # Slow query threshold\n                    print(f\"üêå SLOW QUERY: {description} took {execution_time:.3f}s\")\n                else:\n                    print(f\"‚ö° Query: {description} took {execution_time:.3f}s\")\n                \n                return result\n            return wrapper\n        return decorator\n    \n    def get_report(self):\n        if not self.queries:\n            return \"No queries recorded\"\n        \n        total_time = sum(q[\"execution_time\"] for q in self.queries)\n        avg_time = total_time / len(self.queries)\n        slowest = max(self.queries, key=lambda q: q[\"execution_time\"])\n        \n        report = f\"\"\"\nüìä QUERY PERFORMANCE REPORT\n================================\nTotal Queries: {len(self.queries)}\nTotal Time: {total_time:.3f}s\nAverage Time: {avg_time:.3f}s\nSlowest Query: {slowest['description']} ({slowest['execution_time']:.3f}s)\n\nQuery Details:\n\"\"\"\n        for i, query in enumerate(self.queries, 1):\n            report += f\"{i}. {query['description']}: {query['execution_time']:.3f}s\\n\"\n        \n        return report\n\nprofiler = QueryProfiler()\n\n# BAD: N+1 Query Problem\n@profiler.profile_query(\"Bad: N+1 queries for posts with authors\")\nasync def get_posts_bad_way(db: Session, limit: int = 10):\n    \"\"\"Demonstrates N+1 query problem\"\"\"\n    \n    # 1 query to get posts\n    posts = db.query(Post).limit(limit).all()\n    \n    result = []\n    for post in posts:\n        # N queries to get authors (one for each post)\n        author = db.query(User).filter(User.id == post.author_id).first()\n        result.append({\n            \"id\": post.id,\n            \"title\": post.title,\n            \"author_name\": author.username if author else \"Unknown\"\n        })\n    \n    return result\n\n# GOOD: Eager Loading with JOIN\n@profiler.profile_query(\"Good: Single query with JOIN\")\nasync def get_posts_good_way(db: Session, limit: int = 10):\n    \"\"\"Optimized with eager loading\"\"\"\n    \n    # Single query with JOIN\n    posts = (\n        db.query(Post, User.username)\n        .join(User, Post.author_id == User.id)\n        .limit(limit)\n        .all()\n    )\n    \n    return [\n        {\n            \"id\": post.id,\n            \"title\": post.title,\n            \"author_name\": username\n        }\n        for post, username in posts\n    ]\n\n# BETTER: Optimized query with specific fields\n@profiler.profile_query(\"Better: Optimized field selection\")\nasync def get_posts_optimized(db: Session, limit: int = 10):\n    \"\"\"Optimized with specific field selection\"\"\"\n    \n    # Select only needed fields\n    result = (\n        db.query(\n            Post.id,\n            Post.title,\n            Post.created_at,\n            User.username.label('author_name')\n        )\n        .join(User, Post.author_id == User.id)\n        .filter(Post.status == 'published')  # Add index on status\n        .order_by(Post.created_at.desc())    # Add index on created_at\n        .limit(limit)\n        .all()\n    )\n    \n    return [\n        {\n            \"id\": row.id,\n            \"title\": row.title,\n            \"author_name\": row.author_name,\n            \"created_at\": row.created_at.isoformat()\n        }\n        for row in result\n    ]\n\n# Database indexing examples\nasync def create_performance_indexes(db: Session):\n    \"\"\"Create indexes for better performance\"\"\"\n    \n    indexes = [\n        # Index on frequently queried columns\n        \"CREATE INDEX IF NOT EXISTS idx_posts_status ON posts(status)\",\n        \"CREATE INDEX IF NOT EXISTS idx_posts_created_at ON posts(created_at)\",\n        \"CREATE INDEX IF NOT EXISTS idx_posts_author_id ON posts(author_id)\",\n        \n        # Composite indexes for common query combinations\n        \"CREATE INDEX IF NOT EXISTS idx_posts_status_created ON posts(status, created_at DESC)\",\n        \"CREATE INDEX IF NOT EXISTS idx_posts_author_status ON posts(author_id, status)\",\n        \n        # Index on search fields (if using text search)\n        \"CREATE INDEX IF NOT EXISTS idx_posts_title_gin ON posts USING gin(to_tsvector('english', title))\",\n        \n        # Index on foreign keys\n        \"CREATE INDEX IF NOT EXISTS idx_comments_post_id ON comments(post_id)\",\n        \"CREATE INDEX IF NOT EXISTS idx_comments_author_id ON comments(author_id)\"\n    ]\n    \n    for index_sql in indexes:\n        try:\n            await db.execute(text(index_sql))\n            print(f\"‚úÖ Created index: {index_sql}\")\n        except Exception as e:\n            print(f\"‚ùå Index creation failed: {e}\")\n    \n    await db.commit()\n\n# Query caching with database-level cache\nclass QueryCache:\n    def __init__(self, redis_cache: RedisCache):\n        self.cache = redis_cache\n    \n    async def cached_query(self, query_key: str, query_func, ttl: int = 300):\n        \"\"\"Execute query with caching\"\"\"\n        \n        # Try cache first\n        cached_result = await self.cache.get_json(f\"query:{query_key}\")\n        if cached_result:\n            return cached_result\n        \n        # Execute query\n        start_time = time.time()\n        result = await query_func()\n        execution_time = time.time() - start_time\n        \n        # Cache the result\n        cache_data = {\n            \"data\": result,\n            \"execution_time\": execution_time,\n            \"cached_at\": time.time()\n        }\n        \n        await self.cache.set_json(f\"query:{query_key}\", cache_data, ttl)\n        \n        print(f\"üíæ Cached query '{query_key}' (execution: {execution_time:.3f}s)\")\n        return cache_data\n\n# Usage in FastAPI endpoints\n@app.get(\"/posts/popular\")\nasync def get_popular_posts(\n    cache: RedisCache = Depends(get_cache),\n    db: Session = Depends(get_database)\n):\n    \"\"\"Get popular posts with query caching\"\"\"\n    \n    query_cache = QueryCache(cache)\n    \n    async def expensive_query():\n        # Complex query that takes time\n        result = await db.execute(text(\"\"\"\n            SELECT \n                p.id,\n                p.title,\n                p.content,\n                u.username as author_name,\n                COUNT(DISTINCT c.id) as comment_count,\n                COUNT(DISTINCT l.id) as like_count,\n                AVG(r.rating) as avg_rating\n            FROM posts p\n            JOIN users u ON p.author_id = u.id\n            LEFT JOIN comments c ON p.id = c.post_id\n            LEFT JOIN likes l ON p.id = l.post_id\n            LEFT JOIN ratings r ON p.id = r.post_id\n            WHERE p.status = 'published'\n              AND p.created_at > NOW() - INTERVAL '30 days'\n            GROUP BY p.id, p.title, p.content, u.username\n            HAVING COUNT(DISTINCT c.id) > 5 OR COUNT(DISTINCT l.id) > 10\n            ORDER BY (COUNT(DISTINCT c.id) + COUNT(DISTINCT l.id)) DESC\n            LIMIT 10\n        \"\"\"))\n        \n        return [dict(row) for row in result]\n    \n    # Use cached query\n    cache_result = await query_cache.cached_query(\n        \"popular_posts_30days\", \n        expensive_query, \n        ttl=1800  # 30 minutes\n    )\n    \n    return {\n        \"posts\": cache_result[\"data\"],\n        \"from_cache\": \"cached_at\" in cache_result,\n        \"execution_time\": cache_result.get(\"execution_time\", 0)\n    }\n```\n\n---\n\n### 4Ô∏è‚É£ Performance Profiling (10 minutes)\n\n**Application Profiling Tools:**\n\n```python\n# profiler.py\nimport cProfile\nimport pstats\nimport io\nfrom functools import wraps\nimport time\nimport psutil\nimport tracemalloc\nfrom fastapi import FastAPI, Request\nfrom contextual import middleware\nimport asyncio\n\nclass PerformanceProfiler:\n    def __init__(self):\n        self.request_times = []\n        self.memory_usage = []\n        \n    def profile_endpoint(self, func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Memory profiling\n            tracemalloc.start()\n            process = psutil.Process()\n            memory_before = process.memory_info().rss / 1024 / 1024  # MB\n            \n            # Time profiling\n            start_time = time.time()\n            \n            # Execute function\n            result = await func(*args, **kwargs)\n            \n            # Calculate metrics\n            end_time = time.time()\n            execution_time = end_time - start_time\n            \n            memory_after = process.memory_info().rss / 1024 / 1024  # MB\n            memory_diff = memory_after - memory_before\n            \n            # Memory trace\n            current, peak = tracemalloc.get_traced_memory()\n            tracemalloc.stop()\n            \n            # Log performance data\n            perf_data = {\n                \"function\": func.__name__,\n                \"execution_time\": execution_time,\n                \"memory_before\": memory_before,\n                \"memory_after\": memory_after,\n                \"memory_diff\": memory_diff,\n                \"memory_peak\": peak / 1024 / 1024,  # MB\n                \"timestamp\": start_time\n            }\n            \n            self.request_times.append(perf_data)\n            \n            # Print warnings for slow operations\n            if execution_time > 1.0:\n                print(f\"üêå SLOW OPERATION: {func.__name__} took {execution_time:.3f}s\")\n            \n            if memory_diff > 50:  # More than 50MB increase\n                print(f\"üß† HIGH MEMORY: {func.__name__} used {memory_diff:.1f}MB extra\")\n            \n            return result\n        \n        return wrapper\n    \n    def get_stats(self):\n        if not self.request_times:\n            return {\"message\": \"No performance data available\"}\n        \n        execution_times = [r[\"execution_time\"] for r in self.request_times]\n        memory_diffs = [r[\"memory_diff\"] for r in self.request_times]\n        \n        return {\n            \"total_requests\": len(self.request_times),\n            \"avg_execution_time\": sum(execution_times) / len(execution_times),\n            \"max_execution_time\": max(execution_times),\n            \"min_execution_time\": min(execution_times),\n            \"avg_memory_usage\": sum(memory_diffs) / len(memory_diffs),\n            \"max_memory_usage\": max(memory_diffs),\n            \"recent_requests\": self.request_times[-10:]  # Last 10 requests\n        }\n\n# Global profiler instance\nprofiler = PerformanceProfiler()\n\n# FastAPI middleware for automatic profiling\n@middleware(\"http\")\nasync def performance_middleware(request: Request, call_next):\n    start_time = time.time()\n    \n    # Memory tracking\n    process = psutil.Process()\n    memory_before = process.memory_info().rss / 1024 / 1024\n    \n    # Process request\n    response = await call_next(request)\n    \n    # Calculate metrics\n    end_time = time.time()\n    execution_time = end_time - start_time\n    memory_after = process.memory_info().rss / 1024 / 1024\n    \n    # Add performance headers\n    response.headers[\"X-Response-Time\"] = f\"{execution_time:.3f}s\"\n    response.headers[\"X-Memory-Usage\"] = f\"{memory_after - memory_before:.1f}MB\"\n    \n    # Log slow requests\n    if execution_time > 2.0:\n        print(f\"üêå SLOW REQUEST: {request.method} {request.url} took {execution_time:.3f}s\")\n    \n    return response\n\n# CPU Profiling decorator\ndef cpu_profile(func):\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        profiler = cProfile.Profile()\n        profiler.enable()\n        \n        result = await func(*args, **kwargs)\n        \n        profiler.disable()\n        \n        # Generate profile report\n        s = io.StringIO()\n        ps = pstats.Stats(profiler, stream=s)\n        ps.sort_stats('cumulative')\n        ps.print_stats(10)  # Top 10 functions\n        \n        print(f\"\\nüìä CPU PROFILE for {func.__name__}:\")\n        print(s.getvalue())\n        \n        return result\n    \n    return wrapper\n\n# Memory leak detection\nclass MemoryTracker:\n    def __init__(self):\n        self.snapshots = []\n    \n    async def take_snapshot(self, description: str):\n        \"\"\"Take memory snapshot\"\"\"\n        if not tracemalloc.is_tracing():\n            tracemalloc.start()\n        \n        snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append({\n            \"description\": description,\n            \"snapshot\": snapshot,\n            \"timestamp\": time.time()\n        })\n    \n    def compare_snapshots(self, index1: int = -2, index2: int = -1):\n        \"\"\"Compare two snapshots to detect memory leaks\"\"\"\n        if len(self.snapshots) < 2:\n            return \"Need at least 2 snapshots to compare\"\n        \n        snap1 = self.snapshots[index1][\"snapshot\"]\n        snap2 = self.snapshots[index2][\"snapshot\"]\n        \n        top_stats = snap2.compare_to(snap1, 'lineno')\n        \n        print(f\"\\nüß† MEMORY COMPARISON:\")\n        print(f\"From: {self.snapshots[index1]['description']}\")\n        print(f\"To: {self.snapshots[index2]['description']}\")\n        print(\"\\nTop 10 memory differences:\")\n        \n        for stat in top_stats[:10]:\n            print(stat)\n        \n        return top_stats\n\n# Usage examples\nmemory_tracker = MemoryTracker()\n\n@app.get(\"/profile/start\")\nasync def start_profiling():\n    \"\"\"Start memory profiling\"\"\"\n    await memory_tracker.take_snapshot(\"Profiling started\")\n    return {\"message\": \"Memory profiling started\"}\n\n@app.get(\"/profile/snapshot/{description}\")\nasync def take_snapshot(description: str):\n    \"\"\"Take memory snapshot\"\"\"\n    await memory_tracker.take_snapshot(description)\n    return {\"message\": f\"Snapshot taken: {description}\"}\n\n@app.get(\"/profile/compare\")\nasync def compare_memory():\n    \"\"\"Compare memory snapshots\"\"\"\n    stats = memory_tracker.compare_snapshots()\n    return {\"message\": \"Memory comparison complete - check logs\"}\n\n@app.get(\"/profile/stats\")\nasync def get_performance_stats():\n    \"\"\"Get performance statistics\"\"\"\n    return profiler.get_stats()\n\n# Load testing endpoint\n@app.get(\"/test/load/{num_requests}\")\nasync def simulate_load(num_requests: int):\n    \"\"\"Simulate load for testing\"\"\"\n    \n    @profiler.profile_endpoint\n    async def simulate_work():\n        # Simulate CPU work\n        result = sum(i * i for i in range(1000))\n        \n        # Simulate I/O work\n        await asyncio.sleep(0.1)\n        \n        # Simulate memory allocation\n        data = [i for i in range(1000)]\n        \n        return result\n    \n    start_time = time.time()\n    \n    # Run concurrent requests\n    tasks = [simulate_work() for _ in range(num_requests)]\n    results = await asyncio.gather(*tasks)\n    \n    end_time = time.time()\n    \n    return {\n        \"num_requests\": num_requests,\n        \"total_time\": end_time - start_time,\n        \"requests_per_second\": num_requests / (end_time - start_time),\n        \"avg_response_time\": (end_time - start_time) / num_requests\n    }\n```\n\n---\n\n### 5Ô∏è‚É£ Production Caching Setup (5 minutes)\n\n**Production Configuration:**\n\n```python\n# production_cache.py\nimport os\nfrom typing import Optional\nimport redis.asyncio as redis\nfrom fastapi import FastAPI\nimport json\nimport hashlib\n\nclass ProductionCache:\n    def __init__(self):\n        # Environment-based configuration\n        redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n        self.redis = redis.from_url(redis_url, \n                                   decode_responses=True,\n                                   max_connections=20,\n                                   retry_on_timeout=True)\n        \n        # Cache configuration\n        self.default_ttl = int(os.getenv(\"CACHE_DEFAULT_TTL\", \"300\"))\n        self.enabled = os.getenv(\"CACHE_ENABLED\", \"true\").lower() == \"true\"\n        \n    async def health_check(self) -> bool:\n        \"\"\"Check cache health\"\"\"\n        try:\n            await self.redis.ping()\n            return True\n        except Exception:\n            return False\n    \n    async def get_or_set(self, key: str, fetch_func, ttl: Optional[int] = None):\n        \"\"\"Get from cache or fetch and set\"\"\"\n        if not self.enabled:\n            return await fetch_func()\n        \n        # Try cache first\n        cached = await self.get_json(key)\n        if cached:\n            return cached\n        \n        # Fetch and cache\n        data = await fetch_func()\n        await self.set_json(key, data, ttl or self.default_ttl)\n        return data\n\n# Environment-specific cache settings\nCACHE_SETTINGS = {\n    \"development\": {\n        \"redis_url\": \"redis://localhost:6379/0\",\n        \"default_ttl\": 60,  # Short TTL for development\n        \"enabled\": True\n    },\n    \"production\": {\n        \"redis_url\": os.getenv(\"REDIS_URL\"),\n        \"default_ttl\": 300,\n        \"enabled\": True\n    },\n    \"testing\": {\n        \"redis_url\": \"redis://localhost:6379/15\",  # Different DB for tests\n        \"default_ttl\": 10,\n        \"enabled\": False  # Disable caching in tests\n    }\n}\n\n# Cache warming for production\nasync def warm_production_cache():\n    \"\"\"Warm up cache with essential data\"\"\"\n    cache = ProductionCache()\n    \n    warming_tasks = [\n        # Popular posts\n        warm_popular_posts(cache),\n        # User profiles  \n        warm_user_profiles(cache),\n        # Category data\n        warm_categories(cache),\n        # Static content\n        warm_static_content(cache)\n    ]\n    \n    await asyncio.gather(*warming_tasks)\n    print(\"üî• Production cache warming complete!\")\n\nasync def warm_popular_posts(cache: ProductionCache):\n    \"\"\"Warm popular posts cache\"\"\"\n    # Implementation here\n    pass\n```\n\n---\n\n### üè† Homework: Profile and Optimize\n\n**Task:** Add Redis caching to expensive endpoint and optimize performance\n\n```python\n# Performance optimization challenge:\n\n# 1. Add Redis caching to your blog API\n# 2. Implement cache invalidation strategy\n# 3. Profile a slow endpoint and optimize it\n# 4. Add performance monitoring middleware\n# 5. Create cache warming script\n# 6. Implement cache-aside pattern\n# 7. Add memory leak detection\n\n# Deliverables:\n# - Before/after performance benchmarks\n# - Cache hit rate analysis  \n# - Query optimization report\n# - Memory usage analysis\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Caching = Dramatic performance improvement for I/O operations\n‚úÖ Redis = Distributed caching for production applications\n‚úÖ Query optimization = Use indexes, avoid N+1 problems\n‚úÖ Profiling = Measure first, optimize second\n‚úÖ Cache strategies = Choose the right pattern for your use case\n\n---\n\n<a name=\"hour-34\"></a>\n## üìÖ Hour 34: Message Queues & Background Processing\n\n### üéØ Learning Objectives\n- Understand message queues and when to use them\n- Implement background job processing with Celery\n- Handle task retries, failures, and monitoring\n- Build scalable asynchronous processing systems\n- Create email notifications and file processing tasks\n\n### üìñ What to Teach\n\n**\"Today we learn to handle heavy work in the background while keeping our API responses fast!\"**\n\n---\n\n### 1Ô∏è‚É£ Message Queue Concepts (10 minutes)\n\n**Why Message Queues?**\n\n```python\n# WITHOUT MESSAGE QUEUES (Bad for user experience)\n@app.post(\"/send-newsletter\")\nasync def send_newsletter_bad(emails: List[str], content: str):\n    \"\"\"Bad: Blocks the request until all emails are sent\"\"\"\n    \n    start_time = time.time()\n    \n    # This blocks the HTTP request for minutes!\n    for email in emails:  # Could be 10,000 emails!\n        await send_email(email, content)  # 2 seconds each = 20,000 seconds!\n        \n    end_time = time.time()\n    \n    return {\n        \"message\": f\"Newsletter sent to {len(emails)} recipients\",\n        \"time_taken\": f\"{end_time - start_time:.1f} seconds\"\n    }\n\n# WITH MESSAGE QUEUES (Good user experience)\n@app.post(\"/send-newsletter\")\nasync def send_newsletter_good(emails: List[str], content: str):\n    \"\"\"Good: Returns immediately, work happens in background\"\"\"\n    \n    # Queue the work for background processing\n    task = send_newsletter_task.delay(emails, content)\n    \n    return {\n        \"message\": f\"Newsletter queued for {len(emails)} recipients\",\n        \"task_id\": task.id,\n        \"status\": \"processing\"\n    }\n```\n\n**Message Queue Components:**\n\n```\nProducer ‚Üí Queue ‚Üí Consumer(s)\n   ‚Üì         ‚Üì         ‚Üì\n FastAPI   Redis    Celery Worker(s)\n\nFlow:\n1. FastAPI receives request\n2. Creates job and adds to queue\n3. Returns immediately to user\n4. Worker picks up job from queue\n5. Worker processes job in background\n6. Results stored (database, cache, etc.)\n```\n\n**Real-World Use Cases:**\n- ‚úÖ **Email notifications** (newsletters, confirmations)\n- ‚úÖ **Image processing** (resizing, thumbnails, filters)\n- ‚úÖ **Report generation** (PDF creation, data exports)\n- ‚úÖ **Data synchronization** (third-party API sync)\n- ‚úÖ **Cleanup tasks** (log rotation, temporary file deletion)\n\n---\n\n### 2Ô∏è‚É£ Celery Setup and Configuration (15 minutes)\n\n**Install Celery and Redis:**\n\n```bash\npip install celery redis flower\n# flower is for monitoring Celery tasks\n```\n\n**Celery Configuration:**\n\n```python\n# celery_app.py\nfrom celery import Celery\nimport os\nfrom kombu import Queue\n\n# Celery configuration\ncelery_app = Celery(\n    \"blog_worker\",\n    broker=os.getenv(\"CELERY_BROKER_URL\", \"redis://localhost:6379/0\"),\n    backend=os.getenv(\"CELERY_RESULT_BACKEND\", \"redis://localhost:6379/0\"),\n    include=[\"tasks.email_tasks\", \"tasks.image_tasks\", \"tasks.report_tasks\"]\n)\n\n# Configuration settings\ncelery_app.conf.update(\n    # Task routing\n    task_routes={\n        'tasks.email_tasks.*': {'queue': 'email'},\n        'tasks.image_tasks.*': {'queue': 'images'},\n        'tasks.report_tasks.*': {'queue': 'reports'},\n    },\n    \n    # Queue definitions\n    task_queues=(\n        Queue('email', routing_key='email'),\n        Queue('images', routing_key='images'),\n        Queue('reports', routing_key='reports'),\n        Queue('default', routing_key='default'),\n    ),\n    \n    # Task settings\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    timezone='UTC',\n    enable_utc=True,\n    \n    # Result backend settings\n    result_expires=3600,  # 1 hour\n    \n    # Worker settings\n    worker_prefetch_multiplier=1,\n    task_acks_late=True,\n    \n    # Retry settings\n    task_default_retry_delay=60,  # 1 minute\n    task_max_retries=3,\n    \n    # Rate limiting\n    task_annotations={\n        'tasks.email_tasks.send_email': {'rate_limit': '10/m'},  # 10 emails per minute\n        'tasks.image_tasks.process_image': {'rate_limit': '5/m'},  # 5 images per minute\n    }\n)\n\n# Beat scheduler for periodic tasks\nfrom celery.schedules import crontab\n\ncelery_app.conf.beat_schedule = {\n    # Daily cleanup task\n    'cleanup-temp-files': {\n        'task': 'tasks.cleanup_tasks.cleanup_temp_files',\n        'schedule': crontab(hour=2, minute=0),  # 2 AM daily\n    },\n    \n    # Weekly report generation\n    'weekly-analytics-report': {\n        'task': 'tasks.report_tasks.generate_weekly_report',\n        'schedule': crontab(hour=9, minute=0, day_of_week=1),  # 9 AM Monday\n    },\n    \n    # Hourly health check\n    'health-check': {\n        'task': 'tasks.monitoring_tasks.health_check',\n        'schedule': crontab(minute=0),  # Every hour\n    },\n}\n```\n\n**Task Implementation:**\n\n```python\n# tasks/email_tasks.py\nfrom celery import current_task\nfrom celery.exceptions import Retry\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport time\nimport logging\nfrom typing import List\nimport requests\n\nlogger = logging.getLogger(__name__)\n\n@celery_app.task(bind=True, max_retries=3)\ndef send_email(self, to_email: str, subject: str, body: str, html_body: str = None):\n    \"\"\"Send a single email with retry logic\"\"\"\n    \n    try:\n        # Update task progress\n        self.update_state(state='PROGRESS', meta={'step': 'connecting'})\n        \n        # Email configuration\n        smtp_server = os.getenv(\"SMTP_SERVER\", \"smtp.gmail.com\")\n        smtp_port = int(os.getenv(\"SMTP_PORT\", \"587\"))\n        smtp_username = os.getenv(\"SMTP_USERNAME\")\n        smtp_password = os.getenv(\"SMTP_PASSWORD\")\n        \n        # Create message\n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = subject\n        msg['From'] = smtp_username\n        msg['To'] = to_email\n        \n        # Add text content\n        text_part = MIMEText(body, 'plain')\n        msg.attach(text_part)\n        \n        # Add HTML content if provided\n        if html_body:\n            html_part = MIMEText(html_body, 'html')\n            msg.attach(html_part)\n        \n        # Update progress\n        self.update_state(state='PROGRESS', meta={'step': 'sending'})\n        \n        # Send email\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(smtp_username, smtp_password)\n            server.send_message(msg)\n        \n        logger.info(f\"‚úÖ Email sent successfully to {to_email}\")\n        \n        return {\n            'status': 'success',\n            'to_email': to_email,\n            'subject': subject,\n            'sent_at': time.time()\n        }\n        \n    except smtplib.SMTPException as e:\n        logger.error(f\"‚ùå SMTP error sending email to {to_email}: {str(e)}\")\n        \n        # Retry with exponential backoff\n        countdown = 60 * (2 ** self.request.retries)  # 60s, 120s, 240s\n        \n        raise self.retry(countdown=countdown, exc=e)\n        \n    except Exception as e:\n        logger.error(f\"‚ùå Unexpected error sending email to {to_email}: {str(e)}\")\n        \n        # Don't retry on unexpected errors\n        return {\n            'status': 'failed',\n            'to_email': to_email,\n            'error': str(e),\n            'failed_at': time.time()\n        }\n\n@celery_app.task(bind=True)\ndef send_bulk_emails(self, email_list: List[dict], template: str):\n    \"\"\"Send emails to multiple recipients with progress tracking\"\"\"\n    \n    total_emails = len(email_list)\n    sent_count = 0\n    failed_count = 0\n    results = []\n    \n    for i, email_data in enumerate(email_list):\n        try:\n            # Update overall progress\n            progress = int((i / total_emails) * 100)\n            self.update_state(\n                state='PROGRESS',\n                meta={\n                    'current': i,\n                    'total': total_emails,\n                    'progress': progress,\n                    'sent': sent_count,\n                    'failed': failed_count\n                }\n            )\n            \n            # Send individual email\n            result = send_email.delay(\n                email_data['email'],\n                email_data['subject'],\n                template.format(**email_data.get('variables', {}))\n            )\n            \n            # Wait for result (or use result.get() for synchronous)\n            email_result = result.get(timeout=30)\n            \n            if email_result['status'] == 'success':\n                sent_count += 1\n            else:\n                failed_count += 1\n            \n            results.append(email_result)\n            \n        except Exception as e:\n            failed_count += 1\n            results.append({\n                'status': 'failed',\n                'to_email': email_data['email'],\n                'error': str(e)\n            })\n            \n            logger.error(f\"Error in bulk email {i}: {str(e)}\")\n    \n    return {\n        'total_emails': total_emails,\n        'sent_count': sent_count,\n        'failed_count': failed_count,\n        'results': results,\n        'completed_at': time.time()\n    }\n\n@celery_app.task\ndef send_newsletter(subscriber_emails: List[str], newsletter_content: str):\n    \"\"\"Send newsletter to all subscribers\"\"\"\n    \n    logger.info(f\"üìß Starting newsletter send to {len(subscriber_emails)} subscribers\")\n    \n    # Prepare email data\n    email_jobs = []\n    for email in subscriber_emails:\n        email_jobs.append({\n            'email': email,\n            'subject': 'Weekly Newsletter - Blog Updates',\n            'variables': {'subscriber_email': email}\n        })\n    \n    # Use bulk email task\n    result = send_bulk_emails.delay(email_jobs, newsletter_content)\n    \n    return {\n        'newsletter_task_id': result.id,\n        'subscriber_count': len(subscriber_emails),\n        'status': 'processing'\n    }\n```\n\n**Image Processing Tasks:**\n\n```python\n# tasks/image_tasks.py\nfrom PIL import Image\nimport os\nimport boto3\nfrom io import BytesIO\nimport requests\nimport hashlib\n\n@celery_app.task(bind=True, max_retries=3)\ndef process_uploaded_image(self, image_path: str, user_id: int):\n    \"\"\"Process uploaded image: resize, create thumbnails, upload to S3\"\"\"\n    \n    try:\n        self.update_state(state='PROGRESS', meta={'step': 'loading_image'})\n        \n        # Load image\n        with Image.open(image_path) as img:\n            original_size = img.size\n            \n            # Convert to RGB if necessary\n            if img.mode in ('RGBA', 'LA', 'P'):\n                img = img.convert('RGB')\n            \n            # Generate different sizes\n            sizes = {\n                'thumbnail': (150, 150),\n                'medium': (500, 500),\n                'large': (1200, 1200)\n            }\n            \n            processed_images = {}\n            \n            for size_name, dimensions in sizes.items():\n                self.update_state(\n                    state='PROGRESS', \n                    meta={'step': f'creating_{size_name}', 'progress': 25}\n                )\n                \n                # Resize image\n                resized_img = img.copy()\n                resized_img.thumbnail(dimensions, Image.Resampling.LANCZOS)\n                \n                # Save to buffer\n                buffer = BytesIO()\n                resized_img.save(buffer, format='JPEG', quality=85, optimize=True)\n                buffer.seek(0)\n                \n                # Generate filename\n                filename = f\"user_{user_id}_{size_name}_{int(time.time())}.jpg\"\n                \n                # Upload to S3 (or save locally)\n                s3_url = upload_to_s3(buffer, filename)\n                \n                processed_images[size_name] = {\n                    'url': s3_url,\n                    'size': resized_img.size,\n                    'filename': filename\n                }\n            \n            self.update_state(state='PROGRESS', meta={'step': 'cleanup', 'progress': 90})\n            \n            # Cleanup original file\n            os.remove(image_path)\n            \n            return {\n                'status': 'success',\n                'original_size': original_size,\n                'processed_images': processed_images,\n                'user_id': user_id,\n                'processed_at': time.time()\n            }\n            \n    except Exception as e:\n        logger.error(f\"‚ùå Image processing failed: {str(e)}\")\n        \n        # Retry with backoff\n        countdown = 30 * (2 ** self.request.retries)\n        raise self.retry(countdown=countdown, exc=e)\n\ndef upload_to_s3(file_buffer: BytesIO, filename: str) -> str:\n    \"\"\"Upload file to S3 and return URL\"\"\"\n    \n    # S3 configuration\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n        region_name=os.getenv('AWS_REGION', 'us-east-1')\n    )\n    \n    bucket_name = os.getenv('S3_BUCKET_NAME')\n    \n    try:\n        # Upload file\n        s3_client.upload_fileobj(\n            file_buffer,\n            bucket_name,\n            filename,\n            ExtraArgs={\n                'ContentType': 'image/jpeg',\n                'CacheControl': 'max-age=31536000',  # 1 year\n            }\n        )\n        \n        # Return public URL\n        s3_url = f\"https://{bucket_name}.s3.amazonaws.com/{filename}\"\n        return s3_url\n        \n    except Exception as e:\n        logger.error(f\"‚ùå S3 upload failed: {str(e)}\")\n        \n        # Fallback: save locally\n        local_path = f\"uploads/{filename}\"\n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        \n        with open(local_path, 'wb') as f:\n            file_buffer.seek(0)\n            f.write(file_buffer.read())\n        \n        return f\"/uploads/{filename}\"\n\n@celery_app.task\ndef generate_image_thumbnails_batch(image_urls: List[str]):\n    \"\"\"Generate thumbnails for multiple images\"\"\"\n    \n    results = []\n    \n    for url in image_urls:\n        try:\n            # Download image\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            \n            # Process image\n            task = process_uploaded_image.delay(url, 0)  # user_id=0 for batch\n            result = task.get(timeout=60)\n            \n            results.append({\n                'original_url': url,\n                'status': 'success',\n                'thumbnails': result['processed_images']\n            })\n            \n        except Exception as e:\n            results.append({\n                'original_url': url,\n                'status': 'failed',\n                'error': str(e)\n            })\n    \n    return results\n```\n\n---\n\n### 3Ô∏è‚É£ Task Monitoring and Retry Logic (15 minutes)\n\n**Advanced Task Management:**\n\n```python\n# tasks/monitoring_tasks.py\nimport psutil\nimport subprocess\nfrom datetime import datetime, timedelta\n\n@celery_app.task(bind=True)\ndef long_running_task(self, duration: int):\n    \"\"\"Example long-running task with progress updates\"\"\"\n    \n    total_steps = duration\n    \n    for i in range(total_steps):\n        # Simulate work\n        time.sleep(1)\n        \n        # Update progress\n        progress = int((i / total_steps) * 100)\n        self.update_state(\n            state='PROGRESS',\n            meta={\n                'current': i,\n                'total': total_steps,\n                'progress': progress,\n                'message': f'Processing step {i+1} of {total_steps}'\n            }\n        )\n    \n    return {\n        'status': 'completed',\n        'duration': duration,\n        'completed_at': time.time()\n    }\n\n@celery_app.task(bind=True, max_retries=5)\ndef unreliable_api_call(self, api_url: str, payload: dict):\n    \"\"\"Task that calls unreliable external API with smart retry logic\"\"\"\n    \n    try:\n        self.update_state(state='PROGRESS', meta={'step': 'making_request'})\n        \n        response = requests.post(\n            api_url, \n            json=payload, \n            timeout=30,\n            headers={'User-Agent': 'Blog-API/1.0'}\n        )\n        \n        # Different retry logic based on status code\n        if response.status_code == 429:  # Rate limited\n            # Longer retry for rate limiting\n            retry_after = int(response.headers.get('Retry-After', 60))\n            logger.warning(f\"‚è≥ Rate limited, retrying after {retry_after}s\")\n            raise self.retry(countdown=retry_after)\n        \n        elif response.status_code >= 500:  # Server error\n            # Exponential backoff for server errors\n            countdown = 30 * (2 ** self.request.retries)\n            logger.warning(f\"üîÑ Server error {response.status_code}, retrying in {countdown}s\")\n            raise self.retry(countdown=countdown)\n        \n        elif response.status_code == 400:  # Client error\n            # Don't retry client errors\n            logger.error(f\"‚ùå Client error {response.status_code}: {response.text}\")\n            return {\n                'status': 'failed',\n                'error': 'Client error - not retrying',\n                'response_code': response.status_code,\n                'response_text': response.text\n            }\n        \n        response.raise_for_status()\n        \n        return {\n            'status': 'success',\n            'response_data': response.json(),\n            'response_code': response.status_code,\n            'completed_at': time.time()\n        }\n        \n    except requests.RequestException as e:\n        logger.error(f\"‚ùå Request failed: {str(e)}\")\n        \n        # Final attempt reached\n        if self.request.retries >= self.max_retries:\n            return {\n                'status': 'failed',\n                'error': str(e),\n                'retries_exhausted': True,\n                'failed_at': time.time()\n            }\n        \n        # Retry with backoff\n        countdown = 15 * (2 ** self.request.retries)\n        raise self.retry(countdown=countdown, exc=e)\n\n@celery_app.task\ndef health_check():\n    \"\"\"System health check task\"\"\"\n    \n    health_data = {\n        'timestamp': datetime.now().isoformat(),\n        'system': {},\n        'services': {},\n        'celery': {}\n    }\n    \n    # System metrics\n    health_data['system'] = {\n        'cpu_percent': psutil.cpu_percent(interval=1),\n        'memory_percent': psutil.virtual_memory().percent,\n        'disk_usage': psutil.disk_usage('/').percent,\n        'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else None\n    }\n    \n    # Service checks\n    services = ['redis', 'postgresql']\n    for service in services:\n        try:\n            result = subprocess.run(['systemctl', 'is-active', service], \n                                  capture_output=True, text=True, timeout=5)\n            health_data['services'][service] = {\n                'status': 'active' if result.returncode == 0 else 'inactive',\n                'output': result.stdout.strip()\n            }\n        except Exception as e:\n            health_data['services'][service] = {\n                'status': 'error',\n                'error': str(e)\n            }\n    \n    # Celery worker stats\n    inspect = celery_app.control.inspect()\n    try:\n        stats = inspect.stats()\n        active = inspect.active()\n        \n        health_data['celery'] = {\n            'workers': len(stats) if stats else 0,\n            'active_tasks': sum(len(tasks) for tasks in active.values()) if active else 0,\n            'worker_stats': stats\n        }\n    except Exception as e:\n        health_data['celery'] = {\n            'error': str(e)\n        }\n    \n    # Log health status\n    logger.info(f\"üè• Health check completed: {json.dumps(health_data, indent=2)}\")\n    \n    return health_data\n\n# Custom task class with automatic retry\nclass AutoRetryTask(celery_app.Task):\n    \"\"\"Base task class with automatic retry on failure\"\"\"\n    \n    autoretry_for = (Exception,)\n    retry_kwargs = {'max_retries': 3, 'countdown': 60}\n    retry_backoff = True\n    retry_backoff_max = 600  # 10 minutes\n    retry_jitter = False\n\n@celery_app.task(base=AutoRetryTask)\ndef reliable_data_sync(data_source: str, destination: str):\n    \"\"\"Sync data between systems with automatic retry\"\"\"\n    \n    logger.info(f\"üîÑ Starting data sync from {data_source} to {destination}\")\n    \n    # This will automatically retry on any exception\n    # with exponential backoff\n    \n    try:\n        # Simulate data sync operation\n        if random.random() < 0.3:  # 30% failure rate for testing\n            raise ConnectionError(\"Simulated connection failure\")\n        \n        time.sleep(2)  # Simulate work\n        \n        return {\n            'status': 'success',\n            'source': data_source,\n            'destination': destination,\n            'synced_at': time.time()\n        }\n        \n    except Exception as e:\n        logger.error(f\"‚ùå Data sync failed: {str(e)}\")\n        raise  # This will trigger automatic retry\n```\n\n**Task Result Tracking:**\n\n```python\n# tasks/result_tracking.py\nfrom celery.result import AsyncResult\nfrom typing import Dict, Any\n\nclass TaskTracker:\n    def __init__(self, celery_app):\n        self.celery_app = celery_app\n    \n    def get_task_status(self, task_id: str) -> Dict[str, Any]:\n        \"\"\"Get comprehensive task status\"\"\"\n        \n        result = AsyncResult(task_id, app=self.celery_app)\n        \n        task_info = {\n            'task_id': task_id,\n            'status': result.status,\n            'ready': result.ready(),\n            'successful': result.successful() if result.ready() else None,\n            'failed': result.failed() if result.ready() else None,\n        }\n        \n        if result.ready():\n            if result.successful():\n                task_info['result'] = result.result\n            else:\n                task_info['error'] = str(result.info)\n                task_info['traceback'] = result.traceback\n        else:\n            # Task is still running - get progress info\n            if result.info and isinstance(result.info, dict):\n                task_info['progress'] = result.info\n        \n        return task_info\n    \n    def cancel_task(self, task_id: str) -> Dict[str, Any]:\n        \"\"\"Cancel a running task\"\"\"\n        \n        result = AsyncResult(task_id, app=self.celery_app)\n        \n        if not result.ready():\n            result.revoke(terminate=True)\n            return {\n                'task_id': task_id,\n                'status': 'cancelled',\n                'message': 'Task cancelled successfully'\n            }\n        else:\n            return {\n                'task_id': task_id,\n                'status': result.status,\n                'message': 'Task already completed'\n            }\n    \n    def get_active_tasks(self) -> Dict[str, Any]:\n        \"\"\"Get all active tasks across workers\"\"\"\n        \n        inspect = self.celery_app.control.inspect()\n        \n        try:\n            active = inspect.active()\n            scheduled = inspect.scheduled()\n            reserved = inspect.reserved()\n            \n            return {\n                'active_tasks': active,\n                'scheduled_tasks': scheduled,\n                'reserved_tasks': reserved,\n                'worker_count': len(active) if active else 0\n            }\n        except Exception as e:\n            return {\n                'error': str(e),\n                'message': 'Could not retrieve worker information'\n            }\n\n# Global task tracker\ntask_tracker = TaskTracker(celery_app)\n```\n\n---\n\n### 4Ô∏è‚É£ FastAPI Integration (10 minutes)\n\n**FastAPI Endpoints with Background Tasks:**\n\n```python\n# main.py - FastAPI integration\nfrom fastapi import FastAPI, BackgroundTasks, HTTPException, UploadFile, File\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uuid\nimport os\n\napp = FastAPI(title=\"Background Processing API\")\n\nclass EmailRequest(BaseModel):\n    recipients: List[str]\n    subject: str\n    body: str\n    html_body: Optional[str] = None\n\nclass NewsletterRequest(BaseModel):\n    subscriber_emails: List[str]\n    content: str\n    send_immediately: bool = False\n\n@app.post(\"/send-email\")\nasync def send_email_endpoint(email_request: EmailRequest):\n    \"\"\"Send email via background task\"\"\"\n    \n    if len(email_request.recipients) == 1:\n        # Single email\n        task = send_email.delay(\n            email_request.recipients[0],\n            email_request.subject,\n            email_request.body,\n            email_request.html_body\n        )\n        \n        return {\n            \"message\": \"Email queued for sending\",\n            \"task_id\": task.id,\n            \"recipient_count\": 1\n        }\n    else:\n        # Bulk emails\n        email_jobs = [\n            {\n                'email': recipient,\n                'subject': email_request.subject,\n                'variables': {'recipient': recipient}\n            }\n            for recipient in email_request.recipients\n        ]\n        \n        task = send_bulk_emails.delay(email_jobs, email_request.body)\n        \n        return {\n            \"message\": f\"Bulk email queued for {len(email_request.recipients)} recipients\",\n            \"task_id\": task.id,\n            \"recipient_count\": len(email_request.recipients)\n        }\n\n@app.post(\"/newsletter\")\nasync def send_newsletter_endpoint(newsletter: NewsletterRequest):\n    \"\"\"Send newsletter to subscribers\"\"\"\n    \n    if newsletter.send_immediately:\n        # Use FastAPI background task for small lists\n        if len(newsletter.subscriber_emails) <= 10:\n            background_tasks = BackgroundTasks()\n            \n            def send_immediate_newsletter():\n                for email in newsletter.subscriber_emails:\n                    # Simulate sending\n                    time.sleep(0.1)\n                    print(f\"üìß Newsletter sent to {email}\")\n            \n            background_tasks.add_task(send_immediate_newsletter)\n            \n            return {\n                \"message\": \"Newsletter sent immediately\",\n                \"recipient_count\": len(newsletter.subscriber_emails)\n            }\n    \n    # Use Celery for large lists or scheduled sending\n    task = send_newsletter.delay(\n        newsletter.subscriber_emails, \n        newsletter.content\n    )\n    \n    return {\n        \"message\": f\"Newsletter queued for {len(newsletter.subscriber_emails)} subscribers\",\n        \"task_id\": task.id,\n        \"status\": \"processing\"\n    }\n\n@app.post(\"/upload-image\")\nasync def upload_image(file: UploadFile = File(...), user_id: int = 1):\n    \"\"\"Upload and process image\"\"\"\n    \n    # Validate file type\n    if not file.content_type.startswith('image/'):\n        raise HTTPException(status_code=400, detail=\"File must be an image\")\n    \n    # Save uploaded file temporarily\n    upload_dir = \"temp_uploads\"\n    os.makedirs(upload_dir, exist_ok=True)\n    \n    file_extension = file.filename.split('.')[-1] if '.' in file.filename else 'jpg'\n    temp_filename = f\"{uuid.uuid4()}.{file_extension}\"\n    temp_path = os.path.join(upload_dir, temp_filename)\n    \n    # Save file\n    with open(temp_path, \"wb\") as buffer:\n        content = await file.read()\n        buffer.write(content)\n    \n    # Queue image processing\n    task = process_uploaded_image.delay(temp_path, user_id)\n    \n    return {\n        \"message\": \"Image uploaded and queued for processing\",\n        \"task_id\": task.id,\n        \"filename\": file.filename,\n        \"file_size\": len(content)\n    }\n\n@app.get(\"/tasks/{task_id}\")\nasync def get_task_status(task_id: str):\n    \"\"\"Get task status and result\"\"\"\n    \n    task_info = task_tracker.get_task_status(task_id)\n    \n    if not task_info:\n        raise HTTPException(status_code=404, detail=\"Task not found\")\n    \n    return task_info\n\n@app.delete(\"/tasks/{task_id}\")\nasync def cancel_task(task_id: str):\n    \"\"\"Cancel a running task\"\"\"\n    \n    result = task_tracker.cancel_task(task_id)\n    return result\n\n@app.get(\"/tasks\")\nasync def get_active_tasks():\n    \"\"\"Get all active tasks\"\"\"\n    \n    return task_tracker.get_active_tasks()\n\n@app.post(\"/test/load-test\")\nasync def create_load_test(num_tasks: int = 100, task_duration: int = 5):\n    \"\"\"Create multiple background tasks for testing\"\"\"\n    \n    task_ids = []\n    \n    for i in range(num_tasks):\n        task = long_running_task.delay(task_duration)\n        task_ids.append(task.id)\n    \n    return {\n        \"message\": f\"Created {num_tasks} background tasks\",\n        \"task_ids\": task_ids[:10],  # Return first 10 IDs\n        \"total_tasks\": len(task_ids)\n    }\n\n# Periodic task status endpoint\n@app.get(\"/health/workers\")\nasync def get_worker_health():\n    \"\"\"Get Celery worker health status\"\"\"\n    \n    inspect = celery_app.control.inspect()\n    \n    try:\n        # Get worker statistics\n        stats = inspect.stats()\n        active = inspect.active()\n        reserved = inspect.reserved()\n        \n        if not stats:\n            return {\n                \"status\": \"error\",\n                \"message\": \"No workers available\",\n                \"workers\": []\n            }\n        \n        worker_info = []\n        for worker_name, worker_stats in stats.items():\n            worker_info.append({\n                \"name\": worker_name,\n                \"status\": \"active\",\n                \"active_tasks\": len(active.get(worker_name, [])),\n                \"reserved_tasks\": len(reserved.get(worker_name, [])),\n                \"total_tasks\": worker_stats.get('total', 0),\n                \"pool_processes\": worker_stats.get('pool', {}).get('processes', 0)\n            })\n        \n        return {\n            \"status\": \"healthy\",\n            \"worker_count\": len(worker_info),\n            \"workers\": worker_info\n        }\n        \n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": str(e),\n            \"workers\": []\n        }\n```\n\n---\n\n### 5Ô∏è‚É£ Running and Monitoring (5 minutes)\n\n**Start Celery Workers:**\n\n```bash\n# Start Redis (message broker)\nredis-server\n\n# Start Celery worker (in separate terminal)\ncelery -A celery_app worker --loglevel=info --concurrency=4\n\n# Start Celery worker with specific queues\ncelery -A celery_app worker --loglevel=info --queues=email,images\n\n# Start Celery Beat (periodic tasks scheduler)\ncelery -A celery_app beat --loglevel=info\n\n# Start Flower (monitoring web UI)\ncelery -A celery_app flower --port=5555\n\n# Production: Start all services\n# Worker processes\ncelery multi start worker1 -A celery_app --pidfile=\"./celery/%n.pid\" --logfile=\"./celery/%n%I.log\"\n\n# Beat scheduler  \ncelery -A celery_app beat --pidfile=\"./celery/beat.pid\" --logfile=\"./celery/beat.log\" --detach\n\n# Flower monitoring\ncelery -A celery_app flower --port=5555 --persistent=True --db=./celery/flower\n```\n\n**Production Configuration:**\n\n```python\n# production_celery.py\nimport os\nfrom celery import Celery\n\ndef create_celery_app():\n    \"\"\"Create production Celery app\"\"\"\n    \n    # Environment-based configuration\n    broker_url = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')\n    result_backend = os.getenv('CELERY_RESULT_BACKEND', 'redis://localhost:6379/0')\n    \n    celery = Celery('production_app')\n    \n    # Production settings\n    celery.conf.update(\n        broker_url=broker_url,\n        result_backend=result_backend,\n        \n        # Performance settings\n        worker_prefetch_multiplier=1,\n        task_acks_late=True,\n        worker_max_tasks_per_child=1000,\n        \n        # Security\n        worker_hijack_root_logger=False,\n        worker_log_format=\"[%(asctime)s: %(levelname)s/%(processName)s] %(message)s\",\n        \n        # Monitoring\n        worker_send_task_events=True,\n        task_send_sent_event=True,\n        \n        # Error handling\n        task_reject_on_worker_lost=True,\n        task_ignore_result=False,\n    )\n    \n    return celery\n\n# Deployment script\ndef deploy_workers():\n    \"\"\"Deploy Celery workers in production\"\"\"\n    \n    # Stop existing workers\n    os.system(\"celery multi stopwait worker1 worker2 worker3\")\n    \n    # Start new workers\n    os.system(\"\"\"\n        celery multi start worker1 worker2 worker3 \\\n        -A production_celery:create_celery_app \\\n        --pidfile=./run/celery/%n.pid \\\n        --logfile=./logs/celery/%n%I.log \\\n        --loglevel=INFO \\\n        --concurrency=4\n    \"\"\")\n    \n    print(\"‚úÖ Celery workers deployed successfully\")\n\nif __name__ == \"__main__\":\n    deploy_workers()\n```\n\n---\n\n### üè† Homework: Create Background Task System\n\n**Task:** Implement a complete background processing system\n\n```python\n# Create these background tasks:\n# 1. Email notification system with retry logic\n# 2. Image processing pipeline (resize, thumbnail, S3 upload)\n# 3. Report generation task with progress tracking\n# 4. Data backup task that runs daily\n# 5. API sync task with external services\n\n# Requirements:\n# - Proper error handling and retries\n# - Progress tracking for long-running tasks\n# - Task monitoring and health checks\n# - Queue management (separate queues for different task types)\n# - Production-ready configuration\n\n# Bonus:\n# - Create a web dashboard to monitor tasks\n# - Implement task scheduling with Celery Beat\n# - Add metrics and alerting for failed tasks\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Message Queues = Decouple heavy work from HTTP requests\n‚úÖ Celery = Distributed task queue for Python applications\n‚úÖ Retry Logic = Handle failures gracefully with backoff strategies\n‚úÖ Task Monitoring = Track progress and debug issues effectively\n‚úÖ Background Processing = Essential for scalable web applications\n\n---\n\n<a name=\"hour-35\"></a>\n## üìÖ Hour 35: File Uploads, Media Handling & S3\n\n### üéØ Learning Objectives\n- Handle secure file uploads with validation\n- Implement streaming uploads for large files\n- Store files in AWS S3 and CloudFront\n- Create file processing pipelines\n- Build image galleries and file management systems\n\n### üìñ What to Teach\n\n**\"Today we master file handling from upload to cloud storage with security and performance!\"**\n\n---\n\n### 1Ô∏è‚É£ Secure File Upload Implementation (15 minutes)\n\n**File Upload Security & Validation:**\n\n```python\n# file_upload.py\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Form\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom pydantic import BaseModel\nfrom typing import List, Optional, BinaryIO\nimport aiofiles\nimport os\nimport uuid\nimport hashlib\nimport magic\nimport PIL.Image\nfrom pathlib import Path\nimport asyncio\nfrom datetime import datetime, timedelta\n\napp = FastAPI(title=\"Advanced File Upload API\")\nsecurity = HTTPBearer()\n\n# File upload configuration\nclass FileConfig:\n    # File size limits (bytes)\n    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB\n    MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB for images\n    MAX_VIDEO_SIZE = 100 * 1024 * 1024  # 100MB for videos\n    \n    # Allowed file types\n    ALLOWED_IMAGE_TYPES = {\n        'image/jpeg': '.jpg',\n        'image/png': '.png',\n        'image/gif': '.gif',\n        'image/webp': '.webp'\n    }\n    \n    ALLOWED_DOCUMENT_TYPES = {\n        'application/pdf': '.pdf',\n        'application/msword': '.doc',\n        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',\n        'text/plain': '.txt',\n        'text/csv': '.csv'\n    }\n    \n    ALLOWED_VIDEO_TYPES = {\n        'video/mp4': '.mp4',\n        'video/avi': '.avi',\n        'video/quicktime': '.mov',\n        'video/x-msvideo': '.avi'\n    }\n    \n    # Storage paths\n    UPLOAD_DIR = Path(\"uploads\")\n    TEMP_DIR = Path(\"temp\")\n    PROCESSED_DIR = Path(\"processed\")\n\nclass FileValidator:\n    \"\"\"Comprehensive file validation\"\"\"\n    \n    def __init__(self):\n        self.config = FileConfig()\n    \n    async def validate_file(self, file: UploadFile, file_type: str = \"any\") -> dict:\n        \"\"\"Validate uploaded file with comprehensive checks\"\"\"\n        \n        validation_result = {\n            'valid': False,\n            'errors': [],\n            'file_info': {},\n            'security_checks': {}\n        }\n        \n        try:\n            # 1. Basic file info\n            file_size = 0\n            content = await file.read()\n            file_size = len(content)\n            await file.seek(0)  # Reset file pointer\n            \n            validation_result['file_info'] = {\n                'filename': file.filename,\n                'content_type': file.content_type,\n                'size': file_size,\n                'size_mb': round(file_size / (1024 * 1024), 2)\n            }\n            \n            # 2. File size validation\n            max_size = self._get_max_size_for_type(file_type)\n            if file_size > max_size:\n                validation_result['errors'].append(\n                    f\"File too large: {validation_result['file_info']['size_mb']}MB \"\n                    f\"(max: {max_size / (1024 * 1024)}MB)\"\n                )\n            \n            if file_size == 0:\n                validation_result['errors'].append(\"Empty file not allowed\")\n            \n            # 3. MIME type validation using python-magic\n            try:\n                detected_mime = magic.from_buffer(content[:2048], mime=True)\n                validation_result['file_info']['detected_mime'] = detected_mime\n                \n                # Check if detected MIME matches declared MIME\n                if detected_mime != file.content_type:\n                    validation_result['security_checks']['mime_mismatch'] = True\n                    validation_result['errors'].append(\n                        f\"MIME type mismatch: declared {file.content_type}, \"\n                        f\"detected {detected_mime}\"\n                    )\n            except Exception as e:\n                validation_result['errors'].append(f\"MIME detection failed: {str(e)}\")\n            \n            # 4. File type validation\n            allowed_types = self._get_allowed_types_for_category(file_type)\n            if file.content_type not in allowed_types:\n                validation_result['errors'].append(\n                    f\"File type not allowed: {file.content_type}\"\n                )\n            \n            # 5. Filename validation\n            if not file.filename or len(file.filename) > 255:\n                validation_result['errors'].append(\"Invalid filename\")\n            \n            # Check for dangerous characters\n            dangerous_chars = ['..', '/', '\\\\', '<', '>', ':', '\"', '|', '?', '*']\n            if any(char in file.filename for char in dangerous_chars):\n                validation_result['errors'].append(\"Filename contains dangerous characters\")\n            \n            # 6. Content validation (specific to file type)\n            if file_type == \"image\":\n                image_validation = await self._validate_image_content(content)\n                validation_result.update(image_validation)\n            \n            # 7. Malware scanning (basic)\n            malware_check = await self._basic_malware_scan(content)\n            validation_result['security_checks']['malware_scan'] = malware_check\n            \n            # 8. Generate file hash for deduplication\n            file_hash = hashlib.sha256(content).hexdigest()\n            validation_result['file_info']['sha256'] = file_hash\n            \n            # Set validation status\n            validation_result['valid'] = len(validation_result['errors']) == 0\n            \n            return validation_result\n            \n        except Exception as e:\n            validation_result['errors'].append(f\"Validation error: {str(e)}\")\n            return validation_result\n    \n    def _get_max_size_for_type(self, file_type: str) -> int:\n        \"\"\"Get maximum file size for type\"\"\"\n        size_map = {\n            'image': self.config.MAX_IMAGE_SIZE,\n            'video': self.config.MAX_VIDEO_SIZE,\n            'document': self.config.MAX_FILE_SIZE,\n            'any': self.config.MAX_FILE_SIZE\n        }\n        return size_map.get(file_type, self.config.MAX_FILE_SIZE)\n    \n    def _get_allowed_types_for_category(self, category: str) -> dict:\n        \"\"\"Get allowed MIME types for category\"\"\"\n        type_map = {\n            'image': self.config.ALLOWED_IMAGE_TYPES,\n            'document': self.config.ALLOWED_DOCUMENT_TYPES,\n            'video': self.config.ALLOWED_VIDEO_TYPES,\n            'any': {**self.config.ALLOWED_IMAGE_TYPES, \n                   **self.config.ALLOWED_DOCUMENT_TYPES}\n        }\n        return type_map.get(category, {})\n    \n    async def _validate_image_content(self, content: bytes) -> dict:\n        \"\"\"Validate image-specific content\"\"\"\n        result = {'image_validation': {}}\n        \n        try:\n            # Try to open image with PIL\n            from io import BytesIO\n            image = PIL.Image.open(BytesIO(content))\n            \n            result['image_validation'] = {\n                'dimensions': image.size,\n                'format': image.format,\n                'mode': image.mode,\n                'valid': True\n            }\n            \n            # Check for extremely large images\n            width, height = image.size\n            if width * height > 50_000_000:  # 50 megapixels\n                result['errors'] = result.get('errors', [])\n                result['errors'].append(\"Image resolution too high\")\n            \n        except Exception as e:\n            result['errors'] = result.get('errors', [])\n            result['errors'].append(f\"Invalid image file: {str(e)}\")\n            result['image_validation']['valid'] = False\n        \n        return result\n    \n    async def _basic_malware_scan(self, content: bytes) -> dict:\n        \"\"\"Basic malware detection\"\"\"\n        \n        # Simple signature-based detection\n        malware_signatures = [\n            b'<?php',  # PHP code injection\n            b'<script',  # JavaScript injection\n            b'eval(',  # Code evaluation\n            b'system(',  # System calls\n            b'exec(',  # Command execution\n        ]\n        \n        threats_found = []\n        for signature in malware_signatures:\n            if signature.lower() in content.lower():\n                threats_found.append(signature.decode('utf-8', errors='ignore'))\n        \n        return {\n            'scan_completed': True,\n            'threats_found': threats_found,\n            'is_safe': len(threats_found) == 0\n        }\n\n# File upload endpoints\nfile_validator = FileValidator()\n\n@app.post(\"/upload/single\")\nasync def upload_single_file(\n    file: UploadFile = File(...),\n    file_type: str = Form(\"any\"),\n    description: Optional[str] = Form(None),\n    user_id: int = Form(...),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Upload a single file with comprehensive validation\"\"\"\n    \n    # Validate file\n    validation = await file_validator.validate_file(file, file_type)\n    \n    if not validation['valid']:\n        raise HTTPException(\n            status_code=400,\n            detail={\n                \"message\": \"File validation failed\",\n                \"errors\": validation['errors'],\n                \"file_info\": validation['file_info']\n            }\n        )\n    \n    try:\n        # Generate unique filename\n        file_extension = Path(file.filename).suffix\n        unique_filename = f\"{uuid.uuid4()}{file_extension}\"\n        \n        # Create user directory\n        user_dir = FileConfig.UPLOAD_DIR / str(user_id)\n        user_dir.mkdir(parents=True, exist_ok=True)\n        \n        file_path = user_dir / unique_filename\n        \n        # Save file asynchronously\n        async with aiofiles.open(file_path, 'wb') as f:\n            content = await file.read()\n            await f.write(content)\n        \n        # Create file record\n        file_record = {\n            'id': str(uuid.uuid4()),\n            'original_filename': file.filename,\n            'stored_filename': unique_filename,\n            'file_path': str(file_path),\n            'user_id': user_id,\n            'description': description,\n            'file_type': file_type,\n            'mime_type': file.content_type,\n            'file_size': validation['file_info']['size'],\n            'file_hash': validation['file_info']['sha256'],\n            'upload_timestamp': datetime.now().isoformat(),\n            'validation_result': validation\n        }\n        \n        # TODO: Save file_record to database\n        \n        return {\n            \"message\": \"File uploaded successfully\",\n            \"file_id\": file_record['id'],\n            \"filename\": unique_filename,\n            \"size\": validation['file_info']['size_mb'],\n            \"type\": file.content_type\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Upload failed: {str(e)}\")\n\n@app.post(\"/upload/multiple\")\nasync def upload_multiple_files(\n    files: List[UploadFile] = File(...),\n    file_type: str = Form(\"any\"),\n    user_id: int = Form(...),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Upload multiple files with batch processing\"\"\"\n    \n    if len(files) > 10:  # Limit batch size\n        raise HTTPException(status_code=400, detail=\"Maximum 10 files per batch\")\n    \n    upload_results = []\n    failed_uploads = []\n    \n    for file in files:\n        try:\n            # Validate each file\n            validation = await file_validator.validate_file(file, file_type)\n            \n            if not validation['valid']:\n                failed_uploads.append({\n                    'filename': file.filename,\n                    'errors': validation['errors']\n                })\n                continue\n            \n            # Process valid files\n            file_extension = Path(file.filename).suffix\n            unique_filename = f\"{uuid.uuid4()}{file_extension}\"\n            \n            user_dir = FileConfig.UPLOAD_DIR / str(user_id)\n            user_dir.mkdir(parents=True, exist_ok=True)\n            \n            file_path = user_dir / unique_filename\n            \n            # Save file\n            async with aiofiles.open(file_path, 'wb') as f:\n                content = await file.read()\n                await f.write(content)\n            \n            upload_results.append({\n                'original_filename': file.filename,\n                'stored_filename': unique_filename,\n                'size_mb': validation['file_info']['size_mb'],\n                'file_hash': validation['file_info']['sha256']\n            })\n            \n        except Exception as e:\n            failed_uploads.append({\n                'filename': file.filename,\n                'error': str(e)\n            })\n    \n    return {\n        \"message\": f\"Batch upload completed\",\n        \"successful_uploads\": len(upload_results),\n        \"failed_uploads\": len(failed_uploads),\n        \"results\": upload_results,\n        \"failures\": failed_uploads\n    }\n```\n\n**Streaming Upload for Large Files:**\n\n```python\n# streaming_upload.py\nimport aiofiles\nfrom fastapi import Request, Response\nimport asyncio\n\nclass StreamingUploadHandler:\n    \"\"\"Handle streaming uploads for large files\"\"\"\n    \n    def __init__(self, chunk_size: int = 8192):\n        self.chunk_size = chunk_size\n    \n    async def handle_streaming_upload(\n        self, \n        request: Request, \n        file_path: Path,\n        max_size: int = 100 * 1024 * 1024  # 100MB\n    ) -> dict:\n        \"\"\"Handle streaming file upload with progress tracking\"\"\"\n        \n        total_size = 0\n        chunks_received = 0\n        \n        try:\n            async with aiofiles.open(file_path, 'wb') as f:\n                async for chunk in self._read_chunks(request):\n                    chunk_size = len(chunk)\n                    total_size += chunk_size\n                    chunks_received += 1\n                    \n                    # Size limit check\n                    if total_size > max_size:\n                        # Cleanup partial file\n                        await f.close()\n                        file_path.unlink(missing_ok=True)\n                        \n                        raise HTTPException(\n                            status_code=413,\n                            detail=f\"File too large. Max size: {max_size / (1024*1024):.1f}MB\"\n                        )\n                    \n                    await f.write(chunk)\n                    \n                    # Progress callback (could emit WebSocket events)\n                    progress = (total_size / max_size) * 100\n                    if chunks_received % 100 == 0:  # Every 100 chunks\n                        await self._emit_progress(progress, total_size)\n        \n            return {\n                'total_size': total_size,\n                'chunks_received': chunks_received,\n                'file_path': str(file_path)\n            }\n            \n        except Exception as e:\n            # Cleanup on error\n            file_path.unlink(missing_ok=True)\n            raise e\n    \n    async def _read_chunks(self, request: Request):\n        \"\"\"Read request body in chunks\"\"\"\n        async for chunk in request.stream():\n            if chunk:\n                yield chunk\n    \n    async def _emit_progress(self, progress: float, bytes_received: int):\n        \"\"\"Emit progress updates (WebSocket, SSE, etc.)\"\"\"\n        # Could emit to WebSocket or store in Redis for polling\n        print(f\"üìà Upload progress: {progress:.1f}% ({bytes_received:,} bytes)\")\n\n@app.post(\"/upload/streaming/{file_id}\")\nasync def streaming_upload(\n    file_id: str,\n    request: Request,\n    user_id: int,\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Handle streaming file upload\"\"\"\n    \n    # Validate file_id and user permissions\n    if not file_id or len(file_id) != 36:  # UUID length\n        raise HTTPException(status_code=400, detail=\"Invalid file ID\")\n    \n    # Create file path\n    user_dir = FileConfig.UPLOAD_DIR / str(user_id)\n    user_dir.mkdir(parents=True, exist_ok=True)\n    \n    file_path = user_dir / f\"{file_id}.tmp\"\n    \n    # Handle streaming upload\n    streaming_handler = StreamingUploadHandler()\n    \n    try:\n        result = await streaming_handler.handle_streaming_upload(\n            request, \n            file_path,\n            max_size=FileConfig.MAX_FILE_SIZE\n        )\n        \n        return {\n            \"message\": \"Streaming upload completed\",\n            \"file_id\": file_id,\n            \"size\": result['total_size'],\n            \"chunks\": result['chunks_received']\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/upload/progress/{file_id}\")\nasync def get_upload_progress(file_id: str):\n    \"\"\"Get upload progress for streaming uploads\"\"\"\n    \n    # In production, store progress in Redis\n    # This is a simplified example\n    \n    progress_data = {\n        'file_id': file_id,\n        'progress': 75.5,\n        'bytes_received': 7550000,\n        'status': 'uploading',\n        'eta_seconds': 30\n    }\n    \n    return progress_data\n```\n\n---\n\n### 2Ô∏è‚É£ AWS S3 Integration (15 minutes)\n\n**S3 Configuration and Upload:**\n\n```python\n# s3_handler.py\nimport boto3\nfrom botocore.exceptions import ClientError, NoCredentialsError\nfrom botocore.config import Config\nimport os\nfrom typing import BinaryIO, Optional, Dict, Any\nimport mimetypes\nfrom urllib.parse import quote\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass S3FileManager:\n    \"\"\"Advanced S3 file management with multipart uploads\"\"\"\n    \n    def __init__(self):\n        # S3 configuration\n        self.bucket_name = os.getenv('AWS_S3_BUCKET', 'my-app-uploads')\n        self.region = os.getenv('AWS_REGION', 'us-east-1')\n        self.access_key = os.getenv('AWS_ACCESS_KEY_ID')\n        self.secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n        \n        # CloudFront distribution (optional)\n        self.cloudfront_domain = os.getenv('CLOUDFRONT_DOMAIN')\n        \n        # Create S3 client with retry configuration\n        self.s3_client = boto3.client(\n            's3',\n            aws_access_key_id=self.access_key,\n            aws_secret_access_key=self.secret_key,\n            region_name=self.region,\n            config=Config(\n                region_name=self.region,\n                retries={'max_attempts': 3, 'mode': 'adaptive'},\n                max_pool_connections=50\n            )\n        )\n        \n        # Thread pool for async operations\n        self.executor = ThreadPoolExecutor(max_workers=10)\n    \n    async def upload_file(\n        self,\n        file_path: str,\n        s3_key: str,\n        content_type: Optional[str] = None,\n        metadata: Optional[Dict[str, str]] = None,\n        public_read: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Upload file to S3 with comprehensive options\"\"\"\n        \n        try:\n            # Auto-detect content type if not provided\n            if not content_type:\n                content_type, _ = mimetypes.guess_type(file_path)\n                if not content_type:\n                    content_type = 'application/octet-stream'\n            \n            # Prepare upload arguments\n            upload_args = {\n                'Bucket': self.bucket_name,\n                'Key': s3_key,\n                'ContentType': content_type\n            }\n            \n            # Add metadata\n            if metadata:\n                upload_args['Metadata'] = metadata\n            \n            # Set permissions\n            if public_read:\n                upload_args['ACL'] = 'public-read'\n            \n            # Add cache control for static assets\n            if content_type.startswith(('image/', 'video/', 'audio/')):\n                upload_args['CacheControl'] = 'max-age=31536000'  # 1 year\n            \n            # Upload file asynchronously\n            loop = asyncio.get_event_loop()\n            \n            def _upload():\n                return self.s3_client.upload_file(\n                    file_path,\n                    **upload_args\n                )\n            \n            await loop.run_in_executor(self.executor, _upload)\n            \n            # Generate URLs\n            s3_url = f\"https://{self.bucket_name}.s3.{self.region}.amazonaws.com/{quote(s3_key)}\"\n            \n            # Use CloudFront URL if available\n            if self.cloudfront_domain:\n                cdn_url = f\"https://{self.cloudfront_domain}/{quote(s3_key)}\"\n            else:\n                cdn_url = s3_url\n            \n            logger.info(f\"‚úÖ File uploaded to S3: {s3_key}\")\n            \n            return {\n                'success': True,\n                's3_key': s3_key,\n                's3_url': s3_url,\n                'cdn_url': cdn_url,\n                'bucket': self.bucket_name,\n                'content_type': content_type\n            }\n            \n        except FileNotFoundError:\n            logger.error(f\"‚ùå File not found: {file_path}\")\n            return {'success': False, 'error': 'File not found'}\n            \n        except NoCredentialsError:\n            logger.error(\"‚ùå AWS credentials not found\")\n            return {'success': False, 'error': 'AWS credentials not configured'}\n            \n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            error_message = e.response['Error']['Message']\n            \n            logger.error(f\"‚ùå S3 upload failed: {error_code} - {error_message}\")\n            \n            return {\n                'success': False,\n                'error': f\"S3 error: {error_code}\",\n                'details': error_message\n            }\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Unexpected upload error: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n    \n    async def upload_multipart(\n        self,\n        file_path: str,\n        s3_key: str,\n        part_size: int = 100 * 1024 * 1024,  # 100MB parts\n        content_type: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Upload large file using multipart upload\"\"\"\n        \n        try:\n            file_size = os.path.getsize(file_path)\n            \n            if file_size < part_size:\n                # Use regular upload for small files\n                return await self.upload_file(file_path, s3_key, content_type)\n            \n            # Auto-detect content type\n            if not content_type:\n                content_type, _ = mimetypes.guess_type(file_path)\n                if not content_type:\n                    content_type = 'application/octet-stream'\n            \n            logger.info(f\"üîÑ Starting multipart upload: {s3_key} ({file_size:,} bytes)\")\n            \n            # Initiate multipart upload\n            loop = asyncio.get_event_loop()\n            \n            def _initiate_multipart():\n                return self.s3_client.create_multipart_upload(\n                    Bucket=self.bucket_name,\n                    Key=s3_key,\n                    ContentType=content_type\n                )\n            \n            response = await loop.run_in_executor(self.executor, _initiate_multipart)\n            upload_id = response['UploadId']\n            \n            # Upload parts\n            parts = []\n            part_number = 1\n            \n            with open(file_path, 'rb') as f:\n                while True:\n                    chunk = f.read(part_size)\n                    if not chunk:\n                        break\n                    \n                    def _upload_part():\n                        return self.s3_client.upload_part(\n                            Bucket=self.bucket_name,\n                            Key=s3_key,\n                            PartNumber=part_number,\n                            UploadId=upload_id,\n                            Body=chunk\n                        )\n                    \n                    part_response = await loop.run_in_executor(self.executor, _upload_part)\n                    \n                    parts.append({\n                        'ETag': part_response['ETag'],\n                        'PartNumber': part_number\n                    })\n                    \n                    logger.info(f\"üì§ Uploaded part {part_number} ({len(chunk):,} bytes)\")\n                    part_number += 1\n            \n            # Complete multipart upload\n            def _complete_multipart():\n                return self.s3_client.complete_multipart_upload(\n                    Bucket=self.bucket_name,\n                    Key=s3_key,\n                    UploadId=upload_id,\n                    MultipartUpload={'Parts': parts}\n                )\n            \n            await loop.run_in_executor(self.executor, _complete_multipart)\n            \n            # Generate URLs\n            s3_url = f\"https://{self.bucket_name}.s3.{self.region}.amazonaws.com/{quote(s3_key)}\"\n            cdn_url = f\"https://{self.cloudfront_domain}/{quote(s3_key)}\" if self.cloudfront_domain else s3_url\n            \n            logger.info(f\"‚úÖ Multipart upload completed: {s3_key}\")\n            \n            return {\n                'success': True,\n                's3_key': s3_key,\n                's3_url': s3_url,\n                'cdn_url': cdn_url,\n                'file_size': file_size,\n                'parts_uploaded': len(parts),\n                'upload_method': 'multipart'\n            }\n            \n        except Exception as e:\n            # Cleanup failed multipart upload\n            if 'upload_id' in locals():\n                try:\n                    await loop.run_in_executor(\n                        self.executor,\n                        lambda: self.s3_client.abort_multipart_upload(\n                            Bucket=self.bucket_name,\n                            Key=s3_key,\n                            UploadId=upload_id\n                        )\n                    )\n                except:\n                    pass\n            \n            logger.error(f\"‚ùå Multipart upload failed: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n    \n    async def generate_presigned_url(\n        self,\n        s3_key: str,\n        expiration: int = 3600,\n        http_method: str = 'GET'\n    ) -> Dict[str, Any]:\n        \"\"\"Generate presigned URL for secure file access\"\"\"\n        \n        try:\n            loop = asyncio.get_event_loop()\n            \n            def _generate_url():\n                return self.s3_client.generate_presigned_url(\n                    http_method.upper(),\n                    Params={'Bucket': self.bucket_name, 'Key': s3_key},\n                    ExpiresIn=expiration\n                )\n            \n            presigned_url = await loop.run_in_executor(self.executor, _generate_url)\n            \n            return {\n                'success': True,\n                'presigned_url': presigned_url,\n                'expires_in': expiration,\n                's3_key': s3_key\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Presigned URL generation failed: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n    \n    async def delete_file(self, s3_key: str) -> Dict[str, Any]:\n        \"\"\"Delete file from S3\"\"\"\n        \n        try:\n            loop = asyncio.get_event_loop()\n            \n            def _delete():\n                return self.s3_client.delete_object(\n                    Bucket=self.bucket_name,\n                    Key=s3_key\n                )\n            \n            await loop.run_in_executor(self.executor, _delete)\n            \n            logger.info(f\"üóëÔ∏è File deleted from S3: {s3_key}\")\n            \n            return {\n                'success': True,\n                's3_key': s3_key,\n                'message': 'File deleted successfully'\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå S3 delete failed: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n    \n    async def list_files(\n        self,\n        prefix: str = '',\n        max_keys: int = 1000\n    ) -> Dict[str, Any]:\n        \"\"\"List files in S3 bucket\"\"\"\n        \n        try:\n            loop = asyncio.get_event_loop()\n            \n            def _list_objects():\n                return self.s3_client.list_objects_v2(\n                    Bucket=self.bucket_name,\n                    Prefix=prefix,\n                    MaxKeys=max_keys\n                )\n            \n            response = await loop.run_in_executor(self.executor, _list_objects)\n            \n            files = []\n            if 'Contents' in response:\n                for obj in response['Contents']:\n                    files.append({\n                        'key': obj['Key'],\n                        'size': obj['Size'],\n                        'last_modified': obj['LastModified'].isoformat(),\n                        'url': f\"https://{self.bucket_name}.s3.{self.region}.amazonaws.com/{quote(obj['Key'])}\"\n                    })\n            \n            return {\n                'success': True,\n                'files': files,\n                'count': len(files),\n                'prefix': prefix\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå S3 list failed: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n\n# Initialize S3 manager\ns3_manager = S3FileManager()\n```\n\n**S3 Upload Endpoints:**\n\n```python\n# S3 integration with FastAPI\n@app.post(\"/upload/s3\")\nasync def upload_to_s3(\n    file: UploadFile = File(...),\n    folder: str = Form(\"uploads\"),\n    user_id: int = Form(...),\n    make_public: bool = Form(False),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Upload file directly to S3\"\"\"\n    \n    # Validate file\n    validation = await file_validator.validate_file(file, \"any\")\n    \n    if not validation['valid']:\n        raise HTTPException(status_code=400, detail=validation['errors'])\n    \n    try:\n        # Save file temporarily\n        temp_dir = FileConfig.TEMP_DIR\n        temp_dir.mkdir(exist_ok=True)\n        \n        temp_filename = f\"{uuid.uuid4()}{Path(file.filename).suffix}\"\n        temp_path = temp_dir / temp_filename\n        \n        # Save uploaded file\n        async with aiofiles.open(temp_path, 'wb') as f:\n            content = await file.read()\n            await f.write(content)\n        \n        # Generate S3 key\n        file_hash = validation['file_info']['sha256'][:12]  # First 12 chars of hash\n        s3_key = f\"{folder}/user_{user_id}/{file_hash}_{temp_filename}\"\n        \n        # Upload to S3\n        if validation['file_info']['size'] > 100 * 1024 * 1024:  # 100MB\n            # Use multipart upload for large files\n            result = await s3_manager.upload_multipart(\n                str(temp_path),\n                s3_key,\n                content_type=file.content_type\n            )\n        else:\n            # Regular upload for smaller files\n            metadata = {\n                'original-filename': file.filename,\n                'user-id': str(user_id),\n                'upload-timestamp': datetime.now().isoformat()\n            }\n            \n            result = await s3_manager.upload_file(\n                str(temp_path),\n                s3_key,\n                content_type=file.content_type,\n                metadata=metadata,\n                public_read=make_public\n            )\n        \n        # Cleanup temp file\n        temp_path.unlink(missing_ok=True)\n        \n        if result['success']:\n            return {\n                \"message\": \"File uploaded to S3 successfully\",\n                \"s3_key\": result['s3_key'],\n                \"url\": result['cdn_url'],\n                \"size\": validation['file_info']['size'],\n                \"content_type\": file.content_type\n            }\n        else:\n            raise HTTPException(status_code=500, detail=result['error'])\n            \n    except Exception as e:\n        # Cleanup on error\n        if 'temp_path' in locals():\n            temp_path.unlink(missing_ok=True)\n        \n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/files/s3/{s3_key:path}/download\")\nasync def download_from_s3(\n    s3_key: str,\n    expires_in: int = 3600,\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Generate secure download URL for S3 file\"\"\"\n    \n    result = await s3_manager.generate_presigned_url(\n        s3_key,\n        expiration=expires_in,\n        http_method='GET'\n    )\n    \n    if result['success']:\n        return {\n            \"download_url\": result['presigned_url'],\n            \"expires_in\": expires_in,\n            \"s3_key\": s3_key\n        }\n    else:\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n@app.delete(\"/files/s3/{s3_key:path}\")\nasync def delete_from_s3(\n    s3_key: str,\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Delete file from S3\"\"\"\n    \n    result = await s3_manager.delete_file(s3_key)\n    \n    if result['success']:\n        return {\"message\": \"File deleted successfully\", \"s3_key\": s3_key}\n    else:\n        raise HTTPException(status_code=500, detail=result['error'])\n```\n\n---\n\n### 3Ô∏è‚É£ Image Processing Pipeline (10 minutes)\n\n**Advanced Image Processing:**\n\n```python\n# image_processing.py\nfrom PIL import Image, ImageOps, ImageFilter, ImageEnhance\nimport asyncio\nfrom concurrent.futures import ProcessPoolExecutor\nimport io\nimport base64\nfrom typing import Tuple, List, Dict, Any\n\nclass ImageProcessor:\n    \"\"\"Advanced image processing with async support\"\"\"\n    \n    def __init__(self):\n        self.executor = ProcessPoolExecutor(max_workers=4)\n    \n    async def process_image_variants(\n        self,\n        image_path: str,\n        variants: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Process multiple image variants asynchronously\"\"\"\n        \n        loop = asyncio.get_event_loop()\n        \n        def _process_all_variants():\n            results = {}\n            \n            with Image.open(image_path) as img:\n                # Convert to RGB if needed\n                if img.mode in ('RGBA', 'LA', 'P'):\n                    img = img.convert('RGB')\n                \n                for variant_name, config in variants.items():\n                    try:\n                        processed_img = self._apply_transformations(img.copy(), config)\n                        \n                        # Save variant\n                        output_buffer = io.BytesIO()\n                        format_type = config.get('format', 'JPEG')\n                        quality = config.get('quality', 85)\n                        \n                        processed_img.save(\n                            output_buffer,\n                            format=format_type,\n                            quality=quality,\n                            optimize=True\n                        )\n                        \n                        results[variant_name] = {\n                            'buffer': output_buffer.getvalue(),\n                            'size': processed_img.size,\n                            'format': format_type,\n                            'file_size': len(output_buffer.getvalue())\n                        }\n                        \n                    except Exception as e:\n                        results[variant_name] = {'error': str(e)}\n            \n            return results\n        \n        return await loop.run_in_executor(self.executor, _process_all_variants)\n    \n    def _apply_transformations(self, img: Image.Image, config: Dict[str, Any]) -> Image.Image:\n        \"\"\"Apply image transformations based on configuration\"\"\"\n        \n        # Resize/thumbnail\n        if 'size' in config:\n            size = config['size']\n            if isinstance(size, (list, tuple)) and len(size) == 2:\n                if config.get('crop', False):\n                    img = ImageOps.fit(img, size, Image.Resampling.LANCZOS)\n                else:\n                    img.thumbnail(size, Image.Resampling.LANCZOS)\n        \n        # Rotation\n        if 'rotate' in config:\n            img = img.rotate(config['rotate'], expand=True)\n        \n        # Filters\n        filters = config.get('filters', [])\n        for filter_config in filters:\n            filter_type = filter_config.get('type')\n            \n            if filter_type == 'blur':\n                radius = filter_config.get('radius', 2)\n                img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n            \n            elif filter_type == 'sharpen':\n                img = img.filter(ImageFilter.SHARPEN)\n            \n            elif filter_type == 'enhance':\n                factor = filter_config.get('factor', 1.2)\n                enhancer = ImageEnhance.Sharpness(img)\n                img = enhancer.enhance(factor)\n        \n        # Color adjustments\n        if 'brightness' in config:\n            enhancer = ImageEnhance.Brightness(img)\n            img = enhancer.enhance(config['brightness'])\n        \n        if 'contrast' in config:\n            enhancer = ImageEnhance.Contrast(img)\n            img = enhancer.enhance(config['contrast'])\n        \n        if 'saturation' in config:\n            enhancer = ImageEnhance.Color(img)\n            img = enhancer.enhance(config['saturation'])\n        \n        return img\n    \n    async def create_responsive_images(self, image_path: str, base_name: str) -> Dict[str, Any]:\n        \"\"\"Create responsive image set for web use\"\"\"\n        \n        variants = {\n            'thumbnail': {\n                'size': (150, 150),\n                'crop': True,\n                'quality': 80,\n                'format': 'JPEG'\n            },\n            'small': {\n                'size': (400, 300),\n                'quality': 85,\n                'format': 'JPEG'\n            },\n            'medium': {\n                'size': (800, 600),\n                'quality': 85,\n                'format': 'JPEG'\n            },\n            'large': {\n                'size': (1200, 900),\n                'quality': 90,\n                'format': 'JPEG'\n            },\n            'webp_small': {\n                'size': (400, 300),\n                'quality': 80,\n                'format': 'WebP'\n            },\n            'webp_medium': {\n                'size': (800, 600),\n                'quality': 80,\n                'format': 'WebP'\n            }\n        }\n        \n        results = await self.process_image_variants(image_path, variants)\n        \n        # Upload all variants to S3\n        upload_tasks = []\n        for variant_name, variant_data in results.items():\n            if 'error' not in variant_data:\n                # Create temp file and upload\n                temp_path = f\"/tmp/{base_name}_{variant_name}.{variant_data['format'].lower()}\"\n                \n                with open(temp_path, 'wb') as f:\n                    f.write(variant_data['buffer'])\n                \n                s3_key = f\"images/{base_name}/{variant_name}.{variant_data['format'].lower()}\"\n                upload_task = s3_manager.upload_file(temp_path, s3_key)\n                upload_tasks.append((variant_name, upload_task))\n        \n        # Wait for all uploads\n        upload_results = {}\n        for variant_name, upload_task in upload_tasks:\n            upload_result = await upload_task\n            upload_results[variant_name] = upload_result\n        \n        return {\n            'variants': results,\n            'uploads': upload_results,\n            'responsive_set': self._create_responsive_html(upload_results)\n        }\n    \n    def _create_responsive_html(self, upload_results: Dict[str, Any]) -> str:\n        \"\"\"Generate HTML for responsive images\"\"\"\n        \n        # Create srcset for different sizes\n        jpeg_variants = []\n        webp_variants = []\n        \n        for variant_name, upload_result in upload_results.items():\n            if upload_result.get('success'):\n                url = upload_result['cdn_url']\n                \n                if 'webp' in variant_name:\n                    if 'small' in variant_name:\n                        webp_variants.append(f\"{url} 400w\")\n                    elif 'medium' in variant_name:\n                        webp_variants.append(f\"{url} 800w\")\n                else:\n                    if 'small' in variant_name:\n                        jpeg_variants.append(f\"{url} 400w\")\n                    elif 'medium' in variant_name:\n                        jpeg_variants.append(f\"{url} 800w\")\n                    elif 'large' in variant_name:\n                        jpeg_variants.append(f\"{url} 1200w\")\n        \n        # Generate picture element\n        html = \"<picture>\\n\"\n        \n        if webp_variants:\n            html += f'  <source srcset=\"{\", \".join(webp_variants)}\" type=\"image/webp\">\\n'\n        \n        if jpeg_variants:\n            html += f'  <img srcset=\"{\", \".join(jpeg_variants)}\" '\n            html += 'sizes=\"(max-width: 400px) 100vw, (max-width: 800px) 50vw, 33vw\" '\n            html += f'src=\"{upload_results.get(\"medium\", {}).get(\"cdn_url\", \"\")}\" '\n            html += 'alt=\"Responsive image\" loading=\"lazy\">\\n'\n        \n        html += \"</picture>\"\n        \n        return html\n\n# Initialize image processor\nimage_processor = ImageProcessor()\n```\n\n---\n\n### 4Ô∏è‚É£ File Management System (10 minutes)\n\n**Complete File Management API:**\n\n```python\n# file_management.py\nfrom sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom datetime import datetime\nfrom typing import List, Optional\nimport json\n\n# Database models\nBase = declarative_base()\n\nclass FileRecord(Base):\n    __tablename__ = \"files\"\n    \n    id = Column(String, primary_key=True)\n    user_id = Column(Integer, nullable=False)\n    original_filename = Column(String(255), nullable=False)\n    stored_filename = Column(String(255), nullable=False)\n    s3_key = Column(String(512))\n    file_type = Column(String(50))\n    mime_type = Column(String(100))\n    file_size = Column(Integer)\n    file_hash = Column(String(64))\n    description = Column(Text)\n    tags = Column(Text)  # JSON array\n    is_public = Column(Boolean, default=False)\n    upload_timestamp = Column(DateTime, default=datetime.utcnow)\n    last_accessed = Column(DateTime)\n    download_count = Column(Integer, default=0)\n    processing_status = Column(String(50), default='pending')\n    metadata = Column(Text)  # JSON\n\nclass FileManager:\n    \"\"\"Complete file management system\"\"\"\n    \n    def __init__(self, db_session: Session):\n        self.db = db_session\n    \n    def create_file_record(self, file_data: Dict[str, Any]) -> FileRecord:\n        \"\"\"Create new file record in database\"\"\"\n        \n        file_record = FileRecord(\n            id=file_data['id'],\n            user_id=file_data['user_id'],\n            original_filename=file_data['original_filename'],\n            stored_filename=file_data['stored_filename'],\n            s3_key=file_data.get('s3_key'),\n            file_type=file_data['file_type'],\n            mime_type=file_data['mime_type'],\n            file_size=file_data['file_size'],\n            file_hash=file_data['file_hash'],\n            description=file_data.get('description'),\n            tags=json.dumps(file_data.get('tags', [])),\n            is_public=file_data.get('is_public', False),\n            metadata=json.dumps(file_data.get('metadata', {}))\n        )\n        \n        self.db.add(file_record)\n        self.db.commit()\n        self.db.refresh(file_record)\n        \n        return file_record\n    \n    def get_user_files(\n        self,\n        user_id: int,\n        file_type: Optional[str] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> List[FileRecord]:\n        \"\"\"Get paginated file list for user\"\"\"\n        \n        query = self.db.query(FileRecord).filter(FileRecord.user_id == user_id)\n        \n        if file_type:\n            query = query.filter(FileRecord.file_type == file_type)\n        \n        return query.order_by(FileRecord.upload_timestamp.desc()).offset(offset).limit(limit).all()\n    \n    def search_files(\n        self,\n        user_id: int,\n        search_term: str,\n        file_type: Optional[str] = None\n    ) -> List[FileRecord]:\n        \"\"\"Search files by filename or description\"\"\"\n        \n        query = self.db.query(FileRecord).filter(FileRecord.user_id == user_id)\n        \n        # Search in filename and description\n        search_filter = (\n            FileRecord.original_filename.ilike(f\"%{search_term}%\") |\n            FileRecord.description.ilike(f\"%{search_term}%\")\n        )\n        query = query.filter(search_filter)\n        \n        if file_type:\n            query = query.filter(FileRecord.file_type == file_type)\n        \n        return query.order_by(FileRecord.upload_timestamp.desc()).all()\n    \n    def update_file_access(self, file_id: str):\n        \"\"\"Update file access statistics\"\"\"\n        \n        file_record = self.db.query(FileRecord).filter(FileRecord.id == file_id).first()\n        if file_record:\n            file_record.last_accessed = datetime.utcnow()\n            file_record.download_count += 1\n            self.db.commit()\n    \n    def delete_file_record(self, file_id: str, user_id: int) -> bool:\n        \"\"\"Delete file record (user must own the file)\"\"\"\n        \n        file_record = self.db.query(FileRecord).filter(\n            FileRecord.id == file_id,\n            FileRecord.user_id == user_id\n        ).first()\n        \n        if file_record:\n            self.db.delete(file_record)\n            self.db.commit()\n            return True\n        \n        return False\n\n# File management endpoints\n@app.get(\"/files/user/{user_id}\")\nasync def get_user_files(\n    user_id: int,\n    file_type: Optional[str] = None,\n    page: int = 1,\n    per_page: int = 20,\n    db: Session = Depends(get_db),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Get paginated file list for user\"\"\"\n    \n    file_manager = FileManager(db)\n    \n    offset = (page - 1) * per_page\n    files = file_manager.get_user_files(user_id, file_type, per_page, offset)\n    \n    # Convert to response format\n    file_list = []\n    for file_record in files:\n        file_data = {\n            'id': file_record.id,\n            'filename': file_record.original_filename,\n            'file_type': file_record.file_type,\n            'mime_type': file_record.mime_type,\n            'file_size': file_record.file_size,\n            'description': file_record.description,\n            'upload_date': file_record.upload_timestamp.isoformat(),\n            'download_count': file_record.download_count,\n            'is_public': file_record.is_public\n        }\n        \n        # Add download URL\n        if file_record.s3_key:\n            presigned_result = await s3_manager.generate_presigned_url(file_record.s3_key)\n            if presigned_result['success']:\n                file_data['download_url'] = presigned_result['presigned_url']\n        \n        file_list.append(file_data)\n    \n    return {\n        'files': file_list,\n        'page': page,\n        'per_page': per_page,\n        'total_files': len(file_list)\n    }\n\n@app.get(\"/files/search\")\nasync def search_files(\n    user_id: int,\n    q: str,\n    file_type: Optional[str] = None,\n    db: Session = Depends(get_db),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Search user files\"\"\"\n    \n    if len(q) < 3:\n        raise HTTPException(status_code=400, detail=\"Search term must be at least 3 characters\")\n    \n    file_manager = FileManager(db)\n    files = file_manager.search_files(user_id, q, file_type)\n    \n    search_results = []\n    for file_record in files:\n        search_results.append({\n            'id': file_record.id,\n            'filename': file_record.original_filename,\n            'description': file_record.description,\n            'file_type': file_record.file_type,\n            'upload_date': file_record.upload_timestamp.isoformat(),\n            'relevance_score': 1.0  # Could implement proper relevance scoring\n        })\n    \n    return {\n        'results': search_results,\n        'query': q,\n        'total_results': len(search_results)\n    }\n\n@app.post(\"/files/{file_id}/process-images\")\nasync def process_file_images(\n    file_id: str,\n    user_id: int,\n    db: Session = Depends(get_db),\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n):\n    \"\"\"Process uploaded images to create responsive variants\"\"\"\n    \n    file_manager = FileManager(db)\n    \n    # Get file record\n    file_record = db.query(FileRecord).filter(\n        FileRecord.id == file_id,\n        FileRecord.user_id == user_id,\n        FileRecord.file_type == 'image'\n    ).first()\n    \n    if not file_record:\n        raise HTTPException(status_code=404, detail=\"Image file not found\")\n    \n    if not file_record.s3_key:\n        raise HTTPException(status_code=400, detail=\"File not stored in S3\")\n    \n    # Download original file temporarily\n    temp_dir = Path(\"/tmp/image_processing\")\n    temp_dir.mkdir(exist_ok=True)\n    \n    original_path = temp_dir / f\"original_{file_id}\"\n    \n    # Download from S3 (simplified - in production use proper S3 download)\n    # This would download the S3 object to local temp file\n    \n    # Process images\n    base_name = f\"{file_id}_{int(datetime.now().timestamp())}\"\n    processing_result = await image_processor.create_responsive_images(\n        str(original_path), \n        base_name\n    )\n    \n    # Update file record with processing results\n    metadata = json.loads(file_record.metadata or '{}')\n    metadata['responsive_images'] = processing_result['uploads']\n    metadata['responsive_html'] = processing_result['responsive_set']\n    \n    file_record.metadata = json.dumps(metadata)\n    file_record.processing_status = 'completed'\n    db.commit()\n    \n    # Cleanup temp file\n    original_path.unlink(missing_ok=True)\n    \n    return {\n        'message': 'Image processing completed',\n        'file_id': file_id,\n        'variants_created': len(processing_result['variants']),\n        'responsive_html': processing_result['responsive_set']\n    }\n```\n\n---\n\n### üè† Homework: Build File Management System\n\n**Task:** Create a complete file management system with S3 integration\n\n```python\n# Build a system with:\n# 1. Secure file upload with comprehensive validation\n# 2. S3 storage with multipart uploads for large files\n# 3. Image processing pipeline with responsive variants\n# 4. File organization (folders, tags, search)\n# 5. User permissions and file sharing\n# 6. File versioning and backup\n# 7. CDN integration with CloudFront\n# 8. File analytics (downloads, popular files)\n\n# Requirements:\n# - Handle files up to 1GB\n# - Support image, video, and document types\n# - Automatic thumbnail generation\n# - Virus scanning integration\n# - File deduplication\n# - Bandwidth optimization\n\n# Bonus:\n# - File compression and optimization\n# - Automatic backups to multiple regions\n# - File expiration and cleanup\n# - Integration with office document preview\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ File Security = Always validate uploads thoroughly\n‚úÖ S3 Integration = Scalable cloud storage with CDN\n‚úÖ Streaming Uploads = Handle large files efficiently  \n‚úÖ Image Processing = Create responsive variants automatically\n‚úÖ File Management = Complete CRUD with search and analytics\n\n---\n\n<a name=\"hour-36\"></a>\n## üìÖ Hour 36: Security Best Practices\n\n### üéØ Learning Objectives\n- Implement OWASP Top 10 security measures\n- Handle authentication and authorization securely\n- Protect against common web vulnerabilities\n- Implement security headers and middleware\n- Create secure API endpoints with proper validation\n\n### üìñ What to Teach\n\n**\"Today we build fortress-level security into our applications - because security is not optional!\"**\n\n---\n\n### 1Ô∏è‚É£ OWASP Top 10 Implementation (15 minutes)\n\n**1. Injection Prevention (SQL, NoSQL, Command Injection):**\n\n```python\n# secure_database.py\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import text\nimport bleach\nimport re\nfrom typing import Any, List, Dict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass SecureDatabase:\n    \"\"\"Database operations with injection prevention\"\"\"\n    \n    def __init__(self, db_session: Session):\n        self.db = db_session\n    \n    def safe_query_with_params(self, query: str, params: Dict[str, Any]) -> List[Any]:\n        \"\"\"Execute parameterized queries safely\"\"\"\n        \n        try:\n            # Use SQLAlchemy's text() with bound parameters\n            # This automatically prevents SQL injection\n            result = self.db.execute(text(query), params)\n            return result.fetchall()\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Database query failed: {str(e)}\")\n            raise ValueError(\"Database query failed\")\n    \n    def sanitize_search_input(self, search_term: str) -> str:\n        \"\"\"Sanitize user input for search queries\"\"\"\n        \n        if not search_term:\n            return \"\"\n        \n        # Remove potential SQL injection patterns\n        dangerous_patterns = [\n            r\"(\\%27)|(\\')|(\\-\\-)|(\\%23)|(#)\",  # SQL comment patterns\n            r\"((\\%3D)|(=))[^\\n]*((\\%27)|(\\')|(\\-\\-)|(\\%3B)|(;))\",  # SQL injection patterns\n            r\"w*((\\%27)|(\\'))((\\%6F)|o|(\\%4F))((\\%72)|r|(\\%52))\",  # 'or' patterns\n            r\"((\\%27)|(\\'))union\",  # UNION patterns\n        ]\n        \n        for pattern in dangerous_patterns:\n            if re.search(pattern, search_term, re.IGNORECASE):\n                logger.warning(f\"‚ö†Ô∏è Potential SQL injection attempt blocked: {search_term}\")\n                raise ValueError(\"Invalid search parameters\")\n        \n        # Sanitize and limit length\n        sanitized = bleach.clean(search_term, tags=[], strip=True)\n        return sanitized[:100]  # Limit length\n    \n    def get_user_posts_safe(self, user_id: int, search_term: str = None) -> List[Any]:\n        \"\"\"Safe user posts retrieval with optional search\"\"\"\n        \n        # Validate user_id\n        if not isinstance(user_id, int) or user_id <= 0:\n            raise ValueError(\"Invalid user ID\")\n        \n        base_query = \"\"\"\n            SELECT p.id, p.title, p.content, p.created_at \n            FROM posts p \n            WHERE p.user_id = :user_id AND p.deleted_at IS NULL\n        \"\"\"\n        \n        params = {'user_id': user_id}\n        \n        if search_term:\n            sanitized_search = self.sanitize_search_input(search_term)\n            base_query += \" AND (p.title ILIKE :search OR p.content ILIKE :search)\"\n            params['search'] = f\"%{sanitized_search}%\"\n        \n        base_query += \" ORDER BY p.created_at DESC LIMIT 50\"\n        \n        return self.safe_query_with_params(base_query, params)\n\n# Command injection prevention\nclass SecureSystemOperations:\n    \"\"\"Secure system operations without command injection\"\"\"\n    \n    @staticmethod\n    def validate_filename(filename: str) -> str:\n        \"\"\"Validate and sanitize filename\"\"\"\n        \n        if not filename:\n            raise ValueError(\"Filename cannot be empty\")\n        \n        # Remove dangerous characters\n        dangerous_chars = ['..', '/', '\\\\', '|', ';', '&', '$', '>', '<', '`', '!']\n        for char in dangerous_chars:\n            if char in filename:\n                raise ValueError(f\"Filename contains dangerous character: {char}\")\n        \n        # Only allow alphanumeric, dots, hyphens, underscores\n        if not re.match(r'^[a-zA-Z0-9._-]+$', filename):\n            raise ValueError(\"Filename contains invalid characters\")\n        \n        # Limit length\n        if len(filename) > 255:\n            raise ValueError(\"Filename too long\")\n        \n        return filename\n    \n    @staticmethod\n    def safe_file_operations(operation: str, filename: str) -> Dict[str, Any]:\n        \"\"\"Safe file operations without command injection\"\"\"\n        \n        validated_filename = SecureSystemOperations.validate_filename(filename)\n        \n        import os\n        import subprocess\n        from pathlib import Path\n        \n        # Define safe operations with whitelisted commands\n        safe_operations = {\n            'get_file_info': ['stat', '-c', '%s %Y'],  # size and modification time\n            'check_permissions': ['test', '-r'],  # readable test\n            'get_file_type': ['file', '--mime-type', '-b']  # MIME type detection\n        }\n        \n        if operation not in safe_operations:\n            raise ValueError(f\"Operation not allowed: {operation}\")\n        \n        try:\n            # Build command safely\n            cmd = safe_operations[operation] + [validated_filename]\n            \n            # Execute with security constraints\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                timeout=10,  # Prevent hanging\n                cwd='/safe/directory',  # Restrict working directory\n                env={'PATH': '/usr/bin:/bin'}  # Minimal environment\n            )\n            \n            return {\n                'success': result.returncode == 0,\n                'output': result.stdout.strip(),\n                'error': result.stderr.strip() if result.stderr else None\n            }\n            \n        except subprocess.TimeoutExpired:\n            logger.error(f\"‚ùå Command timeout for operation: {operation}\")\n            return {'success': False, 'error': 'Operation timed out'}\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Safe operation failed: {str(e)}\")\n            return {'success': False, 'error': str(e)}\n```\n\n**2. Broken Authentication Prevention:**\n\n```python\n# secure_auth.py\nfrom fastapi import HTTPException, Depends, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom passlib.context import CryptContext\nfrom jose import JWTError, jwt\nfrom datetime import datetime, timedelta\nimport bcrypt\nimport secrets\nimport redis\nimport hashlib\nfrom typing import Optional, Dict, Any\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass SecureAuthentication:\n    \"\"\"Secure authentication with best practices\"\"\"\n    \n    def __init__(self):\n        # Password hashing\n        self.pwd_context = CryptContext(\n            schemes=[\"bcrypt\"],\n            deprecated=\"auto\",\n            bcrypt__rounds=12  # Strong hashing rounds\n        )\n        \n        # JWT configuration\n        self.SECRET_KEY = os.getenv(\"JWT_SECRET_KEY\", secrets.token_urlsafe(32))\n        self.ALGORITHM = \"HS256\"\n        self.ACCESS_TOKEN_EXPIRE_MINUTES = 30\n        self.REFRESH_TOKEN_EXPIRE_DAYS = 7\n        \n        # Rate limiting\n        self.redis_client = redis.Redis(host='localhost', port=6379, db=1)\n        \n        # Password policy\n        self.password_policy = {\n            'min_length': 8,\n            'max_length': 128,\n            'require_uppercase': True,\n            'require_lowercase': True,\n            'require_numbers': True,\n            'require_special': True,\n            'special_chars': \"!@#$%^&*()_+-=[]{}|;:,.<>?\"\n        }\n    \n    def validate_password_strength(self, password: str) -> Dict[str, Any]:\n        \"\"\"Comprehensive password strength validation\"\"\"\n        \n        validation_result = {\n            'valid': False,\n            'errors': [],\n            'strength_score': 0\n        }\n        \n        # Length check\n        if len(password) < self.password_policy['min_length']:\n            validation_result['errors'].append(\n                f\"Password must be at least {self.password_policy['min_length']} characters\"\n            )\n        elif len(password) > self.password_policy['max_length']:\n            validation_result['errors'].append(\n                f\"Password must be less than {self.password_policy['max_length']} characters\"\n            )\n        else:\n            validation_result['strength_score'] += 25\n        \n        # Character requirements\n        has_upper = any(c.isupper() for c in password)\n        has_lower = any(c.islower() for c in password)\n        has_digit = any(c.isdigit() for c in password)\n        has_special = any(c in self.password_policy['special_chars'] for c in password)\n        \n        if self.password_policy['require_uppercase'] and not has_upper:\n            validation_result['errors'].append(\"Password must contain uppercase letters\")\n        elif has_upper:\n            validation_result['strength_score'] += 20\n        \n        if self.password_policy['require_lowercase'] and not has_lower:\n            validation_result['errors'].append(\"Password must contain lowercase letters\")\n        elif has_lower:\n            validation_result['strength_score'] += 15\n        \n        if self.password_policy['require_numbers'] and not has_digit:\n            validation_result['errors'].append(\"Password must contain numbers\")\n        elif has_digit:\n            validation_result['strength_score'] += 20\n        \n        if self.password_policy['require_special'] and not has_special:\n            validation_result['errors'].append(\"Password must contain special characters\")\n        elif has_special:\n            validation_result['strength_score'] += 20\n        \n        # Common password check (simplified)\n        common_passwords = [\n            'password', '123456', 'password123', 'admin', 'qwerty',\n            'letmein', 'welcome', 'monkey', '1234567890'\n        ]\n        \n        if password.lower() in common_passwords:\n            validation_result['errors'].append(\"Password is too common\")\n            validation_result['strength_score'] = 0\n        \n        # Set validity\n        validation_result['valid'] = len(validation_result['errors']) == 0\n        \n        return validation_result\n    \n    def hash_password(self, password: str) -> str:\n        \"\"\"Hash password securely with salt\"\"\"\n        \n        # Validate password strength first\n        validation = self.validate_password_strength(password)\n        if not validation['valid']:\n            raise ValueError(f\"Password validation failed: {', '.join(validation['errors'])}\")\n        \n        return self.pwd_context.hash(password)\n    \n    def verify_password(self, plain_password: str, hashed_password: str) -> bool:\n        \"\"\"Verify password against hash\"\"\"\n        return self.pwd_context.verify(plain_password, hashed_password)\n    \n    def check_rate_limit(self, identifier: str, action: str, limit: int, window: int) -> bool:\n        \"\"\"Rate limiting for authentication attempts\"\"\"\n        \n        key = f\"rate_limit:{action}:{identifier}\"\n        current_time = int(time.time())\n        window_start = current_time - window\n        \n        try:\n            # Remove old entries\n            self.redis_client.zremrangebyscore(key, 0, window_start)\n            \n            # Count recent attempts\n            recent_attempts = self.redis_client.zcard(key)\n            \n            if recent_attempts >= limit:\n                logger.warning(f\"‚ö†Ô∏è Rate limit exceeded for {identifier} on {action}\")\n                return False\n            \n            # Add current attempt\n            self.redis_client.zadd(key, {str(current_time): current_time})\n            self.redis_client.expire(key, window)\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Rate limiting error: {str(e)}\")\n            return True  # Allow on Redis errors (fail open)\n    \n    def create_access_token(self, data: Dict[str, Any]) -> str:\n        \"\"\"Create JWT access token\"\"\"\n        \n        to_encode = data.copy()\n        expire = datetime.utcnow() + timedelta(minutes=self.ACCESS_TOKEN_EXPIRE_MINUTES)\n        to_encode.update({\"exp\": expire, \"type\": \"access\"})\n        \n        return jwt.encode(to_encode, self.SECRET_KEY, algorithm=self.ALGORITHM)\n    \n    def create_refresh_token(self, data: Dict[str, Any]) -> str:\n        \"\"\"Create JWT refresh token\"\"\"\n        \n        to_encode = data.copy()\n        expire = datetime.utcnow() + timedelta(days=self.REFRESH_TOKEN_EXPIRE_DAYS)\n        to_encode.update({\"exp\": expire, \"type\": \"refresh\"})\n        \n        return jwt.encode(to_encode, self.SECRET_KEY, algorithm=self.ALGORITHM)\n    \n    def verify_token(self, token: str, token_type: str = \"access\") -> Dict[str, Any]:\n        \"\"\"Verify and decode JWT token\"\"\"\n        \n        try:\n            payload = jwt.decode(token, self.SECRET_KEY, algorithms=[self.ALGORITHM])\n            \n            # Check token type\n            if payload.get(\"type\") != token_type:\n                raise JWTError(\"Invalid token type\")\n            \n            return payload\n            \n        except JWTError as e:\n            logger.warning(f\"‚ö†Ô∏è Token verification failed: {str(e)}\")\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Invalid authentication credentials\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n    \n    def revoke_token(self, token: str) -> bool:\n        \"\"\"Revoke token (add to blacklist)\"\"\"\n        \n        try:\n            payload = jwt.decode(token, self.SECRET_KEY, algorithms=[self.ALGORITHM])\n            jti = payload.get(\"jti\", hashlib.sha256(token.encode()).hexdigest())\n            exp = payload.get(\"exp\", int(time.time()) + 3600)\n            \n            # Add to blacklist with expiration\n            self.redis_client.setex(f\"blacklist:{jti}\", exp - int(time.time()), \"revoked\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Token revocation failed: {str(e)}\")\n            return False\n    \n    def is_token_blacklisted(self, token: str) -> bool:\n        \"\"\"Check if token is blacklisted\"\"\"\n        \n        try:\n            payload = jwt.decode(token, self.SECRET_KEY, algorithms=[self.ALGORITHM])\n            jti = payload.get(\"jti\", hashlib.sha256(token.encode()).hexdigest())\n            \n            return self.redis_client.exists(f\"blacklist:{jti}\")\n            \n        except:\n            return False\n\n# Authentication endpoints\nauth_handler = SecureAuthentication()\nsecurity = HTTPBearer()\n\n@app.post(\"/auth/register\")\nasync def register_user(\n    username: str = Form(...),\n    email: str = Form(...),\n    password: str = Form(...),\n    request: Request = None\n):\n    \"\"\"Secure user registration\"\"\"\n    \n    # Rate limiting\n    client_ip = request.client.host\n    if not auth_handler.check_rate_limit(client_ip, \"register\", 5, 300):  # 5 attempts per 5 minutes\n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=\"Too many registration attempts. Please try again later.\"\n        )\n    \n    # Validate input\n    if len(username) < 3 or len(username) > 50:\n        raise HTTPException(status_code=400, detail=\"Username must be 3-50 characters\")\n    \n    if not re.match(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\", email):\n        raise HTTPException(status_code=400, detail=\"Invalid email format\")\n    \n    # Hash password securely\n    try:\n        hashed_password = auth_handler.hash_password(password)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    \n    # TODO: Save user to database\n    # user = create_user(username, email, hashed_password)\n    \n    return {\n        \"message\": \"User registered successfully\",\n        \"username\": username,\n        \"email\": email\n    }\n\n@app.post(\"/auth/login\")\nasync def login_user(\n    username: str = Form(...),\n    password: str = Form(...),\n    request: Request = None\n):\n    \"\"\"Secure user login\"\"\"\n    \n    # Rate limiting\n    client_ip = request.client.host\n    if not auth_handler.check_rate_limit(client_ip, \"login\", 5, 300):  # 5 attempts per 5 minutes\n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=\"Too many login attempts. Please try again later.\"\n        )\n    \n    # Additional rate limiting per user\n    if not auth_handler.check_rate_limit(username, \"user_login\", 3, 600):  # 3 per 10 minutes per user\n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=\"Too many login attempts for this account. Please try again later.\"\n        )\n    \n    # TODO: Get user from database\n    # user = get_user_by_username(username)\n    \n    # Simulate user lookup (replace with actual database call)\n    user = {\n        'id': 1,\n        'username': username,\n        'email': 'user@example.com',\n        'hashed_password': auth_handler.hash_password('correct_password'),  # Example\n        'is_active': True,\n        'is_verified': True\n    }\n    \n    # Verify user exists and is active\n    if not user or not user['is_active']:\n        # Use generic error to prevent user enumeration\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\"\n        )\n    \n    # Verify password\n    if not auth_handler.verify_password(password, user['hashed_password']):\n        logger.warning(f\"‚ö†Ô∏è Failed login attempt for user: {username} from IP: {client_ip}\")\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\"\n        )\n    \n    # Create tokens\n    token_data = {\n        \"sub\": str(user['id']),\n        \"username\": user['username'],\n        \"jti\": secrets.token_urlsafe(32)  # Unique token ID for revocation\n    }\n    \n    access_token = auth_handler.create_access_token(token_data)\n    refresh_token = auth_handler.create_refresh_token(token_data)\n    \n    logger.info(f\"‚úÖ Successful login for user: {username} from IP: {client_ip}\")\n    \n    return {\n        \"access_token\": access_token,\n        \"refresh_token\": refresh_token,\n        \"token_type\": \"bearer\",\n        \"expires_in\": auth_handler.ACCESS_TOKEN_EXPIRE_MINUTES * 60\n    }\n\nasync def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Get current authenticated user\"\"\"\n    \n    token = credentials.credentials\n    \n    # Check if token is blacklisted\n    if auth_handler.is_token_blacklisted(token):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Token has been revoked\"\n        )\n    \n    # Verify token\n    payload = auth_handler.verify_token(token, \"access\")\n    \n    user_id = payload.get(\"sub\")\n    if not user_id:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token payload\"\n        )\n    \n    # TODO: Get user from database\n    # user = get_user_by_id(user_id)\n    \n    # Simulate user lookup\n    user = {\n        'id': int(user_id),\n        'username': payload.get('username'),\n        'is_active': True\n    }\n    \n    if not user or not user['is_active']:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"User not found or inactive\"\n        )\n    \n    return user\n```\n\n**3. Sensitive Data Exposure Prevention:**\n\n```python\n# data_protection.py\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport base64\nimport os\nimport json\nfrom typing import Any, Dict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataProtection:\n    \"\"\"Protect sensitive data with encryption\"\"\"\n    \n    def __init__(self):\n        # Get encryption key from environment or generate\n        self.encryption_key = self._get_or_create_key()\n        self.fernet = Fernet(self.encryption_key)\n        \n        # Fields that should be encrypted\n        self.encrypted_fields = {\n            'ssn', 'social_security_number',\n            'credit_card', 'card_number',\n            'phone_number', 'phone',\n            'personal_data', 'sensitive_info'\n        }\n        \n        # Fields that should be masked in logs\n        self.masked_fields = {\n            'password', 'token', 'secret',\n            'api_key', 'private_key',\n            'ssn', 'credit_card'\n        }\n    \n    def _get_or_create_key(self) -> bytes:\n        \"\"\"Get encryption key from environment or create new one\"\"\"\n        \n        key_env = os.getenv('DATA_ENCRYPTION_KEY')\n        if key_env:\n            return base64.urlsafe_b64decode(key_env.encode())\n        \n        # Generate new key (in production, store securely)\n        password = os.getenv('ENCRYPTION_PASSWORD', 'default-password').encode()\n        salt = os.getenv('ENCRYPTION_SALT', 'default-salt').encode()\n        \n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=salt,\n            iterations=100000,\n        )\n        \n        key = base64.urlsafe_b64encode(kdf.derive(password))\n        logger.warning(\"‚ö†Ô∏è Generated new encryption key - store securely!\")\n        \n        return key\n    \n    def encrypt_sensitive_data(self, data: str) -> str:\n        \"\"\"Encrypt sensitive data\"\"\"\n        \n        if not data:\n            return data\n        \n        try:\n            encrypted_data = self.fernet.encrypt(data.encode())\n            return base64.urlsafe_b64encode(encrypted_data).decode()\n        except Exception as e:\n            logger.error(f\"‚ùå Encryption failed: {str(e)}\")\n            raise ValueError(\"Failed to encrypt data\")\n    \n    def decrypt_sensitive_data(self, encrypted_data: str) -> str:\n        \"\"\"Decrypt sensitive data\"\"\"\n        \n        if not encrypted_data:\n            return encrypted_data\n        \n        try:\n            decoded_data = base64.urlsafe_b64decode(encrypted_data.encode())\n            decrypted_data = self.fernet.decrypt(decoded_data)\n            return decrypted_data.decode()\n        except Exception as e:\n            logger.error(f\"‚ùå Decryption failed: {str(e)}\")\n            raise ValueError(\"Failed to decrypt data\")\n    \n    def process_user_data(self, user_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process user data with automatic encryption\"\"\"\n        \n        processed_data = user_data.copy()\n        \n        for field, value in user_data.items():\n            if field.lower() in self.encrypted_fields and value:\n                processed_data[field] = self.encrypt_sensitive_data(str(value))\n                logger.info(f\"üîí Encrypted field: {field}\")\n        \n        return processed_data\n    \n    def mask_sensitive_logs(self, log_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Mask sensitive data in logs\"\"\"\n        \n        masked_data = {}\n        \n        for key, value in log_data.items():\n            if key.lower() in self.masked_fields:\n                if isinstance(value, str) and len(value) > 4:\n                    # Show only first 2 and last 2 characters\n                    masked_data[key] = f\"{value[:2]}***{value[-2:]}\"\n                else:\n                    masked_data[key] = \"***\"\n            else:\n                masked_data[key] = value\n        \n        return masked_data\n    \n    def secure_compare(self, a: str, b: str) -> bool:\n        \"\"\"Timing-safe string comparison\"\"\"\n        \n        import hmac\n        return hmac.compare_digest(a.encode('utf-8'), b.encode('utf-8'))\n\n# Data protection middleware\ndata_protector = DataProtection()\n\nclass SecurityHeaders:\n    \"\"\"Security headers middleware\"\"\"\n    \n    @staticmethod\n    def get_security_headers() -> Dict[str, str]:\n        \"\"\"Get comprehensive security headers\"\"\"\n        \n        return {\n            # XSS Protection\n            \"X-Content-Type-Options\": \"nosniff\",\n            \"X-Frame-Options\": \"DENY\",\n            \"X-XSS-Protection\": \"1; mode=block\",\n            \n            # HTTPS enforcement\n            \"Strict-Transport-Security\": \"max-age=31536000; includeSubDomains\",\n            \n            # Content Security Policy\n            \"Content-Security-Policy\": (\n                \"default-src 'self'; \"\n                \"script-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com; \"\n                \"style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; \"\n                \"font-src 'self' https://fonts.gstatic.com; \"\n                \"img-src 'self' data: https:; \"\n                \"connect-src 'self'; \"\n                \"frame-ancestors 'none';\"\n            ),\n            \n            # Referrer policy\n            \"Referrer-Policy\": \"strict-origin-when-cross-origin\",\n            \n            # Permissions policy\n            \"Permissions-Policy\": (\n                \"geolocation=(), \"\n                \"microphone=(), \"\n                \"camera=()\"\n            ),\n            \n            # Cache control for sensitive pages\n            \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n            \"Pragma\": \"no-cache\",\n            \"Expires\": \"0\"\n        }\n\n@app.middleware(\"http\")\nasync def add_security_headers(request: Request, call_next):\n    \"\"\"Add security headers to all responses\"\"\"\n    \n    response = await call_next(request)\n    \n    # Add security headers\n    security_headers = SecurityHeaders.get_security_headers()\n    for header, value in security_headers.items():\n        response.headers[header] = value\n    \n    return response\n```\n\n---\n\n### 2Ô∏è‚É£ Input Validation and Sanitization (10 minutes)\n\n**Comprehensive Input Validation:**\n\n```python\n# input_validation.py\nfrom pydantic import BaseModel, validator, Field\nfrom typing import Optional, List, Dict, Any\nimport re\nimport html\nimport bleach\nfrom urllib.parse import urlparse\nimport ipaddress\n\nclass SecureInputValidator:\n    \"\"\"Comprehensive input validation and sanitization\"\"\"\n    \n    @staticmethod\n    def sanitize_html_input(content: str, allowed_tags: List[str] = None) -> str:\n        \"\"\"Sanitize HTML content to prevent XSS\"\"\"\n        \n        if not content:\n            return \"\"\n        \n        # Default allowed tags for rich text\n        default_allowed_tags = [\n            'p', 'br', 'strong', 'em', 'u', 'ol', 'ul', 'li',\n            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n            'blockquote', 'code', 'pre'\n        ]\n        \n        allowed_tags = allowed_tags or default_allowed_tags\n        \n        # Allowed attributes for tags\n        allowed_attributes = {\n            'a': ['href', 'title'],\n            'img': ['src', 'alt', 'width', 'height'],\n            'code': ['class']\n        }\n        \n        # Clean HTML with bleach\n        cleaned_content = bleach.clean(\n            content,\n            tags=allowed_tags,\n            attributes=allowed_attributes,\n            strip=True  # Remove disallowed tags entirely\n        )\n        \n        return cleaned_content\n    \n    @staticmethod\n    def validate_email(email: str) -> bool:\n        \"\"\"Validate email address format\"\"\"\n        \n        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        return bool(re.match(email_pattern, email))\n    \n    @staticmethod\n    def validate_url(url: str, allowed_schemes: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Validate URL and check for security issues\"\"\"\n        \n        if not url:\n            return {'valid': False, 'error': 'URL is empty'}\n        \n        allowed_schemes = allowed_schemes or ['http', 'https']\n        \n        try:\n            parsed = urlparse(url)\n            \n            # Check scheme\n            if parsed.scheme not in allowed_schemes:\n                return {\n                    'valid': False, \n                    'error': f'Scheme not allowed: {parsed.scheme}'\n                }\n            \n            # Check for localhost/private IPs (prevent SSRF)\n            if parsed.hostname:\n                try:\n                    ip = ipaddress.ip_address(parsed.hostname)\n                    if ip.is_private or ip.is_loopback:\n                        return {\n                            'valid': False,\n                            'error': 'Private/localhost URLs not allowed'\n                        }\n                except ValueError:\n                    # Not an IP address, check hostname\n                    forbidden_hosts = [\n                        'localhost', '127.0.0.1', '0.0.0.0',\n                        'metadata.google.internal',  # Cloud metadata\n                        '169.254.169.254'  # AWS metadata\n                    ]\n                    \n                    if parsed.hostname.lower() in forbidden_hosts:\n                        return {\n                            'valid': False,\n                            'error': 'Forbidden hostname'\n                        }\n            \n            return {\n                'valid': True,\n                'parsed': parsed,\n                'clean_url': parsed.geturl()\n            }\n            \n        except Exception as e:\n            return {'valid': False, 'error': f'Invalid URL: {str(e)}'}\n    \n    @staticmethod\n    def validate_phone_number(phone: str, country_code: str = None) -> Dict[str, Any]:\n        \"\"\"Validate phone number format\"\"\"\n        \n        if not phone:\n            return {'valid': False, 'error': 'Phone number is empty'}\n        \n        # Remove all non-digit characters\n        digits_only = re.sub(r'\\D', '', phone)\n        \n        # Basic validation\n        if len(digits_only) < 10 or len(digits_only) > 15:\n            return {\n                'valid': False,\n                'error': 'Phone number must be 10-15 digits'\n            }\n        \n        # Format validation by country (simplified)\n        patterns = {\n            'US': r'^\\+?1?[2-9]\\d{2}[2-9]\\d{2}\\d{4}$',\n            'UK': r'^\\+?44[1-9]\\d{8,9}$',\n            'international': r'^\\+?[1-9]\\d{10,14}$'\n        }\n        \n        pattern = patterns.get(country_code, patterns['international'])\n        \n        if not re.match(pattern, phone):\n            return {\n                'valid': False,\n                'error': f'Invalid phone format for {country_code or \"international\"}'\n            }\n        \n        return {\n            'valid': True,\n            'formatted': digits_only,\n            'display': f\"+{digits_only}\" if not phone.startswith('+') else phone\n        }\n\n# Pydantic models with validation\nclass SecureUserInput(BaseModel):\n    username: str = Field(..., min_length=3, max_length=50)\n    email: str = Field(..., max_length=254)\n    full_name: Optional[str] = Field(None, max_length=100)\n    bio: Optional[str] = Field(None, max_length=500)\n    website: Optional[str] = Field(None, max_length=200)\n    phone: Optional[str] = Field(None, max_length=20)\n    \n    @validator('username')\n    def validate_username(cls, v):\n        \"\"\"Validate username format\"\"\"\n        \n        # Only alphanumeric, underscore, hyphen allowed\n        if not re.match(r'^[a-zA-Z0-9_-]+$', v):\n            raise ValueError('Username can only contain letters, numbers, underscore, and hyphen')\n        \n        # Check for reserved usernames\n        reserved = [\n            'admin', 'root', 'administrator', 'moderator',\n            'api', 'www', 'mail', 'ftp', 'support'\n        ]\n        \n        if v.lower() in reserved:\n            raise ValueError('Username is reserved')\n        \n        return v.lower()  # Normalize to lowercase\n    \n    @validator('email')\n    def validate_email_format(cls, v):\n        \"\"\"Validate email format\"\"\"\n        \n        if not SecureInputValidator.validate_email(v):\n            raise ValueError('Invalid email format')\n        \n        return v.lower()  # Normalize to lowercase\n    \n    @validator('full_name')\n    def sanitize_full_name(cls, v):\n        \"\"\"Sanitize full name\"\"\"\n        \n        if not v:\n            return v\n        \n        # Remove HTML and dangerous characters\n        sanitized = html.escape(v.strip())\n        \n        # Only allow letters, spaces, hyphens, apostrophes\n        if not re.match(r\"^[a-zA-Z\\s\\-']+$\", sanitized):\n            raise ValueError('Full name contains invalid characters')\n        \n        return sanitized\n    \n    @validator('bio')\n    def sanitize_bio(cls, v):\n        \"\"\"Sanitize bio content\"\"\"\n        \n        if not v:\n            return v\n        \n        # Allow basic formatting but remove dangerous content\n        sanitized = SecureInputValidator.sanitize_html_input(\n            v,\n            allowed_tags=['p', 'br', 'strong', 'em']\n        )\n        \n        return sanitized\n    \n    @validator('website')\n    def validate_website_url(cls, v):\n        \"\"\"Validate website URL\"\"\"\n        \n        if not v:\n            return v\n        \n        url_validation = SecureInputValidator.validate_url(v, ['http', 'https'])\n        \n        if not url_validation['valid']:\n            raise ValueError(f\"Invalid website URL: {url_validation['error']}\")\n        \n        return url_validation['clean_url']\n    \n    @validator('phone')\n    def validate_phone_format(cls, v):\n        \"\"\"Validate phone number\"\"\"\n        \n        if not v:\n            return v\n        \n        phone_validation = SecureInputValidator.validate_phone_number(v)\n        \n        if not phone_validation['valid']:\n            raise ValueError(f\"Invalid phone number: {phone_validation['error']}\")\n        \n        return phone_validation['formatted']\n\nclass SecurePostInput(BaseModel):\n    title: str = Field(..., min_length=1, max_length=200)\n    content: str = Field(..., min_length=1, max_length=10000)\n    tags: Optional[List[str]] = Field(None, max_items=10)\n    category: Optional[str] = Field(None, max_length=50)\n    is_published: bool = False\n    \n    @validator('title')\n    def sanitize_title(cls, v):\n        \"\"\"Sanitize post title\"\"\"\n        \n        # Remove HTML and normalize whitespace\n        sanitized = html.escape(v.strip())\n        sanitized = re.sub(r'\\s+', ' ', sanitized)\n        \n        return sanitized\n    \n    @validator('content')\n    def sanitize_content(cls, v):\n        \"\"\"Sanitize post content\"\"\"\n        \n        # Allow rich text formatting but sanitize\n        sanitized = SecureInputValidator.sanitize_html_input(\n            v,\n            allowed_tags=[\n                'p', 'br', 'strong', 'em', 'u', 'ol', 'ul', 'li',\n                'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n                'blockquote', 'code', 'pre', 'a'\n            ]\n        )\n        \n        return sanitized\n    \n    @validator('tags')\n    def sanitize_tags(cls, v):\n        \"\"\"Sanitize and validate tags\"\"\"\n        \n        if not v:\n            return v\n        \n        sanitized_tags = []\n        \n        for tag in v:\n            # Remove HTML and normalize\n            clean_tag = html.escape(tag.strip().lower())\n            \n            # Validate tag format (letters, numbers, hyphens only)\n            if re.match(r'^[a-zA-Z0-9-]+$', clean_tag) and len(clean_tag) <= 30:\n                sanitized_tags.append(clean_tag)\n        \n        # Remove duplicates\n        return list(set(sanitized_tags))\n    \n    @validator('category')\n    def validate_category(cls, v):\n        \"\"\"Validate category\"\"\"\n        \n        if not v:\n            return v\n        \n        # Predefined allowed categories\n        allowed_categories = [\n            'technology', 'science', 'business', 'health',\n            'entertainment', 'sports', 'politics', 'lifestyle'\n        ]\n        \n        clean_category = v.strip().lower()\n        \n        if clean_category not in allowed_categories:\n            raise ValueError(f'Invalid category. Allowed: {\", \".join(allowed_categories)}')\n        \n        return clean_category\n```\n\n---\n\n### 3Ô∏è‚É£ CORS and CSRF Protection (10 minutes)\n\n**CORS and CSRF Implementation:**\n\n```python\n# security_middleware.py\nfrom fastapi import Request, HTTPException, status\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport secrets\nimport time\nfrom typing import Dict, List\nimport hmac\nimport hashlib\n\nclass CSRFProtection:\n    \"\"\"CSRF protection middleware\"\"\"\n    \n    def __init__(self, secret_key: str = None):\n        self.secret_key = secret_key or secrets.token_urlsafe(32)\n        self.token_lifetime = 3600  # 1 hour\n    \n    def generate_csrf_token(self, session_id: str) -> str:\n        \"\"\"Generate CSRF token for session\"\"\"\n        \n        timestamp = str(int(time.time()))\n        message = f\"{session_id}:{timestamp}\"\n        \n        signature = hmac.new(\n            self.secret_key.encode(),\n            message.encode(),\n            hashlib.sha256\n        ).hexdigest()\n        \n        token = f\"{message}:{signature}\"\n        return base64.urlsafe_b64encode(token.encode()).decode()\n    \n    def validate_csrf_token(self, token: str, session_id: str) -> bool:\n        \"\"\"Validate CSRF token\"\"\"\n        \n        try:\n            decoded_token = base64.urlsafe_b64decode(token.encode()).decode()\n            parts = decoded_token.split(':')\n            \n            if len(parts) != 3:\n                return False\n            \n            token_session_id, timestamp, signature = parts\n            \n            # Check session ID matches\n            if token_session_id != session_id:\n                return False\n            \n            # Check token age\n            token_time = int(timestamp)\n            if time.time() - token_time > self.token_lifetime:\n                return False\n            \n            # Verify signature\n            message = f\"{token_session_id}:{timestamp}\"\n            expected_signature = hmac.new(\n                self.secret_key.encode(),\n                message.encode(),\n                hashlib.sha256\n            ).hexdigest()\n            \n            return hmac.compare_digest(signature, expected_signature)\n            \n        except Exception:\n            return False\n\nclass SecurityMiddleware(BaseHTTPMiddleware):\n    \"\"\"Comprehensive security middleware\"\"\"\n    \n    def __init__(self, app, allowed_hosts: List[str] = None):\n        super().__init__(app)\n        self.allowed_hosts = allowed_hosts or ['localhost', '127.0.0.1']\n        self.csrf_protection = CSRFProtection()\n        \n        # Rate limiting storage (use Redis in production)\n        self.rate_limit_storage = {}\n    \n    async def dispatch(self, request: Request, call_next):\n        \"\"\"Apply security checks to all requests\"\"\"\n        \n        # 1. Host header validation\n        if not self._validate_host_header(request):\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"Invalid Host header\"\n            )\n        \n        # 2. Content type validation for POST/PUT requests\n        if request.method in [\"POST\", \"PUT\", \"PATCH\"]:\n            if not self._validate_content_type(request):\n                raise HTTPException(\n                    status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,\n                    detail=\"Unsupported content type\"\n                )\n        \n        # 3. CSRF protection for state-changing operations\n        if request.method in [\"POST\", \"PUT\", \"DELETE\", \"PATCH\"]:\n            if not await self._validate_csrf(request):\n                raise HTTPException(\n                    status_code=status.HTTP_403_FORBIDDEN,\n                    detail=\"CSRF token missing or invalid\"\n                )\n        \n        # 4. Request size validation\n        if not await self._validate_request_size(request):\n            raise HTTPException(\n                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,\n                detail=\"Request body too large\"\n            )\n        \n        # Process request\n        response = await call_next(request)\n        \n        # Add security headers\n        self._add_security_headers(response)\n        \n        return response\n    \n    def _validate_host_header(self, request: Request) -> bool:\n        \"\"\"Validate Host header to prevent Host header injection\"\"\"\n        \n        host = request.headers.get(\"host\", \"\").lower()\n        \n        if not host:\n            return False\n        \n        # Extract hostname (remove port if present)\n        hostname = host.split(':')[0]\n        \n        return hostname in self.allowed_hosts\n    \n    def _validate_content_type(self, request: Request) -> bool:\n        \"\"\"Validate Content-Type header\"\"\"\n        \n        content_type = request.headers.get(\"content-type\", \"\").lower()\n        \n        allowed_types = [\n            'application/json',\n            'application/x-www-form-urlencoded',\n            'multipart/form-data',\n            'text/plain'\n        ]\n        \n        # Check if content type starts with allowed type\n        return any(content_type.startswith(allowed) for allowed in allowed_types)\n    \n    async def _validate_csrf(self, request: Request) -> bool:\n        \"\"\"Validate CSRF token\"\"\"\n        \n        # Skip CSRF for API endpoints with proper authentication\n        if request.url.path.startswith('/api/'):\n            auth_header = request.headers.get('Authorization')\n            if auth_header and auth_header.startswith('Bearer '):\n                return True  # API endpoints use token auth instead of CSRF\n        \n        # Get CSRF token from header or form data\n        csrf_token = request.headers.get('X-CSRF-Token')\n        \n        if not csrf_token:\n            # Try to get from form data\n            if hasattr(request, 'form'):\n                form = await request.form()\n                csrf_token = form.get('csrf_token')\n        \n        if not csrf_token:\n            return False\n        \n        # Get session ID (in production, use actual session)\n        session_id = request.cookies.get('session_id', 'default_session')\n        \n        return self.csrf_protection.validate_csrf_token(csrf_token, session_id)\n    \n    async def _validate_request_size(self, request: Request) -> bool:\n        \"\"\"Validate request body size\"\"\"\n        \n        content_length = request.headers.get('content-length')\n        \n        if content_length:\n            try:\n                size = int(content_length)\n                max_size = 50 * 1024 * 1024  # 50MB limit\n                \n                return size <= max_size\n            except ValueError:\n                return False\n        \n        return True\n    \n    def _add_security_headers(self, response):\n        \"\"\"Add security headers to response\"\"\"\n        \n        security_headers = {\n            \"X-Content-Type-Options\": \"nosniff\",\n            \"X-Frame-Options\": \"DENY\",\n            \"X-XSS-Protection\": \"1; mode=block\",\n            \"Referrer-Policy\": \"strict-origin-when-cross-origin\"\n        }\n        \n        for header, value in security_headers.items():\n            response.headers[header] = value\n\n# Configure CORS properly\ndef setup_cors(app):\n    \"\"\"Setup CORS with secure configuration\"\"\"\n    \n    # Production CORS settings\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\n            \"https://yourdomain.com\",\n            \"https://app.yourdomain.com\",\n            # Add your trusted domains\n        ],\n        allow_credentials=True,\n        allow_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n        allow_headers=[\n            \"Authorization\",\n            \"Content-Type\", \n            \"X-CSRF-Token\",\n            \"X-Requested-With\"\n        ],\n        expose_headers=[\"X-CSRF-Token\"],\n        max_age=3600,  # Cache preflight for 1 hour\n    )\n\n# CSRF token endpoints\n@app.get(\"/auth/csrf-token\")\nasync def get_csrf_token(request: Request):\n    \"\"\"Get CSRF token for forms\"\"\"\n    \n    session_id = request.cookies.get('session_id', 'default_session')\n    \n    csrf_middleware = SecurityMiddleware(None)\n    csrf_token = csrf_middleware.csrf_protection.generate_csrf_token(session_id)\n    \n    return {\n        \"csrf_token\": csrf_token,\n        \"expires_in\": 3600\n    }\n```\n\n---\n\n### 4Ô∏è‚É£ API Security Implementation (10 minutes)\n\n**Secure API Design:**\n\n```python\n# secure_api.py\nfrom fastapi import Depends, HTTPException, Security, status\nfrom fastapi.security.api_key import APIKeyHeader, APIKeyQuery\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nimport hashlib\nimport hmac\nfrom datetime import datetime\nimport json\n\n# Rate limiting\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n# API Key authentication\napi_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\napi_key_query = APIKeyQuery(name=\"api_key\", auto_error=False)\n\nclass APIKeyManager:\n    \"\"\"Manage API keys securely\"\"\"\n    \n    def __init__(self):\n        # In production, store in database with proper encryption\n        self.api_keys = {\n            \"test_key_123\": {\n                \"user_id\": 1,\n                \"name\": \"Test Application\",\n                \"permissions\": [\"read\", \"write\"],\n                \"rate_limit\": 100,  # requests per minute\n                \"is_active\": True,\n                \"created_at\": datetime.now()\n            }\n        }\n    \n    def validate_api_key(self, api_key: str) -> Dict[str, Any]:\n        \"\"\"Validate API key and return key info\"\"\"\n        \n        if not api_key:\n            return None\n        \n        # Hash the API key for comparison (if stored hashed)\n        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n        \n        key_info = self.api_keys.get(api_key)\n        \n        if key_info and key_info.get('is_active', False):\n            return key_info\n        \n        return None\n    \n    def check_api_permissions(self, key_info: Dict[str, Any], required_permission: str) -> bool:\n        \"\"\"Check if API key has required permission\"\"\"\n        \n        permissions = key_info.get('permissions', [])\n        return required_permission in permissions or 'admin' in permissions\n\napi_key_manager = APIKeyManager()\n\nasync def get_api_key(\n    api_key_header: str = Security(api_key_header),\n    api_key_query: str = Security(api_key_query)\n) -> Dict[str, Any]:\n    \"\"\"Validate API key from header or query parameter\"\"\"\n    \n    api_key = api_key_header or api_key_query\n    \n    if not api_key:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"API key required\",\n            headers={\"WWW-Authenticate\": \"ApiKey\"},\n        )\n    \n    key_info = api_key_manager.validate_api_key(api_key)\n    \n    if not key_info:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid or expired API key\"\n        )\n    \n    return key_info\n\n# Webhook signature validation\nclass WebhookValidator:\n    \"\"\"Validate webhook signatures\"\"\"\n    \n    @staticmethod\n    def validate_github_signature(payload: bytes, signature: str, secret: str) -> bool:\n        \"\"\"Validate GitHub webhook signature\"\"\"\n        \n        if not signature.startswith('sha256='):\n            return False\n        \n        expected_signature = hmac.new(\n            secret.encode(),\n            payload,\n            hashlib.sha256\n        ).hexdigest()\n        \n        received_signature = signature[7:]  # Remove 'sha256=' prefix\n        \n        return hmac.compare_digest(expected_signature, received_signature)\n    \n    @staticmethod\n    def validate_stripe_signature(payload: bytes, sig_header: str, endpoint_secret: str) -> bool:\n        \"\"\"Validate Stripe webhook signature\"\"\"\n        \n        try:\n            import stripe\n            stripe.Webhook.construct_event(payload, sig_header, endpoint_secret)\n            return True\n        except (stripe.error.SignatureVerificationError, ValueError):\n            return False\n\n# Secure API endpoints with comprehensive protection\n@app.post(\"/api/v1/posts\")\n@limiter.limit(\"10/minute\")  # Rate limiting\nasync def create_post(\n    request: Request,\n    post_data: SecurePostInput,\n    api_key_info: Dict[str, Any] = Depends(get_api_key),\n    current_user: Dict[str, Any] = Depends(get_current_user)\n):\n    \"\"\"Create post with comprehensive security\"\"\"\n    \n    # Check API permissions\n    if not api_key_manager.check_api_permissions(api_key_info, \"write\"):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Insufficient API permissions\"\n        )\n    \n    # Input validation is handled by Pydantic model (SecurePostInput)\n    \n    # Log the action (with masked sensitive data)\n    log_data = {\n        'action': 'create_post',\n        'user_id': current_user['id'],\n        'api_key': api_key_info.get('name', 'Unknown'),\n        'ip_address': request.client.host,\n        'timestamp': datetime.now().isoformat(),\n        'post_title': post_data.title[:50],  # Only log first 50 chars\n    }\n    \n    logger.info(f\"üìù Post creation: {json.dumps(log_data)}\")\n    \n    # Create post (database logic would go here)\n    post_id = f\"post_{secrets.token_urlsafe(8)}\"\n    \n    return {\n        \"message\": \"Post created successfully\",\n        \"post_id\": post_id,\n        \"title\": post_data.title\n    }\n\n@app.webhook(\"/webhooks/github\")\nasync def github_webhook(request: Request):\n    \"\"\"Secure GitHub webhook handler\"\"\"\n    \n    # Get signature\n    signature = request.headers.get('X-Hub-Signature-256')\n    if not signature:\n        raise HTTPException(status_code=400, detail=\"Missing signature\")\n    \n    # Get payload\n    payload = await request.body()\n    \n    # Validate signature\n    webhook_secret = os.getenv('GITHUB_WEBHOOK_SECRET')\n    if not WebhookValidator.validate_github_signature(payload, signature, webhook_secret):\n        logger.warning(f\"‚ö†Ô∏è Invalid GitHub webhook signature from {request.client.host}\")\n        raise HTTPException(status_code=401, detail=\"Invalid signature\")\n    \n    # Process webhook\n    try:\n        data = json.loads(payload.decode())\n        event_type = request.headers.get('X-GitHub-Event')\n        \n        logger.info(f\"üì® GitHub webhook received: {event_type}\")\n        \n        # Handle different event types\n        if event_type == 'push':\n            # Handle push events\n            pass\n        elif event_type == 'pull_request':\n            # Handle PR events\n            pass\n        \n        return {\"status\": \"success\", \"event\": event_type}\n        \n    except json.JSONDecodeError:\n        raise HTTPException(status_code=400, detail=\"Invalid JSON payload\")\n\n# Security monitoring endpoint\n@app.get(\"/admin/security/logs\")\nasync def get_security_logs(\n    current_user: Dict[str, Any] = Depends(get_current_user),\n    api_key_info: Dict[str, Any] = Depends(get_api_key)\n):\n    \"\"\"Get security logs (admin only)\"\"\"\n    \n    # Check admin permissions\n    if not api_key_manager.check_api_permissions(api_key_info, \"admin\"):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin access required\"\n        )\n    \n    # In production, fetch from security logging system\n    security_logs = [\n        {\n            \"timestamp\": \"2024-01-15T10:30:00Z\",\n            \"event\": \"login_attempt\",\n            \"status\": \"failed\",\n            \"ip\": \"192.168.1.100\",\n            \"user_agent\": \"Mozilla/5.0...\"\n        },\n        {\n            \"timestamp\": \"2024-01-15T10:25:00Z\",\n            \"event\": \"rate_limit_exceeded\",\n            \"ip\": \"10.0.0.5\",\n            \"endpoint\": \"/api/v1/posts\"\n        }\n    ]\n    \n    return {\n        \"logs\": security_logs,\n        \"total\": len(security_logs)\n    }\n```\n\n---\n\n### üè† Homework: Implement Security Fortress\n\n**Task:** Build comprehensive security for your blog application\n\n```python\n# Implement complete security measures:\n# 1. OWASP Top 10 protection (all vulnerabilities)\n# 2. Secure authentication with MFA support\n# 3. Comprehensive input validation and sanitization\n# 4. CORS and CSRF protection\n# 5. API security with rate limiting and key management\n# 6. Security headers and middleware\n# 7. Audit logging and monitoring\n# 8. Vulnerability scanning integration\n\n# Requirements:\n# - Password policy enforcement\n# - Session management with secure cookies\n# - SQL injection prevention\n# - XSS protection with CSP\n# - File upload security\n# - API rate limiting\n# - Security incident response\n\n# Bonus:\n# - Implement security dashboard\n# - Add intrusion detection\n# - Create security audit reports\n# - Integrate with security scanning tools\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ OWASP Top 10 = Essential security checklist for web apps\n‚úÖ Input Validation = Never trust user input, validate everything\n‚úÖ Authentication = Multi-layered security with proper sessions\n‚úÖ CORS & CSRF = Protect against cross-origin attacks\n‚úÖ API Security = Rate limiting, authentication, and monitoring\n\n---\n\n<a name=\"hour-37\"></a>\n## üìÖ Hour 37: GraphQL Introduction\n\n### üéØ Learning Objectives\n- Understand GraphQL concepts vs REST APIs\n- Implement GraphQL schema and resolvers\n- Handle queries, mutations, and subscriptions\n- Add authentication and authorization to GraphQL\n- Optimize GraphQL performance and prevent common issues\n\n### üìñ What to Teach\n\n**\"Today we master GraphQL - the query language that lets clients ask for exactly what they need!\"**\n\n---\n\n### 1Ô∏è‚É£ GraphQL vs REST Comparison (10 minutes)\n\n**REST API Problems GraphQL Solves:**\n\n```python\n# REST API - Multiple requests needed\n\"\"\"\n‚ùå REST API Problems:\n\n1. Over-fetching:\n   GET /api/users/123\n   Returns: { id, name, email, bio, avatar, created_at, updated_at, settings... }\n   Client only needs: { id, name, email }\n\n2. Under-fetching:\n   GET /api/posts/456           # Get post\n   GET /api/users/123           # Get author info\n   GET /api/posts/456/comments  # Get comments\n   GET /api/comments/*/authors  # Get comment authors\n   \n   4 requests to show a blog post with comments!\n\n3. API Versioning:\n   /api/v1/users vs /api/v2/users\n   Multiple versions to maintain\n\n4. Fixed Response Structure:\n   Can't customize what fields to include\n\"\"\"\n\n# GraphQL - Single request with exact data\n\"\"\"\n‚úÖ GraphQL Solutions:\n\n1. Exact Data Fetching:\n   query {\n     user(id: 123) {\n       id\n       name\n       email\n     }\n   }\n   Returns exactly: { \"user\": { \"id\": 123, \"name\": \"John\", \"email\": \"john@example.com\" }}\n\n2. Single Request for Complex Data:\n   query {\n     post(id: 456) {\n       title\n       content\n       author {\n         name\n         avatar\n       }\n       comments {\n         content\n         author {\n           name\n         }\n       }\n     }\n   }\n\n3. No Versioning Needed:\n   Schema evolution without breaking existing clients\n\n4. Flexible Response Structure:\n   Client specifies exactly what they need\n\"\"\"\n```\n\n**When to Use GraphQL vs REST:**\n\n```python\n# Use GraphQL when:\ngraphql_use_cases = {\n    \"Mobile Applications\": \"Reduce bandwidth and battery usage\",\n    \"Complex UIs\": \"Many interconnected data requirements\", \n    \"Rapid Development\": \"Frontend teams need flexible data fetching\",\n    \"Multiple Clients\": \"Different apps need different data subsets\",\n    \"Real-time Features\": \"Built-in subscription support\",\n    \"Developer Experience\": \"Self-documenting schema and tooling\"\n}\n\n# Use REST when:\nrest_use_cases = {\n    \"Simple CRUD\": \"Basic create, read, update, delete operations\",\n    \"File Operations\": \"File uploads, downloads, streaming\",\n    \"Caching\": \"HTTP caching is critical\",\n    \"Team Familiarity\": \"Team is not ready for GraphQL complexity\",\n    \"Third-party Integration\": \"Existing REST services integration\",\n    \"Microservices\": \"Service-to-service communication\"\n}\n\n# GraphQL Benefits:\nbenefits = \"\"\"\n‚úÖ Single endpoint (/graphql)\n‚úÖ Client specifies data requirements\n‚úÖ Strongly typed schema\n‚úÖ Introspection and tooling\n‚úÖ Real-time subscriptions\n‚úÖ No over/under fetching\n‚úÖ Backward compatible evolution\n\"\"\"\n\n# GraphQL Challenges:\nchallenges = \"\"\"\n‚ö†Ô∏è Learning curve and complexity\n‚ö†Ô∏è Caching is more complex\n‚ö†Ô∏è File uploads need special handling\n‚ö†Ô∏è Query complexity attacks\n‚ö†Ô∏è N+1 query problems\n‚ö†Ô∏è HTTP status codes less meaningful\n\"\"\"\n```\n\n---\n\n### 2Ô∏è‚É£ GraphQL Schema and Resolvers (15 minutes)\n\n**Setting up GraphQL with Strawberry:**\n\n```bash\n# Install GraphQL dependencies\npip install strawberry-graphql[fastapi] graphql-core\n```\n\n**Schema Definition:**\n\n```python\n# graphql_schema.py\nimport strawberry\nfrom typing import List, Optional, AsyncGenerator\nfrom datetime import datetime\nimport asyncio\nfrom dataclasses import dataclass\n\n# Type Definitions\n@strawberry.type\nclass User:\n    id: int\n    username: str\n    email: str\n    full_name: Optional[str] = None\n    bio: Optional[str] = None\n    avatar_url: Optional[str] = None\n    created_at: datetime\n    is_active: bool\n    \n    # Computed field\n    @strawberry.field\n    def display_name(self) -> str:\n        return self.full_name or self.username\n\n@strawberry.type \nclass Post:\n    id: int\n    title: str\n    content: str\n    excerpt: Optional[str] = None\n    author_id: int\n    created_at: datetime\n    updated_at: Optional[datetime] = None\n    published: bool = False\n    tags: List[str] = strawberry.field(default_factory=list)\n    \n    # Relationship fields (resolved separately)\n    @strawberry.field\n    async def author(self, info) -> User:\n        \"\"\"Get post author - this is a resolver\"\"\"\n        # Access the database through context\n        db = info.context[\"db\"]\n        user_service = info.context[\"user_service\"]\n        \n        return await user_service.get_user_by_id(self.author_id)\n    \n    @strawberry.field\n    async def comments(self, info) -> List[\"Comment\"]:\n        \"\"\"Get post comments\"\"\"\n        comment_service = info.context[\"comment_service\"] \n        return await comment_service.get_comments_by_post_id(self.id)\n    \n    @strawberry.field\n    def word_count(self) -> int:\n        \"\"\"Computed field for word count\"\"\"\n        return len(self.content.split())\n\n@strawberry.type\nclass Comment:\n    id: int\n    content: str\n    post_id: int\n    author_id: int\n    created_at: datetime\n    parent_id: Optional[int] = None  # For nested comments\n    \n    @strawberry.field\n    async def author(self, info) -> User:\n        user_service = info.context[\"user_service\"]\n        return await user_service.get_user_by_id(self.author_id)\n    \n    @strawberry.field\n    async def replies(self, info) -> List[\"Comment\"]:\n        \"\"\"Get nested replies to this comment\"\"\"\n        comment_service = info.context[\"comment_service\"]\n        return await comment_service.get_replies(self.id)\n\n# Input Types for Mutations\n@strawberry.input\nclass CreatePostInput:\n    title: str\n    content: str\n    tags: Optional[List[str]] = None\n    published: bool = False\n\n@strawberry.input\nclass UpdatePostInput:\n    title: Optional[str] = None\n    content: Optional[str] = None\n    tags: Optional[List[str]] = None\n    published: Optional[bool] = None\n\n@strawberry.input\nclass CreateCommentInput:\n    content: str\n    post_id: int\n    parent_id: Optional[int] = None\n\n# Custom Scalars\n@strawberry.scalar(\n    serialize=lambda v: v.isoformat(),\n    parse_value=lambda v: datetime.fromisoformat(v)\n)\nclass DateTime:\n    \"\"\"Custom DateTime scalar\"\"\"\n    pass\n\n# Pagination Types\n@strawberry.type\nclass PageInfo:\n    has_next_page: bool\n    has_previous_page: bool\n    start_cursor: Optional[str] = None\n    end_cursor: Optional[str] = None\n\n@strawberry.type\nclass PostConnection:\n    edges: List[\"PostEdge\"]\n    page_info: PageInfo\n    total_count: int\n\n@strawberry.type\nclass PostEdge:\n    node: Post\n    cursor: str\n\n# Error Types\n@strawberry.type\nclass ValidationError:\n    field: str\n    message: str\n\n@strawberry.type\nclass PostResult:\n    post: Optional[Post] = None\n    errors: List[ValidationError] = strawberry.field(default_factory=list)\n```\n\n**Query Resolvers:**\n\n```python\n# graphql_queries.py\nfrom typing import List, Optional\nimport strawberry\n\n@strawberry.type\nclass Query:\n    \"\"\"GraphQL Query root\"\"\"\n    \n    @strawberry.field\n    async def users(\n        self, \n        info,\n        first: Optional[int] = 10,\n        search: Optional[str] = None\n    ) -> List[User]:\n        \"\"\"Get list of users with optional search\"\"\"\n        \n        user_service = info.context[\"user_service\"]\n        \n        # Apply search filter if provided\n        if search:\n            return await user_service.search_users(search, limit=first)\n        \n        return await user_service.get_users(limit=first)\n    \n    @strawberry.field\n    async def user(self, info, id: int) -> Optional[User]:\n        \"\"\"Get user by ID\"\"\"\n        \n        user_service = info.context[\"user_service\"]\n        return await user_service.get_user_by_id(id)\n    \n    @strawberry.field\n    async def posts(\n        self,\n        info,\n        first: Optional[int] = 10,\n        after: Optional[str] = None,\n        author_id: Optional[int] = None,\n        published_only: bool = True,\n        search: Optional[str] = None\n    ) -> PostConnection:\n        \"\"\"Get paginated posts with filtering\"\"\"\n        \n        post_service = info.context[\"post_service\"]\n        \n        # Build filters\n        filters = {\n            \"limit\": first,\n            \"after_cursor\": after,\n            \"author_id\": author_id,\n            \"published_only\": published_only,\n            \"search\": search\n        }\n        \n        result = await post_service.get_posts_paginated(**filters)\n        \n        # Convert to GraphQL connection format\n        edges = [\n            PostEdge(node=post, cursor=post_service.encode_cursor(post.id))\n            for post in result[\"posts\"]\n        ]\n        \n        page_info = PageInfo(\n            has_next_page=result[\"has_next\"],\n            has_previous_page=result[\"has_previous\"],\n            start_cursor=edges[0].cursor if edges else None,\n            end_cursor=edges[-1].cursor if edges else None\n        )\n        \n        return PostConnection(\n            edges=edges,\n            page_info=page_info,\n            total_count=result[\"total_count\"]\n        )\n    \n    @strawberry.field\n    async def post(self, info, id: int) -> Optional[Post]:\n        \"\"\"Get single post by ID\"\"\"\n        \n        post_service = info.context[\"post_service\"]\n        return await post_service.get_post_by_id(id)\n    \n    @strawberry.field\n    async def popular_tags(self, info, limit: int = 10) -> List[str]:\n        \"\"\"Get most popular tags\"\"\"\n        \n        post_service = info.context[\"post_service\"]\n        return await post_service.get_popular_tags(limit)\n    \n    @strawberry.field\n    async def search(\n        self,\n        info,\n        query: str,\n        type: Optional[str] = None,  # \"posts\", \"users\", \"all\"\n        limit: int = 20\n    ) -> \"SearchResult\":\n        \"\"\"Universal search across content types\"\"\"\n        \n        search_service = info.context[\"search_service\"]\n        \n        if type == \"posts\":\n            posts = await search_service.search_posts(query, limit)\n            return SearchResult(posts=posts, users=[], total_count=len(posts))\n        \n        elif type == \"users\":\n            users = await search_service.search_users(query, limit)\n            return SearchResult(posts=[], users=users, total_count=len(users))\n        \n        else:  # Search all\n            results = await search_service.universal_search(query, limit)\n            return SearchResult(\n                posts=results[\"posts\"],\n                users=results[\"users\"],\n                total_count=results[\"total_count\"]\n            )\n\n@strawberry.type\nclass SearchResult:\n    posts: List[Post]\n    users: List[User]\n    total_count: int\n```\n\n**Mutation Resolvers:**\n\n```python\n# graphql_mutations.py\nimport strawberry\nfrom typing import List\n\n@strawberry.type\nclass Mutation:\n    \"\"\"GraphQL Mutation root\"\"\"\n    \n    @strawberry.field\n    async def create_post(\n        self,\n        info,\n        input: CreatePostInput\n    ) -> PostResult:\n        \"\"\"Create a new blog post\"\"\"\n        \n        # Get current user from context (authentication)\n        current_user = info.context.get(\"current_user\")\n        if not current_user:\n            return PostResult(\n                errors=[ValidationError(field=\"auth\", message=\"Authentication required\")]\n            )\n        \n        # Validate input\n        errors = []\n        \n        if not input.title or len(input.title.strip()) < 3:\n            errors.append(ValidationError(\n                field=\"title\",\n                message=\"Title must be at least 3 characters\"\n            ))\n        \n        if not input.content or len(input.content.strip()) < 10:\n            errors.append(ValidationError(\n                field=\"content\", \n                message=\"Content must be at least 10 characters\"\n            ))\n        \n        if errors:\n            return PostResult(errors=errors)\n        \n        # Create post\n        post_service = info.context[\"post_service\"]\n        \n        try:\n            post = await post_service.create_post(\n                title=input.title,\n                content=input.content,\n                author_id=current_user[\"id\"],\n                tags=input.tags or [],\n                published=input.published\n            )\n            \n            return PostResult(post=post)\n            \n        except Exception as e:\n            return PostResult(\n                errors=[ValidationError(field=\"general\", message=str(e))]\n            )\n    \n    @strawberry.field\n    async def update_post(\n        self,\n        info,\n        id: int,\n        input: UpdatePostInput\n    ) -> PostResult:\n        \"\"\"Update existing post\"\"\"\n        \n        current_user = info.context.get(\"current_user\")\n        if not current_user:\n            return PostResult(\n                errors=[ValidationError(field=\"auth\", message=\"Authentication required\")]\n            )\n        \n        post_service = info.context[\"post_service\"]\n        \n        # Check if post exists and user owns it\n        existing_post = await post_service.get_post_by_id(id)\n        if not existing_post:\n            return PostResult(\n                errors=[ValidationError(field=\"id\", message=\"Post not found\")]\n            )\n        \n        if existing_post.author_id != current_user[\"id\"]:\n            return PostResult(\n                errors=[ValidationError(field=\"auth\", message=\"Not authorized to edit this post\")]\n            )\n        \n        # Update post\n        try:\n            updated_post = await post_service.update_post(id, input)\n            return PostResult(post=updated_post)\n            \n        except Exception as e:\n            return PostResult(\n                errors=[ValidationError(field=\"general\", message=str(e))]\n            )\n    \n    @strawberry.field\n    async def delete_post(self, info, id: int) -> bool:\n        \"\"\"Delete a post\"\"\"\n        \n        current_user = info.context.get(\"current_user\")\n        if not current_user:\n            return False\n        \n        post_service = info.context[\"post_service\"]\n        \n        # Check ownership\n        post = await post_service.get_post_by_id(id)\n        if not post or post.author_id != current_user[\"id\"]:\n            return False\n        \n        return await post_service.delete_post(id)\n    \n    @strawberry.field\n    async def create_comment(\n        self,\n        info,\n        input: CreateCommentInput\n    ) -> \"CommentResult\":\n        \"\"\"Create a comment on a post\"\"\"\n        \n        current_user = info.context.get(\"current_user\")\n        if not current_user:\n            return CommentResult(\n                errors=[ValidationError(field=\"auth\", message=\"Authentication required\")]\n            )\n        \n        # Validate input\n        if not input.content or len(input.content.strip()) < 1:\n            return CommentResult(\n                errors=[ValidationError(field=\"content\", message=\"Comment cannot be empty\")]\n            )\n        \n        comment_service = info.context[\"comment_service\"]\n        \n        try:\n            comment = await comment_service.create_comment(\n                content=input.content,\n                post_id=input.post_id,\n                author_id=current_user[\"id\"],\n                parent_id=input.parent_id\n            )\n            \n            return CommentResult(comment=comment)\n            \n        except Exception as e:\n            return CommentResult(\n                errors=[ValidationError(field=\"general\", message=str(e))]\n            )\n\n@strawberry.type\nclass CommentResult:\n    comment: Optional[Comment] = None\n    errors: List[ValidationError] = strawberry.field(default_factory=list)\n```\n\n**Subscriptions for Real-time Updates:**\n\n```python\n# graphql_subscriptions.py\nimport strawberry\nimport asyncio\nfrom typing import AsyncGenerator\n\n@strawberry.type\nclass Subscription:\n    \"\"\"GraphQL Subscription root for real-time updates\"\"\"\n    \n    @strawberry.subscription\n    async def post_comments(self, info, post_id: int) -> AsyncGenerator[Comment, None]:\n        \"\"\"Subscribe to new comments on a post\"\"\"\n        \n        # In production, use Redis pub/sub or WebSocket manager\n        comment_service = info.context[\"comment_service\"]\n        \n        # Create a subscription channel\n        channel = f\"post_comments_{post_id}\"\n        \n        try:\n            while True:\n                # Wait for new comments (this would be event-driven in production)\n                new_comment = await comment_service.wait_for_new_comment(post_id)\n                \n                if new_comment:\n                    yield new_comment\n                \n                # Small delay to prevent busy waiting\n                await asyncio.sleep(1)\n                \n        except asyncio.CancelledError:\n            # Cleanup subscription\n            await comment_service.cleanup_subscription(channel)\n            raise\n    \n    @strawberry.subscription\n    async def user_notifications(self, info) -> AsyncGenerator[\"Notification\", None]:\n        \"\"\"Subscribe to user notifications\"\"\"\n        \n        current_user = info.context.get(\"current_user\")\n        if not current_user:\n            return\n        \n        notification_service = info.context[\"notification_service\"]\n        user_id = current_user[\"id\"]\n        \n        try:\n            async for notification in notification_service.subscribe_user_notifications(user_id):\n                yield notification\n                \n        except asyncio.CancelledError:\n            await notification_service.cleanup_user_subscription(user_id)\n            raise\n\n@strawberry.type\nclass Notification:\n    id: int\n    title: str\n    message: str\n    type: str  # \"comment\", \"like\", \"follow\", etc.\n    created_at: datetime\n    read: bool = False\n```\n\n---\n\n### 3Ô∏è‚É£ GraphQL Performance Optimization (10 minutes)\n\n**Solving N+1 Query Problem:**\n\n```python\n# graphql_dataloaders.py\nfrom aiodataloader import DataLoader\nfrom typing import List, Dict, Any\nimport asyncio\n\nclass UserDataLoader(DataLoader):\n    \"\"\"Batch load users to prevent N+1 queries\"\"\"\n    \n    def __init__(self, user_service):\n        super().__init__()\n        self.user_service = user_service\n    \n    async def batch_load_fn(self, user_ids: List[int]) -> List[User]:\n        \"\"\"Load multiple users in a single query\"\"\"\n        \n        # Single database query for all requested user IDs\n        users = await self.user_service.get_users_by_ids(user_ids)\n        \n        # Create a mapping for fast lookup\n        user_map = {user.id: user for user in users}\n        \n        # Return users in the same order as requested IDs\n        return [user_map.get(user_id) for user_id in user_ids]\n\nclass PostDataLoader(DataLoader):\n    \"\"\"Batch load posts\"\"\"\n    \n    def __init__(self, post_service):\n        super().__init__()\n        self.post_service = post_service\n    \n    async def batch_load_fn(self, post_ids: List[int]) -> List[Post]:\n        posts = await self.post_service.get_posts_by_ids(post_ids)\n        post_map = {post.id: post for post in posts}\n        return [post_map.get(post_id) for post_id in post_ids]\n\nclass CommentsByPostDataLoader(DataLoader):\n    \"\"\"Batch load comments by post IDs\"\"\"\n    \n    def __init__(self, comment_service):\n        super().__init__()\n        self.comment_service = comment_service\n    \n    async def batch_load_fn(self, post_ids: List[int]) -> List[List[Comment]]:\n        \"\"\"Load comments for multiple posts\"\"\"\n        \n        # Single query to get all comments for all posts\n        all_comments = await self.comment_service.get_comments_by_post_ids(post_ids)\n        \n        # Group comments by post_id\n        comments_by_post = {}\n        for comment in all_comments:\n            if comment.post_id not in comments_by_post:\n                comments_by_post[comment.post_id] = []\n            comments_by_post[comment.post_id].append(comment)\n        \n        # Return in requested order\n        return [comments_by_post.get(post_id, []) for post_id in post_ids]\n\n# Context factory with dataloaders\ndef create_graphql_context(request, user_service, post_service, comment_service):\n    \"\"\"Create GraphQL context with dataloaders\"\"\"\n    \n    return {\n        \"request\": request,\n        \"user_service\": user_service,\n        \"post_service\": post_service,\n        \"comment_service\": comment_service,\n        \n        # DataLoaders for performance\n        \"user_loader\": UserDataLoader(user_service),\n        \"post_loader\": PostDataLoader(post_service),\n        \"comments_by_post_loader\": CommentsByPostDataLoader(comment_service),\n        \n        # Authentication context\n        \"current_user\": getattr(request, 'user', None)\n    }\n\n# Update resolvers to use dataloaders\n@strawberry.type\nclass OptimizedPost:\n    id: int\n    title: str\n    content: str\n    author_id: int\n    \n    @strawberry.field\n    async def author(self, info) -> User:\n        \"\"\"Use dataloader to prevent N+1 queries\"\"\"\n        user_loader = info.context[\"user_loader\"]\n        return await user_loader.load(self.author_id)\n    \n    @strawberry.field\n    async def comments(self, info) -> List[Comment]:\n        \"\"\"Use dataloader for comments\"\"\"\n        comments_loader = info.context[\"comments_by_post_loader\"]\n        return await comments_loader.load(self.id)\n```\n\n**Query Complexity Analysis:**\n\n```python\n# graphql_security.py\nimport strawberry\nfrom strawberry.extensions import Extension\nfrom typing import Dict, Any\n\nclass QueryComplexityAnalyzer(Extension):\n    \"\"\"Prevent overly complex GraphQL queries\"\"\"\n    \n    def __init__(self, max_complexity: int = 1000, max_depth: int = 10):\n        self.max_complexity = max_complexity\n        self.max_depth = max_depth\n    \n    async def on_request_start(self):\n        \"\"\"Analyze query before execution\"\"\"\n        \n        query = self.execution_context.query\n        \n        # Calculate query depth\n        depth = self._calculate_query_depth(query)\n        if depth > self.max_depth:\n            raise Exception(f\"Query depth {depth} exceeds maximum allowed depth {self.max_depth}\")\n        \n        # Calculate query complexity\n        complexity = self._calculate_query_complexity(query)\n        if complexity > self.max_complexity:\n            raise Exception(f\"Query complexity {complexity} exceeds maximum allowed complexity {self.max_complexity}\")\n    \n    def _calculate_query_depth(self, query) -> int:\n        \"\"\"Calculate maximum depth of GraphQL query\"\"\"\n        # Implementation would parse the query AST\n        # This is a simplified version\n        return query.count('{') + query.count('(')\n    \n    def _calculate_query_complexity(self, query) -> int:\n        \"\"\"Calculate complexity score based on field costs\"\"\"\n        \n        # Define field costs\n        field_costs = {\n            'posts': 10,      # Expensive list query\n            'users': 8,       # User list\n            'comments': 5,    # Comments\n            'author': 2,      # Single relation\n            'user': 1,        # Single field\n        }\n        \n        complexity = 0\n        \n        # Simple complexity calculation (production would use proper AST parsing)\n        for field, cost in field_costs.items():\n            if field in query:\n                complexity += cost\n        \n        return complexity\n\nclass QueryDepthLimiter(Extension):\n    \"\"\"Limit query depth to prevent deeply nested queries\"\"\"\n    \n    def __init__(self, max_depth: int = 10):\n        self.max_depth = max_depth\n    \n    def on_validate(self):\n        \"\"\"Validate query depth during GraphQL validation phase\"\"\"\n        \n        # This would integrate with GraphQL validation\n        pass\n\n# Rate limiting for GraphQL\nclass GraphQLRateLimit(Extension):\n    \"\"\"Rate limit GraphQL queries\"\"\"\n    \n    def __init__(self, redis_client, requests_per_minute: int = 60):\n        self.redis = redis_client\n        self.requests_per_minute = requests_per_minute\n    \n    async def on_request_start(self):\n        \"\"\"Check rate limit before executing query\"\"\"\n        \n        # Get client identifier\n        request = self.execution_context.context[\"request\"]\n        client_id = request.client.host\n        \n        # Check rate limit\n        key = f\"graphql_rate_limit:{client_id}\"\n        current_requests = await self.redis.incr(key)\n        \n        if current_requests == 1:\n            await self.redis.expire(key, 60)  # 1 minute window\n        \n        if current_requests > self.requests_per_minute:\n            raise Exception(\"Rate limit exceeded. Please try again later.\")\n```\n\n---\n\n### 4Ô∏è‚É£ FastAPI Integration (10 minutes)\n\n**Complete GraphQL FastAPI Setup:**\n\n```python\n# main.py - GraphQL FastAPI integration\nfrom fastapi import FastAPI, Depends, HTTPException, Request\nfrom strawberry.fastapi import GraphQLRouter\nimport strawberry\nfrom contextlib import asynccontextmanager\n\n# Create GraphQL schema\n@strawberry.type\nclass Query:\n    # Include all query resolvers\n    pass\n\n@strawberry.type  \nclass Mutation:\n    # Include all mutation resolvers\n    pass\n\n@strawberry.type\nclass Subscription:\n    # Include all subscription resolvers\n    pass\n\n# Create schema with security extensions\nschema = strawberry.Schema(\n    query=Query,\n    mutation=Mutation,\n    subscription=Subscription,\n    extensions=[\n        QueryComplexityAnalyzer(max_complexity=500, max_depth=8),\n        GraphQLRateLimit(redis_client, requests_per_minute=100)\n    ]\n)\n\n# Authentication dependency\nasync def get_current_user_optional(request: Request):\n    \"\"\"Get current user for GraphQL context (optional)\"\"\"\n    \n    auth_header = request.headers.get(\"Authorization\")\n    \n    if not auth_header or not auth_header.startswith(\"Bearer \"):\n        return None\n    \n    token = auth_header.split(\" \")[1]\n    \n    try:\n        # Validate JWT token\n        payload = auth_handler.verify_token(token)\n        user_id = payload.get(\"sub\")\n        \n        # Get user from database\n        # user = await user_service.get_user_by_id(user_id)\n        \n        # Simulate user\n        return {\n            \"id\": int(user_id),\n            \"username\": payload.get(\"username\")\n        }\n        \n    except Exception:\n        return None\n\n# GraphQL context provider\nasync def get_context(\n    request: Request,\n    current_user = Depends(get_current_user_optional)\n):\n    \"\"\"Provide context for GraphQL resolvers\"\"\"\n    \n    return {\n        \"request\": request,\n        \"current_user\": current_user,\n        \"user_service\": user_service,\n        \"post_service\": post_service,\n        \"comment_service\": comment_service,\n        \n        # DataLoaders for performance\n        \"user_loader\": UserDataLoader(user_service),\n        \"post_loader\": PostDataLoader(post_service),\n        \"comments_by_post_loader\": CommentsByPostDataLoader(comment_service),\n    }\n\n# Create GraphQL router\ngraphql_app = GraphQLRouter(\n    schema,\n    context_getter=get_context,\n    debug=True  # Disable in production\n)\n\n# FastAPI app\napp = FastAPI(title=\"Blog API with GraphQL\")\n\n# Mount GraphQL\napp.include_router(graphql_app, prefix=\"/graphql\")\n\n# GraphQL Playground (development only)\n@app.get(\"/graphql-playground\")\nasync def graphql_playground():\n    \"\"\"GraphQL Playground for development\"\"\"\n    \n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <meta charset=utf-8/>\n        <meta name=\"viewport\" content=\"user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, minimal-ui\">\n        <title>GraphQL Playground</title>\n        <link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/graphql-playground-react/build/static/css/index.css\" />\n        <link rel=\"shortcut icon\" href=\"//cdn.jsdelivr.net/npm/graphql-playground-react/build/favicon.png\" />\n        <script src=\"//cdn.jsdelivr.net/npm/graphql-playground-react/build/static/js/middleware.js\"></script>\n    </head>\n    <body>\n        <div id=\"root\">\n            <style>\n                body { background-color: rgb(23, 42, 58); font-family: Open Sans, sans-serif; height: 90vh; }\n                #root { height: 100%; width: 100%; display: flex; align-items: center; justify-content: center; }\n                .loading { font-size: 32px; font-weight: 200; color: rgba(255, 255, 255, .6); margin-left: 20px; }\n                img { width: 78px; height: 78px; }\n                .title { font-weight: 400; }\n            </style>\n            <img src=\"//cdn.jsdelivr.net/npm/graphql-playground-react/build/logo.png\" alt=\"\">\n            <div class=\"loading\"> Loading\n                <span class=\"title\">GraphQL Playground</span>\n            </div>\n        </div>\n        <script>window.addEventListener('load', function (event) {\n            GraphQLPlayground.init(document.getElementById('root'), {\n                endpoint: '/graphql'\n            })\n        })</script>\n    </body>\n    </html>\n    \"\"\"\n\n# Example REST to GraphQL migration endpoint\n@app.get(\"/api/v1/posts/{post_id}\")\nasync def get_post_rest(post_id: int):\n    \"\"\"REST endpoint - compare with GraphQL\"\"\"\n    \n    # Fixed response structure - can't customize fields\n    post = await post_service.get_post_by_id(post_id)\n    \n    if not post:\n        raise HTTPException(status_code=404, detail=\"Post not found\")\n    \n    # Always returns all fields\n    return {\n        \"id\": post.id,\n        \"title\": post.title,\n        \"content\": post.content,\n        \"excerpt\": post.excerpt,\n        \"author_id\": post.author_id,\n        \"created_at\": post.created_at,\n        \"updated_at\": post.updated_at,\n        \"published\": post.published,\n        \"tags\": post.tags\n    }\n\n# GraphQL comparison\n\"\"\"\nGraphQL Query (flexible):\n\nquery GetPost($id: Int!) {\n  post(id: $id) {\n    title\n    content\n    author {\n      name\n      avatar_url\n    }\n    comments {\n      content\n      author {\n        name\n      }\n    }\n  }\n}\n\nBenefits:\n- Single request for post + author + comments\n- Only fetch needed fields\n- Type safety\n- Self-documenting\n\"\"\"\n\n# Error handling for GraphQL\nfrom strawberry.types import ExecutionResult\n\n@app.middleware(\"http\")\nasync def graphql_error_handler(request: Request, call_next):\n    \"\"\"Handle GraphQL errors and logging\"\"\"\n    \n    response = await call_next(request)\n    \n    # Log GraphQL errors for monitoring\n    if hasattr(response, 'body') and b'\"errors\"' in response.body:\n        logger.warning(f\"GraphQL errors in request to {request.url}\")\n    \n    return response\n```\n\n**GraphQL Client Usage Examples:**\n\n```javascript\n// Frontend GraphQL usage examples\nconst QUERIES = {\n  // Simple query\n  GET_POSTS: `\n    query GetPosts($first: Int, $search: String) {\n      posts(first: $first, search: $search) {\n        edges {\n          node {\n            id\n            title\n            excerpt\n            author {\n              displayName\n              avatarUrl\n            }\n            createdAt\n          }\n        }\n        pageInfo {\n          hasNextPage\n          endCursor\n        }\n      }\n    }\n  `,\n  \n  // Complex nested query\n  GET_POST_DETAIL: `\n    query GetPostDetail($id: Int!) {\n      post(id: $id) {\n        id\n        title\n        content\n        tags\n        wordCount\n        author {\n          displayName\n          bio\n          avatarUrl\n        }\n        comments {\n          id\n          content\n          createdAt\n          author {\n            displayName\n          }\n          replies {\n            id\n            content\n            author {\n              displayName\n            }\n          }\n        }\n      }\n    }\n  `,\n  \n  // Mutation\n  CREATE_POST: `\n    mutation CreatePost($input: CreatePostInput!) {\n      createPost(input: $input) {\n        post {\n          id\n          title\n          published\n        }\n        errors {\n          field\n          message\n        }\n      }\n    }\n  `,\n  \n  // Subscription\n  SUBSCRIBE_COMMENTS: `\n    subscription SubscribeToComments($postId: Int!) {\n      postComments(postId: $postId) {\n        id\n        content\n        author {\n          displayName\n        }\n      }\n    }\n  `\n};\n\n// Using with fetch\nasync function fetchPosts() {\n  const response = await fetch('/graphql', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      'Authorization': 'Bearer ' + localStorage.getItem('token')\n    },\n    body: JSON.stringify({\n      query: QUERIES.GET_POSTS,\n      variables: { first: 10 }\n    })\n  });\n  \n  const result = await response.json();\n  \n  if (result.errors) {\n    console.error('GraphQL errors:', result.errors);\n  } else {\n    console.log('Posts:', result.data.posts);\n  }\n}\n\n// Using with Apollo Client or urql would be even simpler\n```\n\n---\n\n### üè† Homework: Build GraphQL API\n\n**Task:** Convert your REST blog API to GraphQL\n\n```python\n# Create a complete GraphQL API with:\n# 1. Schema with Users, Posts, Comments, and Tags\n# 2. Queries with filtering, pagination, and search\n# 3. Mutations for CRUD operations\n# 4. Subscriptions for real-time updates\n# 5. DataLoaders for N+1 query prevention\n# 6. Authentication and authorization\n# 7. Query complexity analysis and rate limiting\n# 8. Error handling and validation\n\n# Schema Requirements:\n# - User management (register, login, profile)\n# - Post management (create, edit, publish, delete)\n# - Comment system (nested comments, replies)\n# - Tag system (popular tags, filtering)\n# - Real-time notifications\n\n# Performance Requirements:\n# - Efficient data fetching (no N+1 queries)\n# - Query complexity limits\n# - Rate limiting\n# - Caching strategies\n\n# Bonus Features:\n# - File upload mutations\n# - Advanced search with filters\n# - Analytics queries (post views, user activity)\n# - Admin queries for moderation\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ GraphQL = Query language for flexible data fetching\n‚úÖ Schema & Resolvers = Define API structure and data sources\n‚úÖ DataLoaders = Solve N+1 query performance problems\n‚úÖ Security = Query limits, authentication, and validation\n‚úÖ Real-time = Subscriptions for live updates\n\n---\n\n<a name=\"hour-38\"></a>\n## üìÖ Hour 38: Microservices & API Gateway Concepts\n\n### üéØ Learning Objectives\n- Understand microservices architecture patterns\n- Implement service-to-service communication\n- Build API Gateway with routing and middleware\n- Handle service discovery and load balancing\n- Implement distributed system patterns (Circuit Breaker, Retry, etc.)\n\n### üìñ What to Teach\n\n**\"Today we break our monolith into microservices and learn to orchestrate distributed systems!\"**\n\n---\n\n### 1Ô∏è‚É£ Microservices Architecture Overview (10 minutes)\n\n**Monolith vs Microservices:**\n\n```python\n# MONOLITH ARCHITECTURE\n\"\"\"\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ           Blog Application              ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ  Users  ‚îÇ ‚îÇ  Posts  ‚îÇ ‚îÇ Comments  ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ Service ‚îÇ ‚îÇ Service ‚îÇ ‚îÇ  Service  ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îÇ              ‚îÇ                          ‚îÇ\n‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n‚îÇ      ‚îÇ    Database       ‚îÇ             ‚îÇ\n‚îÇ      ‚îÇ   (PostgreSQL)    ‚îÇ             ‚îÇ\n‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nMonolith Challenges:\n‚ùå Single point of failure\n‚ùå Difficult to scale individual components\n‚ùå Technology lock-in\n‚ùå Large team coordination issues\n‚ùå Deployment of entire application required\n‚ùå Database bottlenecks\n\"\"\"\n\n# MICROSERVICES ARCHITECTURE\n\"\"\"\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ User Service ‚îÇ    ‚îÇ Post Service ‚îÇ    ‚îÇComment Service‚îÇ\n‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ ‚îÇUsers DB  ‚îÇ ‚îÇ    ‚îÇ ‚îÇPosts DB  ‚îÇ ‚îÇ    ‚îÇ ‚îÇComments DB‚îÇ ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ                     ‚îÇ                     ‚îÇ\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                             ‚îÇ\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n              ‚îÇ       API Gateway           ‚îÇ\n              ‚îÇ   (Routing, Auth, etc.)     ‚îÇ\n              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                             ‚îÇ\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n              ‚îÇ      Load Balancer          ‚îÇ\n              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nMicroservices Benefits:\n‚úÖ Independent deployment and scaling\n‚úÖ Technology diversity\n‚úÖ Team autonomy\n‚úÖ Fault isolation\n‚úÖ Better resource utilization\n\nMicroservices Challenges:\n‚ö†Ô∏è Network complexity\n‚ö†Ô∏è Data consistency issues\n‚ö†Ô∏è Service discovery\n‚ö†Ô∏è Monitoring and debugging\n‚ö†Ô∏è Increased operational overhead\n\"\"\"\n\n# When to use Microservices\nmicroservices_decision_matrix = {\n    \"Team Size\": \"Multiple teams (3+ teams)\",\n    \"Application Complexity\": \"High complexity with distinct domains\",\n    \"Scalability Needs\": \"Different scaling requirements per component\",\n    \"Technology Diversity\": \"Need different tech stacks\",\n    \"Deployment Frequency\": \"Frequent, independent deployments\",\n    \"Fault Tolerance\": \"Need service isolation\",\n    \n    \"Stay with Monolith if\": {\n        \"Small team\": \"< 10 developers\",\n        \"Simple application\": \"CRUD with minimal business logic\",\n        \"Uniform scaling\": \"All components scale together\",\n        \"Rapid prototyping\": \"MVP or early-stage product\",\n        \"Limited ops experience\": \"No DevOps/infrastructure expertise\"\n    }\n}\n```\n\n**Service Decomposition Strategies:**\n\n```python\n# Domain-Driven Design (DDD) approach to service boundaries\nclass BlogDomainAnalysis:\n    \"\"\"Analyze blog domain to identify service boundaries\"\"\"\n    \n    def __init__(self):\n        self.bounded_contexts = {\n            \"User Management\": {\n                \"entities\": [\"User\", \"Profile\", \"Preferences\"],\n                \"operations\": [\"register\", \"login\", \"update_profile\"],\n                \"data\": \"user_database\",\n                \"team\": \"auth_team\"\n            },\n            \n            \"Content Management\": {\n                \"entities\": [\"Post\", \"Draft\", \"Category\", \"Tag\"],\n                \"operations\": [\"create_post\", \"publish\", \"edit\", \"search\"],\n                \"data\": \"content_database\", \n                \"team\": \"content_team\"\n            },\n            \n            \"Engagement\": {\n                \"entities\": [\"Comment\", \"Like\", \"Share\", \"View\"],\n                \"operations\": [\"comment\", \"like\", \"track_view\"],\n                \"data\": \"engagement_database\",\n                \"team\": \"engagement_team\"\n            },\n            \n            \"Notification\": {\n                \"entities\": [\"Notification\", \"EmailTemplate\", \"Preference\"],\n                \"operations\": [\"send_notification\", \"email\", \"push\"],\n                \"data\": \"notification_database\",\n                \"team\": \"notification_team\"\n            },\n            \n            \"Analytics\": {\n                \"entities\": [\"Event\", \"Metric\", \"Report\"],\n                \"operations\": [\"track_event\", \"generate_report\"],\n                \"data\": \"analytics_database\",\n                \"team\": \"analytics_team\"\n            }\n        }\n    \n    def get_service_dependencies(self) -> dict:\n        \"\"\"Map service dependencies\"\"\"\n        \n        return {\n            \"content_service\": {\n                \"depends_on\": [\"user_service\"],  # Need author info\n                \"consumed_by\": [\"engagement_service\", \"notification_service\"]\n            },\n            \n            \"engagement_service\": {\n                \"depends_on\": [\"user_service\", \"content_service\"],\n                \"consumed_by\": [\"notification_service\", \"analytics_service\"]\n            },\n            \n            \"notification_service\": {\n                \"depends_on\": [\"user_service\", \"content_service\", \"engagement_service\"],\n                \"consumed_by\": []  # Leaf service\n            },\n            \n            \"user_service\": {\n                \"depends_on\": [],  # Core service\n                \"consumed_by\": [\"content_service\", \"engagement_service\", \"notification_service\"]\n            },\n            \n            \"analytics_service\": {\n                \"depends_on\": [\"user_service\", \"content_service\", \"engagement_service\"],\n                \"consumed_by\": []  # Leaf service\n            }\n        }\n\n# Service sizing guidelines\nservice_sizing_guide = {\n    \"team_size\": \"6-8 developers max per service\",\n    \"codebase\": \"Can be rewritten in 2-4 weeks\",\n    \"database\": \"Single service owns its data\",\n    \"deployment\": \"Independent deployment pipeline\",\n    \"monitoring\": \"Separate metrics and logging\",\n    \"api_surface\": \"Well-defined, stable interface\"\n}\n```\n\n---\n\n### 2Ô∏è‚É£ Service-to-Service Communication (15 minutes)\n\n**HTTP API Communication:**\n\n```python\n# service_client.py\nimport httpx\nimport asyncio\nfrom typing import Dict, Any, Optional, List\nimport logging\nfrom datetime import datetime, timedelta\nimport json\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nlogger = logging.getLogger(__name__)\n\nclass ServiceClient:\n    \"\"\"Base class for service-to-service HTTP communication\"\"\"\n    \n    def __init__(\n        self, \n        base_url: str, \n        service_name: str,\n        timeout: int = 30,\n        retries: int = 3\n    ):\n        self.base_url = base_url.rstrip('/')\n        self.service_name = service_name\n        self.timeout = timeout\n        self.retries = retries\n        \n        # Create HTTP client with connection pooling\n        self.client = httpx.AsyncClient(\n            timeout=httpx.Timeout(timeout),\n            limits=httpx.Limits(max_keepalive_connections=20, max_connections=100)\n        )\n        \n    async def __aenter__(self):\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.client.aclose()\n    \n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10)\n    )\n    async def _make_request(\n        self, \n        method: str, \n        endpoint: str, \n        data: Optional[Dict] = None,\n        params: Optional[Dict] = None,\n        headers: Optional[Dict] = None\n    ) -> Dict[Any, Any]:\n        \"\"\"Make HTTP request with retry logic\"\"\"\n        \n        url = f\"{self.base_url}{endpoint}\"\n        \n        # Add service identification headers\n        request_headers = {\n            \"User-Agent\": f\"ServiceClient/{self.service_name}\",\n            \"X-Service-Name\": self.service_name,\n            \"Content-Type\": \"application/json\"\n        }\n        \n        if headers:\n            request_headers.update(headers)\n        \n        try:\n            logger.info(f\"üîÑ {method} {url} - Service: {self.service_name}\")\n            \n            response = await self.client.request(\n                method=method,\n                url=url,\n                json=data,\n                params=params,\n                headers=request_headers\n            )\n            \n            # Log response time for monitoring\n            response_time = response.elapsed.total_seconds()\n            logger.info(f\"‚úÖ {method} {url} - {response.status_code} - {response_time:.3f}s\")\n            \n            # Handle different response codes\n            if response.status_code == 404:\n                return None\n            \n            elif response.status_code >= 500:\n                # Server error - should retry\n                response.raise_for_status()\n            \n            elif response.status_code >= 400:\n                # Client error - don't retry\n                error_msg = f\"Client error: {response.status_code}\"\n                try:\n                    error_detail = response.json()\n                    error_msg += f\" - {error_detail}\"\n                except:\n                    error_msg += f\" - {response.text}\"\n                \n                raise ValueError(error_msg)\n            \n            return response.json()\n            \n        except httpx.TimeoutException:\n            logger.error(f\"‚è±Ô∏è Timeout calling {self.service_name} - {url}\")\n            raise\n        \n        except httpx.ConnectError:\n            logger.error(f\"üîå Connection error to {self.service_name} - {url}\")\n            raise\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error calling {self.service_name} - {url}: {str(e)}\")\n            raise\n    \n    async def get(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Dict]:\n        \"\"\"GET request\"\"\"\n        return await self._make_request(\"GET\", endpoint, params=params)\n    \n    async def post(self, endpoint: str, data: Dict) -> Dict:\n        \"\"\"POST request\"\"\"\n        return await self._make_request(\"POST\", endpoint, data=data)\n    \n    async def put(self, endpoint: str, data: Dict) -> Dict:\n        \"\"\"PUT request\"\"\"\n        return await self._make_request(\"PUT\", endpoint, data=data)\n    \n    async def delete(self, endpoint: str) -> bool:\n        \"\"\"DELETE request\"\"\"\n        result = await self._make_request(\"DELETE\", endpoint)\n        return result is not None\n\nclass UserServiceClient(ServiceClient):\n    \"\"\"Client for User Service API\"\"\"\n    \n    def __init__(self, base_url: str = \"http://user-service:8000\"):\n        super().__init__(base_url, \"user-service\")\n    \n    async def get_user(self, user_id: int) -> Optional[Dict]:\n        \"\"\"Get user by ID\"\"\"\n        return await self.get(f\"/api/v1/users/{user_id}\")\n    \n    async def get_users_by_ids(self, user_ids: List[int]) -> List[Dict]:\n        \"\"\"Batch get users by IDs\"\"\"\n        response = await self.post(\"/api/v1/users/batch\", {\"user_ids\": user_ids})\n        return response.get(\"users\", [])\n    \n    async def validate_user_token(self, token: str) -> Optional[Dict]:\n        \"\"\"Validate user authentication token\"\"\"\n        return await self.post(\"/api/v1/auth/validate\", {\"token\": token})\n\nclass ContentServiceClient(ServiceClient):\n    \"\"\"Client for Content Service API\"\"\"\n    \n    def __init__(self, base_url: str = \"http://content-service:8000\"):\n        super().__init__(base_url, \"content-service\")\n    \n    async def get_post(self, post_id: int) -> Optional[Dict]:\n        \"\"\"Get post by ID\"\"\"\n        return await self.get(f\"/api/v1/posts/{post_id}\")\n    \n    async def create_post(self, post_data: Dict, author_id: int) -> Dict:\n        \"\"\"Create new post\"\"\"\n        return await self.post(\"/api/v1/posts\", {\n            **post_data,\n            \"author_id\": author_id\n        })\n    \n    async def get_posts_by_author(self, author_id: int, limit: int = 10) -> List[Dict]:\n        \"\"\"Get posts by author\"\"\"\n        response = await self.get(f\"/api/v1/posts\", params={\n            \"author_id\": author_id,\n            \"limit\": limit\n        })\n        return response.get(\"posts\", [])\n\nclass NotificationServiceClient(ServiceClient):\n    \"\"\"Client for Notification Service API\"\"\"\n    \n    def __init__(self, base_url: str = \"http://notification-service:8000\"):\n        super().__init__(base_url, \"notification-service\")\n    \n    async def send_notification(\n        self, \n        user_id: int, \n        notification_type: str, \n        data: Dict\n    ) -> Dict:\n        \"\"\"Send notification to user\"\"\"\n        return await self.post(\"/api/v1/notifications\", {\n            \"user_id\": user_id,\n            \"type\": notification_type,\n            \"data\": data\n        })\n    \n    async def send_email(\n        self, \n        to_email: str, \n        subject: str, \n        template: str, \n        variables: Dict\n    ) -> Dict:\n        \"\"\"Send email notification\"\"\"\n        return await self.post(\"/api/v1/notifications/email\", {\n            \"to_email\": to_email,\n            \"subject\": subject,\n            \"template\": template,\n            \"variables\": variables\n        })\n\n# Service registry for dependency injection\nclass ServiceRegistry:\n    \"\"\"Registry for service clients\"\"\"\n    \n    def __init__(self):\n        self._clients = {}\n    \n    def register_service(self, name: str, client: ServiceClient):\n        \"\"\"Register a service client\"\"\"\n        self._clients[name] = client\n    \n    def get_service(self, name: str) -> ServiceClient:\n        \"\"\"Get service client by name\"\"\"\n        if name not in self._clients:\n            raise ValueError(f\"Service '{name}' not registered\")\n        return self._clients[name]\n    \n    async def health_check_all(self) -> Dict[str, Any]:\n        \"\"\"Check health of all registered services\"\"\"\n        \n        health_results = {}\n        \n        for service_name, client in self._clients.items():\n            try:\n                # Try to call health endpoint\n                start_time = datetime.now()\n                health_data = await client.get(\"/health\")\n                response_time = (datetime.now() - start_time).total_seconds()\n                \n                health_results[service_name] = {\n                    \"status\": \"healthy\",\n                    \"response_time\": response_time,\n                    \"details\": health_data\n                }\n                \n            except Exception as e:\n                health_results[service_name] = {\n                    \"status\": \"unhealthy\", \n                    \"error\": str(e),\n                    \"response_time\": None\n                }\n        \n        return health_results\n\n# Initialize service registry\nservice_registry = ServiceRegistry()\n\n# Register services\nasync def setup_service_clients():\n    \"\"\"Setup service clients\"\"\"\n    \n    service_registry.register_service(\"users\", UserServiceClient())\n    service_registry.register_service(\"content\", ContentServiceClient()) \n    service_registry.register_service(\"notifications\", NotificationServiceClient())\n```\n\n**Event-Driven Communication:**\n\n```python\n# event_system.py\nimport json\nimport asyncio\nfrom typing import Dict, Any, Callable, List\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nimport uuid\nimport aioredis\nfrom enum import Enum\n\nclass EventType(Enum):\n    \"\"\"Domain events for microservices communication\"\"\"\n    \n    USER_REGISTERED = \"user.registered\"\n    USER_UPDATED = \"user.updated\"\n    POST_CREATED = \"post.created\"\n    POST_PUBLISHED = \"post.published\"\n    COMMENT_CREATED = \"comment.created\"\n    NOTIFICATION_SENT = \"notification.sent\"\n\n@dataclass\nclass DomainEvent:\n    \"\"\"Base domain event\"\"\"\n    \n    event_id: str\n    event_type: EventType\n    aggregate_id: str\n    data: Dict[str, Any]\n    timestamp: datetime\n    version: int = 1\n    \n    @classmethod\n    def create(cls, event_type: EventType, aggregate_id: str, data: Dict[str, Any]):\n        \"\"\"Create new domain event\"\"\"\n        return cls(\n            event_id=str(uuid.uuid4()),\n            event_type=event_type,\n            aggregate_id=aggregate_id,\n            data=data,\n            timestamp=datetime.utcnow()\n        )\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization\"\"\"\n        return {\n            **asdict(self),\n            \"event_type\": self.event_type.value,\n            \"timestamp\": self.timestamp.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"DomainEvent\":\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            event_id=data[\"event_id\"],\n            event_type=EventType(data[\"event_type\"]),\n            aggregate_id=data[\"aggregate_id\"],\n            data=data[\"data\"],\n            timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n            version=data.get(\"version\", 1)\n        )\n\nclass EventBus:\n    \"\"\"Event bus for microservices communication using Redis\"\"\"\n    \n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis_url = redis_url\n        self.redis = None\n        self.subscribers = {}\n        self.running = False\n    \n    async def connect(self):\n        \"\"\"Connect to Redis\"\"\"\n        self.redis = await aioredis.from_url(self.redis_url)\n    \n    async def disconnect(self):\n        \"\"\"Disconnect from Redis\"\"\"\n        if self.redis:\n            await self.redis.close()\n    \n    async def publish_event(self, event: DomainEvent):\n        \"\"\"Publish domain event\"\"\"\n        \n        if not self.redis:\n            await self.connect()\n        \n        # Serialize event\n        event_data = json.dumps(event.to_dict())\n        \n        # Publish to Redis streams\n        stream_name = f\"events:{event.event_type.value}\"\n        \n        await self.redis.xadd(stream_name, event_data)\n        \n        logger.info(f\"üì§ Published event: {event.event_type.value} - {event.event_id}\")\n    \n    async def subscribe_to_events(\n        self, \n        event_types: List[EventType], \n        handler: Callable[[DomainEvent], None],\n        consumer_group: str = \"default\"\n    ):\n        \"\"\"Subscribe to domain events\"\"\"\n        \n        if not self.redis:\n            await self.connect()\n        \n        # Create consumer group for each event type\n        for event_type in event_types:\n            stream_name = f\"events:{event_type.value}\"\n            \n            try:\n                await self.redis.xgroup_create(stream_name, consumer_group, id=\"0\", mkstream=True)\n            except Exception:\n                # Group already exists\n                pass\n        \n        # Store handler\n        for event_type in event_types:\n            if event_type not in self.subscribers:\n                self.subscribers[event_type] = []\n            self.subscribers[event_type].append(handler)\n        \n        logger.info(f\"üì• Subscribed to events: {[et.value for et in event_types]}\")\n    \n    async def start_consuming(self, consumer_name: str = None):\n        \"\"\"Start consuming events\"\"\"\n        \n        if not consumer_name:\n            consumer_name = f\"consumer-{uuid.uuid4().hex[:8]}\"\n        \n        self.running = True\n        \n        while self.running:\n            try:\n                # Check each subscribed event type\n                for event_type in self.subscribers.keys():\n                    stream_name = f\"events:{event_type.value}\"\n                    \n                    # Read from stream\n                    messages = await self.redis.xreadgroup(\n                        \"default\",\n                        consumer_name,\n                        {stream_name: \">\"},\n                        count=10,\n                        block=1000  # 1 second timeout\n                    )\n                    \n                    # Process messages\n                    for stream, msgs in messages:\n                        for msg_id, fields in msgs:\n                            try:\n                                # Parse event\n                                event_data = json.loads(fields[b'data'].decode())\n                                event = DomainEvent.from_dict(event_data)\n                                \n                                # Call handlers\n                                handlers = self.subscribers.get(event.event_type, [])\n                                for handler in handlers:\n                                    try:\n                                        await handler(event)\n                                    except Exception as e:\n                                        logger.error(f\"‚ùå Event handler error: {str(e)}\")\n                                \n                                # Acknowledge message\n                                await self.redis.xack(stream_name, \"default\", msg_id)\n                                \n                            except Exception as e:\n                                logger.error(f\"‚ùå Error processing event: {str(e)}\")\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Event consumption error: {str(e)}\")\n                await asyncio.sleep(5)  # Wait before retry\n    \n    def stop_consuming(self):\n        \"\"\"Stop consuming events\"\"\"\n        self.running = False\n\n# Event handlers for different services\nclass ContentServiceEventHandler:\n    \"\"\"Handle events in content service\"\"\"\n    \n    def __init__(self, content_service, notification_client):\n        self.content_service = content_service\n        self.notification_client = notification_client\n    \n    async def handle_user_registered(self, event: DomainEvent):\n        \"\"\"Handle user registration event\"\"\"\n        \n        user_id = event.data[\"user_id\"]\n        username = event.data[\"username\"]\n        \n        logger.info(f\"üë§ New user registered: {username} (ID: {user_id})\")\n        \n        # Send welcome notification\n        await self.notification_client.send_notification(\n            user_id=user_id,\n            notification_type=\"welcome\",\n            data={\n                \"username\": username,\n                \"message\": \"Welcome to our blog platform!\"\n            }\n        )\n    \n    async def handle_post_published(self, event: DomainEvent):\n        \"\"\"Handle post publication event\"\"\"\n        \n        post_id = event.data[\"post_id\"]\n        author_id = event.data[\"author_id\"]\n        title = event.data[\"title\"]\n        \n        logger.info(f\"üìù Post published: {title} (ID: {post_id})\")\n        \n        # Notify followers (simplified)\n        followers = await self.content_service.get_author_followers(author_id)\n        \n        for follower_id in followers:\n            await self.notification_client.send_notification(\n                user_id=follower_id,\n                notification_type=\"new_post\",\n                data={\n                    \"post_id\": post_id,\n                    \"title\": title,\n                    \"author_id\": author_id\n                }\n            )\n\n# Event publishing examples\nclass PostService:\n    \"\"\"Content service that publishes domain events\"\"\"\n    \n    def __init__(self, event_bus: EventBus):\n        self.event_bus = event_bus\n    \n    async def create_post(self, title: str, content: str, author_id: int) -> Dict:\n        \"\"\"Create post and publish event\"\"\"\n        \n        # Create post in database\n        post_data = {\n            \"id\": 123,  # From database\n            \"title\": title,\n            \"content\": content,\n            \"author_id\": author_id,\n            \"created_at\": datetime.utcnow().isoformat()\n        }\n        \n        # Publish domain event\n        event = DomainEvent.create(\n            event_type=EventType.POST_CREATED,\n            aggregate_id=str(post_data[\"id\"]),\n            data=post_data\n        )\n        \n        await self.event_bus.publish_event(event)\n        \n        return post_data\n    \n    async def publish_post(self, post_id: int) -> bool:\n        \"\"\"Publish post and emit event\"\"\"\n        \n        # Update post status in database\n        # post = update_post_status(post_id, \"published\")\n        \n        # Publish domain event\n        event = DomainEvent.create(\n            event_type=EventType.POST_PUBLISHED,\n            aggregate_id=str(post_id),\n            data={\n                \"post_id\": post_id,\n                \"author_id\": 1,  # From database\n                \"title\": \"Sample Post\",  # From database\n                \"published_at\": datetime.utcnow().isoformat()\n            }\n        )\n        \n        await self.event_bus.publish_event(event)\n        \n        return True\n```\n\n---\n\n### 3Ô∏è‚É£ API Gateway Implementation (15 minutes)\n\n**API Gateway with FastAPI:**\n\n```python\n# api_gateway.py\nfrom fastapi import FastAPI, Request, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import Response\nimport httpx\nimport asyncio\nfrom typing import Dict, Any, Optional\nimport time\nimport logging\nfrom urllib.parse import urljoin\nimport json\n\nlogger = logging.getLogger(__name__)\n\nclass APIGateway:\n    \"\"\"API Gateway for microservices routing\"\"\"\n    \n    def __init__(self):\n        self.app = FastAPI(title=\"API Gateway\", version=\"1.0.0\")\n        self.service_registry = {}\n        self.load_balancers = {}\n        self.circuit_breakers = {}\n        \n        # HTTP client for service communication\n        self.client = httpx.AsyncClient(\n            timeout=httpx.Timeout(30.0),\n            limits=httpx.Limits(max_keepalive_connections=100, max_connections=200)\n        )\n        \n        self._setup_middleware()\n        self._setup_routes()\n    \n    def _setup_middleware(self):\n        \"\"\"Setup gateway middleware\"\"\"\n        \n        # CORS\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],  # Configure for production\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        \n        # Request/Response logging\n        @self.app.middleware(\"http\")\n        async def log_requests(request: Request, call_next):\n            start_time = time.time()\n            \n            response = await call_next(request)\n            \n            process_time = time.time() - start_time\n            logger.info(\n                f\"üö™ Gateway: {request.method} {request.url.path} - \"\n                f\"{response.status_code} - {process_time:.3f}s\"\n            )\n            \n            return response\n    \n    def register_service(\n        self, \n        name: str, \n        instances: list, \n        health_endpoint: str = \"/health\"\n    ):\n        \"\"\"Register a microservice with multiple instances\"\"\"\n        \n        self.service_registry[name] = {\n            \"instances\": instances,\n            \"health_endpoint\": health_endpoint,\n            \"healthy_instances\": instances.copy()\n        }\n        \n        # Initialize load balancer\n        self.load_balancers[name] = RoundRobinLoadBalancer(instances)\n        \n        # Initialize circuit breaker\n        self.circuit_breakers[name] = CircuitBreaker(\n            failure_threshold=5,\n            timeout=60,\n            expected_exception=httpx.RequestError\n        )\n        \n        logger.info(f\"üìù Registered service: {name} with {len(instances)} instances\")\n    \n    def _setup_routes(self):\n        \"\"\"Setup gateway routes\"\"\"\n        \n        # Health check endpoint\n        @self.app.get(\"/health\")\n        async def gateway_health():\n            \"\"\"Gateway health check\"\"\"\n            \n            service_health = {}\n            \n            for service_name in self.service_registry.keys():\n                try:\n                    healthy_count = len(self.service_registry[service_name][\"healthy_instances\"])\n                    total_count = len(self.service_registry[service_name][\"instances\"])\n                    \n                    service_health[service_name] = {\n                        \"healthy\": healthy_count,\n                        \"total\": total_count,\n                        \"status\": \"healthy\" if healthy_count > 0 else \"unhealthy\"\n                    }\n                except Exception as e:\n                    service_health[service_name] = {\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    }\n            \n            return {\n                \"gateway\": \"healthy\",\n                \"services\": service_health,\n                \"timestamp\": time.time()\n            }\n        \n        # Route all API requests\n        @self.app.api_route(\n            \"/api/{service_name}/{path:path}\",\n            methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"]\n        )\n        async def route_to_service(\n            service_name: str,\n            path: str,\n            request: Request\n        ):\n            \"\"\"Route requests to appropriate microservice\"\"\"\n            \n            if service_name not in self.service_registry:\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"Service '{service_name}' not found\"\n                )\n            \n            # Get healthy service instance\n            try:\n                service_url = self.load_balancers[service_name].get_instance()\n                if not service_url:\n                    raise HTTPException(\n                        status_code=503,\n                        detail=f\"No healthy instances available for service '{service_name}'\"\n                    )\n                \n                # Use circuit breaker\n                circuit_breaker = self.circuit_breakers[service_name]\n                \n                return await circuit_breaker.call(\n                    self._forward_request,\n                    service_url,\n                    path,\n                    request\n                )\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Error routing to {service_name}: {str(e)}\")\n                raise HTTPException(status_code=502, detail=str(e))\n    \n    async def _forward_request(\n        self,\n        service_url: str,\n        path: str,\n        request: Request\n    ) -> Response:\n        \"\"\"Forward request to microservice\"\"\"\n        \n        # Build target URL\n        target_url = urljoin(service_url, f\"/api/{path}\")\n        \n        # Get request data\n        body = await request.body()\n        \n        # Forward headers (filter out hop-by-hop headers)\n        forward_headers = {}\n        for name, value in request.headers.items():\n            if name.lower() not in ['host', 'content-length', 'connection']:\n                forward_headers[name] = value\n        \n        # Add gateway headers\n        forward_headers['X-Gateway'] = 'APIGateway/1.0'\n        forward_headers['X-Forwarded-For'] = request.client.host\n        \n        try:\n            # Make request to microservice\n            response = await self.client.request(\n                method=request.method,\n                url=target_url,\n                content=body,\n                headers=forward_headers,\n                params=dict(request.query_params)\n            )\n            \n            # Return response\n            return Response(\n                content=response.content,\n                status_code=response.status_code,\n                headers=dict(response.headers)\n            )\n            \n        except httpx.TimeoutException:\n            logger.error(f\"‚è±Ô∏è Timeout calling service: {target_url}\")\n            raise HTTPException(status_code=504, detail=\"Gateway timeout\")\n        \n        except httpx.RequestError as e:\n            logger.error(f\"üîå Request error: {target_url} - {str(e)}\")\n            raise HTTPException(status_code=502, detail=\"Bad Gateway\")\n    \n    async def start_health_checks(self):\n        \"\"\"Start background health checking\"\"\"\n        \n        asyncio.create_task(self._health_check_loop())\n    \n    async def _health_check_loop(self):\n        \"\"\"Continuously check service health\"\"\"\n        \n        while True:\n            try:\n                for service_name, config in self.service_registry.items():\n                    healthy_instances = []\n                    \n                    for instance_url in config[\"instances\"]:\n                        if await self._check_instance_health(\n                            instance_url, \n                            config[\"health_endpoint\"]\n                        ):\n                            healthy_instances.append(instance_url)\n                    \n                    # Update healthy instances\n                    self.service_registry[service_name][\"healthy_instances\"] = healthy_instances\n                    self.load_balancers[service_name].update_instances(healthy_instances)\n                    \n                    logger.debug(f\"üè• {service_name}: {len(healthy_instances)}/{len(config['instances'])} healthy\")\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Health check error: {str(e)}\")\n            \n            # Check every 30 seconds\n            await asyncio.sleep(30)\n    \n    async def _check_instance_health(self, instance_url: str, health_endpoint: str) -> bool:\n        \"\"\"Check if service instance is healthy\"\"\"\n        \n        try:\n            health_url = urljoin(instance_url, health_endpoint)\n            response = await self.client.get(health_url, timeout=5.0)\n            return response.status_code == 200\n            \n        except Exception:\n            return False\n\nclass RoundRobinLoadBalancer:\n    \"\"\"Simple round-robin load balancer\"\"\"\n    \n    def __init__(self, instances: list):\n        self.instances = instances.copy()\n        self.current_index = 0\n    \n    def get_instance(self) -> Optional[str]:\n        \"\"\"Get next instance using round-robin\"\"\"\n        \n        if not self.instances:\n            return None\n        \n        instance = self.instances[self.current_index]\n        self.current_index = (self.current_index + 1) % len(self.instances)\n        \n        return instance\n    \n    def update_instances(self, instances: list):\n        \"\"\"Update available instances\"\"\"\n        self.instances = instances.copy()\n        if self.instances:\n            self.current_index = self.current_index % len(self.instances)\n        else:\n            self.current_index = 0\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker pattern implementation\"\"\"\n    \n    def __init__(\n        self, \n        failure_threshold: int = 5,\n        timeout: int = 60,\n        expected_exception: type = Exception\n    ):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.expected_exception = expected_exception\n        \n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = \"closed\"  # closed, open, half-open\n    \n    async def call(self, func, *args, **kwargs):\n        \"\"\"Execute function with circuit breaker protection\"\"\"\n        \n        if self.state == \"open\":\n            if self._should_attempt_reset():\n                self.state = \"half-open\"\n            else:\n                raise Exception(\"Circuit breaker is OPEN\")\n        \n        try:\n            result = await func(*args, **kwargs)\n            self._on_success()\n            return result\n            \n        except self.expected_exception as e:\n            self._on_failure()\n            raise e\n    \n    def _should_attempt_reset(self) -> bool:\n        \"\"\"Check if circuit breaker should attempt to reset\"\"\"\n        \n        if self.last_failure_time is None:\n            return False\n        \n        return time.time() - self.last_failure_time >= self.timeout\n    \n    def _on_success(self):\n        \"\"\"Handle successful call\"\"\"\n        self.failure_count = 0\n        self.state = \"closed\"\n    \n    def _on_failure(self):\n        \"\"\"Handle failed call\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.failure_count >= self.failure_threshold:\n            self.state = \"open\"\n\n# Gateway setup\ngateway = APIGateway()\n\n# Register services\ngateway.register_service(\"users\", [\n    \"http://user-service-1:8000\",\n    \"http://user-service-2:8000\"\n])\n\ngateway.register_service(\"posts\", [\n    \"http://content-service-1:8000\",\n    \"http://content-service-2:8000\"\n])\n\ngateway.register_service(\"notifications\", [\n    \"http://notification-service:8000\"\n])\n\n# Start the gateway\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    # Start health checks\n    asyncio.create_task(gateway.start_health_checks())\n    \n    # Run gateway\n    uvicorn.run(gateway.app, host=\"0.0.0.0\", port=8080)\n```\n\n**Service Discovery with Consul:**\n\n```python\n# service_discovery.py\nimport consul\nimport asyncio\nimport json\nfrom typing import List, Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ConsulServiceDiscovery:\n    \"\"\"Service discovery using HashiCorp Consul\"\"\"\n    \n    def __init__(self, consul_host: str = \"localhost\", consul_port: int = 8500):\n        self.consul = consul.Consul(host=consul_host, port=consul_port)\n    \n    async def register_service(\n        self,\n        service_name: str,\n        service_id: str,\n        address: str,\n        port: int,\n        health_check_url: str = None,\n        tags: List[str] = None\n    ):\n        \"\"\"Register service with Consul\"\"\"\n        \n        check = None\n        if health_check_url:\n            check = consul.Check.http(health_check_url, interval=\"10s\", timeout=\"3s\")\n        \n        try:\n            self.consul.agent.service.register(\n                name=service_name,\n                service_id=service_id,\n                address=address,\n                port=port,\n                tags=tags or [],\n                check=check\n            )\n            \n            logger.info(f\"üìù Registered service: {service_name} ({service_id}) at {address}:{port}\")\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Failed to register service {service_name}: {str(e)}\")\n            raise\n    \n    async def deregister_service(self, service_id: str):\n        \"\"\"Deregister service from Consul\"\"\"\n        \n        try:\n            self.consul.agent.service.deregister(service_id)\n            logger.info(f\"üóëÔ∏è Deregistered service: {service_id}\")\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Failed to deregister service {service_id}: {str(e)}\")\n    \n    async def discover_services(self, service_name: str) -> List[Dict[str, Any]]:\n        \"\"\"Discover healthy service instances\"\"\"\n        \n        try:\n            _, services = self.consul.health.service(service_name, passing=True)\n            \n            instances = []\n            for service in services:\n                service_info = service['Service']\n                instances.append({\n                    'id': service_info['ID'],\n                    'address': service_info['Address'],\n                    'port': service_info['Port'],\n                    'tags': service_info['Tags'],\n                    'url': f\"http://{service_info['Address']}:{service_info['Port']}\"\n                })\n            \n            logger.debug(f\"üîç Found {len(instances)} instances of {service_name}\")\n            return instances\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Failed to discover service {service_name}: {str(e)}\")\n            return []\n    \n    async def watch_service(\n        self, \n        service_name: str, \n        callback: callable\n    ):\n        \"\"\"Watch for service changes\"\"\"\n        \n        index = None\n        \n        while True:\n            try:\n                index, services = self.consul.health.service(\n                    service_name, \n                    index=index, \n                    wait=\"30s\"\n                )\n                \n                # Process service changes\n                healthy_instances = []\n                for service in services:\n                    if service['Checks'][0]['Status'] == 'passing':\n                        service_info = service['Service']\n                        healthy_instances.append({\n                            'id': service_info['ID'],\n                            'url': f\"http://{service_info['Address']}:{service_info['Port']}\"\n                        })\n                \n                # Call callback with updated instances\n                await callback(service_name, healthy_instances)\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Error watching service {service_name}: {str(e)}\")\n                await asyncio.sleep(10)\n\n# Enhanced API Gateway with service discovery\nclass DiscoveryAPIGateway(APIGateway):\n    \"\"\"API Gateway with dynamic service discovery\"\"\"\n    \n    def __init__(self, consul_host: str = \"localhost\"):\n        super().__init__()\n        self.service_discovery = ConsulServiceDiscovery(consul_host)\n        self.watched_services = set()\n    \n    async def register_and_watch_service(self, service_name: str):\n        \"\"\"Register service for discovery and start watching\"\"\"\n        \n        if service_name in self.watched_services:\n            return\n        \n        self.watched_services.add(service_name)\n        \n        # Start watching service\n        asyncio.create_task(\n            self.service_discovery.watch_service(\n                service_name,\n                self._on_service_change\n            )\n        )\n        \n        logger.info(f\"üëÅÔ∏è Started watching service: {service_name}\")\n    \n    async def _on_service_change(self, service_name: str, instances: List[Dict]):\n        \"\"\"Handle service instance changes\"\"\"\n        \n        urls = [instance['url'] for instance in instances]\n        \n        # Update service registry\n        if service_name not in self.service_registry:\n            self.service_registry[service_name] = {\n                \"instances\": urls,\n                \"health_endpoint\": \"/health\",\n                \"healthy_instances\": urls\n            }\n            self.load_balancers[service_name] = RoundRobinLoadBalancer(urls)\n        else:\n            self.service_registry[service_name][\"instances\"] = urls\n            self.service_registry[service_name][\"healthy_instances\"] = urls\n            self.load_balancers[service_name].update_instances(urls)\n        \n        logger.info(f\"üîÑ Updated {service_name} instances: {len(urls)} available\")\n```\n\n---\n\n### 4Ô∏è‚É£ Distributed System Patterns (10 minutes)\n\n**Resilience Patterns:**\n\n```python\n# distributed_patterns.py\nimport asyncio\nimport time\nfrom typing import Any, Callable, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass RetryStrategy(Enum):\n    FIXED_DELAY = \"fixed\"\n    EXPONENTIAL_BACKOFF = \"exponential\"\n    LINEAR_BACKOFF = \"linear\"\n\n@dataclass\nclass RetryConfig:\n    max_attempts: int = 3\n    base_delay: float = 1.0\n    max_delay: float = 60.0\n    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF\n    backoff_multiplier: float = 2.0\n\nclass RetryHandler:\n    \"\"\"Configurable retry handler with different strategies\"\"\"\n    \n    def __init__(self, config: RetryConfig):\n        self.config = config\n    \n    async def execute_with_retry(\n        self,\n        func: Callable,\n        *args,\n        **kwargs\n    ) -> Any:\n        \"\"\"Execute function with retry logic\"\"\"\n        \n        last_exception = None\n        \n        for attempt in range(self.config.max_attempts):\n            try:\n                return await func(*args, **kwargs)\n                \n            except Exception as e:\n                last_exception = e\n                \n                if attempt < self.config.max_attempts - 1:\n                    delay = self._calculate_delay(attempt)\n                    \n                    logger.warning(\n                        f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {str(e)}. \"\n                        f\"Retrying in {delay:.2f}s...\"\n                    )\n                    \n                    await asyncio.sleep(delay)\n                else:\n                    logger.error(f\"‚ùå All {self.config.max_attempts} attempts failed\")\n        \n        raise last_exception\n    \n    def _calculate_delay(self, attempt: int) -> float:\n        \"\"\"Calculate delay based on retry strategy\"\"\"\n        \n        if self.config.strategy == RetryStrategy.FIXED_DELAY:\n            return self.config.base_delay\n        \n        elif self.config.strategy == RetryStrategy.LINEAR_BACKOFF:\n            delay = self.config.base_delay * (attempt + 1)\n        \n        elif self.config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:\n            delay = self.config.base_delay * (self.config.backoff_multiplier ** attempt)\n        \n        else:\n            delay = self.config.base_delay\n        \n        return min(delay, self.config.max_delay)\n\nclass BulkheadPattern:\n    \"\"\"Bulkhead pattern for resource isolation\"\"\"\n    \n    def __init__(self, name: str, max_concurrent: int = 10):\n        self.name = name\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.active_requests = 0\n        self.total_requests = 0\n        self.failed_requests = 0\n    \n    async def execute(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with bulkhead protection\"\"\"\n        \n        async with self.semaphore:\n            self.active_requests += 1\n            self.total_requests += 1\n            \n            try:\n                result = await func(*args, **kwargs)\n                return result\n                \n            except Exception as e:\n                self.failed_requests += 1\n                raise e\n            \n            finally:\n                self.active_requests -= 1\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get bulkhead statistics\"\"\"\n        \n        return {\n            \"name\": self.name,\n            \"active_requests\": self.active_requests,\n            \"total_requests\": self.total_requests,\n            \"failed_requests\": self.failed_requests,\n            \"success_rate\": (\n                (self.total_requests - self.failed_requests) / self.total_requests\n                if self.total_requests > 0 else 0\n            )\n        }\n\nclass TimeoutHandler:\n    \"\"\"Timeout handling for distributed calls\"\"\"\n    \n    def __init__(self, timeout: float = 30.0):\n        self.timeout = timeout\n    \n    async def execute_with_timeout(\n        self,\n        func: Callable,\n        *args,\n        **kwargs\n    ) -> Any:\n        \"\"\"Execute function with timeout\"\"\"\n        \n        try:\n            return await asyncio.wait_for(func(*args, **kwargs), timeout=self.timeout)\n            \n        except asyncio.TimeoutError:\n            logger.error(f\"‚è±Ô∏è Operation timed out after {self.timeout}s\")\n            raise TimeoutError(f\"Operation timed out after {self.timeout}s\")\n\nclass DistributedServiceCall:\n    \"\"\"Combined service call with all resilience patterns\"\"\"\n    \n    def __init__(\n        self,\n        service_name: str,\n        retry_config: Optional[RetryConfig] = None,\n        timeout: float = 30.0,\n        bulkhead_limit: int = 10\n    ):\n        self.service_name = service_name\n        self.retry_handler = RetryHandler(retry_config or RetryConfig())\n        self.timeout_handler = TimeoutHandler(timeout)\n        self.bulkhead = BulkheadPattern(service_name, bulkhead_limit)\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Make resilient service call\"\"\"\n        \n        async def wrapped_call():\n            return await self.timeout_handler.execute_with_timeout(func, *args, **kwargs)\n        \n        async def bulkhead_call():\n            return await self.bulkhead.execute(wrapped_call)\n        \n        return await self.retry_handler.execute_with_retry(bulkhead_call)\n\n# Saga Pattern for distributed transactions\nclass SagaStep:\n    \"\"\"Single step in a saga\"\"\"\n    \n    def __init__(\n        self,\n        name: str,\n        action: Callable,\n        compensation: Callable,\n        *args,\n        **kwargs\n    ):\n        self.name = name\n        self.action = action\n        self.compensation = compensation\n        self.args = args\n        self.kwargs = kwargs\n        self.completed = False\n        self.result = None\n\nclass SagaOrchestrator:\n    \"\"\"Orchestrator for saga pattern\"\"\"\n    \n    def __init__(self, saga_name: str):\n        self.saga_name = saga_name\n        self.steps = []\n        self.completed_steps = []\n    \n    def add_step(\n        self,\n        name: str,\n        action: Callable,\n        compensation: Callable,\n        *args,\n        **kwargs\n    ):\n        \"\"\"Add step to saga\"\"\"\n        \n        step = SagaStep(name, action, compensation, *args, **kwargs)\n        self.steps.append(step)\n    \n    async def execute(self) -> dict:\n        \"\"\"Execute saga with compensation on failure\"\"\"\n        \n        logger.info(f\"üé≠ Starting saga: {self.saga_name}\")\n        \n        try:\n            # Execute all steps\n            for step in self.steps:\n                logger.info(f\"‚ö° Executing step: {step.name}\")\n                \n                step.result = await step.action(*step.args, **step.kwargs)\n                step.completed = True\n                self.completed_steps.append(step)\n                \n                logger.info(f\"‚úÖ Completed step: {step.name}\")\n            \n            logger.info(f\"üéâ Saga completed successfully: {self.saga_name}\")\n            \n            return {\n                \"status\": \"success\",\n                \"saga_name\": self.saga_name,\n                \"completed_steps\": len(self.completed_steps),\n                \"results\": [step.result for step in self.completed_steps]\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Saga failed: {self.saga_name} - {str(e)}\")\n            \n            # Compensate completed steps in reverse order\n            await self._compensate()\n            \n            return {\n                \"status\": \"failed\",\n                \"saga_name\": self.saga_name,\n                \"error\": str(e),\n                \"compensated_steps\": len(self.completed_steps)\n            }\n    \n    async def _compensate(self):\n        \"\"\"Run compensation for completed steps\"\"\"\n        \n        logger.info(f\"üîÑ Starting compensation for: {self.saga_name}\")\n        \n        # Compensate in reverse order\n        for step in reversed(self.completed_steps):\n            try:\n                logger.info(f\"‚Ü©Ô∏è Compensating step: {step.name}\")\n                await step.compensation(*step.args, **step.kwargs)\n                logger.info(f\"‚úÖ Compensated step: {step.name}\")\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Compensation failed for step {step.name}: {str(e)}\")\n                # Continue with other compensations\n\n# Example: Blog post creation saga\nasync def create_blog_post_saga(post_data: dict, author_id: int) -> dict:\n    \"\"\"Create blog post with distributed transaction\"\"\"\n    \n    saga = SagaOrchestrator(\"create_blog_post\")\n    \n    # Step 1: Create post in content service\n    saga.add_step(\n        \"create_post\",\n        content_service.create_post,\n        content_service.delete_post,\n        post_data, author_id\n    )\n    \n    # Step 2: Update author statistics\n    saga.add_step(\n        \"update_author_stats\",\n        user_service.increment_post_count,\n        user_service.decrement_post_count,\n        author_id\n    )\n    \n    # Step 3: Send notifications\n    saga.add_step(\n        \"send_notifications\",\n        notification_service.notify_followers,\n        notification_service.cancel_notifications,\n        author_id, \"new_post\"\n    )\n    \n    # Step 4: Index for search\n    saga.add_step(\n        \"index_post\",\n        search_service.index_post,\n        search_service.remove_from_index,\n        post_data\n    )\n    \n    return await saga.execute()\n```\n\n---\n\n### üè† Homework: Build Microservices Architecture\n\n**Task:** Refactor your blog application into microservices\n\n```python\n# Create microservices architecture with:\n# 1. User Service (authentication, profiles, preferences)\n# 2. Content Service (posts, drafts, categories)\n# 3. Engagement Service (comments, likes, shares)\n# 4. Notification Service (email, push, in-app)\n# 5. API Gateway (routing, load balancing, auth)\n\n# Implementation Requirements:\n# - Service-to-service HTTP communication\n# - Event-driven communication with Redis/RabbitMQ\n# - API Gateway with circuit breakers\n# - Service discovery (Consul or custom)\n# - Distributed tracing and logging\n# - Health checks and monitoring\n# - Database per service pattern\n\n# Resilience Patterns:\n# - Circuit breaker for service calls\n# - Retry with exponential backoff\n# - Bulkhead for resource isolation\n# - Timeout handling\n# - Saga pattern for distributed transactions\n\n# Bonus Features:\n# - Service mesh with Istio/Linkerd\n# - Container orchestration with Docker Compose\n# - API versioning strategy\n# - Performance monitoring and metrics\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Microservices = Independent, scalable service architecture\n‚úÖ Service Communication = HTTP APIs + event-driven messaging\n‚úÖ API Gateway = Single entry point with routing and security\n‚úÖ Service Discovery = Dynamic service location and health\n‚úÖ Resilience Patterns = Circuit breakers, retries, and timeouts\n\n---\n\n<a name=\"hour-39\"></a>\n## üìÖ Hour 39: Observability (Logging, Metrics, Tracing)\n\n### üéØ Learning Objectives\n- Implement structured logging with correlation IDs\n- Set up metrics collection and monitoring dashboards\n- Add distributed tracing for microservices\n- Create alerting and incident response systems\n- Build comprehensive observability for production systems\n\n### üìñ What to Teach\n\n**\"Today we add eyes and ears to our applications - because you can't manage what you can't measure!\"**\n\n---\n\n### 1Ô∏è‚É£ Structured Logging Implementation (15 minutes)\n\n**Advanced Logging Setup:**\n\n```python\n# observability/logging_config.py\nimport logging\nimport json\nimport sys\nimport traceback\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\nfrom contextvars import ContextVar\nimport uuid\nfrom functools import wraps\nimport asyncio\n\n# Context variables for request correlation\ncorrelation_id: ContextVar[str] = ContextVar('correlation_id', default='')\nuser_id: ContextVar[str] = ContextVar('user_id', default='')\nrequest_path: ContextVar[str] = ContextVar('request_path', default='')\n\nclass StructuredLogger:\n    \"\"\"Structured logging with correlation IDs and context\"\"\"\n    \n    def __init__(self, service_name: str, environment: str = \"development\"):\n        self.service_name = service_name\n        self.environment = environment\n        self.logger = logging.getLogger(service_name)\n        \n        # Configure structured logging\n        self._setup_logger()\n    \n    def _setup_logger(self):\n        \"\"\"Setup structured JSON logging\"\"\"\n        \n        # Remove existing handlers\n        self.logger.handlers.clear()\n        \n        # Create console handler with structured formatter\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(StructuredFormatter(self.service_name, self.environment))\n        \n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n        \n        # Prevent duplicate logs\n        self.logger.propagate = False\n    \n    def info(self, message: str, **kwargs):\n        \"\"\"Log info message with context\"\"\"\n        self._log(logging.INFO, message, **kwargs)\n    \n    def warning(self, message: str, **kwargs):\n        \"\"\"Log warning message with context\"\"\"\n        self._log(logging.WARNING, message, **kwargs)\n    \n    def error(self, message: str, exception: Exception = None, **kwargs):\n        \"\"\"Log error message with exception details\"\"\"\n        \n        extra_data = kwargs.copy()\n        \n        if exception:\n            extra_data.update({\n                \"exception_type\": type(exception).__name__,\n                \"exception_message\": str(exception),\n                \"exception_traceback\": traceback.format_exc()\n            })\n        \n        self._log(logging.ERROR, message, **extra_data)\n    \n    def debug(self, message: str, **kwargs):\n        \"\"\"Log debug message with context\"\"\"\n        self._log(logging.DEBUG, message, **kwargs)\n    \n    def _log(self, level: int, message: str, **kwargs):\n        \"\"\"Internal log method with context injection\"\"\"\n        \n        # Build log context\n        log_context = {\n            \"correlation_id\": correlation_id.get(),\n            \"user_id\": user_id.get(),\n            \"request_path\": request_path.get(),\n            **kwargs\n        }\n        \n        # Remove empty context values\n        log_context = {k: v for k, v in log_context.items() if v}\n        \n        self.logger.log(level, message, extra={\"context\": log_context})\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logs\"\"\"\n    \n    def __init__(self, service_name: str, environment: str):\n        super().__init__()\n        self.service_name = service_name\n        self.environment = environment\n    \n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format log record as JSON\"\"\"\n        \n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"service\": self.service_name,\n            \"environment\": self.environment,\n            \"message\": record.getMessage(),\n            \"logger\": record.name,\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno\n        }\n        \n        # Add context if available\n        if hasattr(record, 'context'):\n            log_entry.update(record.context)\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_entry[\"exception\"] = {\n                \"type\": record.exc_info[0].__name__,\n                \"message\": str(record.exc_info[1]),\n                \"traceback\": self.formatException(record.exc_info)\n            }\n        \n        return json.dumps(log_entry, ensure_ascii=False)\n\n# Request correlation middleware\nfrom fastapi import Request, Response\nimport time\n\nclass CorrelationMiddleware:\n    \"\"\"Middleware to add correlation ID to requests\"\"\"\n    \n    def __init__(self, app):\n        self.app = app\n    \n    async def __call__(self, scope, receive, send):\n        if scope[\"type\"] == \"http\":\n            # Generate or extract correlation ID\n            headers = dict(scope.get(\"headers\", []))\n            correlation_header = headers.get(b\"x-correlation-id\")\n            \n            if correlation_header:\n                corr_id = correlation_header.decode()\n            else:\n                corr_id = str(uuid.uuid4())\n            \n            # Set context variables\n            correlation_id.set(corr_id)\n            request_path.set(scope.get(\"path\", \"\"))\n            \n            # Add correlation ID to response headers\n            async def send_wrapper(message):\n                if message[\"type\"] == \"http.response.start\":\n                    headers = message.get(\"headers\", [])\n                    headers.append([b\"x-correlation-id\", corr_id.encode()])\n                    message[\"headers\"] = headers\n                await send(message)\n            \n            await self.app(scope, receive, send_wrapper)\n        else:\n            await self.app(scope, receive, send)\n\n# Initialize structured logger\nlogger = StructuredLogger(\"blog-api\", environment=\"production\")\n\n# Logging decorators\ndef log_function_call(func):\n    \"\"\"Decorator to log function calls with performance metrics\"\"\"\n    \n    @wraps(func)\n    async def async_wrapper(*args, **kwargs):\n        start_time = time.time()\n        \n        logger.info(\n            f\"üöÄ Function call started: {func.__name__}\",\n            function=func.__name__,\n            module=func.__module__,\n            args_count=len(args),\n            kwargs_keys=list(kwargs.keys())\n        )\n        \n        try:\n            result = await func(*args, **kwargs)\n            duration = time.time() - start_time\n            \n            logger.info(\n                f\"‚úÖ Function call completed: {func.__name__}\",\n                function=func.__name__,\n                duration_ms=round(duration * 1000, 2),\n                success=True\n            )\n            \n            return result\n            \n        except Exception as e:\n            duration = time.time() - start_time\n            \n            logger.error(\n                f\"‚ùå Function call failed: {func.__name__}\",\n                function=func.__name__,\n                duration_ms=round(duration * 1000, 2),\n                success=False,\n                exception=e\n            )\n            \n            raise\n    \n    @wraps(func)\n    def sync_wrapper(*args, **kwargs):\n        start_time = time.time()\n        \n        logger.info(f\"üöÄ Function call started: {func.__name__}\", function=func.__name__)\n        \n        try:\n            result = func(*args, **kwargs)\n            duration = time.time() - start_time\n            \n            logger.info(\n                f\"‚úÖ Function call completed: {func.__name__}\",\n                function=func.__name__,\n                duration_ms=round(duration * 1000, 2)\n            )\n            \n            return result\n            \n        except Exception as e:\n            duration = time.time() - start_time\n            \n            logger.error(\n                f\"‚ùå Function call failed: {func.__name__}\",\n                function=func.__name__,\n                duration_ms=round(duration * 1000, 2),\n                exception=e\n            )\n            \n            raise\n    \n    if asyncio.iscoroutinefunction(func):\n        return async_wrapper\n    else:\n        return sync_wrapper\n\n# Database query logging\nclass DatabaseLogger:\n    \"\"\"Database query logging with performance tracking\"\"\"\n    \n    def __init__(self, db_name: str):\n        self.db_name = db_name\n        self.slow_query_threshold = 1.0  # 1 second\n    \n    def log_query(self, query: str, params: Dict = None, duration: float = None):\n        \"\"\"Log database query with performance metrics\"\"\"\n        \n        log_data = {\n            \"database\": self.db_name,\n            \"query\": query[:200] + \"...\" if len(query) > 200 else query,\n            \"param_count\": len(params) if params else 0\n        }\n        \n        if duration:\n            log_data[\"duration_ms\"] = round(duration * 1000, 2)\n            \n            if duration > self.slow_query_threshold:\n                logger.warning(\"üêå Slow database query detected\", **log_data)\n            else:\n                logger.info(\"üóÉÔ∏è Database query executed\", **log_data)\n        else:\n            logger.info(\"üóÉÔ∏è Database query started\", **log_data)\n\n# Usage examples\n@log_function_call\nasync def get_user_posts(user_id: int) -> List[Dict]:\n    \"\"\"Get posts for user with logging\"\"\"\n    \n    db_logger = DatabaseLogger(\"blog_db\")\n    \n    # Set user context\n    user_id.set(str(user_id))\n    \n    logger.info(f\"üîç Fetching posts for user\", user_id=user_id)\n    \n    # Simulate database query\n    query = \"SELECT * FROM posts WHERE author_id = $1 ORDER BY created_at DESC\"\n    start_time = time.time()\n    \n    db_logger.log_query(query, {\"author_id\": user_id})\n    \n    try:\n        # Simulate query execution\n        await asyncio.sleep(0.1)\n        posts = [{\"id\": 1, \"title\": \"Sample Post\"}]  # Mock data\n        \n        duration = time.time() - start_time\n        db_logger.log_query(query, {\"author_id\": user_id}, duration)\n        \n        logger.info(f\"üìö Found {len(posts)} posts for user\", post_count=len(posts))\n        \n        return posts\n        \n    except Exception as e:\n        logger.error(\"‚ùå Failed to fetch user posts\", user_id=user_id, exception=e)\n        raise\n\n# Error boundary for unhandled exceptions\nclass ErrorHandler:\n    \"\"\"Global error handler with structured logging\"\"\"\n    \n    def __init__(self):\n        self.logger = logger\n    \n    async def handle_http_exception(self, request: Request, exc: Exception):\n        \"\"\"Handle HTTP exceptions with detailed logging\"\"\"\n        \n        # Extract request info\n        request_info = {\n            \"method\": request.method,\n            \"url\": str(request.url),\n            \"headers\": dict(request.headers),\n            \"client_ip\": request.client.host\n        }\n        \n        # Log error with context\n        self.logger.error(\n            f\"üö® HTTP Exception: {type(exc).__name__}\",\n            exception_type=type(exc).__name__,\n            exception_message=str(exc),\n            request_info=request_info,\n            exception=exc\n        )\n        \n        # Return appropriate error response\n        return {\n            \"error\": \"Internal server error\",\n            \"correlation_id\": correlation_id.get(),\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n\n# Log aggregation for ELK stack or similar\nclass LogAggregator:\n    \"\"\"Send logs to external aggregation service\"\"\"\n    \n    def __init__(self, elasticsearch_url: str = None):\n        self.elasticsearch_url = elasticsearch_url\n    \n    async def send_logs(self, log_entries: List[Dict]):\n        \"\"\"Send structured logs to Elasticsearch\"\"\"\n        \n        if not self.elasticsearch_url:\n            return\n        \n        try:\n            # In production, use proper Elasticsearch client\n            # This is a simplified example\n            logger.debug(f\"üì§ Sending {len(log_entries)} logs to Elasticsearch\")\n            \n        except Exception as e:\n            # Don't fail the application due to logging issues\n            logger.error(\"‚ùå Failed to send logs to aggregator\", exception=e)\n```\n\n---\n\n### 2Ô∏è‚É£ Metrics Collection and Monitoring (15 minutes)\n\n**Prometheus Metrics Setup:**\n\n```python\n# observability/metrics.py\nfrom prometheus_client import Counter, Histogram, Gauge, Summary, CollectorRegistry, generate_latest\nimport time\nfrom typing import Dict, Any\nfrom functools import wraps\nimport asyncio\nimport psutil\nimport threading\n\nclass ApplicationMetrics:\n    \"\"\"Comprehensive application metrics collection\"\"\"\n    \n    def __init__(self, service_name: str):\n        self.service_name = service_name\n        self.registry = CollectorRegistry()\n        \n        # HTTP Metrics\n        self.http_requests_total = Counter(\n            'http_requests_total',\n            'Total HTTP requests',\n            ['method', 'endpoint', 'status_code'],\n            registry=self.registry\n        )\n        \n        self.http_request_duration_seconds = Histogram(\n            'http_request_duration_seconds',\n            'HTTP request duration in seconds',\n            ['method', 'endpoint'],\n            buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0],\n            registry=self.registry\n        )\n        \n        # Database Metrics\n        self.db_queries_total = Counter(\n            'db_queries_total',\n            'Total database queries',\n            ['operation', 'table', 'status'],\n            registry=self.registry\n        )\n        \n        self.db_query_duration_seconds = Histogram(\n            'db_query_duration_seconds', \n            'Database query duration in seconds',\n            ['operation', 'table'],\n            buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],\n            registry=self.registry\n        )\n        \n        # Business Metrics\n        self.active_users = Gauge(\n            'active_users_total',\n            'Number of currently active users',\n            registry=self.registry\n        )\n        \n        self.posts_created_total = Counter(\n            'posts_created_total',\n            'Total posts created',\n            ['category', 'published'],\n            registry=self.registry\n        )\n        \n        self.comments_created_total = Counter(\n            'comments_created_total', \n            'Total comments created',\n            registry=self.registry\n        )\n        \n        # System Metrics\n        self.memory_usage_bytes = Gauge(\n            'memory_usage_bytes',\n            'Memory usage in bytes',\n            registry=self.registry\n        )\n        \n        self.cpu_usage_percent = Gauge(\n            'cpu_usage_percent',\n            'CPU usage percentage',\n            registry=self.registry\n        )\n        \n        # Cache Metrics\n        self.cache_operations_total = Counter(\n            'cache_operations_total',\n            'Total cache operations',\n            ['operation', 'cache_name', 'result'],\n            registry=self.registry\n        )\n        \n        # Queue Metrics\n        self.queue_messages_total = Counter(\n            'queue_messages_total',\n            'Total queue messages',\n            ['queue_name', 'status'],\n            registry=self.registry\n        )\n        \n        self.queue_size = Gauge(\n            'queue_size',\n            'Current queue size',\n            ['queue_name'],\n            registry=self.registry\n        )\n        \n        # Start system metrics collection\n        self._start_system_metrics_collection()\n    \n    def record_http_request(\n        self, \n        method: str, \n        endpoint: str, \n        status_code: int, \n        duration: float\n    ):\n        \"\"\"Record HTTP request metrics\"\"\"\n        \n        self.http_requests_total.labels(\n            method=method,\n            endpoint=endpoint,\n            status_code=str(status_code)\n        ).inc()\n        \n        self.http_request_duration_seconds.labels(\n            method=method,\n            endpoint=endpoint\n        ).observe(duration)\n    \n    def record_db_query(\n        self,\n        operation: str,\n        table: str,\n        duration: float,\n        success: bool = True\n    ):\n        \"\"\"Record database query metrics\"\"\"\n        \n        status = \"success\" if success else \"error\"\n        \n        self.db_queries_total.labels(\n            operation=operation,\n            table=table,\n            status=status\n        ).inc()\n        \n        self.db_query_duration_seconds.labels(\n            operation=operation,\n            table=table\n        ).observe(duration)\n    \n    def record_post_created(self, category: str, published: bool):\n        \"\"\"Record post creation metric\"\"\"\n        \n        self.posts_created_total.labels(\n            category=category,\n            published=str(published).lower()\n        ).inc()\n    \n    def record_comment_created(self):\n        \"\"\"Record comment creation metric\"\"\"\n        \n        self.comments_created_total.inc()\n    \n    def record_cache_operation(\n        self,\n        operation: str,\n        cache_name: str,\n        hit: bool\n    ):\n        \"\"\"Record cache operation metrics\"\"\"\n        \n        result = \"hit\" if hit else \"miss\"\n        \n        self.cache_operations_total.labels(\n            operation=operation,\n            cache_name=cache_name,\n            result=result\n        ).inc()\n    \n    def record_queue_message(\n        self,\n        queue_name: str,\n        status: str\n    ):\n        \"\"\"Record queue message metrics\"\"\"\n        \n        self.queue_messages_total.labels(\n            queue_name=queue_name,\n            status=status\n        ).inc()\n    \n    def set_queue_size(self, queue_name: str, size: int):\n        \"\"\"Set current queue size\"\"\"\n        \n        self.queue_size.labels(queue_name=queue_name).set(size)\n    \n    def set_active_users(self, count: int):\n        \"\"\"Set active user count\"\"\"\n        \n        self.active_users.set(count)\n    \n    def _start_system_metrics_collection(self):\n        \"\"\"Start background thread for system metrics\"\"\"\n        \n        def collect_system_metrics():\n            while True:\n                try:\n                    # Memory usage\n                    memory_info = psutil.virtual_memory()\n                    self.memory_usage_bytes.set(memory_info.used)\n                    \n                    # CPU usage\n                    cpu_percent = psutil.cpu_percent(interval=1)\n                    self.cpu_usage_percent.set(cpu_percent)\n                    \n                except Exception as e:\n                    logger.error(\"‚ùå Failed to collect system metrics\", exception=e)\n                \n                time.sleep(30)  # Collect every 30 seconds\n        \n        thread = threading.Thread(target=collect_system_metrics, daemon=True)\n        thread.start()\n    \n    def get_metrics(self) -> str:\n        \"\"\"Get metrics in Prometheus format\"\"\"\n        \n        return generate_latest(self.registry).decode('utf-8')\n\n# Metrics middleware\nclass MetricsMiddleware:\n    \"\"\"FastAPI middleware for metrics collection\"\"\"\n    \n    def __init__(self, app, metrics: ApplicationMetrics):\n        self.app = app\n        self.metrics = metrics\n    \n    async def __call__(self, scope, receive, send):\n        if scope[\"type\"] == \"http\":\n            start_time = time.time()\n            \n            # Extract request info\n            method = scope[\"method\"]\n            path = scope[\"path\"]\n            \n            # Create response wrapper to capture status code\n            status_code = 200\n            \n            async def send_wrapper(message):\n                nonlocal status_code\n                if message[\"type\"] == \"http.response.start\":\n                    status_code = message[\"status\"]\n                await send(message)\n            \n            try:\n                await self.app(scope, receive, send_wrapper)\n            finally:\n                # Record metrics\n                duration = time.time() - start_time\n                self.metrics.record_http_request(method, path, status_code, duration)\n        else:\n            await self.app(scope, receive, send)\n\n# Metrics decorators\ndef track_database_query(operation: str, table: str):\n    \"\"\"Decorator to track database queries\"\"\"\n    \n    def decorator(func):\n        @wraps(func)\n        async def async_wrapper(*args, **kwargs):\n            start_time = time.time()\n            success = True\n            \n            try:\n                result = await func(*args, **kwargs)\n                return result\n            except Exception as e:\n                success = False\n                raise\n            finally:\n                duration = time.time() - start_time\n                metrics.record_db_query(operation, table, duration, success)\n        \n        @wraps(func)\n        def sync_wrapper(*args, **kwargs):\n            start_time = time.time()\n            success = True\n            \n            try:\n                result = func(*args, **kwargs)\n                return result\n            except Exception as e:\n                success = False\n                raise\n            finally:\n                duration = time.time() - start_time\n                metrics.record_db_query(operation, table, duration, success)\n        \n        if asyncio.iscoroutinefunction(func):\n            return async_wrapper\n        else:\n            return sync_wrapper\n    \n    return decorator\n\n# Initialize metrics\nmetrics = ApplicationMetrics(\"blog-api\")\n\n# Usage examples\n@track_database_query(\"SELECT\", \"posts\")\nasync def get_posts_from_db(limit: int = 10):\n    \"\"\"Example database query with metrics tracking\"\"\"\n    \n    # Simulate database query\n    await asyncio.sleep(0.1)\n    return [{\"id\": 1, \"title\": \"Sample Post\"}]\n\n# Business logic with metrics\nclass PostService:\n    \"\"\"Post service with metrics tracking\"\"\"\n    \n    def __init__(self, metrics: ApplicationMetrics):\n        self.metrics = metrics\n    \n    async def create_post(\n        self,\n        title: str,\n        content: str,\n        category: str,\n        published: bool = False\n    ):\n        \"\"\"Create post with metrics\"\"\"\n        \n        # Create post logic here\n        post = {\n            \"id\": 123,\n            \"title\": title,\n            \"content\": content,\n            \"category\": category,\n            \"published\": published\n        }\n        \n        # Record metrics\n        self.metrics.record_post_created(category, published)\n        \n        return post\n\n# Metrics endpoint\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/metrics\")\nasync def get_metrics():\n    \"\"\"Prometheus metrics endpoint\"\"\"\n    \n    from fastapi import Response\n    \n    metrics_data = metrics.get_metrics()\n    \n    return Response(\n        content=metrics_data,\n        media_type=\"text/plain; version=0.0.4; charset=utf-8\"\n    )\n\n# Health check with metrics\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint with metrics\"\"\"\n    \n    health_data = {\n        \"status\": \"healthy\",\n        \"timestamp\": time.time(),\n        \"service\": \"blog-api\",\n        \"version\": \"1.0.0\"\n    }\n    \n    # You could add metrics about health check frequency\n    metrics.http_requests_total.labels(\n        method=\"GET\",\n        endpoint=\"/health\", \n        status_code=\"200\"\n    ).inc()\n    \n    return health_data\n\n# Custom business metrics\nclass BusinessMetricsCollector:\n    \"\"\"Collect business-specific metrics\"\"\"\n    \n    def __init__(self, metrics: ApplicationMetrics):\n        self.metrics = metrics\n    \n    async def collect_user_metrics(self):\n        \"\"\"Collect user-related metrics\"\"\"\n        \n        # Count active users (example with Redis)\n        try:\n            # active_users = await redis_client.scard(\"active_users\")\n            active_users = 42  # Mock data\n            \n            self.metrics.set_active_users(active_users)\n            \n        except Exception as e:\n            logger.error(\"‚ùå Failed to collect user metrics\", exception=e)\n    \n    async def collect_queue_metrics(self):\n        \"\"\"Collect queue metrics\"\"\"\n        \n        try:\n            # Example queue sizes\n            queue_sizes = {\n                \"email_queue\": 15,\n                \"image_processing_queue\": 8,\n                \"notification_queue\": 3\n            }\n            \n            for queue_name, size in queue_sizes.items():\n                self.metrics.set_queue_size(queue_name, size)\n                \n        except Exception as e:\n            logger.error(\"‚ùå Failed to collect queue metrics\", exception=e)\n\n# Periodic metrics collection\nasync def start_metrics_collection():\n    \"\"\"Start periodic business metrics collection\"\"\"\n    \n    collector = BusinessMetricsCollector(metrics)\n    \n    while True:\n        try:\n            await collector.collect_user_metrics()\n            await collector.collect_queue_metrics()\n            \n        except Exception as e:\n            logger.error(\"‚ùå Error in metrics collection\", exception=e)\n        \n        # Collect every 60 seconds\n        await asyncio.sleep(60)\n```\n\n**Grafana Dashboard Configuration:**\n\n```json\n{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"Blog API Dashboard\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(http_requests_total[5m])\",\n            \"legendFormat\": \"{{method}} {{endpoint}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time\",\n        \"type\": \"graph\", \n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\",\n            \"legendFormat\": \"95th percentile\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))\", \n            \"legendFormat\": \"50th percentile\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"type\": \"singlestat\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(http_requests_total{status_code=~\\\"5..\\\"}[5m]) / rate(http_requests_total[5m]) * 100\",\n            \"legendFormat\": \"Error Rate %\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Active Users\",\n        \"type\": \"singlestat\",\n        \"targets\": [\n          {\n            \"expr\": \"active_users_total\",\n            \"legendFormat\": \"Active Users\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Database Query Performance\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(db_query_duration_seconds_bucket[5m]))\",\n            \"legendFormat\": \"{{operation}} {{table}}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n### 3Ô∏è‚É£ Distributed Tracing (10 minutes)\n\n**OpenTelemetry Integration:**\n\n```python\n# observability/tracing.py\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\nfrom opentelemetry.instrumentation.redis import RedisInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nimport asyncio\nfrom typing import Dict, Any, Optional\nfrom functools import wraps\n\nclass DistributedTracing:\n    \"\"\"Distributed tracing setup with OpenTelemetry\"\"\"\n    \n    def __init__(\n        self, \n        service_name: str,\n        jaeger_endpoint: str = \"http://localhost:14268/api/traces\"\n    ):\n        self.service_name = service_name\n        self.jaeger_endpoint = jaeger_endpoint\n        \n        # Setup tracing\n        self._setup_tracer()\n    \n    def _setup_tracer(self):\n        \"\"\"Configure OpenTelemetry tracer\"\"\"\n        \n        # Create tracer provider\n        trace.set_tracer_provider(TracerProvider())\n        \n        # Configure Jaeger exporter\n        jaeger_exporter = JaegerExporter(\n            agent_host_name=\"localhost\",\n            agent_port=6831,\n            collector_endpoint=self.jaeger_endpoint,\n        )\n        \n        # Create span processor\n        span_processor = BatchSpanProcessor(jaeger_exporter)\n        trace.get_tracer_provider().add_span_processor(span_processor)\n        \n        # Get tracer\n        self.tracer = trace.get_tracer(self.service_name)\n    \n    def instrument_app(self, app):\n        \"\"\"Instrument FastAPI application\"\"\"\n        \n        # Auto-instrument FastAPI\n        FastAPIInstrumentor.instrument_app(app)\n        \n        # Auto-instrument database\n        SQLAlchemyInstrumentor().instrument()\n        \n        # Auto-instrument Redis\n        RedisInstrumentor().instrument()\n        \n        # Auto-instrument HTTP requests\n        RequestsInstrumentor().instrument()\n    \n    def create_span(\n        self,\n        name: str,\n        attributes: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Create a new span\"\"\"\n        \n        span = self.tracer.start_span(name)\n        \n        if attributes:\n            for key, value in attributes.items():\n                span.set_attribute(key, str(value))\n        \n        return span\n    \n    def trace_function(\n        self,\n        span_name: str = None,\n        attributes: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Decorator to trace function execution\"\"\"\n        \n        def decorator(func):\n            @wraps(func)\n            async def async_wrapper(*args, **kwargs):\n                name = span_name or f\"{func.__module__}.{func.__name__}\"\n                \n                with self.tracer.start_as_current_span(name) as span:\n                    # Add function attributes\n                    span.set_attribute(\"function.name\", func.__name__)\n                    span.set_attribute(\"function.module\", func.__module__)\n                    \n                    # Add custom attributes\n                    if attributes:\n                        for key, value in attributes.items():\n                            span.set_attribute(key, str(value))\n                    \n                    try:\n                        result = await func(*args, **kwargs)\n                        span.set_attribute(\"function.success\", True)\n                        return result\n                        \n                    except Exception as e:\n                        span.set_attribute(\"function.success\", False)\n                        span.set_attribute(\"function.error\", str(e))\n                        span.record_exception(e)\n                        raise\n            \n            @wraps(func)\n            def sync_wrapper(*args, **kwargs):\n                name = span_name or f\"{func.__module__}.{func.__name__}\"\n                \n                with self.tracer.start_as_current_span(name) as span:\n                    span.set_attribute(\"function.name\", func.__name__)\n                    span.set_attribute(\"function.module\", func.__module__)\n                    \n                    if attributes:\n                        for key, value in attributes.items():\n                            span.set_attribute(key, str(value))\n                    \n                    try:\n                        result = func(*args, **kwargs)\n                        span.set_attribute(\"function.success\", True)\n                        return result\n                        \n                    except Exception as e:\n                        span.set_attribute(\"function.success\", False)\n                        span.set_attribute(\"function.error\", str(e))\n                        span.record_exception(e)\n                        raise\n            \n            if asyncio.iscoroutinefunction(func):\n                return async_wrapper\n            else:\n                return sync_wrapper\n        \n        return decorator\n\n# Initialize tracing\ntracing = DistributedTracing(\"blog-api\")\n\n# Service-to-service tracing\nclass TracedServiceClient:\n    \"\"\"HTTP client with tracing support\"\"\"\n    \n    def __init__(self, service_name: str, base_url: str, tracer):\n        self.service_name = service_name\n        self.base_url = base_url\n        self.tracer = tracer\n    \n    async def make_request(\n        self,\n        method: str,\n        endpoint: str,\n        data: Optional[Dict] = None\n    ) -> Dict[Any, Any]:\n        \"\"\"Make HTTP request with tracing\"\"\"\n        \n        with self.tracer.start_as_current_span(f\"http_client_{self.service_name}\") as span:\n            # Add HTTP attributes\n            span.set_attribute(\"http.method\", method)\n            span.set_attribute(\"http.url\", f\"{self.base_url}{endpoint}\")\n            span.set_attribute(\"service.name\", self.service_name)\n            \n            try:\n                # Make HTTP request (using httpx or similar)\n                # response = await httpx.request(method, f\"{self.base_url}{endpoint}\", json=data)\n                \n                # Mock response for example\n                await asyncio.sleep(0.1)\n                response_data = {\"status\": \"success\"}\n                \n                span.set_attribute(\"http.status_code\", 200)\n                span.set_attribute(\"http.response_size\", len(str(response_data)))\n                \n                return response_data\n                \n            except Exception as e:\n                span.set_attribute(\"http.status_code\", 500)\n                span.record_exception(e)\n                raise\n\n# Database tracing\nclass TracedDatabase:\n    \"\"\"Database operations with tracing\"\"\"\n    \n    def __init__(self, tracer):\n        self.tracer = tracer\n    \n    @tracing.trace_function(\"db.query\", {\"component\": \"database\"})\n    async def execute_query(\n        self,\n        query: str,\n        params: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"Execute database query with tracing\"\"\"\n        \n        # Current span is automatically available\n        current_span = trace.get_current_span()\n        \n        # Add database-specific attributes\n        current_span.set_attribute(\"db.statement\", query)\n        current_span.set_attribute(\"db.operation\", query.split()[0].upper())\n        \n        if params:\n            current_span.set_attribute(\"db.params_count\", len(params))\n        \n        # Simulate query execution\n        await asyncio.sleep(0.05)\n        \n        result = [{\"id\": 1, \"title\": \"Sample\"}]\n        current_span.set_attribute(\"db.rows_affected\", len(result))\n        \n        return result\n\n# Business logic with tracing\nclass TracedPostService:\n    \"\"\"Post service with comprehensive tracing\"\"\"\n    \n    def __init__(self, tracer):\n        self.tracer = tracer\n        self.db = TracedDatabase(tracer)\n        self.user_service = TracedServiceClient(\"user-service\", \"http://user-service:8000\", tracer)\n    \n    @tracing.trace_function(\"post.create\", {\"operation\": \"create_post\"})\n    async def create_post(\n        self,\n        title: str,\n        content: str,\n        author_id: int\n    ) -> Dict[str, Any]:\n        \"\"\"Create post with distributed tracing\"\"\"\n        \n        current_span = trace.get_current_span()\n        \n        # Add business context\n        current_span.set_attribute(\"post.title\", title)\n        current_span.set_attribute(\"post.author_id\", author_id)\n        current_span.set_attribute(\"post.content_length\", len(content))\n        \n        try:\n            # Step 1: Validate author exists\n            with self.tracer.start_as_current_span(\"post.validate_author\") as span:\n                span.set_attribute(\"user.id\", author_id)\n                \n                author = await self.user_service.make_request(\n                    \"GET\",\n                    f\"/api/v1/users/{author_id}\"\n                )\n                \n                span.set_attribute(\"user.found\", author is not None)\n            \n            # Step 2: Create post in database\n            with self.tracer.start_as_current_span(\"post.save_to_db\") as span:\n                query = \"INSERT INTO posts (title, content, author_id) VALUES ($1, $2, $3) RETURNING id\"\n                \n                result = await self.db.execute_query(\n                    query,\n                    {\"title\": title, \"content\": content, \"author_id\": author_id}\n                )\n                \n                post_id = result[0][\"id\"] if result else None\n                span.set_attribute(\"post.id\", post_id)\n            \n            # Step 3: Send notifications\n            with self.tracer.start_as_current_span(\"post.send_notifications\") as span:\n                notification_data = {\n                    \"type\": \"new_post\",\n                    \"post_id\": post_id,\n                    \"author_id\": author_id\n                }\n                \n                # Simulate notification sending\n                await asyncio.sleep(0.02)\n                \n                span.set_attribute(\"notifications.sent\", True)\n            \n            # Success\n            current_span.set_attribute(\"post.creation_success\", True)\n            \n            return {\n                \"id\": post_id,\n                \"title\": title,\n                \"author_id\": author_id,\n                \"status\": \"created\"\n            }\n            \n        except Exception as e:\n            current_span.set_attribute(\"post.creation_success\", False)\n            current_span.record_exception(e)\n            raise\n\n# Custom span context manager\nclass TraceContext:\n    \"\"\"Context manager for custom spans\"\"\"\n    \n    def __init__(self, tracer, span_name: str, **attributes):\n        self.tracer = tracer\n        self.span_name = span_name\n        self.attributes = attributes\n        self.span = None\n    \n    async def __aenter__(self):\n        self.span = self.tracer.start_span(self.span_name)\n        \n        for key, value in self.attributes.items():\n            self.span.set_attribute(key, str(value))\n        \n        return self.span\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            self.span.record_exception(exc_val)\n            self.span.set_attribute(\"error\", True)\n        else:\n            self.span.set_attribute(\"success\", True)\n        \n        self.span.end()\n\n# Usage examples\nasync def complex_business_operation(user_id: int, data: Dict):\n    \"\"\"Example of complex operation with tracing\"\"\"\n    \n    async with TraceContext(\n        tracing.tracer,\n        \"complex_business_operation\",\n        user_id=user_id,\n        operation_type=\"complex\"\n    ) as span:\n        \n        # Step 1: Validate user\n        async with TraceContext(tracing.tracer, \"validate_user\", user_id=user_id):\n            # Validation logic\n            await asyncio.sleep(0.01)\n        \n        # Step 2: Process data\n        async with TraceContext(tracing.tracer, \"process_data\", data_size=len(data)):\n            # Processing logic\n            await asyncio.sleep(0.05)\n        \n        # Step 3: Save results\n        async with TraceContext(tracing.tracer, \"save_results\"):\n            # Save logic\n            await asyncio.sleep(0.02)\n        \n        span.set_attribute(\"operation_completed\", True)\n        return {\"status\": \"success\"}\n```\n\n---\n\n### 4Ô∏è‚É£ Alerting and Incident Response (10 minutes)\n\n**Comprehensive Alerting System:**\n\n```python\n# observability/alerting.py\nimport asyncio\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport aiohttp\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass AlertSeverity(Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\" \n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\nclass AlertStatus(Enum):\n    ACTIVE = \"active\"\n    RESOLVED = \"resolved\"\n    ACKNOWLEDGED = \"acknowledged\"\n\n@dataclass\nclass Alert:\n    \"\"\"Alert data structure\"\"\"\n    \n    id: str\n    title: str\n    description: str\n    severity: AlertSeverity\n    status: AlertStatus\n    service: str\n    created_at: datetime\n    updated_at: datetime\n    labels: Dict[str, str]\n    annotations: Dict[str, str]\n    resolved_at: Optional[datetime] = None\n\nclass AlertManager:\n    \"\"\"Centralized alert management\"\"\"\n    \n    def __init__(self):\n        self.active_alerts: Dict[str, Alert] = {}\n        self.alert_handlers = []\n        self.notification_channels = []\n    \n    def add_notification_channel(self, channel):\n        \"\"\"Add notification channel (Slack, email, etc.)\"\"\"\n        self.notification_channels.append(channel)\n    \n    def add_alert_handler(self, handler):\n        \"\"\"Add custom alert handler\"\"\"\n        self.alert_handlers.append(handler)\n    \n    async def create_alert(\n        self,\n        alert_id: str,\n        title: str,\n        description: str,\n        severity: AlertSeverity,\n        service: str,\n        labels: Dict[str, str] = None,\n        annotations: Dict[str, str] = None\n    ) -> Alert:\n        \"\"\"Create new alert\"\"\"\n        \n        alert = Alert(\n            id=alert_id,\n            title=title,\n            description=description,\n            severity=severity,\n            status=AlertStatus.ACTIVE,\n            service=service,\n            created_at=datetime.utcnow(),\n            updated_at=datetime.utcnow(),\n            labels=labels or {},\n            annotations=annotations or {}\n        )\n        \n        self.active_alerts[alert_id] = alert\n        \n        logger.warning(f\"üö® Alert created: {title} ({severity.value})\")\n        \n        # Send notifications\n        await self._send_notifications(alert, \"created\")\n        \n        # Run custom handlers\n        for handler in self.alert_handlers:\n            try:\n                await handler.on_alert_created(alert)\n            except Exception as e:\n                logger.error(f\"‚ùå Alert handler error: {str(e)}\")\n        \n        return alert\n    \n    async def resolve_alert(self, alert_id: str) -> bool:\n        \"\"\"Resolve an active alert\"\"\"\n        \n        if alert_id not in self.active_alerts:\n            return False\n        \n        alert = self.active_alerts[alert_id]\n        alert.status = AlertStatus.RESOLVED\n        alert.resolved_at = datetime.utcnow()\n        alert.updated_at = datetime.utcnow()\n        \n        logger.info(f\"‚úÖ Alert resolved: {alert.title}\")\n        \n        # Send notifications\n        await self._send_notifications(alert, \"resolved\")\n        \n        # Remove from active alerts\n        del self.active_alerts[alert_id]\n        \n        return True\n    \n    async def _send_notifications(self, alert: Alert, action: str):\n        \"\"\"Send alert notifications to all channels\"\"\"\n        \n        for channel in self.notification_channels:\n            try:\n                await channel.send_alert(alert, action)\n            except Exception as e:\n                logger.error(f\"‚ùå Notification failed: {str(e)}\")\n\nclass SlackNotificationChannel:\n    \"\"\"Slack notification channel\"\"\"\n    \n    def __init__(self, webhook_url: str):\n        self.webhook_url = webhook_url\n    \n    async def send_alert(self, alert: Alert, action: str):\n        \"\"\"Send alert to Slack\"\"\"\n        \n        color_map = {\n            AlertSeverity.LOW: \"#36a64f\",\n            AlertSeverity.MEDIUM: \"#ffaa00\", \n            AlertSeverity.HIGH: \"#ff6600\",\n            AlertSeverity.CRITICAL: \"#ff0000\"\n        }\n        \n        emoji_map = {\n            \"created\": \"üö®\",\n            \"resolved\": \"‚úÖ\",\n            \"acknowledged\": \"üëÄ\"\n        }\n        \n        message = {\n            \"attachments\": [\n                {\n                    \"color\": color_map.get(alert.severity, \"#cccccc\"),\n                    \"title\": f\"{emoji_map.get(action, '‚ö†Ô∏è')} Alert {action.title()}: {alert.title}\",\n                    \"text\": alert.description,\n                    \"fields\": [\n                        {\n                            \"title\": \"Service\",\n                            \"value\": alert.service,\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Severity\", \n                            \"value\": alert.severity.value.upper(),\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Time\",\n                            \"value\": alert.created_at.isoformat(),\n                            \"short\": True\n                        }\n                    ]\n                }\n            ]\n        }\n        \n        # Add labels and annotations\n        if alert.labels:\n            message[\"attachments\"][0][\"fields\"].append({\n                \"title\": \"Labels\",\n                \"value\": \", \".join([f\"{k}={v}\" for k, v in alert.labels.items()]),\n                \"short\": False\n            })\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(self.webhook_url, json=message) as response:\n                if response.status != 200:\n                    raise Exception(f\"Slack notification failed: {response.status}\")\n\nclass EmailNotificationChannel:\n    \"\"\"Email notification channel\"\"\"\n    \n    def __init__(self, smtp_config: Dict[str, Any]):\n        self.smtp_config = smtp_config\n    \n    async def send_alert(self, alert: Alert, action: str):\n        \"\"\"Send alert via email\"\"\"\n        \n        subject = f\"[{alert.severity.value.upper()}] {alert.title}\"\n        \n        body = f\"\"\"\nAlert {action}: {alert.title}\n\nDescription: {alert.description}\nService: {alert.service}\nSeverity: {alert.severity.value}\nTime: {alert.created_at.isoformat()}\n\nLabels: {', '.join([f\"{k}={v}\" for k, v in alert.labels.items()])}\n\n--\nAutomated Alert System\n\"\"\"\n        \n        # Implementation would use actual SMTP client\n        logger.info(f\"üìß Email alert sent: {subject}\")\n\nclass HealthChecker:\n    \"\"\"System health monitoring with alerting\"\"\"\n    \n    def __init__(self, alert_manager: AlertManager):\n        self.alert_manager = alert_manager\n        self.checks = {}\n        self.check_interval = 60  # seconds\n        self.running = False\n    \n    def add_health_check(\n        self,\n        name: str,\n        check_func: callable,\n        threshold: float = 0.8,\n        severity: AlertSeverity = AlertSeverity.HIGH\n    ):\n        \"\"\"Add health check\"\"\"\n        \n        self.checks[name] = {\n            \"func\": check_func,\n            \"threshold\": threshold,\n            \"severity\": severity,\n            \"last_status\": None,\n            \"consecutive_failures\": 0\n        }\n    \n    async def start_monitoring(self):\n        \"\"\"Start health monitoring loop\"\"\"\n        \n        self.running = True\n        \n        while self.running:\n            try:\n                for name, check_config in self.checks.items():\n                    await self._run_health_check(name, check_config)\n                \n            except Exception as e:\n                logger.error(f\"‚ùå Health check error: {str(e)}\")\n            \n            await asyncio.sleep(self.check_interval)\n    \n    async def _run_health_check(self, name: str, config: Dict):\n        \"\"\"Run individual health check\"\"\"\n        \n        try:\n            # Run check function\n            result = await config[\"func\"]()\n            \n            is_healthy = result >= config[\"threshold\"]\n            \n            if not is_healthy:\n                config[\"consecutive_failures\"] += 1\n                \n                # Create alert after 3 consecutive failures\n                if config[\"consecutive_failures\"] >= 3:\n                    alert_id = f\"health_check_{name}\"\n                    \n                    if alert_id not in self.alert_manager.active_alerts:\n                        await self.alert_manager.create_alert(\n                            alert_id=alert_id,\n                            title=f\"Health Check Failed: {name}\",\n                            description=f\"Health check '{name}' failed with value {result:.2f} (threshold: {config['threshold']})\",\n                            severity=config[\"severity\"],\n                            service=\"health-monitor\",\n                            labels={\n                                \"check_name\": name,\n                                \"value\": str(result),\n                                \"threshold\": str(config[\"threshold\"])\n                            }\n                        )\n            else:\n                config[\"consecutive_failures\"] = 0\n                \n                # Resolve alert if it exists\n                alert_id = f\"health_check_{name}\"\n                await self.alert_manager.resolve_alert(alert_id)\n            \n            config[\"last_status\"] = is_healthy\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Health check '{name}' failed: {str(e)}\")\n            config[\"consecutive_failures\"] += 1\n\n# Metric-based alerting\nclass MetricAlerter:\n    \"\"\"Create alerts based on metrics thresholds\"\"\"\n    \n    def __init__(self, alert_manager: AlertManager, metrics: ApplicationMetrics):\n        self.alert_manager = alert_manager\n        self.metrics = metrics\n        self.thresholds = {}\n    \n    def add_threshold(\n        self,\n        metric_name: str,\n        threshold_value: float,\n        comparison: str = \"greater\",  # \"greater\", \"less\", \"equal\"\n        severity: AlertSeverity = AlertSeverity.MEDIUM,\n        duration: int = 300  # seconds\n    ):\n        \"\"\"Add metric threshold for alerting\"\"\"\n        \n        self.thresholds[metric_name] = {\n            \"value\": threshold_value,\n            \"comparison\": comparison,\n            \"severity\": severity,\n            \"duration\": duration,\n            \"triggered_at\": None\n        }\n    \n    async def check_thresholds(self):\n        \"\"\"Check all metric thresholds\"\"\"\n        \n        for metric_name, config in self.thresholds.items():\n            try:\n                # Get current metric value (simplified)\n                current_value = await self._get_metric_value(metric_name)\n                \n                if current_value is None:\n                    continue\n                \n                # Check threshold\n                threshold_exceeded = self._check_threshold(\n                    current_value,\n                    config[\"value\"],\n                    config[\"comparison\"]\n                )\n                \n                alert_id = f\"metric_threshold_{metric_name}\"\n                \n                if threshold_exceeded:\n                    if not config[\"triggered_at\"]:\n                        config[\"triggered_at\"] = datetime.utcnow()\n                    \n                    # Check if duration exceeded\n                    elif (datetime.utcnow() - config[\"triggered_at\"]).seconds >= config[\"duration\"]:\n                        \n                        if alert_id not in self.alert_manager.active_alerts:\n                            await self.alert_manager.create_alert(\n                                alert_id=alert_id,\n                                title=f\"Metric Threshold Exceeded: {metric_name}\",\n                                description=f\"Metric {metric_name} = {current_value} {config['comparison']} {config['value']} for {config['duration']}s\",\n                                severity=config[\"severity\"],\n                                service=\"metric-monitor\",\n                                labels={\n                                    \"metric\": metric_name,\n                                    \"current_value\": str(current_value),\n                                    \"threshold\": str(config[\"value\"]),\n                                    \"comparison\": config[\"comparison\"]\n                                }\n                            )\n                else:\n                    config[\"triggered_at\"] = None\n                    await self.alert_manager.resolve_alert(alert_id)\n                    \n            except Exception as e:\n                logger.error(f\"‚ùå Threshold check failed for {metric_name}: {str(e)}\")\n    \n    def _check_threshold(self, value: float, threshold: float, comparison: str) -> bool:\n        \"\"\"Check if value exceeds threshold\"\"\"\n        \n        if comparison == \"greater\":\n            return value > threshold\n        elif comparison == \"less\":\n            return value < threshold\n        elif comparison == \"equal\":\n            return abs(value - threshold) < 0.001\n        \n        return False\n    \n    async def _get_metric_value(self, metric_name: str) -> Optional[float]:\n        \"\"\"Get current metric value\"\"\"\n        \n        # This would integrate with actual metrics collection\n        # For example purposes, return mock values\n        metric_values = {\n            \"error_rate\": 0.05,  # 5% error rate\n            \"response_time_p95\": 2.5,  # 2.5 seconds\n            \"cpu_usage\": 75.0,  # 75% CPU\n            \"memory_usage\": 80.0  # 80% memory\n        }\n        \n        return metric_values.get(metric_name)\n\n# Initialize alerting system\nalert_manager = AlertManager()\n\n# Setup notification channels\nslack_channel = SlackNotificationChannel(\"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\")\nalert_manager.add_notification_channel(slack_channel)\n\n# Setup health monitoring\nasync def database_health_check() -> float:\n    \"\"\"Check database health\"\"\"\n    try:\n        # Simulate database ping\n        await asyncio.sleep(0.01)\n        return 1.0  # Healthy\n    except:\n        return 0.0  # Unhealthy\n\nasync def api_response_time_check() -> float:\n    \"\"\"Check API response time\"\"\"\n    # Return percentage of requests under threshold\n    return 0.85  # 85% under threshold\n\nhealth_checker = HealthChecker(alert_manager)\nhealth_checker.add_health_check(\"database\", database_health_check, threshold=0.9)\nhealth_checker.add_health_check(\"api_response_time\", api_response_time_check, threshold=0.8)\n\n# Setup metric alerting\nmetric_alerter = MetricAlerter(alert_manager, metrics)\nmetric_alerter.add_threshold(\"error_rate\", 0.1, \"greater\", AlertSeverity.HIGH)\nmetric_alerter.add_threshold(\"response_time_p95\", 5.0, \"greater\", AlertSeverity.MEDIUM)\n```\n\n---\n\n### üè† Homework: Complete Observability Stack\n\n**Task:** Implement comprehensive observability for your blog application\n\n```python\n# Build a complete observability solution with:\n# 1. Structured logging with correlation IDs\n# 2. Comprehensive metrics collection (business + system)\n# 3. Distributed tracing across microservices\n# 4. Real-time alerting with multiple channels\n# 5. Health monitoring and SLA tracking\n# 6. Performance monitoring and optimization\n# 7. Error tracking and incident response\n# 8. Custom dashboards and reporting\n\n# Technical Requirements:\n# - ELK/EFK stack for log aggregation\n# - Prometheus + Grafana for metrics\n# - Jaeger/Zipkin for distributed tracing  \n# - AlertManager for alerting\n# - Custom business metrics\n# - SLI/SLO monitoring\n\n# Monitoring Scope:\n# - API performance and errors\n# - Database query performance\n# - Cache hit rates and performance\n# - Queue sizes and processing times\n# - User behavior and engagement\n# - Infrastructure metrics\n# - Business KPIs\n\n# Bonus Features:\n# - Anomaly detection with ML\n# - Predictive alerting\n# - Automated incident response\n# - Performance regression detection\n```\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Structured Logging = JSON logs with correlation IDs and context\n‚úÖ Metrics Collection = Prometheus metrics for performance monitoring\n‚úÖ Distributed Tracing = OpenTelemetry for request flow visibility\n‚úÖ Alerting = Multi-channel notifications for incidents\n‚úÖ Observability = Complete visibility into system behavior\n\n---\n\n<a name=\"hour-40\"></a>\n## üß™ Hour 40: Testing & CI for Backend\n\n### üéØ Learning Objectives\n- Implement comprehensive testing strategies (unit, integration, e2e)\n- Build automated testing pipelines with pytest\n- Set up continuous integration with GitHub Actions\n- Create test environments and data management\n- Implement code coverage and quality gates\n- Deploy with confidence using automated testing\n\n### üìñ What to Teach\n\n**\"The final hour - where we build unbreakable systems through testing and automation!\"**\n\n---\n\n### 1Ô∏è‚É£ Comprehensive Testing Framework (15 minutes)\n\n**Advanced Testing Setup with pytest:**\n\n```python\n# tests/conftest.py\nimport pytest\nimport asyncio\nimport asyncpg\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\nimport redis.asyncio as redis\nfrom unittest.mock import AsyncMock, MagicMock\nimport tempfile\nimport os\nfrom typing import AsyncGenerator, Generator\nimport uuid\n\n# Import your application\nfrom app.main import create_app\nfrom app.database import get_database\nfrom app.cache import get_redis\nfrom app.auth import get_current_user\nfrom app.config import Settings\n\n# Test settings\n@pytest.fixture(scope=\"session\")\ndef test_settings() -> Settings:\n    \"\"\"Test configuration\"\"\"\n    return Settings(\n        database_url=\"postgresql://test_user:test_pass@localhost:5433/test_blog\",\n        redis_url=\"redis://localhost:6380/1\",\n        secret_key=\"test_secret_key_for_testing_only\",\n        environment=\"testing\"\n    )\n\n# Database fixtures\n@pytest.fixture(scope=\"session\")\nasync def test_db_pool(test_settings):\n    \"\"\"Create test database connection pool\"\"\"\n    \n    pool = await asyncpg.create_pool(test_settings.database_url)\n    \n    # Create test schema\n    async with pool.acquire() as conn:\n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                email VARCHAR(255) UNIQUE NOT NULL,\n                username VARCHAR(100) UNIQUE NOT NULL,\n                hashed_password VARCHAR(255) NOT NULL,\n                is_active BOOLEAN DEFAULT TRUE,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n        \n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS posts (\n                id SERIAL PRIMARY KEY,\n                title VARCHAR(255) NOT NULL,\n                content TEXT NOT NULL,\n                author_id INTEGER REFERENCES users(id),\n                published BOOLEAN DEFAULT FALSE,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n        \n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS comments (\n                id SERIAL PRIMARY KEY,\n                content TEXT NOT NULL,\n                post_id INTEGER REFERENCES posts(id),\n                author_id INTEGER REFERENCES users(id),\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n    \n    yield pool\n    \n    # Cleanup\n    async with pool.acquire() as conn:\n        await conn.execute(\"DROP TABLE IF EXISTS comments CASCADE\")\n        await conn.execute(\"DROP TABLE IF EXISTS posts CASCADE\") \n        await conn.execute(\"DROP TABLE IF EXISTS users CASCADE\")\n    \n    await pool.close()\n\n@pytest.fixture\nasync def db_conn(test_db_pool):\n    \"\"\"Get database connection for test\"\"\"\n    \n    async with test_db_pool.acquire() as conn:\n        # Start transaction\n        tx = conn.transaction()\n        await tx.start()\n        \n        yield conn\n        \n        # Rollback transaction (cleanup)\n        await tx.rollback()\n\n# Redis fixtures\n@pytest.fixture(scope=\"session\")\nasync def test_redis_client(test_settings):\n    \"\"\"Create test Redis client\"\"\"\n    \n    client = redis.from_url(test_settings.redis_url)\n    \n    yield client\n    \n    # Cleanup\n    await client.flushdb()\n    await client.aclose()\n\n@pytest.fixture\nasync def redis_conn(test_redis_client):\n    \"\"\"Get Redis connection for test\"\"\"\n    \n    # Clear before test\n    await test_redis_client.flushdb()\n    \n    yield test_redis_client\n    \n    # Clear after test\n    await test_redis_client.flushdb()\n\n# Application fixtures\n@pytest.fixture\nasync def app(test_settings, test_db_pool, test_redis_client):\n    \"\"\"Create test FastAPI application\"\"\"\n    \n    app = create_app(test_settings)\n    \n    # Override dependencies\n    app.dependency_overrides[get_database] = lambda: test_db_pool\n    app.dependency_overrides[get_redis] = lambda: test_redis_client\n    \n    yield app\n    \n    # Cleanup overrides\n    app.dependency_overrides.clear()\n\n@pytest.fixture\nasync def client(app) -> AsyncGenerator[AsyncClient, None]:\n    \"\"\"Create async HTTP client\"\"\"\n    \n    async with AsyncClient(app=app, base_url=\"http://testserver\") as client:\n        yield client\n\n@pytest.fixture\ndef sync_client(app) -> Generator[TestClient, None, None]:\n    \"\"\"Create synchronous HTTP client\"\"\"\n    \n    with TestClient(app) as client:\n        yield client\n\n# User fixtures\n@pytest.fixture\nasync def test_user(db_conn):\n    \"\"\"Create test user\"\"\"\n    \n    user_data = {\n        \"email\": \"test@example.com\",\n        \"username\": \"testuser\",\n        \"hashed_password\": \"$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj.qK5XQ5zGO\"  # \"testpass123\"\n    }\n    \n    result = await db_conn.fetchrow(\"\"\"\n        INSERT INTO users (email, username, hashed_password) \n        VALUES ($1, $2, $3) \n        RETURNING id, email, username, is_active, created_at\n    \"\"\", user_data[\"email\"], user_data[\"username\"], user_data[\"hashed_password\"])\n    \n    return dict(result)\n\n@pytest.fixture\nasync def authenticated_user(test_user, app):\n    \"\"\"Create authenticated user for testing\"\"\"\n    \n    # Mock authentication\n    async def mock_get_current_user():\n        return test_user\n    \n    app.dependency_overrides[get_current_user] = mock_get_current_user\n    \n    return test_user\n\n@pytest.fixture\nasync def test_post(test_user, db_conn):\n    \"\"\"Create test post\"\"\"\n    \n    post_data = {\n        \"title\": \"Test Post Title\",\n        \"content\": \"This is test post content for testing purposes.\",\n        \"author_id\": test_user[\"id\"],\n        \"published\": True\n    }\n    \n    result = await db_conn.fetchrow(\"\"\"\n        INSERT INTO posts (title, content, author_id, published) \n        VALUES ($1, $2, $3, $4) \n        RETURNING id, title, content, author_id, published, created_at\n    \"\"\", post_data[\"title\"], post_data[\"content\"], post_data[\"author_id\"], post_data[\"published\"])\n    \n    return dict(result)\n\n# Mock fixtures\n@pytest.fixture\ndef mock_email_service():\n    \"\"\"Mock email service\"\"\"\n    \n    mock = AsyncMock()\n    mock.send_email.return_value = True\n    return mock\n\n@pytest.fixture \ndef mock_file_storage():\n    \"\"\"Mock file storage service\"\"\"\n    \n    mock = AsyncMock()\n    mock.upload_file.return_value = \"https://example.com/file.jpg\"\n    mock.delete_file.return_value = True\n    return mock\n\n# Test data factories\nclass UserFactory:\n    \"\"\"Factory for creating test users\"\"\"\n    \n    @staticmethod\n    def build(**kwargs):\n        \"\"\"Build user data\"\"\"\n        \n        defaults = {\n            \"email\": f\"user_{uuid.uuid4()}@example.com\",\n            \"username\": f\"user_{uuid.uuid4().hex[:8]}\",\n            \"password\": \"testpass123\",\n            \"is_active\": True\n        }\n        \n        return {**defaults, **kwargs}\n    \n    @staticmethod\n    async def create(db_conn, **kwargs):\n        \"\"\"Create user in database\"\"\"\n        \n        user_data = UserFactory.build(**kwargs)\n        \n        # Hash password\n        from app.auth import get_password_hash\n        hashed_password = get_password_hash(user_data.pop(\"password\"))\n        \n        result = await db_conn.fetchrow(\"\"\"\n            INSERT INTO users (email, username, hashed_password, is_active) \n            VALUES ($1, $2, $3, $4) \n            RETURNING id, email, username, is_active, created_at\n        \"\"\", user_data[\"email\"], user_data[\"username\"], hashed_password, user_data[\"is_active\"])\n        \n        return dict(result)\n\nclass PostFactory:\n    \"\"\"Factory for creating test posts\"\"\"\n    \n    @staticmethod\n    def build(author_id: int, **kwargs):\n        \"\"\"Build post data\"\"\"\n        \n        defaults = {\n            \"title\": f\"Test Post {uuid.uuid4().hex[:8]}\",\n            \"content\": \"This is test content for the post.\",\n            \"author_id\": author_id,\n            \"published\": False\n        }\n        \n        return {**defaults, **kwargs}\n    \n    @staticmethod\n    async def create(db_conn, author_id: int, **kwargs):\n        \"\"\"Create post in database\"\"\"\n        \n        post_data = PostFactory.build(author_id, **kwargs)\n        \n        result = await db_conn.fetchrow(\"\"\"\n            INSERT INTO posts (title, content, author_id, published) \n            VALUES ($1, $2, $3, $4) \n            RETURNING id, title, content, author_id, published, created_at, updated_at\n        \"\"\", post_data[\"title\"], post_data[\"content\"], post_data[\"author_id\"], post_data[\"published\"])\n        \n        return dict(result)\n\n# Helper functions\ndef assert_response_data(response, expected_data: dict):\n    \"\"\"Assert response contains expected data\"\"\"\n    \n    response_data = response.json()\n    \n    for key, value in expected_data.items():\n        assert key in response_data\n        assert response_data[key] == value\n\nasync def create_test_data(db_conn, num_users=5, posts_per_user=3):\n    \"\"\"Create test data for integration tests\"\"\"\n    \n    users = []\n    posts = []\n    \n    for i in range(num_users):\n        user = await UserFactory.create(db_conn)\n        users.append(user)\n        \n        for j in range(posts_per_user):\n            post = await PostFactory.create(db_conn, user[\"id\"], published=True)\n            posts.append(post)\n    \n    return users, posts\n\n# Event loop configuration\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for async tests\"\"\"\n    \n    loop = asyncio.new_event_loop()\n    yield loop\n    loop.close()\n\n# Performance testing helpers\n@pytest.fixture\ndef performance_tracker():\n    \"\"\"Track test performance\"\"\"\n    \n    class PerformanceTracker:\n        def __init__(self):\n            self.start_time = None\n            self.measurements = {}\n        \n        def start(self, name: str):\n            import time\n            self.start_time = time.time()\n        \n        def stop(self, name: str):\n            import time\n            if self.start_time:\n                duration = time.time() - self.start_time\n                self.measurements[name] = duration\n                return duration\n            return None\n        \n        def assert_duration_under(self, name: str, max_duration: float):\n            assert name in self.measurements\n            assert self.measurements[name] < max_duration, \\\n                f\"Operation {name} took {self.measurements[name]:.3f}s, expected < {max_duration}s\"\n    \n    return PerformanceTracker()\n```\n\n**Unit Tests:**\n\n```python\n# tests/unit/test_auth.py\nimport pytest\nfrom unittest.mock import patch, AsyncMock\nfrom fastapi import HTTPException\nfrom app.auth import (\n    create_access_token, \n    verify_token, \n    get_password_hash, \n    verify_password,\n    authenticate_user\n)\nfrom app.models import User\n\nclass TestAuthentication:\n    \"\"\"Test authentication functions\"\"\"\n    \n    def test_password_hashing(self):\n        \"\"\"Test password hashing and verification\"\"\"\n        \n        password = \"testpassword123\"\n        \n        # Test hashing\n        hashed = get_password_hash(password)\n        assert hashed != password\n        assert len(hashed) > 50\n        \n        # Test verification\n        assert verify_password(password, hashed) is True\n        assert verify_password(\"wrongpassword\", hashed) is False\n    \n    def test_create_access_token(self):\n        \"\"\"Test JWT token creation\"\"\"\n        \n        data = {\"sub\": \"testuser\", \"user_id\": 123}\n        token = create_access_token(data, expires_delta=None)\n        \n        assert isinstance(token, str)\n        assert len(token) > 100\n        \n        # Verify token contains expected data\n        payload = verify_token(token)\n        assert payload[\"sub\"] == \"testuser\"\n        assert payload[\"user_id\"] == 123\n    \n    def test_verify_token_valid(self):\n        \"\"\"Test token verification with valid token\"\"\"\n        \n        data = {\"sub\": \"testuser\", \"user_id\": 123}\n        token = create_access_token(data)\n        \n        payload = verify_token(token)\n        assert payload[\"sub\"] == \"testuser\"\n        assert payload[\"user_id\"] == 123\n    \n    def test_verify_token_invalid(self):\n        \"\"\"Test token verification with invalid token\"\"\"\n        \n        with pytest.raises(HTTPException) as exc_info:\n            verify_token(\"invalid_token\")\n        \n        assert exc_info.value.status_code == 401\n    \n    @pytest.mark.asyncio\n    async def test_authenticate_user_success(self, db_conn):\n        \"\"\"Test successful user authentication\"\"\"\n        \n        # Create test user\n        hashed_password = get_password_hash(\"testpass123\")\n        \n        await db_conn.execute(\"\"\"\n            INSERT INTO users (email, username, hashed_password) \n            VALUES ($1, $2, $3)\n        \"\"\", \"test@example.com\", \"testuser\", hashed_password)\n        \n        # Test authentication\n        with patch('app.auth.get_user_by_username') as mock_get_user:\n            mock_get_user.return_value = {\n                \"id\": 1,\n                \"username\": \"testuser\",\n                \"email\": \"test@example.com\",\n                \"hashed_password\": hashed_password,\n                \"is_active\": True\n            }\n            \n            user = await authenticate_user(db_conn, \"testuser\", \"testpass123\")\n            assert user is not None\n            assert user[\"username\"] == \"testuser\"\n    \n    @pytest.mark.asyncio\n    async def test_authenticate_user_failure(self, db_conn):\n        \"\"\"Test failed user authentication\"\"\"\n        \n        with patch('app.auth.get_user_by_username') as mock_get_user:\n            mock_get_user.return_value = None\n            \n            user = await authenticate_user(db_conn, \"nonexistent\", \"password\")\n            assert user is None\n\n# tests/unit/test_models.py\nimport pytest\nfrom pydantic import ValidationError\nfrom app.schemas import UserCreate, PostCreate, UserResponse\n\nclass TestSchemas:\n    \"\"\"Test Pydantic schemas\"\"\"\n    \n    def test_user_create_valid(self):\n        \"\"\"Test valid user creation schema\"\"\"\n        \n        user_data = {\n            \"email\": \"test@example.com\",\n            \"username\": \"testuser\",\n            \"password\": \"testpass123\"\n        }\n        \n        user = UserCreate(**user_data)\n        assert user.email == \"test@example.com\"\n        assert user.username == \"testuser\"\n        assert user.password == \"testpass123\"\n    \n    def test_user_create_invalid_email(self):\n        \"\"\"Test invalid email validation\"\"\"\n        \n        with pytest.raises(ValidationError):\n            UserCreate(\n                email=\"invalid_email\",\n                username=\"testuser\",\n                password=\"testpass123\"\n            )\n    \n    def test_user_create_short_password(self):\n        \"\"\"Test short password validation\"\"\"\n        \n        with pytest.raises(ValidationError):\n            UserCreate(\n                email=\"test@example.com\",\n                username=\"testuser\", \n                password=\"123\"  # Too short\n            )\n    \n    def test_post_create_valid(self):\n        \"\"\"Test valid post creation schema\"\"\"\n        \n        post_data = {\n            \"title\": \"Test Post\",\n            \"content\": \"This is test content that is long enough.\",\n            \"published\": True\n        }\n        \n        post = PostCreate(**post_data)\n        assert post.title == \"Test Post\"\n        assert post.published is True\n    \n    def test_post_create_title_too_short(self):\n        \"\"\"Test post title validation\"\"\"\n        \n        with pytest.raises(ValidationError):\n            PostCreate(\n                title=\"Hi\",  # Too short\n                content=\"This is test content that is long enough.\",\n                published=False\n            )\n\n# tests/unit/test_services.py\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom app.services import PostService, UserService\nfrom app.exceptions import NotFoundError, ValidationError\n\nclass TestPostService:\n    \"\"\"Test post service business logic\"\"\"\n    \n    @pytest.fixture\n    def post_service(self):\n        return PostService()\n    \n    @pytest.mark.asyncio\n    async def test_create_post_success(self, post_service):\n        \"\"\"Test successful post creation\"\"\"\n        \n        with patch.object(post_service, 'db') as mock_db:\n            mock_db.fetchrow.return_value = {\n                \"id\": 1,\n                \"title\": \"Test Post\",\n                \"content\": \"Test content\",\n                \"author_id\": 1,\n                \"published\": False\n            }\n            \n            post_data = {\n                \"title\": \"Test Post\",\n                \"content\": \"Test content\",\n                \"published\": False\n            }\n            \n            result = await post_service.create_post(1, post_data)\n            \n            assert result[\"title\"] == \"Test Post\"\n            assert result[\"author_id\"] == 1\n            mock_db.fetchrow.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_get_post_not_found(self, post_service):\n        \"\"\"Test getting non-existent post\"\"\"\n        \n        with patch.object(post_service, 'db') as mock_db:\n            mock_db.fetchrow.return_value = None\n            \n            with pytest.raises(NotFoundError):\n                await post_service.get_post(999)\n    \n    @pytest.mark.asyncio\n    async def test_update_post_unauthorized(self, post_service):\n        \"\"\"Test updating post by non-owner\"\"\"\n        \n        with patch.object(post_service, 'db') as mock_db:\n            mock_db.fetchrow.return_value = {\n                \"id\": 1,\n                \"author_id\": 2  # Different author\n            }\n            \n            with pytest.raises(ValidationError):\n                await post_service.update_post(1, 1, {\"title\": \"New Title\"})\n```\n\n**Integration Tests:**\n\n```python\n# tests/integration/test_api.py\nimport pytest\nfrom httpx import AsyncClient\nfrom app.schemas import UserCreate, PostCreate\n\nclass TestUserAPI:\n    \"\"\"Test user API endpoints\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_create_user(self, client: AsyncClient):\n        \"\"\"Test user registration\"\"\"\n        \n        user_data = {\n            \"email\": \"newuser@example.com\",\n            \"username\": \"newuser\",\n            \"password\": \"newpass123\"\n        }\n        \n        response = await client.post(\"/api/v1/users/\", json=user_data)\n        \n        assert response.status_code == 201\n        \n        data = response.json()\n        assert data[\"email\"] == user_data[\"email\"]\n        assert data[\"username\"] == user_data[\"username\"]\n        assert \"id\" in data\n        assert \"hashed_password\" not in data  # Should not return password\n    \n    @pytest.mark.asyncio\n    async def test_create_user_duplicate_email(self, client: AsyncClient, test_user):\n        \"\"\"Test creating user with duplicate email\"\"\"\n        \n        user_data = {\n            \"email\": test_user[\"email\"],  # Duplicate email\n            \"username\": \"different_username\",\n            \"password\": \"newpass123\"\n        }\n        \n        response = await client.post(\"/api/v1/users/\", json=user_data)\n        \n        assert response.status_code == 400\n        assert \"already exists\" in response.json()[\"detail\"]\n    \n    @pytest.mark.asyncio\n    async def test_login_user(self, client: AsyncClient, test_user):\n        \"\"\"Test user login\"\"\"\n        \n        login_data = {\n            \"username\": test_user[\"username\"],\n            \"password\": \"testpass123\"\n        }\n        \n        response = await client.post(\"/api/v1/auth/login\", data=login_data)\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert \"access_token\" in data\n        assert data[\"token_type\"] == \"bearer\"\n    \n    @pytest.mark.asyncio\n    async def test_get_current_user(self, client: AsyncClient, authenticated_user):\n        \"\"\"Test getting current user profile\"\"\"\n        \n        response = await client.get(\"/api/v1/users/me\")\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert data[\"id\"] == authenticated_user[\"id\"]\n        assert data[\"username\"] == authenticated_user[\"username\"]\n\nclass TestPostAPI:\n    \"\"\"Test post API endpoints\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_create_post(self, client: AsyncClient, authenticated_user):\n        \"\"\"Test creating a new post\"\"\"\n        \n        post_data = {\n            \"title\": \"Integration Test Post\",\n            \"content\": \"This is content for integration testing.\",\n            \"published\": True\n        }\n        \n        response = await client.post(\"/api/v1/posts/\", json=post_data)\n        \n        assert response.status_code == 201\n        \n        data = response.json()\n        assert data[\"title\"] == post_data[\"title\"]\n        assert data[\"content\"] == post_data[\"content\"]\n        assert data[\"author_id\"] == authenticated_user[\"id\"]\n        assert data[\"published\"] is True\n    \n    @pytest.mark.asyncio\n    async def test_get_posts_paginated(self, client: AsyncClient, db_conn):\n        \"\"\"Test getting posts with pagination\"\"\"\n        \n        # Create test data\n        users, posts = await create_test_data(db_conn, num_users=2, posts_per_user=5)\n        \n        response = await client.get(\"/api/v1/posts/?limit=5&offset=0\")\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert \"items\" in data\n        assert \"total\" in data\n        assert \"page\" in data\n        assert len(data[\"items\"]) <= 5\n    \n    @pytest.mark.asyncio\n    async def test_get_post_by_id(self, client: AsyncClient, test_post):\n        \"\"\"Test getting specific post\"\"\"\n        \n        response = await client.get(f\"/api/v1/posts/{test_post['id']}\")\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert data[\"id\"] == test_post[\"id\"]\n        assert data[\"title\"] == test_post[\"title\"]\n    \n    @pytest.mark.asyncio\n    async def test_update_post(self, client: AsyncClient, authenticated_user, test_post):\n        \"\"\"Test updating a post\"\"\"\n        \n        update_data = {\n            \"title\": \"Updated Post Title\",\n            \"content\": \"Updated content for the post.\"\n        }\n        \n        response = await client.put(f\"/api/v1/posts/{test_post['id']}\", json=update_data)\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert data[\"title\"] == update_data[\"title\"]\n        assert data[\"content\"] == update_data[\"content\"]\n    \n    @pytest.mark.asyncio\n    async def test_delete_post(self, client: AsyncClient, authenticated_user, test_post):\n        \"\"\"Test deleting a post\"\"\"\n        \n        response = await client.delete(f\"/api/v1/posts/{test_post['id']}\")\n        \n        assert response.status_code == 204\n        \n        # Verify post is deleted\n        get_response = await client.get(f\"/api/v1/posts/{test_post['id']}\")\n        assert get_response.status_code == 404\n\nclass TestPostSearch:\n    \"\"\"Test post search functionality\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_search_posts_by_title(self, client: AsyncClient, db_conn):\n        \"\"\"Test searching posts by title\"\"\"\n        \n        # Create posts with specific titles\n        user = await UserFactory.create(db_conn)\n        \n        await PostFactory.create(db_conn, user[\"id\"], title=\"Python Tutorial\", published=True)\n        await PostFactory.create(db_conn, user[\"id\"], title=\"JavaScript Guide\", published=True)\n        await PostFactory.create(db_conn, user[\"id\"], title=\"Python Advanced\", published=True)\n        \n        response = await client.get(\"/api/v1/posts/search?q=Python\")\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert len(data[\"items\"]) == 2  # Should find 2 Python posts\n        \n        for post in data[\"items\"]:\n            assert \"Python\" in post[\"title\"]\n    \n    @pytest.mark.asyncio\n    async def test_search_posts_no_results(self, client: AsyncClient):\n        \"\"\"Test search with no matching results\"\"\"\n        \n        response = await client.get(\"/api/v1/posts/search?q=nonexistent\")\n        \n        assert response.status_code == 200\n        \n        data = response.json()\n        assert len(data[\"items\"]) == 0\n        assert data[\"total\"] == 0\n```\n\n**Performance Tests:**\n\n```python\n# tests/performance/test_load.py\nimport pytest\nimport asyncio\nimport time\nfrom httpx import AsyncClient\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass TestPerformance:\n    \"\"\"Test application performance under load\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_requests(self, client: AsyncClient, performance_tracker):\n        \"\"\"Test handling concurrent requests\"\"\"\n        \n        async def make_request():\n            response = await client.get(\"/api/v1/posts/\")\n            return response.status_code\n        \n        # Test with 50 concurrent requests\n        performance_tracker.start(\"concurrent_requests\")\n        \n        tasks = [make_request() for _ in range(50)]\n        results = await asyncio.gather(*tasks)\n        \n        duration = performance_tracker.stop(\"concurrent_requests\")\n        \n        # All requests should succeed\n        assert all(status == 200 for status in results)\n        \n        # Should handle 50 requests in under 5 seconds\n        performance_tracker.assert_duration_under(\"concurrent_requests\", 5.0)\n    \n    @pytest.mark.asyncio\n    async def test_database_query_performance(self, db_conn, performance_tracker):\n        \"\"\"Test database query performance\"\"\"\n        \n        # Create large dataset\n        await create_test_data(db_conn, num_users=10, posts_per_user=100)\n        \n        performance_tracker.start(\"large_query\")\n        \n        # Query large dataset\n        result = await db_conn.fetch(\"\"\"\n            SELECT p.*, u.username \n            FROM posts p \n            JOIN users u ON p.author_id = u.id \n            WHERE p.published = TRUE \n            ORDER BY p.created_at DESC \n            LIMIT 100\n        \"\"\")\n        \n        duration = performance_tracker.stop(\"large_query\")\n        \n        assert len(result) <= 100\n        # Should complete query in under 1 second\n        performance_tracker.assert_duration_under(\"large_query\", 1.0)\n    \n    @pytest.mark.asyncio\n    async def test_memory_usage(self, client: AsyncClient):\n        \"\"\"Test memory usage during load\"\"\"\n        \n        import psutil\n        import os\n        \n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n        \n        # Make many requests\n        for _ in range(100):\n            await client.get(\"/api/v1/posts/\")\n        \n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n        memory_increase = final_memory - initial_memory\n        \n        # Memory increase should be reasonable (< 100MB for 100 requests)\n        assert memory_increase < 100, f\"Memory increased by {memory_increase:.2f}MB\"\n```\n\n---\n\n### 2Ô∏è‚É£ Continuous Integration with GitHub Actions (10 minutes)\n\n**Complete CI/CD Pipeline:**\n\n```yaml\n# .github/workflows/ci.yml\nname: CI/CD Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 2 * * *'  # Nightly builds\n\nenv:\n  PYTHON_VERSION: 3.11\n  NODE_VERSION: 18\n  \njobs:\n  # Lint and code quality\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      \n      - name: Cache dependencies\n        uses: actions/cache@v3\n        with:\n          path: ~/.cache/pip\n          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements-dev.txt\n      \n      - name: Run black (code formatting)\n        run: black --check .\n      \n      - name: Run isort (import sorting)\n        run: isort --check-only .\n      \n      - name: Run flake8 (linting)\n        run: flake8 .\n      \n      - name: Run mypy (type checking)\n        run: mypy .\n      \n      - name: Run bandit (security)\n        run: bandit -r app/\n      \n      - name: Run safety (dependency security)\n        run: safety check\n\n  # Unit tests\n  test-unit:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements-dev.txt\n      \n      - name: Run unit tests\n        run: |\n          pytest tests/unit/ \\\n            --cov=app \\\n            --cov-report=xml \\\n            --cov-report=html \\\n            --junit-xml=test-results.xml \\\n            -v\n      \n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n          flags: unit\n          name: unit-tests\n      \n      - name: Upload test results\n        uses: actions/upload-artifact@v3\n        if: always()\n        with:\n          name: test-results-unit\n          path: |\n            test-results.xml\n            htmlcov/\n\n  # Integration tests\n  test-integration:\n    runs-on: ubuntu-latest\n    \n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: test_pass\n          POSTGRES_USER: test_user\n          POSTGRES_DB: test_blog\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          - 5433:5432\n      \n      redis:\n        image: redis:7-alpine\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          - 6380:6379\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements-dev.txt\n      \n      - name: Wait for services\n        run: |\n          sleep 10\n      \n      - name: Run integration tests\n        env:\n          DATABASE_URL: postgresql://test_user:test_pass@localhost:5433/test_blog\n          REDIS_URL: redis://localhost:6380/1\n        run: |\n          pytest tests/integration/ \\\n            --cov=app \\\n            --cov-report=xml \\\n            --junit-xml=integration-results.xml \\\n            -v\n      \n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n          flags: integration\n          name: integration-tests\n\n  # Performance tests\n  test-performance:\n    runs-on: ubuntu-latest\n    needs: [test-unit, test-integration]\n    \n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: test_pass\n          POSTGRES_USER: test_user  \n          POSTGRES_DB: test_blog\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          - 5433:5432\n      \n      redis:\n        image: redis:7-alpine\n        ports:\n          - 6380:6379\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements-dev.txt\n      \n      - name: Run performance tests\n        env:\n          DATABASE_URL: postgresql://test_user:test_pass@localhost:5433/test_blog\n          REDIS_URL: redis://localhost:6380/1\n        run: |\n          pytest tests/performance/ \\\n            --benchmark-json=benchmark.json \\\n            -v\n      \n      - name: Upload benchmark results\n        uses: actions/upload-artifact@v3\n        with:\n          name: benchmark-results\n          path: benchmark.json\n\n  # Security scanning\n  security:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n      \n      - name: Upload Trivy scan results to GitHub Security tab\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n\n  # Build and test Docker image\n  build:\n    runs-on: ubuntu-latest\n    needs: [lint, test-unit, test-integration]\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      \n      - name: Build Docker image\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: false\n          tags: blog-api:test\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n      \n      - name: Test Docker image\n        run: |\n          docker run --rm blog-api:test python -c \"import app; print('‚úÖ App imports successfully')\"\n\n  # Deploy to staging\n  deploy-staging:\n    runs-on: ubuntu-latest\n    needs: [lint, test-unit, test-integration, build, security]\n    if: github.ref == 'refs/heads/develop'\n    environment: staging\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Deploy to staging\n        env:\n          STAGING_HOST: ${{ secrets.STAGING_HOST }}\n          STAGING_USER: ${{ secrets.STAGING_USER }}\n          STAGING_KEY: ${{ secrets.STAGING_SSH_KEY }}\n        run: |\n          echo \"üöÄ Deploying to staging environment\"\n          # Deployment script here\n      \n      - name: Run smoke tests\n        run: |\n          pytest tests/smoke/ \\\n            --base-url=${{ secrets.STAGING_URL }} \\\n            -v\n\n  # Deploy to production\n  deploy-production:\n    runs-on: ubuntu-latest\n    needs: [lint, test-unit, test-integration, build, security]\n    if: github.ref == 'refs/heads/main'\n    environment: production\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Deploy to production\n        env:\n          PRODUCTION_HOST: ${{ secrets.PRODUCTION_HOST }}\n          PRODUCTION_USER: ${{ secrets.PRODUCTION_USER }}\n          PRODUCTION_KEY: ${{ secrets.PRODUCTION_SSH_KEY }}\n        run: |\n          echo \"üöÄ Deploying to production environment\"\n          # Blue-green deployment script here\n      \n      - name: Run production smoke tests\n        run: |\n          pytest tests/smoke/ \\\n            --base-url=${{ secrets.PRODUCTION_URL }} \\\n            -v\n      \n      - name: Notify deployment success\n        if: success()\n        uses: 8398a7/action-slack@v3\n        with:\n          status: success\n          text: \"‚úÖ Production deployment successful!\"\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n\n  # Nightly comprehensive tests\n  test-nightly:\n    runs-on: ubuntu-latest\n    if: github.event.schedule\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run comprehensive test suite\n        run: |\n          echo \"üåô Running nightly comprehensive tests\"\n          # Extended test suite for nightly builds\n```\n\n**Test Configuration:**\n\n```python\n# pytest.ini\n[tool:pytest]\nminversion = 7.0\naddopts = \n    -ra\n    --strict-markers\n    --strict-config\n    --cov=app\n    --cov-report=term-missing:skip-covered\n    --cov-report=html:htmlcov\n    --cov-report=xml\n    --cov-fail-under=85\n    --tb=short\n    --asyncio-mode=auto\n\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\n\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    performance: Performance tests\n    smoke: Smoke tests\n    slow: Slow running tests\n    \nfilterwarnings =\n    error\n    ignore::UserWarning\n    ignore::DeprecationWarning\n\n# pyproject.toml\n[tool.coverage.run]\nsource = [\"app\"]\nomit = [\n    \"*/tests/*\",\n    \"*/migrations/*\", \n    \"*/__pycache__/*\",\n    \"*/venv/*\",\n    \"*/env/*\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\",\n]\n\n[tool.black]\nline-length = 88\ntarget-version = ['py311']\ninclude = '\\.pyi?$'\nextend-exclude = '''\n/(\n  # directories\n  \\.eggs\n  | \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\nline_length = 88\nknown_first_party = [\"app\", \"tests\"]\n\n[tool.mypy]\npython_version = \"3.11\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\nwarn_unreachable = true\nstrict_equality = true\n\n[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisallow_untyped_defs = false\n```\n\n---\n\n### 3Ô∏è‚É£ Test Environment Management (10 minutes)\n\n**Docker Test Environment:**\n\n```dockerfile\n# docker/test.Dockerfile\nFROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    postgresql-client \\\n    redis-tools \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements\nCOPY requirements-dev.txt .\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements-dev.txt\n\n# Copy application code\nCOPY . .\n\n# Create test user\nRUN adduser --disabled-password --gecos '' testuser && \\\n    chown -R testuser:testuser /app\n\nUSER testuser\n\n# Default command for testing\nCMD [\"pytest\", \"-v\"]\n```\n\n```yaml\n# docker-compose.test.yml\nversion: '3.8'\n\nservices:\n  test-db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: test_blog\n      POSTGRES_USER: test_user\n      POSTGRES_PASSWORD: test_pass\n    ports:\n      - \"5433:5432\"\n    volumes:\n      - test_db_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U test_user -d test_blog\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  test-redis:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n\n  test-app:\n    build:\n      context: .\n      dockerfile: docker/test.Dockerfile\n    environment:\n      DATABASE_URL: postgresql://test_user:test_pass@test-db:5432/test_blog\n      REDIS_URL: redis://test-redis:6379/1\n      SECRET_KEY: test_secret_key\n      ENVIRONMENT: testing\n    depends_on:\n      test-db:\n        condition: service_healthy\n      test-redis:\n        condition: service_healthy\n    volumes:\n      - .:/app\n      - test_coverage:/app/htmlcov\n    command: pytest --cov=app --cov-report=html -v\n\nvolumes:\n  test_db_data:\n  test_coverage:\n```\n\n**Test Data Management:**\n\n```python\n# tests/utils/test_data.py\nimport asyncio\nimport json\nfrom typing import Dict, List, Any\nfrom pathlib import Path\n\nclass TestDataManager:\n    \"\"\"Manage test data fixtures and cleanup\"\"\"\n    \n    def __init__(self, db_pool, redis_client):\n        self.db_pool = db_pool\n        self.redis_client = redis_client\n        self.created_data = []\n    \n    async def create_test_scenario(self, scenario_name: str) -> Dict[str, Any]:\n        \"\"\"Create complete test scenario\"\"\"\n        \n        scenarios = {\n            \"blog_with_users_and_posts\": self._create_blog_scenario,\n            \"empty_blog\": self._create_empty_scenario,\n            \"blog_with_comments\": self._create_comments_scenario,\n            \"performance_test_data\": self._create_performance_data\n        }\n        \n        if scenario_name not in scenarios:\n            raise ValueError(f\"Unknown scenario: {scenario_name}\")\n        \n        return await scenarios[scenario_name]()\n    \n    async def _create_blog_scenario(self) -> Dict[str, Any]:\n        \"\"\"Create blog scenario with users and posts\"\"\"\n        \n        async with self.db_pool.acquire() as conn:\n            # Create users\n            users = []\n            for i in range(3):\n                user = await UserFactory.create(\n                    conn,\n                    email=f\"user{i}@example.com\",\n                    username=f\"user{i}\"\n                )\n                users.append(user)\n                self.created_data.append((\"user\", user[\"id\"]))\n            \n            # Create posts\n            posts = []\n            for user in users:\n                for j in range(5):\n                    post = await PostFactory.create(\n                        conn,\n                        user[\"id\"],\n                        title=f\"Post {j} by {user['username']}\",\n                        published=True\n                    )\n                    posts.append(post)\n                    self.created_data.append((\"post\", post[\"id\"]))\n            \n            return {\n                \"users\": users,\n                \"posts\": posts,\n                \"scenario\": \"blog_with_users_and_posts\"\n            }\n    \n    async def cleanup(self):\n        \"\"\"Clean up all created test data\"\"\"\n        \n        async with self.db_pool.acquire() as conn:\n            # Clean up in reverse order\n            for data_type, data_id in reversed(self.created_data):\n                try:\n                    if data_type == \"post\":\n                        await conn.execute(\"DELETE FROM posts WHERE id = $1\", data_id)\n                    elif data_type == \"user\":\n                        await conn.execute(\"DELETE FROM users WHERE id = $1\", data_id)\n                        \n                except Exception as e:\n                    # Log but don't fail cleanup\n                    print(f\"Warning: Failed to cleanup {data_type} {data_id}: {e}\")\n        \n        # Clear Redis\n        await self.redis_client.flushdb()\n        \n        # Clear tracking\n        self.created_data.clear()\n\n# Test database management\nclass TestDatabaseManager:\n    \"\"\"Manage test database lifecycle\"\"\"\n    \n    def __init__(self, database_url: str):\n        self.database_url = database_url\n        self.pool = None\n    \n    async def setup_database(self):\n        \"\"\"Setup test database with schema\"\"\"\n        \n        # Create connection pool\n        self.pool = await asyncpg.create_pool(self.database_url)\n        \n        # Run migrations/schema setup\n        await self._run_migrations()\n        \n        return self.pool\n    \n    async def _run_migrations(self):\n        \"\"\"Run database migrations for tests\"\"\"\n        \n        async with self.pool.acquire() as conn:\n            # Create tables (simplified for example)\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS users (\n                    id SERIAL PRIMARY KEY,\n                    email VARCHAR(255) UNIQUE NOT NULL,\n                    username VARCHAR(100) UNIQUE NOT NULL,\n                    hashed_password VARCHAR(255) NOT NULL,\n                    is_active BOOLEAN DEFAULT TRUE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n                \n                CREATE TABLE IF NOT EXISTS posts (\n                    id SERIAL PRIMARY KEY,\n                    title VARCHAR(255) NOT NULL,\n                    content TEXT NOT NULL,\n                    author_id INTEGER REFERENCES users(id) ON DELETE CASCADE,\n                    published BOOLEAN DEFAULT FALSE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n                \n                CREATE INDEX IF NOT EXISTS idx_posts_author_id ON posts(author_id);\n                CREATE INDEX IF NOT EXISTS idx_posts_published ON posts(published);\n            \"\"\")\n    \n    async def reset_database(self):\n        \"\"\"Reset database to clean state\"\"\"\n        \n        async with self.pool.acquire() as conn:\n            # Truncate all tables\n            await conn.execute(\"TRUNCATE TABLE posts, users RESTART IDENTITY CASCADE\")\n    \n    async def cleanup_database(self):\n        \"\"\"Cleanup database resources\"\"\"\n        \n        if self.pool:\n            await self.pool.close()\n```\n\n---\n\n### 4Ô∏è‚É£ Quality Gates and Deployment (5 minutes)\n\n**Quality Gates Configuration:**\n\n```python\n# scripts/quality_gate.py\n#!/usr/bin/env python3\n\"\"\"\nQuality gate script for deployment pipeline\n\"\"\"\n\nimport asyncio\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\nimport json\n\nclass QualityGate:\n    \"\"\"Quality gate checker\"\"\"\n    \n    def __init__(self):\n        self.checks = []\n        self.results = {}\n    \n    def add_check(self, name: str, command: List[str], threshold: Dict = None):\n        \"\"\"Add quality check\"\"\"\n        \n        self.checks.append({\n            \"name\": name,\n            \"command\": command,\n            \"threshold\": threshold or {}\n        })\n    \n    async def run_all_checks(self) -> bool:\n        \"\"\"Run all quality checks\"\"\"\n        \n        print(\"üîç Running quality gate checks...\")\n        \n        all_passed = True\n        \n        for check in self.checks:\n            passed = await self._run_check(check)\n            self.results[check[\"name\"]] = passed\n            \n            if not passed:\n                all_passed = False\n        \n        self._print_results()\n        \n        return all_passed\n    \n    async def _run_check(self, check: Dict) -> bool:\n        \"\"\"Run individual check\"\"\"\n        \n        print(f\"  ‚è≥ Running {check['name']}...\")\n        \n        try:\n            result = subprocess.run(\n                check[\"command\"],\n                capture_output=True,\n                text=True,\n                timeout=300  # 5 minute timeout\n            )\n            \n            success = result.returncode == 0\n            \n            if success:\n                print(f\"  ‚úÖ {check['name']} passed\")\n            else:\n                print(f\"  ‚ùå {check['name']} failed\")\n                if result.stdout:\n                    print(f\"     Output: {result.stdout.strip()}\")\n                if result.stderr:\n                    print(f\"     Error: {result.stderr.strip()}\")\n            \n            return success\n            \n        except subprocess.TimeoutExpired:\n            print(f\"  ‚è∞ {check['name']} timed out\")\n            return False\n        except Exception as e:\n            print(f\"  üí• {check['name']} error: {str(e)}\")\n            return False\n    \n    def _print_results(self):\n        \"\"\"Print summary of results\"\"\"\n        \n        passed = sum(1 for result in self.results.values() if result)\n        total = len(self.results)\n        \n        print(f\"\\nüìä Quality Gate Results: {passed}/{total} checks passed\")\n        \n        for name, result in self.results.items():\n            status = \"‚úÖ PASS\" if result else \"‚ùå FAIL\"\n            print(f\"  {status} {name}\")\n        \n        if all(self.results.values()):\n            print(\"\\nüéâ All quality checks passed! Ready for deployment.\")\n        else:\n            print(\"\\nüö´ Quality gate failed. Deployment blocked.\")\n\nasync def main():\n    \"\"\"Main quality gate runner\"\"\"\n    \n    gate = QualityGate()\n    \n    # Add quality checks\n    gate.add_check(\"Code Formatting\", [\"black\", \"--check\", \".\"])\n    gate.add_check(\"Import Sorting\", [\"isort\", \"--check-only\", \".\"])\n    gate.add_check(\"Code Linting\", [\"flake8\", \".\"])\n    gate.add_check(\"Type Checking\", [\"mypy\", \".\"])\n    gate.add_check(\"Security Scan\", [\"bandit\", \"-r\", \"app/\"])\n    gate.add_check(\"Dependency Security\", [\"safety\", \"check\"])\n    \n    # Test checks with coverage requirements\n    gate.add_check(\"Unit Tests\", [\n        \"pytest\", \"tests/unit/\", \n        \"--cov=app\", \n        \"--cov-fail-under=85\",\n        \"--tb=short\"\n    ])\n    \n    gate.add_check(\"Integration Tests\", [\n        \"pytest\", \"tests/integration/\",\n        \"--cov=app\",\n        \"--cov-fail-under=70\",\n        \"--tb=short\"\n    ])\n    \n    # Performance checks\n    gate.add_check(\"Performance Tests\", [\n        \"pytest\", \"tests/performance/\",\n        \"--tb=short\"\n    ])\n    \n    # Run all checks\n    success = await gate.run_all_checks()\n    \n    # Exit with appropriate code\n    sys.exit(0 if success else 1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n**Deployment Scripts:**\n\n```bash\n#!/bin/bash\n# scripts/deploy.sh\nset -e\n\necho \"üöÄ Starting deployment process...\"\n\n# Run quality gate\necho \"üìã Running quality gate...\"\npython scripts/quality_gate.py\n\nif [ $? -ne 0 ]; then\n    echo \"‚ùå Quality gate failed. Aborting deployment.\"\n    exit 1\nfi\n\n# Build Docker image\necho \"üèóÔ∏è Building Docker image...\"\ndocker build -t blog-api:latest .\n\n# Run smoke tests on built image\necho \"üí® Running smoke tests...\"\ndocker run --rm blog-api:latest python -c \"\nimport app\nprint('‚úÖ Application imports successfully')\n\n# Basic health check\nfrom app.main import app\nprint('‚úÖ FastAPI app created successfully')\n\"\n\n# Deploy based on environment\nif [ \"$ENVIRONMENT\" = \"production\" ]; then\n    echo \"üéØ Deploying to production...\"\n    ./scripts/deploy_production.sh\nelse\n    echo \"üß™ Deploying to staging...\"\n    ./scripts/deploy_staging.sh\nfi\n\necho \"‚úÖ Deployment completed successfully!\"\n```\n\n---\n\n### üè† Final Project: Complete Testing & CI System\n\n**Task:** Build a production-ready testing and CI/CD system\n\n```python\n# Create comprehensive testing framework with:\n# 1. Unit tests (>90% coverage)\n# 2. Integration tests for all API endpoints\n# 3. Performance tests with benchmarks\n# 4. Security tests and vulnerability scanning\n# 5. End-to-end tests for critical user journeys\n# 6. GitHub Actions CI/CD pipeline\n# 7. Quality gates for deployment\n# 8. Automated staging and production deployments\n\n# Testing Requirements:\n# - pytest with async support\n# - Test factories and fixtures\n# - Database transaction rollback\n# - Mock external services\n# - Performance benchmarking\n# - Code coverage reports\n# - Test parallelization\n\n# CI/CD Features:\n# - Automated testing on PR/push\n# - Code quality checks (linting, formatting, types)\n# - Security scanning\n# - Docker image building and testing\n# - Deployment to staging/production\n# - Slack/email notifications\n# - Rollback capabilities\n\n# Quality Gates:\n# - 85%+ test coverage\n# - Zero security vulnerabilities\n# - All linting/formatting checks pass\n# - Performance benchmarks met\n# - All tests pass\n\n# Deployment Pipeline:\n# - Blue/green deployment\n# - Database migrations\n# - Health checks\n# - Smoke tests in production\n# - Monitoring alerts setup\n```\n\n---\n\n### üéì Complete 40-Hour Curriculum Summary\n\n**Congratulations! You've completed the full 40-hour Advanced Backend Development curriculum!**\n\n‚úÖ **Part 1** - Python Fundamentals (Hours 1-10)\n‚úÖ **Part 2** - Backend Development (Hours 11-20) \n‚úÖ **Part 3** - Frontend Development (Hours 21-30)\n‚úÖ **Part 4** - Advanced Backend (Hours 31-40)\n\n**What you've mastered:**\n- üêç Python from basics to advanced concepts\n- üöÄ FastAPI with async programming\n- üóÑÔ∏è Database design and optimization\n- ‚ö° Caching and performance tuning\n- üì± Frontend development with React\n- üîí Security and authentication\n- üìä GraphQL APIs\n- üèóÔ∏è Microservices architecture\n- üìà Observability and monitoring\n- üß™ Testing and CI/CD\n\n**You're now ready for:**\n- Senior Backend Developer roles\n- Full-stack development positions\n- System architecture design\n- DevOps and deployment automation\n- Production system optimization\n- Team technical leadership\n\n---\n\n### üìù Key Takeaways\n\n‚úÖ Comprehensive Testing = Unit + Integration + Performance + E2E\n‚úÖ CI/CD Pipeline = Automated quality gates and deployments\n‚úÖ Quality Gates = Coverage, security, performance thresholds\n‚úÖ Test Environment = Containerized, reproducible, automated\n‚úÖ Production Ready = Battle-tested with confidence!\n\n**üéâ Congratulations on completing the ultimate backend mastery journey!**\n\n---"}