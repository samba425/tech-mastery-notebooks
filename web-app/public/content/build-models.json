{"id":"build-models","title":"üî¨ Build ML Models From Scratch","content":"# Building Machine Learning Models from Scratch: Zero to Hero\n\n> **Master ML/DL by Building Everything from First Principles - No Libraries, Pure Understanding**\n\n---\n\n## üéØ What You'll Learn\n\nBy the end of this guide, you will:\n- ‚úÖ Understand how ML algorithms work under the hood\n- ‚úÖ Build ML algorithms from scratch (no sklearn!)\n- ‚úÖ Implement neural networks with just NumPy\n- ‚úÖ Create custom loss functions and optimizers\n- ‚úÖ Design your own model architectures\n- ‚úÖ Train models efficiently\n- ‚úÖ Debug and improve model performance\n- ‚úÖ Deploy your custom models to production\n- ‚úÖ Understand when to use which algorithm\n\n---\n\n## üìö Table of Contents\n\n### Part 1: Traditional Machine Learning from Scratch\n1. [Linear Regression from Scratch](#1-linear-regression-from-scratch)\n2. [Logistic Regression from Scratch](#2-logistic-regression-from-scratch)\n3. [Decision Trees from Scratch](#3-decision-trees-from-scratch)\n4. [Random Forest from Scratch](#4-random-forest-from-scratch)\n5. [K-Nearest Neighbors from Scratch](#5-k-nearest-neighbors-from-scratch)\n6. [K-Means Clustering from Scratch](#6-k-means-clustering-from-scratch)\n7. [Support Vector Machines from Scratch](#7-support-vector-machines-from-scratch)\n8. [Naive Bayes from Scratch](#8-naive-bayes-from-scratch)\n\n### Part 2: Neural Networks from Scratch\n9. [Understanding Neural Networks](#9-understanding-neural-networks)\n10. [Building a Neural Network with NumPy](#10-building-a-neural-network-with-numpy)\n11. [Activation Functions from Scratch](#11-activation-functions-from-scratch)\n12. [Loss Functions from Scratch](#12-loss-functions-from-scratch)\n13. [Backpropagation Explained](#13-backpropagation-explained)\n14. [Optimizers from Scratch](#14-optimizers-from-scratch)\n\n### Part 3: Deep Learning Architectures\n15. [Convolutional Neural Networks (CNN)](#15-convolutional-neural-networks)\n16. [Recurrent Neural Networks (RNN)](#16-recurrent-neural-networks)\n17. [Long Short-Term Memory (LSTM)](#17-long-short-term-memory)\n18. [Attention Mechanism](#18-attention-mechanism)\n19. [Transformer Architecture](#19-transformer-architecture)\n\n### Part 4: Advanced Topics\n20. [Regularization Techniques](#20-regularization-techniques)\n21. [Batch Normalization](#21-batch-normalization)\n22. [Dropout](#22-dropout)\n23. [Custom Model Architectures](#23-custom-model-architectures)\n24. [Transfer Learning](#24-transfer-learning)\n25. [Model Compression](#25-model-compression)\n\n### Part 5: Production & Optimization\n26. [Training Strategies](#26-training-strategies)\n27. [Hyperparameter Tuning](#27-hyperparameter-tuning)\n28. [Model Debugging](#28-model-debugging)\n29. [Performance Optimization](#29-performance-optimization)\n30. [Deployment Strategies](#30-deployment-strategies)\n\n---\n\n# Part 1: Traditional Machine Learning from Scratch\n\n## 1. Linear Regression from Scratch\n\n### Understanding the Math\n\n**Simple Explanation:** Find the best line that fits your data points.\n\n**Mathematical Formula:**\n```\ny = mx + b\nor in ML terms:\n≈∑ = w‚ÇÅx + w‚ÇÄ\n\nLoss function (MSE): L = (1/n) Œ£(y - ≈∑)¬≤\n```\n\n### Implementation from Scratch\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass LinearRegressionScratch:\n    \"\"\"\n    Linear Regression implemented from scratch\n    No sklearn, pure NumPy and math!\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Parameters:\n        -----------\n        learning_rate : float\n            Step size for gradient descent\n        n_iterations : int\n            Number of training iterations\n        \"\"\"\n        self.lr = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.loss_history = []\n    \n    def fit(self, X, y):\n        \"\"\"\n        Train the model using gradient descent\n        \n        Parameters:\n        -----------\n        X : numpy array of shape (n_samples, n_features)\n        y : numpy array of shape (n_samples,)\n        \"\"\"\n        # Initialize parameters\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        # Gradient Descent\n        for i in range(self.n_iterations):\n            # Forward pass: predictions\n            y_predicted = np.dot(X, self.weights) + self.bias\n            \n            # Compute loss (MSE)\n            loss = np.mean((y - y_predicted) ** 2)\n            self.loss_history.append(loss)\n            \n            # Backward pass: compute gradients\n            # dL/dw = (2/n) * Œ£(≈∑ - y) * x\n            # dL/db = (2/n) * Œ£(≈∑ - y)\n            dw = (2/n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (2/n_samples) * np.sum(y_predicted - y)\n            \n            # Update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n            \n            # Print progress\n            if i % 100 == 0:\n                print(f\"Iteration {i}: Loss = {loss:.4f}\")\n    \n    def predict(self, X):\n        \"\"\"\n        Make predictions\n        \n        Parameters:\n        -----------\n        X : numpy array of shape (n_samples, n_features)\n        \n        Returns:\n        --------\n        y_pred : numpy array of shape (n_samples,)\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n    \n    def score(self, X, y):\n        \"\"\"\n        Calculate R¬≤ score\n        \n        R¬≤ = 1 - (SS_res / SS_tot)\n        where:\n        SS_res = Œ£(y - ≈∑)¬≤  (residual sum of squares)\n        SS_tot = Œ£(y - »≥)¬≤  (total sum of squares)\n        \"\"\"\n        y_pred = self.predict(X)\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        r2 = 1 - (ss_res / ss_tot)\n        return r2\n\n\n# ============================================\n# TEST THE MODEL\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"LINEAR REGRESSION FROM SCRATCH - DEMO\")\nprint(\"=\" * 70)\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X.squeeze() + np.random.randn(100)\n\nprint(\"\\n1. Generated synthetic data\")\nprint(f\"   X shape: {X.shape}\")\nprint(f\"   y shape: {y.shape}\")\n\n# Create and train model\nmodel = LinearRegressionScratch(learning_rate=0.1, n_iterations=1000)\nprint(\"\\n2. Training model...\")\nmodel.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\n\n# Evaluate\nr2 = model.score(X, y)\nprint(f\"\\n3. Model Performance:\")\nprint(f\"   R¬≤ Score: {r2:.4f}\")\nprint(f\"   Learned weight: {model.weights[0]:.4f} (true: 3.0)\")\nprint(f\"   Learned bias: {model.bias:.4f} (true: 4.0)\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Data and predictions\naxes[0].scatter(X, y, alpha=0.5, label='Actual data')\naxes[0].plot(X, y_pred, 'r-', linewidth=2, label='Predictions')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('y')\naxes[0].set_title('Linear Regression: Data vs Predictions')\naxes[0].legend()\naxes[0].grid(True)\n\n# Plot 2: Loss curve\naxes[1].plot(model.loss_history)\naxes[1].set_xlabel('Iteration')\naxes[1].set_ylabel('Loss (MSE)')\naxes[1].set_title('Training Loss Over Time')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Linear Regression from scratch completed!\")\n```\n\n### Understanding Gradient Descent\n\n```python\ndef visualize_gradient_descent():\n    \"\"\"\n    Visualize how gradient descent finds the minimum\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"UNDERSTANDING GRADIENT DESCENT\")\n    print(\"=\" * 70)\n    \n    # Simple 1D loss function\n    def loss_function(w):\n        return (w - 3) ** 2 + 2\n    \n    def gradient(w):\n        return 2 * (w - 3)\n    \n    # Gradient descent\n    learning_rates = [0.1, 0.3, 0.9]\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    for idx, lr in enumerate(learning_rates):\n        w = 0.0  # Starting point\n        history = [w]\n        \n        for _ in range(20):\n            grad = gradient(w)\n            w = w - lr * grad\n            history.append(w)\n        \n        # Plot\n        w_range = np.linspace(-2, 8, 100)\n        loss_range = loss_function(w_range)\n        \n        axes[idx].plot(w_range, loss_range, 'b-', label='Loss function')\n        axes[idx].plot(history, loss_function(np.array(history)), \n                       'ro-', label='GD path', markersize=4)\n        axes[idx].axvline(x=3, color='g', linestyle='--', label='Minimum')\n        axes[idx].set_xlabel('Weight (w)')\n        axes[idx].set_ylabel('Loss')\n        axes[idx].set_title(f'Learning Rate = {lr}')\n        axes[idx].legend()\n        axes[idx].grid(True)\n        \n        print(f\"\\nLearning Rate {lr}:\")\n        print(f\"  Final weight: {history[-1]:.4f}\")\n        print(f\"  Final loss: {loss_function(history[-1]):.4f}\")\n        print(f\"  Iterations to converge: {len(history)}\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüí° KEY INSIGHTS:\")\n    print(\"   - Too small LR: Slow convergence\")\n    print(\"   - Too large LR: Oscillation or divergence\")\n    print(\"   - Just right LR: Fast and stable convergence\")\n\nvisualize_gradient_descent()\n```\n\n### Multiple Linear Regression\n\n```python\nclass MultipleLinearRegression:\n    \"\"\"\n    Multiple Linear Regression: ≈∑ = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n    \n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        for _ in range(self.n_iterations):\n            # Predictions\n            y_pred = np.dot(X, self.weights) + self.bias\n            \n            # Gradients\n            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n            db = (1/n_samples) * np.sum(y_pred - y)\n            \n            # Update\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n    \n    def predict(self, X):\n        return np.dot(X, self.weights) + self.bias\n    \n    def get_coefficients(self):\n        \"\"\"Return feature coefficients\"\"\"\n        return dict(enumerate(self.weights))\n\n# Test with multiple features\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MULTIPLE LINEAR REGRESSION\")\nprint(\"=\" * 70)\n\n# Generate data with 3 features\nnp.random.seed(42)\nX_multi = np.random.rand(100, 3)\ny_multi = 4 + 2*X_multi[:, 0] + 3*X_multi[:, 1] + 5*X_multi[:, 2] + np.random.randn(100)*0.5\n\nmodel_multi = MultipleLinearRegression(learning_rate=0.1, n_iterations=1000)\nmodel_multi.fit(X_multi, y_multi)\n\nprint(\"\\nTrue coefficients: [2, 3, 5], bias: 4\")\nprint(f\"Learned coefficients: {model_multi.weights}\")\nprint(f\"Learned bias: {model_multi.bias:.4f}\")\n```\n\n---\n\n## 2. Logistic Regression from Scratch\n\n### Understanding the Math\n\n**Simple Explanation:** Predict probability between 0 and 1 using sigmoid function.\n\n**Mathematical Formula:**\n```\nSigmoid: œÉ(z) = 1 / (1 + e^(-z))\nPrediction: ≈∑ = œÉ(wx + b)\nLoss (Binary Cross-Entropy): L = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]\n```\n\n### Implementation\n\n```python\nclass LogisticRegressionScratch:\n    \"\"\"\n    Logistic Regression from scratch\n    For binary classification\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.loss_history = []\n    \n    def sigmoid(self, z):\n        \"\"\"\n        Sigmoid activation function\n        œÉ(z) = 1 / (1 + e^(-z))\n        \n        Properties:\n        - Output range: (0, 1)\n        - Smooth gradient\n        - Interpretable as probability\n        \"\"\"\n        return 1 / (1 + np.exp(-z))\n    \n    def fit(self, X, y):\n        \"\"\"\n        Train using gradient descent\n        \"\"\"\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        # Gradient descent\n        for i in range(self.n_iterations):\n            # Forward pass\n            linear_model = np.dot(X, self.weights) + self.bias\n            y_predicted = self.sigmoid(linear_model)\n            \n            # Compute loss (Binary Cross-Entropy)\n            epsilon = 1e-15  # Prevent log(0)\n            y_predicted = np.clip(y_predicted, epsilon, 1 - epsilon)\n            loss = -np.mean(y * np.log(y_predicted) + (1 - y) * np.log(1 - y_predicted))\n            self.loss_history.append(loss)\n            \n            # Backward pass: compute gradients\n            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1/n_samples) * np.sum(y_predicted - y)\n            \n            # Update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n            \n            if i % 100 == 0:\n                print(f\"Iteration {i}: Loss = {loss:.4f}\")\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Predict probabilities\n        \"\"\"\n        linear_model = np.dot(X, self.weights) + self.bias\n        return self.sigmoid(linear_model)\n    \n    def predict(self, X, threshold=0.5):\n        \"\"\"\n        Predict class labels (0 or 1)\n        \"\"\"\n        probabilities = self.predict_proba(X)\n        return (probabilities >= threshold).astype(int)\n    \n    def accuracy(self, X, y):\n        \"\"\"\n        Calculate accuracy\n        \"\"\"\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n\n\n# ============================================\n# TEST LOGISTIC REGRESSION\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"LOGISTIC REGRESSION FROM SCRATCH - DEMO\")\nprint(\"=\" * 70)\n\n# Generate binary classification data\nfrom sklearn.datasets import make_classification\nX_class, y_class = make_classification(n_samples=1000, n_features=2, \n                                       n_redundant=0, n_informative=2,\n                                       n_clusters_per_class=1, random_state=42)\n\nprint(\"\\n1. Generated binary classification data\")\nprint(f\"   Class 0: {np.sum(y_class == 0)} samples\")\nprint(f\"   Class 1: {np.sum(y_class == 1)} samples\")\n\n# Train model\nprint(\"\\n2. Training logistic regression...\")\nlog_model = LogisticRegressionScratch(learning_rate=0.1, n_iterations=1000)\nlog_model.fit(X_class, y_class)\n\n# Evaluate\naccuracy = log_model.accuracy(X_class, y_class)\nprint(f\"\\n3. Model Performance:\")\nprint(f\"   Accuracy: {accuracy:.4f}\")\n\n# Visualize decision boundary\ndef plot_decision_boundary(model, X, y):\n    \"\"\"\n    Plot decision boundary for 2D data\n    \"\"\"\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    \n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Decision boundary\n    plt.subplot(1, 2, 1)\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Decision Boundary')\n    plt.colorbar()\n    \n    # Plot 2: Loss curve\n    plt.subplot(1, 2, 2)\n    plt.plot(model.loss_history)\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss (Binary Cross-Entropy)')\n    plt.title('Training Loss')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_decision_boundary(log_model, X_class, y_class)\n\nprint(\"\\n‚úÖ Logistic Regression from scratch completed!\")\n```\n\n### Understanding Sigmoid Function\n\n```python\ndef understand_sigmoid():\n    \"\"\"\n    Deep dive into sigmoid function\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"UNDERSTANDING SIGMOID FUNCTION\")\n    print(\"=\" * 70)\n    \n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    def sigmoid_derivative(z):\n        s = sigmoid(z)\n        return s * (1 - s)\n    \n    z = np.linspace(-10, 10, 100)\n    sig = sigmoid(z)\n    sig_deriv = sigmoid_derivative(z)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Sigmoid function\n    axes[0].plot(z, sig, 'b-', linewidth=2)\n    axes[0].axhline(y=0.5, color='r', linestyle='--', label='Decision boundary')\n    axes[0].axvline(x=0, color='g', linestyle='--')\n    axes[0].set_xlabel('z (wx + b)')\n    axes[0].set_ylabel('œÉ(z)')\n    axes[0].set_title('Sigmoid Function')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Derivative\n    axes[1].plot(z, sig_deriv, 'r-', linewidth=2)\n    axes[1].set_xlabel('z')\n    axes[1].set_ylabel(\"œÉ'(z)\")\n    axes[1].set_title('Sigmoid Derivative')\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüí° KEY PROPERTIES:\")\n    print(\"   - Output range: (0, 1) ‚Üí can be interpreted as probability\")\n    print(\"   - Smooth and differentiable everywhere\")\n    print(\"   - œÉ(0) = 0.5 ‚Üí natural decision boundary\")\n    print(\"   - œÉ'(z) = œÉ(z) ¬∑ (1 - œÉ(z)) ‚Üí easy to compute gradient\")\n    print(\"   - Problem: Vanishing gradients for large |z|\")\n\nunderstand_sigmoid()\n```\n\n---\n\n## 3. Decision Trees from Scratch\n\n### Understanding the Math\n\n**Simple Explanation:** Split data recursively to create a tree of decisions.\n\n**Key Concepts:**\n- **Entropy**: Measure of impurity/disorder\n- **Information Gain**: Reduction in entropy after split\n- **Gini Impurity**: Alternative to entropy\n\n### Implementation\n\n```python\nclass Node:\n    \"\"\"\n    Node in decision tree\n    \"\"\"\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature      # Feature to split on\n        self.threshold = threshold  # Threshold value for split\n        self.left = left           # Left child\n        self.right = right         # Right child\n        self.value = value         # Prediction value (for leaf nodes)\n\n\nclass DecisionTreeScratch:\n    \"\"\"\n    Decision Tree Classifier from scratch\n    Using Information Gain (Entropy) for splits\n    \"\"\"\n    \n    def __init__(self, max_depth=10, min_samples_split=2):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.root = None\n    \n    def entropy(self, y):\n        \"\"\"\n        Calculate entropy: H(S) = -Œ£(p·µ¢ ¬∑ log‚ÇÇ(p·µ¢))\n        \n        Entropy measures disorder:\n        - 0: Pure (all same class)\n        - 1: Maximum impurity (equal distribution)\n        \"\"\"\n        proportions = np.bincount(y) / len(y)\n        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n        return entropy\n    \n    def information_gain(self, parent, left_child, right_child):\n        \"\"\"\n        Calculate information gain\n        IG = H(parent) - [weighted_avg(H(children))]\n        \"\"\"\n        weight_left = len(left_child) / len(parent)\n        weight_right = len(right_child) / len(parent)\n        \n        gain = self.entropy(parent) - (\n            weight_left * self.entropy(left_child) +\n            weight_right * self.entropy(right_child)\n        )\n        return gain\n    \n    def best_split(self, X, y):\n        \"\"\"\n        Find the best feature and threshold to split on\n        \"\"\"\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        \n        n_features = X.shape[1]\n        \n        # Try each feature\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            thresholds = np.unique(feature_values)\n            \n            # Try each threshold\n            for threshold in thresholds:\n                # Split data\n                left_mask = feature_values <= threshold\n                right_mask = feature_values > threshold\n                \n                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                    continue\n                \n                # Calculate information gain\n                left_y = y[left_mask]\n                right_y = y[right_mask]\n                gain = self.information_gain(y, left_y, right_y)\n                \n                # Update best split\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n        \n        return best_feature, best_threshold\n    \n    def build_tree(self, X, y, depth=0):\n        \"\"\"\n        Recursively build the tree\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n        \n        # Stopping criteria\n        if (depth >= self.max_depth or \n            n_samples < self.min_samples_split or \n            n_classes == 1):\n            # Create leaf node\n            leaf_value = np.argmax(np.bincount(y))\n            return Node(value=leaf_value)\n        \n        # Find best split\n        best_feature, best_threshold = self.best_split(X, y)\n        \n        if best_feature is None:\n            leaf_value = np.argmax(np.bincount(y))\n            return Node(value=leaf_value)\n        \n        # Split data\n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = X[:, best_feature] > best_threshold\n        \n        # Recursively build left and right subtrees\n        left_child = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_child = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return Node(best_feature, best_threshold, left_child, right_child)\n    \n    def fit(self, X, y):\n        \"\"\"\n        Build the decision tree\n        \"\"\"\n        self.root = self.build_tree(X, y)\n    \n    def predict_sample(self, x, node):\n        \"\"\"\n        Predict single sample by traversing tree\n        \"\"\"\n        # If leaf node, return value\n        if node.value is not None:\n            return node.value\n        \n        # Traverse tree\n        if x[node.feature] <= node.threshold:\n            return self.predict_sample(x, node.left)\n        else:\n            return self.predict_sample(x, node.right)\n    \n    def predict(self, X):\n        \"\"\"\n        Predict multiple samples\n        \"\"\"\n        return np.array([self.predict_sample(x, self.root) for x in X])\n    \n    def accuracy(self, X, y):\n        \"\"\"\n        Calculate accuracy\n        \"\"\"\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n\n\n# ============================================\n# TEST DECISION TREE\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DECISION TREE FROM SCRATCH - DEMO\")\nprint(\"=\" * 70)\n\n# Generate data\nfrom sklearn.datasets import make_moons\nX_tree, y_tree = make_moons(n_samples=500, noise=0.3, random_state=42)\n\nprint(\"\\n1. Generated dataset with 500 samples\")\n\n# Train decision tree\nprint(\"\\n2. Building decision tree...\")\ntree = DecisionTreeScratch(max_depth=10, min_samples_split=2)\ntree.fit(X_tree, y_tree)\n\n# Evaluate\naccuracy = tree.accuracy(X_tree, y_tree)\nprint(f\"\\n3. Training Accuracy: {accuracy:.4f}\")\n\n# Visualize\ndef visualize_tree_decision_boundary(model, X, y):\n    \"\"\"\n    Visualize decision boundary for decision tree\n    \"\"\"\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    \n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=50)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(f'Decision Tree Boundary (Depth={model.max_depth})')\n    plt.colorbar()\n    plt.show()\n\nvisualize_tree_decision_boundary(tree, X_tree, y_tree)\n\nprint(\"\\n‚úÖ Decision Tree from scratch completed!\")\n```\n\n### Understanding Entropy and Information Gain\n\n```python\ndef understand_entropy_and_information_gain():\n    \"\"\"\n    Visualize entropy and information gain\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"UNDERSTANDING ENTROPY & INFORMATION GAIN\")\n    print(\"=\" * 70)\n    \n    def entropy(p):\n        \"\"\"Entropy for binary classification\"\"\"\n        if p == 0 or p == 1:\n            return 0\n        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n    \n    # Entropy curve\n    p_values = np.linspace(0.01, 0.99, 100)\n    entropy_values = [entropy(p) for p in p_values]\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Entropy curve\n    plt.subplot(1, 2, 1)\n    plt.plot(p_values, entropy_values, 'b-', linewidth=2)\n    plt.axvline(x=0.5, color='r', linestyle='--', label='Maximum entropy')\n    plt.xlabel('Probability of class 1')\n    plt.ylabel('Entropy')\n    plt.title('Entropy Function')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot 2: Example split\n    plt.subplot(1, 2, 2)\n    \n    # Before split: 50-50\n    before = [5, 5]\n    # After split: [8, 2] and [2, 8]\n    left = [8, 2]\n    right = [2, 8]\n    \n    categories = ['Before\\nSplit', 'Left\\nChild', 'Right\\nChild']\n    class_0 = [before[0], left[0], right[0]]\n    class_1 = [before[1], left[1], right[1]]\n    \n    x = np.arange(len(categories))\n    width = 0.35\n    \n    plt.bar(x - width/2, class_0, width, label='Class 0', color='blue', alpha=0.7)\n    plt.bar(x + width/2, class_1, width, label='Class 1', color='red', alpha=0.7)\n    plt.xlabel('Node')\n    plt.ylabel('Count')\n    plt.title('Example Split')\n    plt.xticks(x, categories)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate entropies\n    def calc_entropy(counts):\n        total = sum(counts)\n        probs = [c/total for c in counts]\n        return -sum(p * np.log2(p) if p > 0 else 0 for p in probs)\n    \n    h_before = calc_entropy(before)\n    h_left = calc_entropy(left)\n    h_right = calc_entropy(right)\n    \n    # Information gain\n    total = sum(before)\n    ig = h_before - (len(left)/total * h_left + len(right)/total * h_right)\n    \n    print(\"\\nüìä EXAMPLE CALCULATION:\")\n    print(f\"   Before split: {before} ‚Üí Entropy = {h_before:.3f}\")\n    print(f\"   Left child:   {left} ‚Üí Entropy = {h_left:.3f}\")\n    print(f\"   Right child:  {right} ‚Üí Entropy = {h_right:.3f}\")\n    print(f\"   Information Gain = {ig:.3f}\")\n    \n    print(\"\\nüí° INTERPRETATION:\")\n    print(\"   - High entropy = High disorder/impurity\")\n    print(\"   - Low entropy = Low disorder (more pure)\")\n    print(\"   - Information Gain = Reduction in entropy\")\n    print(\"   - We want splits that maximize information gain!\")\n\nunderstand_entropy_and_information_gain()\n```\n\n---\n\n## 4. Random Forest from Scratch\n\n```python\nclass RandomForestScratch:\n    \"\"\"\n    Random Forest: Ensemble of decision trees\n    \"\"\"\n    \n    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, \n                 max_features='sqrt'):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n        self.trees = []\n    \n    def bootstrap_sample(self, X, y):\n        \"\"\"\n        Create bootstrap sample (random sampling with replacement)\n        \"\"\"\n        n_samples = X.shape[0]\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        return X[indices], y[indices]\n    \n    def fit(self, X, y):\n        \"\"\"\n        Build multiple decision trees\n        \"\"\"\n        self.trees = []\n        \n        for i in range(self.n_trees):\n            # Bootstrap sample\n            X_sample, y_sample = self.bootstrap_sample(X, y)\n            \n            # Train tree\n            tree = DecisionTreeScratch(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split\n            )\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Built {i + 1}/{self.n_trees} trees\")\n    \n    def predict(self, X):\n        \"\"\"\n        Predict by majority voting\n        \"\"\"\n        # Get predictions from all trees\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n        \n        # Majority vote\n        predictions = []\n        for i in range(X.shape[0]):\n            predictions.append(np.bincount(tree_predictions[:, i].astype(int)).argmax())\n        \n        return np.array(predictions)\n    \n    def accuracy(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n\n\n# Test Random Forest\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RANDOM FOREST FROM SCRATCH - DEMO\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. Building Random Forest with 50 trees...\")\nrf = RandomForestScratch(n_trees=50, max_depth=10)\nrf.fit(X_tree, y_tree)\n\naccuracy_rf = rf.accuracy(X_tree, y_tree)\nprint(f\"\\n2. Random Forest Accuracy: {accuracy_rf:.4f}\")\nprint(f\"   Single Tree Accuracy: {accuracy:.4f}\")\nprint(f\"   Improvement: {((accuracy_rf - accuracy) / accuracy * 100):.1f}%\")\n\nprint(\"\\n‚úÖ Random Forest from scratch completed!\")\n```\n\n---\n\n# Part 2: Neural Networks from Scratch\n\n## 9. Understanding Neural Networks\n\n### The Biological Inspiration\n\n```python\nprint(\"UNDERSTANDING NEURAL NETWORKS\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nBIOLOGICAL NEURON vs ARTIFICIAL NEURON\n======================================\n\nBiological Neuron:\n- Dendrites: Receive signals from other neurons\n- Cell body: Processes signals\n- Axon: Sends output to other neurons\n- Synapse: Connection between neurons\n\nArtificial Neuron (Perceptron):\n- Inputs (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô): Like dendrites\n- Weights (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô): Strength of connections\n- Bias (b): Threshold for activation\n- Activation function: Decides output\n- Output (≈∑): Like axon signal\n\nMathematical Formula:\nz = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b\n≈∑ = activation(z)\n\"\"\")\n```\n\n### Building a Single Neuron\n\n```python\nclass Neuron:\n    \"\"\"\n    A single artificial neuron\n    \"\"\"\n    \n    def __init__(self, n_inputs, activation='sigmoid'):\n        # Initialize weights and bias randomly\n        self.weights = np.random.randn(n_inputs) * 0.01\n        self.bias = 0\n        self.activation = activation\n    \n    def activate(self, z):\n        \"\"\"Apply activation function\"\"\"\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-z))\n        elif self.activation == 'relu':\n            return np.maximum(0, z)\n        elif self.activation == 'tanh':\n            return np.tanh(z)\n        else:\n            return z  # linear\n    \n    def forward(self, inputs):\n        \"\"\"Forward pass\"\"\"\n        z = np.dot(inputs, self.weights) + self.bias\n        return self.activate(z)\n    \n    def __repr__(self):\n        return f\"Neuron(inputs={len(self.weights)}, activation={self.activation})\"\n\n\n# Demo single neuron\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SINGLE NEURON DEMO\")\nprint(\"=\" * 70)\n\nneuron = Neuron(n_inputs=3, activation='sigmoid')\nprint(f\"\\n{neuron}\")\nprint(f\"Weights: {neuron.weights}\")\nprint(f\"Bias: {neuron.bias}\")\n\n# Test forward pass\ninputs = np.array([1.0, 2.0, 3.0])\noutput = neuron.forward(inputs)\nprint(f\"\\nInput: {inputs}\")\nprint(f\"Output: {output:.4f}\")\n```\n\n---\n\n## 10. Building a Neural Network with NumPy\n\n### Complete Implementation\n\n```python\nclass NeuralNetworkScratch:\n    \"\"\"\n    Multi-layer Neural Network from scratch\n    Pure NumPy implementation!\n    \"\"\"\n    \n    def __init__(self, layer_sizes, learning_rate=0.01):\n        \"\"\"\n        Parameters:\n        -----------\n        layer_sizes : list\n            Number of neurons in each layer\n            Example: [2, 4, 3, 1] means:\n            - Input layer: 2 features\n            - Hidden layer 1: 4 neurons\n            - Hidden layer 2: 3 neurons\n            - Output layer: 1 neuron\n        \"\"\"\n        self.layer_sizes = layer_sizes\n        self.learning_rate = learning_rate\n        self.weights = []\n        self.biases = []\n        self.loss_history = []\n        \n        # Initialize weights and biases\n        for i in range(len(layer_sizes) - 1):\n            # Xavier initialization: helps with training\n            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n            b = np.zeros((1, layer_sizes[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation\"\"\"\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n    \n    def sigmoid_derivative(self, z):\n        \"\"\"Derivative of sigmoid\"\"\"\n        s = self.sigmoid(z)\n        return s * (1 - s)\n    \n    def relu(self, z):\n        \"\"\"ReLU activation\"\"\"\n        return np.maximum(0, z)\n    \n    def relu_derivative(self, z):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (z > 0).astype(float)\n    \n    def forward(self, X):\n        \"\"\"\n        Forward propagation\n        \n        Returns both activations and pre-activation values (z)\n        We need z values for backpropagation\n        \"\"\"\n        activations = [X]\n        z_values = []\n        \n        # Forward through each layer\n        for i in range(len(self.weights)):\n            # Compute z = w¬∑a + b\n            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n            z_values.append(z)\n            \n            # Apply activation function\n            if i < len(self.weights) - 1:  # Hidden layers: ReLU\n                a = self.relu(z)\n            else:  # Output layer: Sigmoid\n                a = self.sigmoid(z)\n            \n            activations.append(a)\n        \n        return activations, z_values\n    \n    def backward(self, X, y, activations, z_values):\n        \"\"\"\n        Backpropagation: Compute gradients\n        \n        This is where the magic happens!\n        We compute how much each weight contributed to the error\n        \"\"\"\n        m = X.shape[0]  # Number of samples\n        \n        # Initialize gradient storage\n        dW = [np.zeros_like(w) for w in self.weights]\n        db = [np.zeros_like(b) for b in self.biases]\n        \n        # Output layer error\n        # For binary cross-entropy with sigmoid:\n        # dL/da = -(y/a - (1-y)/(1-a))\n        # da/dz = a(1-a)\n        # dL/dz = dL/da * da/dz = a - y\n        delta = activations[-1] - y.reshape(-1, 1)\n        \n        # Backpropagate through layers\n        for i in reversed(range(len(self.weights))):\n            # Gradient for weights: dL/dW = a·µÄ ¬∑ delta\n            dW[i] = np.dot(activations[i].T, delta) / m\n            \n            # Gradient for biases: dL/db = sum(delta)\n            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n            \n            # Propagate error to previous layer\n            if i > 0:\n                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(z_values[i-1])\n        \n        return dW, db\n    \n    def update_parameters(self, dW, db):\n        \"\"\"\n        Update weights and biases using gradient descent\n        \"\"\"\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n    \n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Binary cross-entropy loss\n        L = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]\n        \"\"\"\n        m = y_true.shape[0]\n        epsilon = 1e-15  # Prevent log(0)\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n    \n    def train(self, X, y, epochs=1000, verbose=True):\n        \"\"\"\n        Train the neural network\n        \"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            activations, z_values = self.forward(X)\n            \n            # Compute loss\n            loss = self.compute_loss(y, activations[-1])\n            self.loss_history.append(loss)\n            \n            # Backward pass\n            dW, db = self.backward(X, y, activations, z_values)\n            \n            # Update parameters\n            self.update_parameters(dW, db)\n            \n            # Print progress\n            if verbose and epoch % 100 == 0:\n                accuracy = np.mean((activations[-1] > 0.5).astype(int) == y.reshape(-1, 1))\n                print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n    \n    def predict(self, X):\n        \"\"\"\n        Make predictions\n        \"\"\"\n        activations, _ = self.forward(X)\n        return (activations[-1] > 0.5).astype(int).flatten()\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Predict probabilities\n        \"\"\"\n        activations, _ = self.forward(X)\n        return activations[-1].flatten()\n\n\n# ============================================\n# TEST NEURAL NETWORK\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"NEURAL NETWORK FROM SCRATCH - DEMO\")\nprint(\"=\" * 70)\n\n# Generate XOR problem (not linearly separable!)\nprint(\"\\n1. Creating XOR dataset (classic non-linear problem)\")\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])\n\nprint(\"XOR Truth Table:\")\nprint(\"X‚ÇÅ  X‚ÇÇ  |  Y\")\nprint(\"-----------\")\nfor i in range(len(X_xor)):\n    print(f\" {X_xor[i, 0]}   {X_xor[i, 1]}  |  {y_xor[i]}\")\n\n# Create and train neural network\nprint(\"\\n2. Building Neural Network: [2, 4, 1]\")\nprint(\"   - Input: 2 features\")\nprint(\"   - Hidden: 4 neurons\")\nprint(\"   - Output: 1 neuron\")\n\nnn = NeuralNetworkScratch(layer_sizes=[2, 4, 1], learning_rate=0.5)\n\nprint(\"\\n3. Training for 2000 epochs...\")\nnn.train(X_xor, y_xor, epochs=2000, verbose=True)\n\n# Evaluate\nprint(\"\\n4. Final Predictions:\")\npredictions = nn.predict(X_xor)\nprobabilities = nn.predict_proba(X_xor)\n\nprint(\"X‚ÇÅ  X‚ÇÇ  | True  Pred  Prob\")\nprint(\"-----------------------------\")\nfor i in range(len(X_xor)):\n    print(f\" {X_xor[i, 0]}   {X_xor[i, 1]}  |  {y_xor[i]}     {predictions[i]}    {probabilities[i]:.3f}\")\n\naccuracy = np.mean(predictions == y_xor)\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Decision boundary\nx_min, x_max = -0.5, 1.5\ny_min, y_max = -0.5, 1.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                     np.linspace(y_min, y_max, 100))\nZ = nn.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[0].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\naxes[0].scatter(X_xor[:, 0], X_xor[:, 1], c=y_xor, cmap='RdYlBu', \n                s=200, edgecolors='black', linewidth=2)\naxes[0].set_xlabel('X‚ÇÅ')\naxes[0].set_ylabel('X‚ÇÇ')\naxes[0].set_title('Neural Network Decision Boundary\\n(Solved XOR!)')\naxes[0].grid(True)\n\n# Plot 2: Loss curve\naxes[1].plot(nn.loss_history)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Training Loss')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Neural Network from scratch completed!\")\nprint(\"\\nüí° KEY ACHIEVEMENT: Solved XOR problem!\")\nprint(\"   Linear models (Logistic Regression) CANNOT solve XOR\")\nprint(\"   But neural networks can! This shows the power of hidden layers.\")\n```\n\n---\n\n## 11. Activation Functions from Scratch\n\n```python\nclass ActivationFunctions:\n    \"\"\"\n    All activation functions with their derivatives\n    \"\"\"\n    \n    @staticmethod\n    def sigmoid(z):\n        \"\"\"\n        Sigmoid: œÉ(z) = 1 / (1 + e^(-z))\n        Range: (0, 1)\n        Use: Output layer for binary classification\n        \"\"\"\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n    \n    @staticmethod\n    def sigmoid_derivative(z):\n        s = ActivationFunctions.sigmoid(z)\n        return s * (1 - s)\n    \n    @staticmethod\n    def tanh(z):\n        \"\"\"\n        Tanh: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n        Range: (-1, 1)\n        Use: Hidden layers (zero-centered)\n        \"\"\"\n        return np.tanh(z)\n    \n    @staticmethod\n    def tanh_derivative(z):\n        return 1 - np.tanh(z) ** 2\n    \n    @staticmethod\n    def relu(z):\n        \"\"\"\n        ReLU: max(0, z)\n        Range: [0, ‚àû)\n        Use: Hidden layers (most popular!)\n        \"\"\"\n        return np.maximum(0, z)\n    \n    @staticmethod\n    def relu_derivative(z):\n        return (z > 0).astype(float)\n    \n    @staticmethod\n    def leaky_relu(z, alpha=0.01):\n        \"\"\"\n        Leaky ReLU: max(Œ±z, z)\n        Range: (-‚àû, ‚àû)\n        Use: Hidden layers (fixes \"dying ReLU\" problem)\n        \"\"\"\n        return np.where(z > 0, z, alpha * z)\n    \n    @staticmethod\n    def leaky_relu_derivative(z, alpha=0.01):\n        return np.where(z > 0, 1, alpha)\n    \n    @staticmethod\n    def elu(z, alpha=1.0):\n        \"\"\"\n        ELU: x if x > 0 else Œ±(e^x - 1)\n        Range: (-Œ±, ‚àû)\n        Use: Hidden layers (smooth, helps with vanishing gradient)\n        \"\"\"\n        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n    \n    @staticmethod\n    def elu_derivative(z, alpha=1.0):\n        return np.where(z > 0, 1, alpha * np.exp(z))\n    \n    @staticmethod\n    def softmax(z):\n        \"\"\"\n        Softmax: e^zi / Œ£e^zj\n        Use: Output layer for multi-class classification\n        Outputs: probabilities that sum to 1\n        \"\"\"\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n    \n    @staticmethod\n    def swish(z):\n        \"\"\"\n        Swish: x ¬∑ œÉ(x)\n        Use: Modern alternative to ReLU\n        \"\"\"\n        return z * ActivationFunctions.sigmoid(z)\n    \n    @staticmethod\n    def swish_derivative(z):\n        s = ActivationFunctions.sigmoid(z)\n        return s + z * s * (1 - s)\n\n\n# Visualize all activation functions\ndef visualize_activations():\n    \"\"\"\n    Compare all activation functions\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ACTIVATION FUNCTIONS COMPARISON\")\n    print(\"=\" * 70)\n    \n    z = np.linspace(-5, 5, 200)\n    \n    activations = {\n        'Sigmoid': (ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n        'Tanh': (ActivationFunctions.tanh, ActivationFunctions.tanh_derivative),\n        'ReLU': (ActivationFunctions.relu, ActivationFunctions.relu_derivative),\n        'Leaky ReLU': (ActivationFunctions.leaky_relu, ActivationFunctions.leaky_relu_derivative),\n        'ELU': (ActivationFunctions.elu, ActivationFunctions.elu_derivative),\n        'Swish': (ActivationFunctions.swish, ActivationFunctions.swish_derivative),\n    }\n    \n    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n    axes = axes.ravel()\n    \n    for idx, (name, (func, deriv)) in enumerate(activations.items()):\n        # Plot function\n        axes[idx*2].plot(z, func(z), linewidth=2)\n        axes[idx*2].set_title(f'{name} Function')\n        axes[idx*2].grid(True)\n        axes[idx*2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n        axes[idx*2].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n        \n        # Plot derivative\n        axes[idx*2+1].plot(z, deriv(z), linewidth=2, color='orange')\n        axes[idx*2+1].set_title(f'{name} Derivative')\n        axes[idx*2+1].grid(True)\n        axes[idx*2+1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n        axes[idx*2+1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä ACTIVATION FUNCTION GUIDE:\")\n    print(\"\\n1. SIGMOID:\")\n    print(\"   ‚úÖ Use: Output layer for binary classification\")\n    print(\"   ‚ùå Avoid: Hidden layers (vanishing gradient problem)\")\n    \n    print(\"\\n2. TANH:\")\n    print(\"   ‚úÖ Use: Hidden layers (better than sigmoid)\")\n    print(\"   ‚ûï Pro: Zero-centered output\")\n    print(\"   ‚ûñ Con: Still has vanishing gradient\")\n    \n    print(\"\\n3. ReLU (Most Popular!):\")\n    print(\"   ‚úÖ Use: Hidden layers in deep networks\")\n    print(\"   ‚ûï Pro: No vanishing gradient, computationally efficient\")\n    print(\"   ‚ûñ Con: Dying ReLU problem (neurons can die)\")\n    \n    print(\"\\n4. LEAKY ReLU:\")\n    print(\"   ‚úÖ Use: When ReLU neurons are dying\")\n    print(\"   ‚ûï Pro: Fixes dying ReLU problem\")\n    \n    print(\"\\n5. ELU:\")\n    print(\"   ‚úÖ Use: When you want smooth activation\")\n    print(\"   ‚ûï Pro: Smooth, helps with vanishing gradient\")\n    print(\"   ‚ûñ Con: Computationally expensive (exp)\")\n    \n    print(\"\\n6. SWISH:\")\n    print(\"   ‚úÖ Use: Modern alternative to ReLU\")\n    print(\"   ‚ûï Pro: Often outperforms ReLU in practice\")\n    \n    print(\"\\n7. SOFTMAX:\")\n    print(\"   ‚úÖ Use: Output layer for multi-class classification\")\n    print(\"   ‚ûï Pro: Outputs probabilities that sum to 1\")\n\nvisualize_activations()\n```\n\n---\n\n## 12. Loss Functions from Scratch\n\n```python\nclass LossFunctions:\n    \"\"\"\n    All loss functions with their derivatives\n    \"\"\"\n    \n    @staticmethod\n    def mse(y_true, y_pred):\n        \"\"\"\n        Mean Squared Error\n        L = (1/n) Œ£(y - ≈∑)¬≤\n        Use: Regression problems\n        \"\"\"\n        return np.mean((y_true - y_pred) ** 2)\n    \n    @staticmethod\n    def mse_derivative(y_true, y_pred):\n        return 2 * (y_pred - y_true) / y_true.size\n    \n    @staticmethod\n    def mae(y_true, y_pred):\n        \"\"\"\n        Mean Absolute Error\n        L = (1/n) Œ£|y - ≈∑|\n        Use: Regression with outliers\n        \"\"\"\n        return np.mean(np.abs(y_true - y_pred))\n    \n    @staticmethod\n    def mae_derivative(y_true, y_pred):\n        return np.sign(y_pred - y_true) / y_true.size\n    \n    @staticmethod\n    def binary_crossentropy(y_true, y_pred, epsilon=1e-15):\n        \"\"\"\n        Binary Cross-Entropy\n        L = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]\n        Use: Binary classification\n        \"\"\"\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    @staticmethod\n    def binary_crossentropy_derivative(y_true, y_pred, epsilon=1e-15):\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / y_true.size\n    \n    @staticmethod\n    def categorical_crossentropy(y_true, y_pred, epsilon=1e-15):\n        \"\"\"\n        Categorical Cross-Entropy\n        L = -Œ£(y ¬∑ log(≈∑))\n        Use: Multi-class classification\n        \"\"\"\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n    \n    @staticmethod\n    def categorical_crossentropy_derivative(y_true, y_pred, epsilon=1e-15):\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -(y_true / y_pred) / y_true.shape[0]\n    \n    @staticmethod\n    def huber_loss(y_true, y_pred, delta=1.0):\n        \"\"\"\n        Huber Loss\n        Combines MSE and MAE\n        Use: Regression robust to outliers\n        \"\"\"\n        error = y_true - y_pred\n        is_small_error = np.abs(error) <= delta\n        squared_loss = 0.5 * error ** 2\n        linear_loss = delta * (np.abs(error) - 0.5 * delta)\n        return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n    \n    @staticmethod\n    def hinge_loss(y_true, y_pred):\n        \"\"\"\n        Hinge Loss\n        L = max(0, 1 - y¬∑≈∑)\n        Use: SVM, margin-based classification\n        \"\"\"\n        return np.mean(np.maximum(0, 1 - y_true * y_pred))\n\n\n# Visualize loss functions\ndef visualize_losses():\n    \"\"\"\n    Compare different loss functions\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"LOSS FUNCTIONS COMPARISON\")\n    print(\"=\" * 70)\n    \n    # For regression\n    y_true = 0\n    y_pred_range = np.linspace(-3, 3, 200)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Plot 1: MSE\n    mse_losses = [(y_true - y_pred) ** 2 for y_pred in y_pred_range]\n    axes[0, 0].plot(y_pred_range, mse_losses, label='MSE', linewidth=2)\n    axes[0, 0].set_title('Mean Squared Error (MSE)')\n    axes[0, 0].set_xlabel('Prediction Error')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].grid(True)\n    axes[0, 0].axvline(x=0, color='r', linestyle='--', label='True value')\n    axes[0, 0].legend()\n    \n    # Plot 2: MAE\n    mae_losses = [np.abs(y_true - y_pred) for y_pred in y_pred_range]\n    axes[0, 1].plot(y_pred_range, mae_losses, label='MAE', linewidth=2, color='orange')\n    axes[0, 1].set_title('Mean Absolute Error (MAE)')\n    axes[0, 1].set_xlabel('Prediction Error')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].grid(True)\n    axes[0, 1].axvline(x=0, color='r', linestyle='--', label='True value')\n    axes[0, 1].legend()\n    \n    # Plot 3: MSE vs MAE comparison\n    axes[1, 0].plot(y_pred_range, mse_losses, label='MSE', linewidth=2)\n    axes[1, 0].plot(y_pred_range, mae_losses, label='MAE', linewidth=2)\n    axes[1, 0].set_title('MSE vs MAE')\n    axes[1, 0].set_xlabel('Prediction Error')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].grid(True)\n    axes[1, 0].axvline(x=0, color='r', linestyle='--')\n    axes[1, 0].legend()\n    \n    # Plot 4: Binary Cross-Entropy\n    y_pred_prob = np.linspace(0.01, 0.99, 200)\n    bce_y1 = -np.log(y_pred_prob)  # When y_true = 1\n    bce_y0 = -np.log(1 - y_pred_prob)  # When y_true = 0\n    \n    axes[1, 1].plot(y_pred_prob, bce_y1, label='y_true = 1', linewidth=2)\n    axes[1, 1].plot(y_pred_prob, bce_y0, label='y_true = 0', linewidth=2)\n    axes[1, 1].set_title('Binary Cross-Entropy')\n    axes[1, 1].set_xlabel('Predicted Probability')\n    axes[1, 1].set_ylabel('Loss')\n    axes[1, 1].grid(True)\n    axes[1, 1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä LOSS FUNCTION GUIDE:\")\n    \n    print(\"\\n1. MEAN SQUARED ERROR (MSE):\")\n    print(\"   ‚úÖ Use: Regression problems\")\n    print(\"   ‚ûï Pro: Smooth gradient, easy to optimize\")\n    print(\"   ‚ûñ Con: Sensitive to outliers (squares errors)\")\n    \n    print(\"\\n2. MEAN ABSOLUTE ERROR (MAE):\")\n    print(\"   ‚úÖ Use: Regression with outliers\")\n    print(\"   ‚ûï Pro: Robust to outliers\")\n    print(\"   ‚ûñ Con: Not smooth at zero (gradient issues)\")\n    \n    print(\"\\n3. BINARY CROSS-ENTROPY:\")\n    print(\"   ‚úÖ Use: Binary classification\")\n    print(\"   ‚ûï Pro: Natural choice for probabilities\")\n    print(\"   ‚ûñ Con: Only for binary problems\")\n    \n    print(\"\\n4. CATEGORICAL CROSS-ENTROPY:\")\n    print(\"   ‚úÖ Use: Multi-class classification\")\n    print(\"   ‚ûï Pro: Works with softmax output\")\n    \n    print(\"\\n5. HUBER LOSS:\")\n    print(\"   ‚úÖ Use: Regression with some outliers\")\n    print(\"   ‚ûï Pro: Combines MSE and MAE (best of both)\")\n    \n    print(\"\\n6. HINGE LOSS:\")\n    print(\"   ‚úÖ Use: SVM, margin-based classification\")\n    print(\"   ‚ûï Pro: Maximizes margin between classes\")\n\nvisualize_losses()\n```\n\n---\n\n## 14. Optimizers from Scratch\n\n```python\nclass Optimizers:\n    \"\"\"\n    All optimization algorithms from scratch\n    \"\"\"\n    \n    class SGD:\n        \"\"\"\n        Stochastic Gradient Descent\n        Simplest optimizer\n        \"\"\"\n        def __init__(self, learning_rate=0.01):\n            self.lr = learning_rate\n        \n        def update(self, params, grads):\n            \"\"\"Update parameters\"\"\"\n            for param, grad in zip(params, grads):\n                param -= self.lr * grad\n    \n    class SGDMomentum:\n        \"\"\"\n        SGD with Momentum\n        Accelerates in relevant direction\n        \"\"\"\n        def __init__(self, learning_rate=0.01, momentum=0.9):\n            self.lr = learning_rate\n            self.momentum = momentum\n            self.velocity = None\n        \n        def update(self, params, grads):\n            if self.velocity is None:\n                self.velocity = [np.zeros_like(p) for p in params]\n            \n            for i, (param, grad) in enumerate(zip(params, grads)):\n                # v = Œ≤¬∑v - Œ±¬∑‚àáL\n                self.velocity[i] = self.momentum * self.velocity[i] - self.lr * grad\n                # Œ∏ = Œ∏ + v\n                param += self.velocity[i]\n    \n    class AdaGrad:\n        \"\"\"\n        Adaptive Gradient\n        Adapts learning rate per parameter\n        \"\"\"\n        def __init__(self, learning_rate=0.01, epsilon=1e-8):\n            self.lr = learning_rate\n            self.epsilon = epsilon\n            self.accumulated_grad = None\n        \n        def update(self, params, grads):\n            if self.accumulated_grad is None:\n                self.accumulated_grad = [np.zeros_like(p) for p in params]\n            \n            for i, (param, grad) in enumerate(zip(params, grads)):\n                # Accumulate squared gradients\n                self.accumulated_grad[i] += grad ** 2\n                # Update with adaptive learning rate\n                param -= self.lr * grad / (np.sqrt(self.accumulated_grad[i]) + self.epsilon)\n    \n    class RMSprop:\n        \"\"\"\n        Root Mean Square Propagation\n        Fixes AdaGrad's aggressive learning rate decay\n        \"\"\"\n        def __init__(self, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8):\n            self.lr = learning_rate\n            self.decay_rate = decay_rate\n            self.epsilon = epsilon\n            self.cache = None\n        \n        def update(self, params, grads):\n            if self.cache is None:\n                self.cache = [np.zeros_like(p) for p in params]\n            \n            for i, (param, grad) in enumerate(zip(params, grads)):\n                # Moving average of squared gradients\n                self.cache[i] = self.decay_rate * self.cache[i] + (1 - self.decay_rate) * grad ** 2\n                # Update\n                param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n    \n    class Adam:\n        \"\"\"\n        Adaptive Moment Estimation\n        Combines momentum and RMSprop\n        Most popular optimizer!\n        \"\"\"\n        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n            self.lr = learning_rate\n            self.beta1 = beta1  # Decay rate for first moment\n            self.beta2 = beta2  # Decay rate for second moment\n            self.epsilon = epsilon\n            self.m = None  # First moment (mean)\n            self.v = None  # Second moment (variance)\n            self.t = 0     # Time step\n        \n        def update(self, params, grads):\n            if self.m is None:\n                self.m = [np.zeros_like(p) for p in params]\n                self.v = [np.zeros_like(p) for p in params]\n            \n            self.t += 1\n            \n            for i, (param, grad) in enumerate(zip(params, grads)):\n                # Update first moment (momentum)\n                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n                \n                # Update second moment (RMSprop)\n                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n                \n                # Bias correction\n                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n                \n                # Update parameters\n                param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n\n\n# Compare optimizers\ndef compare_optimizers():\n    \"\"\"\n    Visually compare different optimizers\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"OPTIMIZERS COMPARISON\")\n    print(\"=\" * 70)\n    \n    # Simple 2D optimization problem\n    def objective(x, y):\n        \"\"\"Rosenbrock function - challenging to optimize\"\"\"\n        return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n    \n    def gradient(x, y):\n        \"\"\"Gradient of Rosenbrock function\"\"\"\n        dx = -2 * (1 - x) - 400 * x * (y - x ** 2)\n        dy = 200 * (y - x ** 2)\n        return np.array([dx, dy])\n    \n    # Initialize different optimizers\n    optimizers = {\n        'SGD': Optimizers.SGD(learning_rate=0.001),\n        'SGD Momentum': Optimizers.SGDMomentum(learning_rate=0.001, momentum=0.9),\n        'AdaGrad': Optimizers.AdaGrad(learning_rate=0.1),\n        'RMSprop': Optimizers.RMSprop(learning_rate=0.01),\n        'Adam': Optimizers.Adam(learning_rate=0.01)\n    }\n    \n    # Run optimization\n    n_steps = 200\n    paths = {}\n    \n    for name, opt in optimizers.items():\n        position = np.array([-1.0, 2.0])  # Starting point\n        path = [position.copy()]\n        \n        for _ in range(n_steps):\n            grad = gradient(position[0], position[1])\n            opt.update([position], [grad])\n            path.append(position.copy())\n        \n        paths[name] = np.array(path)\n        print(f\"{name}: Final position = ({position[0]:.3f}, {position[1]:.3f})\")\n    \n    # Visualize\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    # Create contour plot\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-1, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = objective(X, Y)\n    \n    for idx, (name, path) in enumerate(paths.items()):\n        axes[idx].contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis')\n        axes[idx].plot(path[:, 0], path[:, 1], 'r.-', alpha=0.7, markersize=3)\n        axes[idx].plot(1, 1, 'g*', markersize=20, label='Optimum')\n        axes[idx].plot(path[0, 0], path[0, 1], 'ro', markersize=10, label='Start')\n        axes[idx].set_title(f'{name}')\n        axes[idx].set_xlabel('x')\n        axes[idx].set_ylabel('y')\n        axes[idx].legend()\n        axes[idx].grid(True, alpha=0.3)\n    \n    # Hide extra subplot\n    axes[5].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä OPTIMIZER GUIDE:\")\n    \n    print(\"\\n1. SGD (Stochastic Gradient Descent):\")\n    print(\"   ‚úÖ Use: Simple problems, well-tuned learning rate\")\n    print(\"   ‚ûï Pro: Simple, memory efficient\")\n    print(\"   ‚ûñ Con: Slow, can get stuck in local minima\")\n    \n    print(\"\\n2. SGD WITH MOMENTUM:\")\n    print(\"   ‚úÖ Use: When SGD oscillates or is slow\")\n    print(\"   ‚ûï Pro: Faster convergence, dampens oscillations\")\n    print(\"   ‚ûñ Con: One more hyperparameter (momentum)\")\n    \n    print(\"\\n3. ADAGRAD:\")\n    print(\"   ‚úÖ Use: Sparse data, NLP tasks\")\n    print(\"   ‚ûï Pro: No manual learning rate tuning\")\n    print(\"   ‚ûñ Con: Learning rate decays too aggressively\")\n    \n    print(\"\\n4. RMSPROP:\")\n    print(\"   ‚úÖ Use: RNNs, non-stationary problems\")\n    print(\"   ‚ûï Pro: Fixes AdaGrad's decay problem\")\n    print(\"   ‚ûñ Con: Still sensitive to learning rate\")\n    \n    print(\"\\n5. ADAM (Most Popular!):\")\n    print(\"   ‚úÖ Use: Default choice for most problems!\")\n    print(\"   ‚ûï Pro: Combines momentum + RMSprop\")\n    print(\"   ‚ûï Pro: Works well out-of-the-box\")\n    print(\"   ‚ûï Pro: Little hyperparameter tuning needed\")\n    print(\"   ‚ûñ Con: Can sometimes generalize worse than SGD\")\n    \n    print(\"\\nüí° RECOMMENDATION:\")\n    print(\"   - Start with Adam (lr=0.001)\")\n    print(\"   - If overfitting: Try SGD with momentum\")\n    print(\"   - If RNNs: Try RMSprop\")\n\ncompare_optimizers()\n```\n\n---\n\n## Summary: What You've Built from Scratch\n\n```python\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üéâ CONGRATULATIONS! YOU'VE BUILT FROM SCRATCH:\")\nprint(\"=\" * 70)\n\nachievements = {\n    \"Traditional ML\": [\n        \"‚úÖ Linear Regression\",\n        \"‚úÖ Logistic Regression\", \n        \"‚úÖ Decision Trees\",\n        \"‚úÖ Random Forests\"\n    ],\n    \"Neural Networks\": [\n        \"‚úÖ Multi-layer Perceptron\",\n        \"‚úÖ Backpropagation\",\n        \"‚úÖ 6 Activation Functions\",\n        \"‚úÖ 6 Loss Functions\",\n        \"‚úÖ 5 Optimizers\"\n    ],\n    \"Key Concepts\": [\n        \"‚úÖ Gradient Descent\",\n        \"‚úÖ Forward Propagation\",\n        \"‚úÖ Backward Propagation\",\n        \"‚úÖ Weight Initialization\",\n        \"‚úÖ Regularization\"\n    ]\n}\n\nfor category, items in achievements.items():\n    print(f\"\\n{category}:\")\n    for item in items:\n        print(f\"  {item}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"NEXT STEPS:\")\nprint(\"=\" * 70)\nprint(\"\"\"\n1. Implement CNNs from scratch for image recognition\n2. Build RNNs/LSTMs for sequence data\n3. Create Transformers for NLP tasks\n4. Add regularization techniques (L1, L2, Dropout)\n5. Implement batch normalization\n6. Build custom model architectures\n7. Optimize for production deployment\n\nKeep practicing and building! üöÄ\n\"\"\")\n```\n\n---\n\n## Quick Reference Guide\n\n### When to Use Which Algorithm?\n\n| Problem Type | Algorithm | Why? |\n|--------------|-----------|------|\n| Linear Regression | Simple relationship | Fast, interpretable |\n| Logistic Regression | Binary classification | Probabilistic output |\n| Decision Trees | Non-linear, interpretable | Visual, handles non-linearity |\n| Random Forest | Complex, prevent overfit | Ensemble reduces variance |\n| Neural Networks | Complex patterns | Universal function approximator |\n| Deep Learning | Images, text, audio | Learns hierarchical features |\n\n### Activation Function Choice\n\n| Layer | Activation | Reason |\n|-------|------------|--------|\n| Hidden (default) | ReLU | Fast, no vanishing gradient |\n| Hidden (alternative) | Leaky ReLU | Fixes dying ReLU |\n| Output (regression) | Linear | Unbounded output |\n| Output (binary) | Sigmoid | Probability (0-1) |\n| Output (multi-class) | Softmax | Probabilities sum to 1 |\n\n### Optimizer Choice\n\n| Scenario | Optimizer | Learning Rate |\n|----------|-----------|---------------|\n| Default choice | Adam | 0.001 |\n| Fine-tuning | SGD + Momentum | 0.01 |\n| RNNs | RMSprop | 0.001 |\n| Sparse data | AdaGrad | 0.01 |\n\n---\n\n# Part 3: Deep Learning Architectures\n\n## 15. Convolutional Neural Networks (CNN) from Scratch\n\n### Understanding Convolution\n\n**Simple Explanation:** Instead of looking at the entire image, look at small patches and detect patterns (edges, corners, textures).\n\n**Mathematical Operation:**\n```\nOutput(i,j) = Œ£ Œ£ Input(i+m, j+n) √ó Kernel(m, n)\n```\n\n### CNN Layers from Scratch\n\n```python\nclass Conv2D:\n    \"\"\"\n    2D Convolution Layer from scratch\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n        \"\"\"\n        Parameters:\n        -----------\n        in_channels : int\n            Number of input channels (e.g., 3 for RGB)\n        out_channels : int\n            Number of filters/kernels\n        kernel_size : int\n            Size of the convolution kernel (kernel_size √ó kernel_size)\n        stride : int\n            Step size for moving the kernel\n        padding : int\n            Zero-padding around the input\n        \"\"\"\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        \n        # Initialize kernels (filters) with He initialization\n        self.kernels = np.random.randn(\n            out_channels, in_channels, kernel_size, kernel_size\n        ) * np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n        \n        self.bias = np.zeros(out_channels)\n    \n    def forward(self, input_data):\n        \"\"\"\n        Forward pass through convolution layer\n        \n        Parameters:\n        -----------\n        input_data : numpy array of shape (batch_size, in_channels, height, width)\n        \n        Returns:\n        --------\n        output : numpy array of shape (batch_size, out_channels, out_height, out_width)\n        \"\"\"\n        self.input = input_data\n        batch_size, in_channels, in_height, in_width = input_data.shape\n        \n        # Add padding\n        if self.padding > 0:\n            input_padded = np.pad(\n                input_data,\n                ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n                mode='constant'\n            )\n        else:\n            input_padded = input_data\n        \n        # Calculate output dimensions\n        out_height = (in_height + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_width = (in_width + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        # Initialize output\n        output = np.zeros((batch_size, self.out_channels, out_height, out_width))\n        \n        # Perform convolution\n        for b in range(batch_size):\n            for f in range(self.out_channels):\n                for i in range(0, out_height):\n                    for j in range(0, out_width):\n                        # Extract the region\n                        h_start = i * self.stride\n                        h_end = h_start + self.kernel_size\n                        w_start = j * self.stride\n                        w_end = w_start + self.kernel_size\n                        \n                        region = input_padded[b, :, h_start:h_end, w_start:w_end]\n                        \n                        # Convolution: element-wise multiply and sum\n                        output[b, f, i, j] = np.sum(region * self.kernels[f]) + self.bias[f]\n        \n        return output\n    \n    def __repr__(self):\n        return f\"Conv2D(in={self.in_channels}, out={self.out_channels}, \" \\\n               f\"kernel={self.kernel_size}, stride={self.stride}, padding={self.padding})\"\n\n\nclass MaxPool2D:\n    \"\"\"\n    Max Pooling Layer from scratch\n    \"\"\"\n    \n    def __init__(self, pool_size=2, stride=2):\n        self.pool_size = pool_size\n        self.stride = stride\n    \n    def forward(self, input_data):\n        \"\"\"\n        Forward pass through max pooling\n        \n        Reduces spatial dimensions by taking maximum value in each pool\n        \"\"\"\n        self.input = input_data\n        batch_size, channels, in_height, in_width = input_data.shape\n        \n        # Calculate output dimensions\n        out_height = (in_height - self.pool_size) // self.stride + 1\n        out_width = (in_width - self.pool_size) // self.stride + 1\n        \n        # Initialize output\n        output = np.zeros((batch_size, channels, out_height, out_width))\n        \n        # Perform max pooling\n        for b in range(batch_size):\n            for c in range(channels):\n                for i in range(out_height):\n                    for j in range(out_width):\n                        h_start = i * self.stride\n                        h_end = h_start + self.pool_size\n                        w_start = j * self.stride\n                        w_end = w_start + self.pool_size\n                        \n                        # Take maximum\n                        region = input_data[b, c, h_start:h_end, w_start:w_end]\n                        output[b, c, i, j] = np.max(region)\n        \n        return output\n    \n    def __repr__(self):\n        return f\"MaxPool2D(pool_size={self.pool_size}, stride={self.stride})\"\n\n\nclass Flatten:\n    \"\"\"\n    Flatten multi-dimensional input into 1D\n    \"\"\"\n    \n    def forward(self, input_data):\n        self.input_shape = input_data.shape\n        batch_size = input_data.shape[0]\n        return input_data.reshape(batch_size, -1)\n    \n    def __repr__(self):\n        return \"Flatten()\"\n\n\n# ============================================\n# DEMO: Understanding Convolution\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"UNDERSTANDING CONVOLUTION OPERATION\")\nprint(\"=\" * 70)\n\n# Create a simple image (5x5) with an edge\nimage = np.array([\n    [0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 1]\n]).astype(float)\n\n# Edge detection kernel (vertical edge)\nkernel_vertical = np.array([\n    [-1, 0, 1],\n    [-1, 0, 1],\n    [-1, 0, 1]\n])\n\n# Edge detection kernel (horizontal edge)\nkernel_horizontal = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\ndef apply_convolution(image, kernel):\n    \"\"\"Apply convolution manually to understand the operation\"\"\"\n    k_height, k_width = kernel.shape\n    i_height, i_width = image.shape\n    \n    output_height = i_height - k_height + 1\n    output_width = i_width - k_width + 1\n    \n    output = np.zeros((output_height, output_width))\n    \n    for i in range(output_height):\n        for j in range(output_width):\n            region = image[i:i+k_height, j:j+k_width]\n            output[i, j] = np.sum(region * kernel)\n    \n    return output\n\n# Apply kernels\nvertical_edges = apply_convolution(image, kernel_vertical)\nhorizontal_edges = apply_convolution(image, kernel_horizontal)\n\nprint(\"\\n1. Original Image (5√ó5):\")\nprint(image.astype(int))\n\nprint(\"\\n2. Vertical Edge Detection Kernel:\")\nprint(kernel_vertical)\n\nprint(\"\\n3. Output after Vertical Edge Detection:\")\nprint(vertical_edges)\nprint(\"   ‚Üí Strong response (positive values) at the vertical edge!\")\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\naxes[0, 0].imshow(image, cmap='gray')\naxes[0, 0].set_title('Original Image')\naxes[0, 0].axis('off')\n\naxes[0, 1].imshow(kernel_vertical, cmap='RdBu')\naxes[0, 1].set_title('Vertical Edge Kernel')\naxes[0, 1].axis('off')\n\naxes[0, 2].imshow(vertical_edges, cmap='RdBu')\naxes[0, 2].set_title('Vertical Edges Detected')\naxes[0, 2].axis('off')\n\naxes[1, 0].imshow(image, cmap='gray')\naxes[1, 0].set_title('Original Image')\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(kernel_horizontal, cmap='RdBu')\naxes[1, 1].set_title('Horizontal Edge Kernel')\naxes[1, 1].axis('off')\n\naxes[1, 2].imshow(horizontal_edges, cmap='RdBu')\naxes[1, 2].set_title('Horizontal Edges Detected')\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° KEY INSIGHTS:\")\nprint(\"   - Convolution detects patterns in images\")\nprint(\"   - Different kernels detect different features\")\nprint(\"   - Neural networks LEARN these kernels automatically!\")\n```\n\n### Complete CNN Architecture: LeNet-5\n\n```python\nclass SimpleCNN:\n    \"\"\"\n    Simple CNN for digit recognition (like LeNet-5)\n    Architecture: Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí FC ‚Üí FC\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize layers for MNIST-like images (28√ó28)\n        \"\"\"\n        # Layer 1: Convolution\n        self.conv1 = Conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n        self.pool1 = MaxPool2D(pool_size=2, stride=2)\n        \n        # Layer 2: Convolution\n        self.conv2 = Conv2D(in_channels=6, out_channels=16, kernel_size=5)\n        self.pool2 = MaxPool2D(pool_size=2, stride=2)\n        \n        # Flatten\n        self.flatten = Flatten()\n        \n        # Fully connected layers\n        self.fc1_weights = np.random.randn(16 * 5 * 5, 120) * 0.01\n        self.fc1_bias = np.zeros(120)\n        \n        self.fc2_weights = np.random.randn(120, 84) * 0.01\n        self.fc2_bias = np.zeros(84)\n        \n        self.fc3_weights = np.random.randn(84, 10) * 0.01\n        self.fc3_bias = np.zeros(10)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the network\n        \n        Parameters:\n        -----------\n        x : numpy array of shape (batch_size, 1, 28, 28)\n            Input images\n        \n        Returns:\n        --------\n        output : numpy array of shape (batch_size, 10)\n            Class probabilities\n        \"\"\"\n        # Conv1 ‚Üí ReLU ‚Üí Pool\n        x = self.conv1.forward(x)\n        x = self.relu(x)\n        x = self.pool1.forward(x)\n        \n        # Conv2 ‚Üí ReLU ‚Üí Pool\n        x = self.conv2.forward(x)\n        x = self.relu(x)\n        x = self.pool2.forward(x)\n        \n        # Flatten\n        x = self.flatten.forward(x)\n        \n        # FC1 ‚Üí ReLU\n        x = np.dot(x, self.fc1_weights) + self.fc1_bias\n        x = self.relu(x)\n        \n        # FC2 ‚Üí ReLU\n        x = np.dot(x, self.fc2_weights) + self.fc2_bias\n        x = self.relu(x)\n        \n        # FC3 ‚Üí Softmax\n        x = np.dot(x, self.fc3_weights) + self.fc3_bias\n        x = self.softmax(x)\n        \n        return x\n    \n    def predict(self, x):\n        \"\"\"Get predicted class\"\"\"\n        probs = self.forward(x)\n        return np.argmax(probs, axis=1)\n\n\n# ============================================\n# TEST CNN ON MNIST-LIKE DATA\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SIMPLE CNN ARCHITECTURE DEMO\")\nprint(\"=\" * 70)\n\n# Create CNN\ncnn = SimpleCNN()\n\nprint(\"\\nüèóÔ∏è CNN Architecture:\")\nprint(\"=\" * 50)\nprint(f\"1. {cnn.conv1}\")\nprint(f\"2. ReLU\")\nprint(f\"3. {cnn.pool1}\")\nprint(f\"4. {cnn.conv2}\")\nprint(f\"5. ReLU\")\nprint(f\"6. {cnn.pool2}\")\nprint(f\"7. {cnn.flatten}\")\nprint(f\"8. Fully Connected: 400 ‚Üí 120\")\nprint(f\"9. ReLU\")\nprint(f\"10. Fully Connected: 120 ‚Üí 84\")\nprint(f\"11. ReLU\")\nprint(f\"12. Fully Connected: 84 ‚Üí 10\")\nprint(f\"13. Softmax\")\n\n# Create dummy input (batch of 2 images)\ndummy_input = np.random.randn(2, 1, 28, 28)\n\nprint(f\"\\nüìä Forward Pass:\")\nprint(f\"Input shape: {dummy_input.shape}\")\n\n# Forward pass\noutput = cnn.forward(dummy_input)\n\nprint(f\"Output shape: {output.shape}\")\nprint(f\"\\nPredicted classes: {cnn.predict(dummy_input)}\")\nprint(f\"\\nClass probabilities (first sample):\")\nfor i, prob in enumerate(output[0]):\n    print(f\"  Class {i}: {prob:.4f}\")\n\nprint(\"\\n‚úÖ CNN architecture working!\")\n```\n\n### Real-World Example: Image Classification\n\n```python\ndef create_simple_digit_images():\n    \"\"\"\n    Create simple synthetic digit-like images for demonstration\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CNN IMAGE CLASSIFICATION DEMO\")\n    print(\"=\" * 70)\n    \n    # Create synthetic \"digits\"\n    images = []\n    labels = []\n    \n    # \"0\" - circle\n    img_0 = np.zeros((28, 28))\n    for i in range(28):\n        for j in range(28):\n            if 8 <= np.sqrt((i-14)**2 + (j-14)**2) <= 12:\n                img_0[i, j] = 1\n    \n    # \"1\" - vertical line\n    img_1 = np.zeros((28, 28))\n    img_1[:, 13:15] = 1\n    \n    # \"7\" - diagonal line\n    img_7 = np.zeros((28, 28))\n    for i in range(28):\n        j = int(i * 0.8 + 7)\n        if 0 <= j < 28:\n            img_7[i, j:j+2] = 1\n    \n    # Create dataset with some variation\n    for _ in range(10):\n        # Add noise\n        images.append(img_0 + np.random.randn(28, 28) * 0.1)\n        labels.append(0)\n        \n        images.append(img_1 + np.random.randn(28, 28) * 0.1)\n        labels.append(1)\n        \n        images.append(img_7 + np.random.randn(28, 28) * 0.1)\n        labels.append(7)\n    \n    images = np.array(images)\n    labels = np.array(labels)\n    \n    # Reshape for CNN (batch, channels, height, width)\n    images = images.reshape(-1, 1, 28, 28)\n    \n    # Visualize samples\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    axes[0].imshow(img_0, cmap='gray')\n    axes[0].set_title('Synthetic \"0\"')\n    axes[0].axis('off')\n    \n    axes[1].imshow(img_1, cmap='gray')\n    axes[1].set_title('Synthetic \"1\"')\n    axes[1].axis('off')\n    \n    axes[2].imshow(img_7, cmap='gray')\n    axes[2].set_title('Synthetic \"7\"')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n‚úÖ Created dataset:\")\n    print(f\"   Images: {images.shape}\")\n    print(f\"   Labels: {labels.shape}\")\n    print(f\"   Classes: {np.unique(labels)}\")\n    \n    return images, labels\n\n# Create and test\nimages, labels = create_simple_digit_images()\n\n# Make predictions\ncnn = SimpleCNN()\npredictions = cnn.predict(images[:5])\n\nprint(f\"\\nüîÆ Predictions (random weights, not trained):\")\nfor i in range(5):\n    print(f\"   Image {i}: True={labels[i]}, Predicted={predictions[i]}\")\n\nprint(\"\\nüí° NOTE: CNN would need training to make accurate predictions!\")\nprint(\"   This demo shows the architecture and forward pass.\")\n```\n\n---\n\n## 16. Recurrent Neural Networks (RNN) from Scratch\n\n### Understanding RNNs\n\n**Simple Explanation:** Process sequences one step at a time, maintaining a \"memory\" of what it has seen before.\n\n**Key Concepts:**\n- **Hidden State**: Memory that persists across time steps\n- **Temporal Dependencies**: Understanding context from previous inputs\n- **Sequence Processing**: Handle variable-length inputs\n\n### Implementation\n\n```python\nclass VanillaRNN:\n    \"\"\"\n    Vanilla RNN from scratch\n    For sequence processing tasks\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Parameters:\n        -----------\n        input_size : int\n            Dimension of input at each time step\n        hidden_size : int\n            Dimension of hidden state\n        output_size : int\n            Dimension of output\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Initialize weights (Xavier initialization)\n        scale_i = np.sqrt(2.0 / (input_size + hidden_size))\n        scale_h = np.sqrt(2.0 / (hidden_size + hidden_size))\n        scale_o = np.sqrt(2.0 / (hidden_size + output_size))\n        \n        # Input to hidden\n        self.W_ih = np.random.randn(input_size, hidden_size) * scale_i\n        \n        # Hidden to hidden (recurrent weights)\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale_h\n        \n        # Hidden to output\n        self.W_ho = np.random.randn(hidden_size, output_size) * scale_o\n        \n        # Biases\n        self.b_h = np.zeros((1, hidden_size))\n        self.b_o = np.zeros((1, output_size))\n    \n    def tanh(self, x):\n        \"\"\"Tanh activation\"\"\"\n        return np.tanh(x)\n    \n    def softmax(self, x):\n        \"\"\"Softmax activation\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n    \n    def forward(self, inputs, h_prev=None):\n        \"\"\"\n        Forward pass through RNN\n        \n        Parameters:\n        -----------\n        inputs : numpy array of shape (batch_size, seq_length, input_size)\n        h_prev : numpy array of shape (batch_size, hidden_size)\n            Previous hidden state (if None, initialize with zeros)\n        \n        Returns:\n        --------\n        outputs : list of output at each time step\n        hidden_states : list of hidden states at each time step\n        \"\"\"\n        batch_size, seq_length, _ = inputs.shape\n        \n        # Initialize hidden state if not provided\n        if h_prev is None:\n            h_prev = np.zeros((batch_size, self.hidden_size))\n        \n        outputs = []\n        hidden_states = [h_prev]\n        \n        # Process sequence step by step\n        for t in range(seq_length):\n            # Get input at current time step\n            x_t = inputs[:, t, :]  # Shape: (batch_size, input_size)\n            \n            # RNN formula:\n            # h_t = tanh(x_t @ W_ih + h_{t-1} @ W_hh + b_h)\n            h_t = self.tanh(\n                np.dot(x_t, self.W_ih) +\n                np.dot(h_prev, self.W_hh) +\n                self.b_h\n            )\n            \n            # Output at time t:\n            # y_t = h_t @ W_ho + b_o\n            y_t = np.dot(h_t, self.W_ho) + self.b_o\n            y_t = self.softmax(y_t)\n            \n            outputs.append(y_t)\n            hidden_states.append(h_t)\n            \n            # Update hidden state\n            h_prev = h_t\n        \n        return outputs, hidden_states\n    \n    def predict(self, inputs):\n        \"\"\"Get predictions for sequence\"\"\"\n        outputs, _ = self.forward(inputs)\n        # Return last output (for sequence classification)\n        return np.argmax(outputs[-1], axis=1)\n\n\n# ============================================\n# VISUALIZE RNN COMPUTATION\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RNN ARCHITECTURE & COMPUTATION\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nRNN UNROLLED IN TIME:\n\n   Input:    x‚ÇÄ      x‚ÇÅ      x‚ÇÇ      x‚ÇÉ\n              ‚Üì       ‚Üì       ‚Üì       ‚Üì\n   Hidden:  [h‚ÇÄ] ‚Üí [h‚ÇÅ] ‚Üí [h‚ÇÇ] ‚Üí [h‚ÇÉ]\n              ‚Üì       ‚Üì       ‚Üì       ‚Üì\n   Output:   y‚ÇÄ      y‚ÇÅ      y‚ÇÇ      y‚ÇÉ\n\nAt each time step t:\n   h_t = tanh(x_t @ W_ih + h_{t-1} @ W_hh + b_h)\n   y_t = softmax(h_t @ W_ho + b_o)\n\nKey Points:\n‚úÖ Same weights (W_ih, W_hh, W_ho) used at all time steps\n‚úÖ Hidden state h_t carries information from previous steps\n‚úÖ Can process variable-length sequences\n\"\"\")\n\n# Create RNN\nrnn = VanillaRNN(input_size=10, hidden_size=20, output_size=5)\n\nprint(f\"\\nüèóÔ∏è RNN Configuration:\")\nprint(f\"   Input size: {rnn.input_size}\")\nprint(f\"   Hidden size: {rnn.hidden_size}\")\nprint(f\"   Output size: {rnn.output_size}\")\nprint(f\"\\n   Parameters:\")\nprint(f\"   - W_ih: {rnn.W_ih.shape} (input ‚Üí hidden)\")\nprint(f\"   - W_hh: {rnn.W_hh.shape} (hidden ‚Üí hidden)\")\nprint(f\"   - W_ho: {rnn.W_ho.shape} (hidden ‚Üí output)\")\n\n# Test with dummy sequence\nbatch_size = 2\nseq_length = 5\ndummy_sequence = np.random.randn(batch_size, seq_length, 10)\n\nprint(f\"\\nüìä Forward Pass:\")\nprint(f\"   Input shape: {dummy_sequence.shape}\")\n\noutputs, hidden_states = rnn.forward(dummy_sequence)\n\nprint(f\"   Number of outputs: {len(outputs)}\")\nprint(f\"   Output shape at each step: {outputs[0].shape}\")\nprint(f\"   Number of hidden states: {len(hidden_states)}\")\nprint(f\"   Hidden state shape: {hidden_states[0].shape}\")\n\nprint(f\"\\nüîÆ Predictions: {rnn.predict(dummy_sequence)}\")\n\nprint(\"\\n‚úÖ RNN forward pass complete!\")\n```\n\n### RNN Applications: Sentiment Analysis\n\n```python\nclass SentimentRNN:\n    \"\"\"\n    RNN for sentiment analysis (positive/negative)\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim=50, hidden_size=128):\n        \"\"\"\n        Parameters:\n        -----------\n        vocab_size : int\n            Size of vocabulary\n        embedding_dim : int\n            Dimension of word embeddings\n        hidden_size : int\n            Size of RNN hidden state\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        \n        # Embedding layer (maps words to vectors)\n        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n        \n        # RNN\n        self.rnn = VanillaRNN(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            output_size=2  # Binary: positive/negative\n        )\n    \n    def embed_sequence(self, sequence):\n        \"\"\"\n        Convert sequence of word indices to embeddings\n        \n        Parameters:\n        -----------\n        sequence : numpy array of shape (batch_size, seq_length)\n            Word indices\n        \n        Returns:\n        --------\n        embedded : numpy array of shape (batch_size, seq_length, embedding_dim)\n        \"\"\"\n        batch_size, seq_length = sequence.shape\n        embedded = np.zeros((batch_size, seq_length, self.embedding_dim))\n        \n        for i in range(batch_size):\n            for t in range(seq_length):\n                word_idx = sequence[i, t]\n                embedded[i, t] = self.embeddings[word_idx]\n        \n        return embedded\n    \n    def predict_sentiment(self, sequence):\n        \"\"\"\n        Predict sentiment for text sequence\n        \n        Returns:\n        --------\n        sentiment : 0 (negative) or 1 (positive)\n        probability : confidence of prediction\n        \"\"\"\n        # Embed words\n        embedded = self.embed_sequence(sequence)\n        \n        # Pass through RNN\n        outputs, _ = self.rnn.forward(embedded)\n        \n        # Use last output for classification\n        final_output = outputs[-1]\n        \n        sentiment = np.argmax(final_output, axis=1)\n        probability = np.max(final_output, axis=1)\n        \n        return sentiment, probability\n\n\n# ============================================\n# DEMO: SENTIMENT ANALYSIS\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RNN SENTIMENT ANALYSIS DEMO\")\nprint(\"=\" * 70)\n\n# Simple vocabulary\nvocab = {\n    'great': 1, 'good': 2, 'excellent': 3, 'amazing': 4, 'love': 5,\n    'bad': 6, 'terrible': 7, 'awful': 8, 'hate': 9, 'worst': 10,\n    'movie': 11, 'film': 12, 'this': 13, 'is': 14, 'the': 15,\n    '<PAD>': 0  # Padding token\n}\n\n# Example sentences (as word indices)\nsentences = [\n    [13, 14, 1, 11],      # \"this is great movie\"\n    [13, 11, 14, 7],      # \"this movie is terrible\"\n    [4, 12],              # \"amazing film\"\n    [9, 13, 11]           # \"hate this movie\"\n]\n\n# Pad sequences to same length\nmax_len = max(len(s) for s in sentences)\npadded = np.array([s + [0] * (max_len - len(s)) for s in sentences])\n\nprint(\"\\nüìù Example Sentences (as word indices):\")\nprint(padded)\n\n# Create sentiment model\nsentiment_model = SentimentRNN(vocab_size=len(vocab), embedding_dim=50, hidden_size=64)\n\nprint(f\"\\nüèóÔ∏è Sentiment RNN Architecture:\")\nprint(f\"   Vocabulary size: {len(vocab)}\")\nprint(f\"   Embedding dimension: {sentiment_model.embedding_dim}\")\nprint(f\"   Hidden size: {sentiment_model.hidden_size}\")\nprint(f\"   Output: 2 classes (negative=0, positive=1)\")\n\n# Make predictions\nsentiments, probabilities = sentiment_model.predict_sentiment(padded)\n\nprint(f\"\\nüîÆ Predictions (untrained model):\")\nsentence_texts = [\n    \"this is great movie\",\n    \"this movie is terrible\",\n    \"amazing film\",\n    \"hate this movie\"\n]\n\nfor i, text in enumerate(sentence_texts):\n    sentiment_label = \"Positive üòä\" if sentiments[i] == 1 else \"Negative üòû\"\n    print(f\"   '{text}': {sentiment_label} (confidence: {probabilities[i]:.2f})\")\n\nprint(\"\\nüí° NOTE: Model needs training to make accurate predictions!\")\nprint(\"   This demonstrates the RNN architecture for NLP tasks.\")\n```\n\n---\n\n## 17. Long Short-Term Memory (LSTM) from Scratch\n\n### Understanding LSTM\n\n**Simple Explanation:** Advanced RNN that can remember information for long sequences by using \"gates\" that control what to remember and forget.\n\n**Key Innovation:** Solves the vanishing gradient problem of vanilla RNNs.\n\n### LSTM Architecture\n\n```python\nclass LSTMCell:\n    \"\"\"\n    Single LSTM cell from scratch\n    \n    LSTM has 4 gates:\n    1. Forget gate: What to forget from previous memory\n    2. Input gate: What new information to store\n    3. Output gate: What to output\n    4. Cell gate: Candidate values to add to memory\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        # Initialize weights for all 4 gates\n        # Each gate has weights for input (x) and hidden state (h)\n        scale = np.sqrt(2.0 / (input_size + hidden_size))\n        \n        # Forget gate\n        self.W_f = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_f = np.zeros((1, hidden_size))\n        \n        # Input gate\n        self.W_i = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_i = np.zeros((1, hidden_size))\n        \n        # Cell gate (candidate values)\n        self.W_c = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_c = np.zeros((1, hidden_size))\n        \n        # Output gate\n        self.W_o = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_o = np.zeros((1, hidden_size))\n    \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n    \n    def tanh(self, x):\n        return np.tanh(x)\n    \n    def forward(self, x_t, h_prev, c_prev):\n        \"\"\"\n        Forward pass through LSTM cell\n        \n        Parameters:\n        -----------\n        x_t : numpy array of shape (batch_size, input_size)\n            Input at current time step\n        h_prev : numpy array of shape (batch_size, hidden_size)\n            Previous hidden state\n        c_prev : numpy array of shape (batch_size, hidden_size)\n            Previous cell state\n        \n        Returns:\n        --------\n        h_t : numpy array\n            New hidden state\n        c_t : numpy array\n            New cell state\n        \"\"\"\n        # Concatenate input and previous hidden state\n        combined = np.concatenate([x_t, h_prev], axis=1)\n        \n        # 1. Forget gate: What to forget from previous cell state\n        # f_t = œÉ(W_f @ [x_t, h_{t-1}] + b_f)\n        f_t = self.sigmoid(np.dot(combined, self.W_f) + self.b_f)\n        \n        # 2. Input gate: What new information to store\n        # i_t = œÉ(W_i @ [x_t, h_{t-1}] + b_i)\n        i_t = self.sigmoid(np.dot(combined, self.W_i) + self.b_i)\n        \n        # 3. Cell gate: Candidate values to add\n        # cÃÉ_t = tanh(W_c @ [x_t, h_{t-1}] + b_c)\n        c_tilde = self.tanh(np.dot(combined, self.W_c) + self.b_c)\n        \n        # 4. Update cell state\n        # c_t = f_t ‚äô c_{t-1} + i_t ‚äô cÃÉ_t\n        # (forget old + remember new)\n        c_t = f_t * c_prev + i_t * c_tilde\n        \n        # 5. Output gate: What to output\n        # o_t = œÉ(W_o @ [x_t, h_{t-1}] + b_o)\n        o_t = self.sigmoid(np.dot(combined, self.W_o) + self.b_o)\n        \n        # 6. Hidden state\n        # h_t = o_t ‚äô tanh(c_t)\n        h_t = o_t * self.tanh(c_t)\n        \n        return h_t, c_t, (f_t, i_t, c_tilde, o_t)  # Return gates for visualization\n\n\nclass LSTM:\n    \"\"\"\n    Complete LSTM network\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # LSTM cell\n        self.cell = LSTMCell(input_size, hidden_size)\n        \n        # Output layer\n        self.W_out = np.random.randn(hidden_size, output_size) * 0.01\n        self.b_out = np.zeros((1, output_size))\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n    \n    def forward(self, inputs, h_prev=None, c_prev=None):\n        \"\"\"\n        Forward pass through LSTM\n        \n        Parameters:\n        -----------\n        inputs : numpy array of shape (batch_size, seq_length, input_size)\n        h_prev, c_prev : Initial hidden and cell states\n        \n        Returns:\n        --------\n        outputs : List of outputs at each time step\n        hidden_states : List of hidden states\n        cell_states : List of cell states\n        \"\"\"\n        batch_size, seq_length, _ = inputs.shape\n        \n        # Initialize states\n        if h_prev is None:\n            h_prev = np.zeros((batch_size, self.hidden_size))\n        if c_prev is None:\n            c_prev = np.zeros((batch_size, self.hidden_size))\n        \n        outputs = []\n        hidden_states = []\n        cell_states = []\n        gate_values = []\n        \n        # Process sequence\n        for t in range(seq_length):\n            x_t = inputs[:, t, :]\n            \n            # LSTM cell forward\n            h_t, c_t, gates = self.cell.forward(x_t, h_prev, c_prev)\n            \n            # Output\n            y_t = np.dot(h_t, self.W_out) + self.b_out\n            y_t = self.softmax(y_t)\n            \n            outputs.append(y_t)\n            hidden_states.append(h_t)\n            cell_states.append(c_t)\n            gate_values.append(gates)\n            \n            # Update states\n            h_prev = h_t\n            c_prev = c_t\n        \n        return outputs, hidden_states, cell_states, gate_values\n\n\n# ============================================\n# VISUALIZE LSTM GATES\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"LSTM ARCHITECTURE & GATES\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nLSTM CELL STRUCTURE:\n\n         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    x_t  ‚îÇ                      ‚îÇ\n    ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ                      ‚îÇ\n         ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n  h_{t-1}‚îÇ   ‚îÇ f ‚îÇ  ‚îÇ i ‚îÇ       ‚îÇ\n    ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  h_t\n         ‚îÇ     ‚Üì      ‚Üì          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚Üí\n  c_{t-1}‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n    ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ‚îÄ‚îÄ‚îÄ‚îÇ   c_t      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  c_t\n         ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚Üí\n         ‚îÇ         ‚Üì             ‚îÇ\n         ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n         ‚îÇ       ‚îÇ o ‚îÇ           ‚îÇ\n         ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nFour Gates:\n1. Forget gate (f): Decides what to forget from cell state\n2. Input gate (i): Decides what new information to store\n3. Cell gate (cÃÉ): Creates candidate values\n4. Output gate (o): Decides what to output\n\nKey Equations:\n   f_t = œÉ(W_f¬∑[x_t, h_{t-1}] + b_f)    [Forget]\n   i_t = œÉ(W_i¬∑[x_t, h_{t-1}] + b_i)    [Input]\n   cÃÉ_t = tanh(W_c¬∑[x_t, h_{t-1}] + b_c) [Candidate]\n   c_t = f_t‚äôc_{t-1} + i_t‚äôcÃÉ_t          [Cell update]\n   o_t = œÉ(W_o¬∑[x_t, h_{t-1}] + b_o)    [Output]\n   h_t = o_t‚äôtanh(c_t)                  [Hidden state]\n\nüí° Key Advantages over Vanilla RNN:\n   ‚úÖ Can learn long-term dependencies\n   ‚úÖ Solves vanishing gradient problem\n   ‚úÖ Cell state acts as \"highway\" for information\n\"\"\")\n\n# Create LSTM\nlstm = LSTM(input_size=10, hidden_size=20, output_size=5)\n\nprint(f\"\\nüèóÔ∏è LSTM Configuration:\")\nprint(f\"   Input size: {lstm.input_size}\")\nprint(f\"   Hidden size: {lstm.hidden_size}\")\nprint(f\"   Output size: {lstm.output_size}\")\nprint(f\"\\n   Parameters per cell:\")\nprint(f\"   - Forget gate: {lstm.cell.W_f.shape}\")\nprint(f\"   - Input gate: {lstm.cell.W_i.shape}\")\nprint(f\"   - Cell gate: {lstm.cell.W_c.shape}\")\nprint(f\"   - Output gate: {lstm.cell.W_o.shape}\")\n\n# Test forward pass\nbatch_size = 2\nseq_length = 5\ntest_input = np.random.randn(batch_size, seq_length, 10)\n\nprint(f\"\\nüìä Forward Pass:\")\noutputs, hidden_states, cell_states, gate_values = lstm.forward(test_input)\n\nprint(f\"   Input: {test_input.shape}\")\nprint(f\"   Outputs: {len(outputs)} time steps, each {outputs[0].shape}\")\nprint(f\"   Hidden states: {len(hidden_states)} states, each {hidden_states[0].shape}\")\nprint(f\"   Cell states: {len(cell_states)} states, each {cell_states[0].shape}\")\n\n# Visualize gate activations\nprint(f\"\\nüö™ Gate Activations at First Time Step:\")\nf_t, i_t, c_tilde, o_t = gate_values[0]\nprint(f\"   Forget gate mean: {np.mean(f_t):.3f} (closer to 0 = forget more)\")\nprint(f\"   Input gate mean: {np.mean(i_t):.3f} (closer to 1 = remember more)\")\nprint(f\"   Output gate mean: {np.mean(o_t):.3f} (closer to 1 = output more)\")\n\nprint(\"\\n‚úÖ LSTM forward pass complete!\")\n```\n\n### RNN vs LSTM Comparison\n\n```python\ndef compare_rnn_lstm():\n    \"\"\"\n    Compare vanilla RNN vs LSTM\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RNN vs LSTM COMPARISON\")\n    print(\"=\" * 70)\n    \n    comparison = pd.DataFrame({\n        'Aspect': [\n            'Memory Length',\n            'Gradient Flow',\n            'Parameters',\n            'Training Speed',\n            'Long Sequences',\n            'Short Sequences',\n            'Complexity'\n        ],\n        'Vanilla RNN': [\n            'Short-term',\n            'Vanishing gradient ‚ùå',\n            'Fewer (faster)',\n            'Fast',\n            'Poor ‚ùå',\n            'Good ‚úÖ',\n            'Simple'\n        ],\n        'LSTM': [\n            'Long-term ‚úÖ',\n            'Better gradient flow ‚úÖ',\n            'More (4√ó weights)',\n            'Slower',\n            'Excellent ‚úÖ',\n            'Good ‚úÖ',\n            'Complex'\n        ]\n    })\n    \n    print(\"\\nüìä Feature Comparison:\")\n    print(comparison.to_string(index=False))\n    \n    print(\"\\nüí° WHEN TO USE:\")\n    print(\"\\nVanilla RNN:\")\n    print(\"   ‚úÖ Short sequences (< 10 steps)\")\n    print(\"   ‚úÖ Simple patterns\")\n    print(\"   ‚úÖ Fast training needed\")\n    print(\"   ‚úÖ Limited computational resources\")\n    \n    print(\"\\nLSTM:\")\n    print(\"   ‚úÖ Long sequences (> 20 steps)\")\n    print(\"   ‚úÖ Complex dependencies\")\n    print(\"   ‚úÖ Text generation, translation\")\n    print(\"   ‚úÖ Time series with long patterns\")\n    print(\"   ‚úÖ When accuracy > speed\")\n    \n    print(\"\\nüéØ COMMON APPLICATIONS:\")\n    print(\"\\nRNN:\")\n    print(\"   - Simple sequence classification\")\n    print(\"   - Short text sentiment analysis\")\n    print(\"   - Basic time series prediction\")\n    \n    print(\"\\nLSTM:\")\n    print(\"   - Machine translation\")\n    print(\"   - Text generation\")\n    print(\"   - Speech recognition\")\n    print(\"   - Video analysis\")\n    print(\"   - Music composition\")\n    print(\"   - Stock price prediction\")\n\ncompare_rnn_lstm()\n```\n\n---\n\n## 19. Transformer Architecture from Scratch\n\n### Understanding Self-Attention\n\n**Simple Explanation:** Instead of processing sequences step-by-step like RNNs, Transformers look at the entire sequence at once and learn which parts are important.\n\n**Key Innovation:** Self-attention mechanism - each word can \"attend\" to every other word.\n\n```python\nclass SelfAttention:\n    \"\"\"\n    Self-Attention mechanism from scratch\n    The heart of Transformers!\n    \"\"\"\n    \n    def __init__(self, embed_dim, num_heads=1):\n        \"\"\"\n        Parameters:\n        -----------\n        embed_dim : int\n            Dimension of input embeddings\n        num_heads : int\n            Number of attention heads (for multi-head attention)\n        \"\"\"\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Query, Key, Value weight matrices\n        self.W_q = np.random.randn(embed_dim, embed_dim) * 0.01\n        self.W_k = np.random.randn(embed_dim, embed_dim) * 0.01\n        self.W_v = np.random.randn(embed_dim, embed_dim) * 0.01\n        \n        # Output projection\n        self.W_o = np.random.randn(embed_dim, embed_dim) * 0.01\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        \"\"\"\n        Compute attention: Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n        \n        Parameters:\n        -----------\n        Q : Query matrix (batch, seq_len, embed_dim)\n        K : Key matrix (batch, seq_len, embed_dim)\n        V : Value matrix (batch, seq_len, embed_dim)\n        mask : Optional mask (prevents attending to certain positions)\n        \n        Returns:\n        --------\n        output : Attended values\n        attention_weights : Attention scores\n        \"\"\"\n        # Calculate attention scores\n        # scores = Q @ K^T / ‚àöd_k\n        d_k = K.shape[-1]\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n        \n        # Apply mask if provided (for padding or future positions)\n        if mask is not None:\n            scores = scores + mask\n        \n        # Softmax to get attention weights\n        # This tells us \"how much attention to pay to each position\"\n        attention_weights = self.softmax(scores)\n        \n        # Weighted sum of values\n        # output = attention_weights @ V\n        output = np.matmul(attention_weights, V)\n        \n        return output, attention_weights\n    \n    def softmax(self, x):\n        \"\"\"Numerical stable softmax\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n    \n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass through self-attention\n        \n        Parameters:\n        -----------\n        x : numpy array of shape (batch_size, seq_length, embed_dim)\n        \n        Returns:\n        --------\n        output : Attended output\n        attention_weights : Attention scores\n        \"\"\"\n        batch_size, seq_length, embed_dim = x.shape\n        \n        # Linear projections: Q = XW_q, K = XW_k, V = XW_v\n        Q = np.dot(x, self.W_q)\n        K = np.dot(x, self.W_k)\n        V = np.dot(x, self.W_v)\n        \n        # Compute attention\n        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Final linear projection\n        output = np.dot(output, self.W_o)\n        \n        return output, attention_weights\n\n\nclass PositionalEncoding:\n    \"\"\"\n    Add position information to embeddings\n    Since Transformers don't have inherent notion of order\n    \"\"\"\n    \n    def __init__(self, max_seq_length, embed_dim):\n        \"\"\"\n        Create positional encodings\n        PE(pos, 2i) = sin(pos / 10000^(2i/d))\n        PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n        \"\"\"\n        self.max_seq_length = max_seq_length\n        self.embed_dim = embed_dim\n        \n        # Create position encodings\n        pe = np.zeros((max_seq_length, embed_dim))\n        position = np.arange(0, max_seq_length).reshape(-1, 1)\n        div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n        \n        pe[:, 0::2] = np.sin(position * div_term)\n        pe[:, 1::2] = np.cos(position * div_term)\n        \n        self.pe = pe\n    \n    def forward(self, x):\n        \"\"\"\n        Add positional encoding to input\n        \n        Parameters:\n        -----------\n        x : numpy array of shape (batch_size, seq_length, embed_dim)\n        \"\"\"\n        batch_size, seq_length, embed_dim = x.shape\n        return x + self.pe[:seq_length, :]\n\n\n# ============================================\n# VISUALIZE SELF-ATTENTION\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SELF-ATTENTION MECHANISM\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nHOW SELF-ATTENTION WORKS:\n\nInput Sentence: \"The cat sat on the mat\"\n\nStep 1: Create Query, Key, Value vectors for each word\n   Q = input @ W_q  (What am I looking for?)\n   K = input @ W_k  (What do I contain?)\n   V = input @ W_v  (What should I output?)\n\nStep 2: Calculate attention scores\n   Scores = Q @ K^T / ‚àöd_k\n   \n   This creates a matrix showing how much each word \n   should \"attend\" to every other word.\n\nStep 3: Apply softmax to get attention weights\n   Weights = softmax(Scores)\n   \n   Now each word has a distribution over all words.\n\nStep 4: Weighted sum of values\n   Output = Weights @ V\n   \n   Each word's output is influenced by all other words!\n\nüí° KEY ADVANTAGES:\n   ‚úÖ Parallel computation (unlike RNNs)\n   ‚úÖ Captures long-range dependencies easily\n   ‚úÖ Attention weights are interpretable\n   ‚úÖ No vanishing gradient problem\n\"\"\")\n\n# Demo self-attention\nembed_dim = 64\nseq_length = 6\n\n# Create sample input (e.g., embedded words)\nsample_input = np.random.randn(1, seq_length, embed_dim)\n\n# Create attention layer\nattention = SelfAttention(embed_dim=embed_dim)\n\nprint(f\"\\nüèóÔ∏è Self-Attention Configuration:\")\nprint(f\"   Embedding dimension: {embed_dim}\")\nprint(f\"   Sequence length: {seq_length}\")\nprint(f\"   Weight matrices: Q, K, V each {attention.W_q.shape}\")\n\n# Forward pass\noutput, attention_weights = attention.forward(sample_input)\n\nprint(f\"\\nüìä Forward Pass:\")\nprint(f\"   Input: {sample_input.shape}\")\nprint(f\"   Output: {output.shape}\")\nprint(f\"   Attention weights: {attention_weights.shape}\")\n\n# Visualize attention weights\nplt.figure(figsize=(10, 8))\nplt.imshow(attention_weights[0], cmap='viridis', aspect='auto')\nplt.colorbar(label='Attention Weight')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.title('Self-Attention Weights Heatmap\\n(Shows which positions attend to which)')\n\n# Add grid\nfor i in range(seq_length + 1):\n    plt.axhline(i - 0.5, color='white', linewidth=0.5)\n    plt.axvline(i - 0.5, color='white', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° INTERPRETATION:\")\nprint(\"   Brighter colors = Stronger attention\")\nprint(\"   Each row shows what that position attends to\")\nprint(\"   Diagonal = positions attend to themselves\")\n\nprint(\"\\n‚úÖ Self-Attention demo complete!\")\n```\n\n### Simple Transformer Implementation\n\n```python\nclass TransformerBlock:\n    \"\"\"\n    Single Transformer block\n    Contains: Self-Attention + Feed-Forward + Layer Norms + Residuals\n    \"\"\"\n    \n    def __init__(self, embed_dim, num_heads=4, ff_dim=256):\n        \"\"\"\n        Parameters:\n        -----------\n        embed_dim : int\n            Embedding dimension\n        num_heads : int\n            Number of attention heads\n        ff_dim : int\n            Feed-forward hidden dimension\n        \"\"\"\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        \n        # Multi-head self-attention\n        self.attention = SelfAttention(embed_dim, num_heads)\n        \n        # Feed-forward network\n        self.ff_W1 = np.random.randn(embed_dim, ff_dim) * 0.01\n        self.ff_b1 = np.zeros((1, ff_dim))\n        self.ff_W2 = np.random.randn(ff_dim, embed_dim) * 0.01\n        self.ff_b2 = np.zeros((1, embed_dim))\n        \n        # Layer normalization parameters\n        self.ln1_gamma = np.ones((1, embed_dim))\n        self.ln1_beta = np.zeros((1, embed_dim))\n        self.ln2_gamma = np.ones((1, embed_dim))\n        self.ln2_beta = np.zeros((1, embed_dim))\n    \n    def layer_norm(self, x, gamma, beta, epsilon=1e-6):\n        \"\"\"Layer Normalization\"\"\"\n        mean = np.mean(x, axis=-1, keepdims=True)\n        std = np.std(x, axis=-1, keepdims=True)\n        return gamma * (x - mean) / (std + epsilon) + beta\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def feed_forward(self, x):\n        \"\"\"\n        Feed-forward network: FFN(x) = ReLU(xW1 + b1)W2 + b2\n        \"\"\"\n        # First layer\n        x = np.dot(x, self.ff_W1) + self.ff_b1\n        x = self.relu(x)\n        \n        # Second layer\n        x = np.dot(x, self.ff_W2) + self.ff_b2\n        \n        return x\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through transformer block\n        \n        Architecture:\n        1. Multi-head self-attention\n        2. Add & Norm (residual connection + layer norm)\n        3. Feed-forward network\n        4. Add & Norm\n        \"\"\"\n        # Multi-head self-attention + residual + layer norm\n        attn_output, attn_weights = self.attention.forward(x)\n        x = self.layer_norm(x + attn_output, self.ln1_gamma, self.ln1_beta)  # Add & Norm\n        \n        # Feed-forward + residual + layer norm\n        ff_output = self.feed_forward(x)\n        x = self.layer_norm(x + ff_output, self.ln2_gamma, self.ln2_beta)  # Add & Norm\n        \n        return x, attn_weights\n\n\nclass SimpleTransformer:\n    \"\"\"\n    Simple Transformer for sequence classification\n    \"\"\"\n    \n    def __init__(self, vocab_size, embed_dim=128, num_heads=4, \n                 num_layers=2, max_seq_length=100, num_classes=2):\n        \"\"\"\n        Parameters:\n        -----------\n        vocab_size : int\n            Size of vocabulary\n        embed_dim : int\n            Embedding dimension\n        num_heads : int\n            Number of attention heads\n        num_layers : int\n            Number of transformer blocks\n        max_seq_length : int\n            Maximum sequence length\n        num_classes : int\n            Number of output classes\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.num_classes = num_classes\n        \n        # Token embeddings\n        self.token_embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n        \n        # Positional encodings\n        self.pos_encoding = PositionalEncoding(max_seq_length, embed_dim)\n        \n        # Transformer blocks\n        self.transformer_blocks = [\n            TransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ]\n        \n        # Classification head\n        self.classifier_W = np.random.randn(embed_dim, num_classes) * 0.01\n        self.classifier_b = np.zeros((1, num_classes))\n    \n    def embed(self, token_ids):\n        \"\"\"\n        Convert token IDs to embeddings\n        \"\"\"\n        batch_size, seq_length = token_ids.shape\n        embeddings = np.zeros((batch_size, seq_length, self.embed_dim))\n        \n        for i in range(batch_size):\n            for j in range(seq_length):\n                embeddings[i, j] = self.token_embeddings[token_ids[i, j]]\n        \n        return embeddings\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n    \n    def forward(self, token_ids):\n        \"\"\"\n        Forward pass through transformer\n        \n        Parameters:\n        -----------\n        token_ids : numpy array of shape (batch_size, seq_length)\n            Input token IDs\n        \n        Returns:\n        --------\n        logits : numpy array of shape (batch_size, num_classes)\n            Class logits\n        attention_weights : List of attention weights from each layer\n        \"\"\"\n        # 1. Token embeddings\n        x = self.embed(token_ids)\n        \n        # 2. Add positional encodings\n        x = self.pos_encoding.forward(x)\n        \n        # 3. Pass through transformer blocks\n        all_attention_weights = []\n        for transformer_block in self.transformer_blocks:\n            x, attn_weights = transformer_block.forward(x)\n            all_attention_weights.append(attn_weights)\n        \n        # 4. Global average pooling (average over sequence length)\n        x = np.mean(x, axis=1)\n        \n        # 5. Classification head\n        logits = np.dot(x, self.classifier_W) + self.classifier_b\n        probs = self.softmax(logits)\n        \n        return probs, all_attention_weights\n    \n    def predict(self, token_ids):\n        \"\"\"Get predicted class\"\"\"\n        probs, _ = self.forward(token_ids)\n        return np.argmax(probs, axis=1)\n\n\n# ============================================\n# DEMO: TRANSFORMER FOR SENTIMENT ANALYSIS\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRANSFORMER FOR SEQUENCE CLASSIFICATION\")\nprint(\"=\" * 70)\n\n# Simple vocabulary\nvocab = {\n    '<PAD>': 0, 'great': 1, 'good': 2, 'excellent': 3, \n    'bad': 4, 'terrible': 5, 'movie': 6, 'film': 7,\n    'this': 8, 'is': 9, 'the': 10\n}\n\n# Create transformer\ntransformer = SimpleTransformer(\n    vocab_size=len(vocab),\n    embed_dim=64,\n    num_heads=4,\n    num_layers=2,\n    max_seq_length=20,\n    num_classes=2\n)\n\nprint(f\"\\nüèóÔ∏è Transformer Architecture:\")\nprint(f\"   Vocabulary size: {len(vocab)}\")\nprint(f\"   Embedding dimension: {transformer.embed_dim}\")\nprint(f\"   Number of layers: {len(transformer.transformer_blocks)}\")\nprint(f\"   Number of heads: 4\")\nprint(f\"   Output classes: {transformer.num_classes}\")\n\n# Example sentences\nsentences = np.array([\n    [8, 9, 1, 6, 0, 0],  # \"this is great movie\"\n    [8, 6, 9, 5, 0, 0],  # \"this movie is terrible\"\n    [3, 7, 0, 0, 0, 0],  # \"excellent film\"\n])\n\nprint(f\"\\nüìä Forward Pass:\")\nprint(f\"   Input shape: {sentences.shape}\")\n\n# Forward pass\nprobs, attention_weights = transformer.forward(sentences)\npredictions = transformer.predict(sentences)\n\nprint(f\"   Output probabilities: {probs.shape}\")\nprint(f\"   Number of attention layers: {len(attention_weights)}\")\n\nprint(f\"\\nüîÆ Predictions:\")\nfor i in range(len(sentences)):\n    sentiment = \"Positive üòä\" if predictions[i] == 1 else \"Negative üòû\"\n    print(f\"   Sentence {i+1}: {sentiment} (prob: {probs[i, predictions[i]]:.3f})\")\n\n# Visualize attention from first layer\nplt.figure(figsize=(8, 6))\nplt.imshow(attention_weights[0][0], cmap='viridis', aspect='auto')\nplt.colorbar(label='Attention Weight')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.title('First Layer Attention Weights\\n(First sample)')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Transformer demo complete!\")\nprint(\"\\nüí° KEY DIFFERENCES from RNN/LSTM:\")\nprint(\"   ‚úÖ Processes entire sequence in parallel (much faster)\")\nprint(\"   ‚úÖ No vanishing gradient problem\")\nprint(\"   ‚úÖ Better at capturing long-range dependencies\")\nprint(\"   ‚úÖ Attention weights are interpretable\")\nprint(\"   ‚ùå Requires more memory\")\nprint(\"   ‚ùå Needs more data to train effectively\")\n```\n\n---\n\n# Part 4: Advanced Techniques\n\n## 20. Regularization Techniques from Scratch\n\n### L1 and L2 Regularization\n\n**Simple Explanation:** Add a penalty for large weights to prevent overfitting.\n\n```python\nclass RegularizedLinearRegression:\n    \"\"\"\n    Linear Regression with L1 (Lasso) and L2 (Ridge) regularization\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, n_iterations=1000, \n                 regularization='l2', lambda_param=0.01):\n        \"\"\"\n        Parameters:\n        -----------\n        regularization : str\n            'l1' for Lasso, 'l2' for Ridge, 'elastic_net' for both\n        lambda_param : float\n            Regularization strength\n        \"\"\"\n        self.lr = learning_rate\n        self.n_iterations = n_iterations\n        self.regularization = regularization\n        self.lambda_param = lambda_param\n        self.weights = None\n        self.bias = None\n        self.loss_history = []\n    \n    def fit(self, X, y):\n        \"\"\"\n        Train with regularization\n        \"\"\"\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        for i in range(self.n_iterations):\n            # Predictions\n            y_pred = np.dot(X, self.weights) + self.bias\n            \n            # Compute loss (MSE + regularization)\n            mse_loss = np.mean((y - y_pred) ** 2)\n            \n            if self.regularization == 'l1':\n                # L1: Œª Œ£|w|\n                reg_loss = self.lambda_param * np.sum(np.abs(self.weights))\n            elif self.regularization == 'l2':\n                # L2: Œª Œ£w¬≤\n                reg_loss = self.lambda_param * np.sum(self.weights ** 2)\n            elif self.regularization == 'elastic_net':\n                # Elastic Net: Œ±¬∑L1 + (1-Œ±)¬∑L2\n                reg_loss = self.lambda_param * (\n                    0.5 * np.sum(np.abs(self.weights)) +\n                    0.5 * np.sum(self.weights ** 2)\n                )\n            else:\n                reg_loss = 0\n            \n            total_loss = mse_loss + reg_loss\n            self.loss_history.append(total_loss)\n            \n            # Compute gradients\n            dw = (2/n_samples) * np.dot(X.T, (y_pred - y))\n            db = (2/n_samples) * np.sum(y_pred - y)\n            \n            # Add regularization gradient\n            if self.regularization == 'l1':\n                dw += self.lambda_param * np.sign(self.weights)\n            elif self.regularization == 'l2':\n                dw += 2 * self.lambda_param * self.weights\n            elif self.regularization == 'elastic_net':\n                dw += self.lambda_param * (\n                    0.5 * np.sign(self.weights) +\n                    0.5 * 2 * self.weights\n                )\n            \n            # Update weights\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n            \n            if i % 100 == 0:\n                print(f\"Iteration {i}: Loss = {total_loss:.4f}\")\n    \n    def predict(self, X):\n        return np.dot(X, self.weights) + self.bias\n\n\n# ============================================\n# DEMO: REGULARIZATION COMPARISON\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"REGULARIZATION TECHNIQUES COMPARISON\")\nprint(\"=\" * 70)\n\n# Generate data with noise and many features\nnp.random.seed(42)\nn_samples = 100\nn_features = 20\n\n# Only first 5 features are actually relevant\nX_train = np.random.randn(n_samples, n_features)\ntrue_weights = np.array([3, -2, 1, 0.5, -1.5] + [0] * 15)  # Most weights are 0\ny_train = X_train @ true_weights + np.random.randn(n_samples) * 0.5\n\nprint(f\"\\nüìä Dataset:\")\nprint(f\"   Samples: {n_samples}\")\nprint(f\"   Features: {n_features}\")\nprint(f\"   Relevant features: 5\")\nprint(f\"   Irrelevant features: 15\")\n\n# Train models with different regularization\nmodels = {\n    'No Regularization': RegularizedLinearRegression(\n        regularization=None, learning_rate=0.01, n_iterations=1000\n    ),\n    'L2 (Ridge)': RegularizedLinearRegression(\n        regularization='l2', lambda_param=0.1, learning_rate=0.01, n_iterations=1000\n    ),\n    'L1 (Lasso)': RegularizedLinearRegression(\n        regularization='l1', lambda_param=0.1, learning_rate=0.01, n_iterations=1000\n    ),\n    'Elastic Net': RegularizedLinearRegression(\n        regularization='elastic_net', lambda_param=0.1, learning_rate=0.01, n_iterations=1000\n    )\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.ravel()\n\nfor idx, (name, model) in enumerate(models.items()):\n    print(f\"\\n{'='*50}\")\n    print(f\"Training: {name}\")\n    print(f\"{'='*50}\")\n    model.fit(X_train, y_train)\n    \n    # Plot learned weights\n    axes[idx].bar(range(n_features), model.weights, alpha=0.7, label='Learned')\n    axes[idx].bar(range(n_features), true_weights, alpha=0.7, label='True')\n    axes[idx].set_xlabel('Feature Index')\n    axes[idx].set_ylabel('Weight Value')\n    axes[idx].set_title(f'{name}\\nSparsity: {np.sum(np.abs(model.weights) < 0.01)}/{n_features} weights ‚âà 0')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° KEY INSIGHTS:\")\nprint(\"\\n1. NO REGULARIZATION:\")\nprint(\"   ‚ùå Overfits: Learns non-zero weights for irrelevant features\")\nprint(\"   ‚ùå Weights are large and unstable\")\n\nprint(\"\\n2. L2 (RIDGE):\")\nprint(\"   ‚úÖ Shrinks all weights towards zero\")\nprint(\"   ‚ùå Doesn't eliminate features (no sparsity)\")\nprint(\"   ‚úÖ Good when all features are somewhat relevant\")\n\nprint(\"\\n3. L1 (LASSO):\")\nprint(\"   ‚úÖ Creates sparse solutions (many weights = exactly 0)\")\nprint(\"   ‚úÖ Performs feature selection automatically\")\nprint(\"   ‚úÖ Good when only few features are relevant\")\n\nprint(\"\\n4. ELASTIC NET:\")\nprint(\"   ‚úÖ Combines L1 and L2 benefits\")\nprint(\"   ‚úÖ Some sparsity + stable solutions\")\nprint(\"   ‚úÖ Best of both worlds\")\n```\n\n---\n\n## 22. Dropout from Scratch\n\n**Simple Explanation:** Randomly \"turn off\" neurons during training to prevent over-reliance on specific neurons.\n\n```python\nclass Dropout:\n    \"\"\"\n    Dropout layer from scratch\n    \"\"\"\n    \n    def __init__(self, drop_prob=0.5):\n        \"\"\"\n        Parameters:\n        -----------\n        drop_prob : float\n            Probability of dropping a neuron (0.0 - 1.0)\n        \"\"\"\n        self.drop_prob = drop_prob\n        self.mask = None\n    \n    def forward(self, X, training=True):\n        \"\"\"\n        Forward pass with dropout\n        \n        During training: Randomly drop neurons\n        During inference: Use all neurons (scaled)\n        \"\"\"\n        if training:\n            # Create binary mask\n            self.mask = (np.random.rand(*X.shape) > self.drop_prob).astype(float)\n            \n            # Scale remaining neurons to maintain expected value\n            # (Inverted dropout)\n            return X * self.mask / (1 - self.drop_prob)\n        else:\n            # No dropout during inference\n            return X\n    \n    def backward(self, grad):\n        \"\"\"\n        Backward pass: gradient only flows through non-dropped neurons\n        \"\"\"\n        return grad * self.mask / (1 - self.drop_prob)\n\n\nclass NeuralNetworkWithDropout:\n    \"\"\"\n    Neural Network with Dropout regularization\n    \"\"\"\n    \n    def __init__(self, layer_sizes, drop_probs=None):\n        \"\"\"\n        Parameters:\n        -----------\n        layer_sizes : list\n            Neurons in each layer\n        drop_probs : list\n            Dropout probability for each layer\n        \"\"\"\n        self.layer_sizes = layer_sizes\n        self.weights = []\n        self.biases = []\n        \n        # Initialize weights\n        for i in range(len(layer_sizes) - 1):\n            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n            b = np.zeros((1, layer_sizes[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n        \n        # Initialize dropout layers\n        if drop_probs is None:\n            drop_probs = [0.2] * (len(layer_sizes) - 2) + [0.0]  # No dropout on output\n        \n        self.dropout_layers = [Dropout(p) for p in drop_probs]\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, X, training=True):\n        \"\"\"\n        Forward pass with dropout\n        \"\"\"\n        activations = [X]\n        \n        for i in range(len(self.weights)):\n            # Linear transformation\n            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n            \n            # Activation\n            if i < len(self.weights) - 1:\n                a = self.relu(z)\n                # Apply dropout\n                a = self.dropout_layers[i].forward(a, training=training)\n            else:\n                a = self.softmax(z)\n            \n            activations.append(a)\n        \n        return activations[-1]\n    \n    def predict(self, X):\n        \"\"\"Prediction with dropout OFF\"\"\"\n        probs = self.forward(X, training=False)\n        return np.argmax(probs, axis=1)\n\n\n# ============================================\n# DEMO: DROPOUT EFFECT\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DROPOUT REGULARIZATION DEMO\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nHOW DROPOUT WORKS:\n\nDuring Training:\n   Input ‚Üí [√ó] [‚úì] [√ó] [‚úì] [‚úì] ‚Üí Hidden Layer\n           Randomly drop neurons\n\nDuring Inference:\n   Input ‚Üí [‚úì] [‚úì] [‚úì] [‚úì] [‚úì] ‚Üí Hidden Layer\n           Use all neurons\n\nüí° KEY BENEFITS:\n   ‚úÖ Prevents co-adaptation of neurons\n   ‚úÖ Forces network to learn robust features\n   ‚úÖ Acts as ensemble of many networks\n   ‚úÖ Simple and effective regularization\n\nüí° TYPICAL VALUES:\n   - Hidden layers: 0.5 (drop 50%)\n   - Input layer: 0.2 (drop 20%)\n   - Output layer: 0.0 (no dropout)\n\"\"\")\n\n# Demonstrate dropout effect\nlayer = Dropout(drop_prob=0.5)\nX_sample = np.random.randn(5, 10)\n\nprint(f\"\\nüìä Dropout Example:\")\nprint(f\"   Input shape: {X_sample.shape}\")\nprint(f\"   Drop probability: 0.5\")\n\n# Training mode: apply dropout\nX_dropped = layer.forward(X_sample, training=True)\n\nprint(f\"\\n   Original values (first neuron):\")\nprint(f\"   {X_sample[0, :5]}\")\nprint(f\"\\n   After dropout (first neuron):\")\nprint(f\"   {X_dropped[0, :5]}\")\nprint(f\"\\n   Dropped neurons: {np.sum(X_dropped[0] == 0)}/{X_sample.shape[1]}\")\n\n# Inference mode: no dropout\nX_inference = layer.forward(X_sample, training=False)\nprint(f\"\\n   Inference mode (no dropout):\")\nprint(f\"   {X_inference[0, :5]}\")\n\n# Compare models with and without dropout\nprint(f\"\\nüèóÔ∏è Network Architecture:\")\nnn_no_dropout = NeuralNetworkWithDropout([10, 50, 50, 5], drop_probs=[0.0, 0.0, 0.0])\nnn_with_dropout = NeuralNetworkWithDropout([10, 50, 50, 5], drop_probs=[0.2, 0.5, 0.0])\n\nprint(f\"   Without dropout: {[0.0, 0.0, 0.0]}\")\nprint(f\"   With dropout: {[0.2, 0.5, 0.0]}\")\n\n# Generate test data\nX_test = np.random.randn(100, 10)\n\n# Forward pass\noutput_no_dropout = nn_no_dropout.forward(X_test, training=True)\noutput_with_dropout = nn_with_dropout.forward(X_test, training=True)\n\nprint(f\"\\nüìä Output variance:\")\nprint(f\"   Without dropout: {np.var(output_no_dropout):.4f}\")\nprint(f\"   With dropout: {np.var(output_with_dropout):.4f}\")\n\nprint(\"\\n‚úÖ Dropout demonstration complete!\")\n```\n\n---\n\n## 21. Batch Normalization from Scratch\n\n**Simple Explanation:** Normalize activations within each mini-batch to stabilize and speed up training.\n\n```python\nclass BatchNormalization:\n    \"\"\"\n    Batch Normalization from scratch\n    \"\"\"\n    \n    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n        \"\"\"\n        Parameters:\n        -----------\n        num_features : int\n            Number of features (channels)\n        epsilon : float\n            Small constant for numerical stability\n        momentum : float\n            Momentum for running mean/variance\n        \"\"\"\n        self.epsilon = epsilon\n        self.momentum = momentum\n        \n        # Learnable parameters\n        self.gamma = np.ones((1, num_features))  # Scale\n        self.beta = np.zeros((1, num_features))  # Shift\n        \n        # Running statistics (for inference)\n        self.running_mean = np.zeros((1, num_features))\n        self.running_var = np.ones((1, num_features))\n    \n    def forward(self, X, training=True):\n        \"\"\"\n        Forward pass through batch normalization\n        \n        During training:\n        1. Calculate batch mean and variance\n        2. Normalize: x_norm = (x - mean) / sqrt(var + Œµ)\n        3. Scale and shift: y = Œ≥¬∑x_norm + Œ≤\n        \n        During inference:\n        - Use running mean and variance\n        \"\"\"\n        if training:\n            # Calculate batch statistics\n            batch_mean = np.mean(X, axis=0, keepdims=True)\n            batch_var = np.var(X, axis=0, keepdims=True)\n            \n            # Normalize\n            X_norm = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n            \n            # Update running statistics\n            self.running_mean = (\n                self.momentum * self.running_mean +\n                (1 - self.momentum) * batch_mean\n            )\n            self.running_var = (\n                self.momentum * self.running_var +\n                (1 - self.momentum) * batch_var\n            )\n            \n            # Store for backward pass\n            self.X = X\n            self.X_norm = X_norm\n            self.batch_mean = batch_mean\n            self.batch_var = batch_var\n        else:\n            # Use running statistics during inference\n            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n        \n        # Scale and shift\n        output = self.gamma * X_norm + self.beta\n        \n        return output\n    \n    def __repr__(self):\n        return f\"BatchNorm(features={self.gamma.shape[1]}, epsilon={self.epsilon})\"\n\n\n# ============================================\n# DEMO: BATCH NORMALIZATION\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"BATCH NORMALIZATION\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nWHY BATCH NORMALIZATION?\n\nProblem: Internal Covariate Shift\n   - Distribution of layer inputs changes during training\n   - Makes training unstable and slow\n   - Requires careful weight initialization\n\nSolution: Batch Normalization\n   - Normalizes each mini-batch\n   - Reduces internal covariate shift\n   - Allows higher learning rates\n   - Acts as regularization\n\nFormula:\n   1. Œº = mean(X)                    [batch mean]\n   2. œÉ¬≤ = var(X)                    [batch variance]\n   3. xÃÇ = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)       [normalize]\n   4. y = Œ≥¬∑xÃÇ + Œ≤                   [scale & shift]\n\nüí° BENEFITS:\n   ‚úÖ Faster training (2-3x speedup)\n   ‚úÖ Higher learning rates possible\n   ‚úÖ Less sensitive to initialization\n   ‚úÖ Acts as regularization\n   ‚úÖ Reduces need for dropout\n\"\"\")\n\n# Create batch norm layer\nbn = BatchNormalization(num_features=10)\n\n# Generate sample data with different scales\nX_sample = np.random.randn(32, 10) * np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nprint(f\"\\nüìä Batch Normalization Demo:\")\nprint(f\"   Input shape: {X_sample.shape}\")\nprint(f\"   Features: {bn.gamma.shape[1]}\")\n\n# Forward pass (training)\nX_normalized = bn.forward(X_sample, training=True)\n\nprint(f\"\\n   Before BatchNorm:\")\nprint(f\"   Mean: {np.mean(X_sample, axis=0)[:5]}\")\nprint(f\"   Std: {np.std(X_sample, axis=0)[:5]}\")\n\nprint(f\"\\n   After BatchNorm:\")\nprint(f\"   Mean: {np.mean(X_normalized, axis=0)[:5]}\")\nprint(f\"   Std: {np.std(X_normalized, axis=0)[:5]}\")\nprint(f\"   (Should be close to 0 mean, 1 std)\")\n\n# Visualize effect\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Before\naxes[0].boxplot(X_sample[:, :5])\naxes[0].set_xlabel('Feature Index')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Before Batch Normalization\\n(Different scales)')\naxes[0].grid(True, alpha=0.3)\n\n# After\naxes[1].boxplot(X_normalized[:, :5])\naxes[1].set_xlabel('Feature Index')\naxes[1].set_ylabel('Value')\naxes[1].set_title('After Batch Normalization\\n(Normalized scales)')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Batch Normalization demo complete!\")\n```\n\n---\n\n# Part 5: Production & Deployment\n\n## 26. Training Strategies\n\n### Learning Rate Schedules\n\n```python\nclass LearningRateScheduler:\n    \"\"\"\n    Various learning rate schedules\n    \"\"\"\n    \n    @staticmethod\n    def step_decay(initial_lr, epoch, drop_rate=0.5, epochs_drop=10):\n        \"\"\"\n        Step decay: lr = initial_lr * drop_rate^floor(epoch/epochs_drop)\n        \"\"\"\n        return initial_lr * (drop_rate ** np.floor(epoch / epochs_drop))\n    \n    @staticmethod\n    def exponential_decay(initial_lr, epoch, decay_rate=0.95):\n        \"\"\"\n        Exponential decay: lr = initial_lr * decay_rate^epoch\n        \"\"\"\n        return initial_lr * (decay_rate ** epoch)\n    \n    @staticmethod\n    def cosine_annealing(initial_lr, epoch, max_epochs):\n        \"\"\"\n        Cosine annealing: lr = min_lr + 0.5*(max_lr-min_lr)*(1+cos(œÄ*epoch/max_epochs))\n        \"\"\"\n        min_lr = initial_lr * 0.01\n        return min_lr + 0.5 * (initial_lr - min_lr) * (\n            1 + np.cos(np.pi * epoch / max_epochs)\n        )\n    \n    @staticmethod\n    def warmup_cosine(initial_lr, epoch, warmup_epochs=5, max_epochs=100):\n        \"\"\"\n        Warmup then cosine: Linear warmup followed by cosine decay\n        \"\"\"\n        if epoch < warmup_epochs:\n            return initial_lr * (epoch + 1) / warmup_epochs\n        else:\n            return LearningRateScheduler.cosine_annealing(\n                initial_lr, epoch - warmup_epochs, max_epochs - warmup_epochs\n            )\n\n\n# Visualize learning rate schedules\ndef visualize_lr_schedules():\n    \"\"\"\n    Compare different learning rate schedules\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"LEARNING RATE SCHEDULES\")\n    print(\"=\" * 70)\n    \n    initial_lr = 0.1\n    max_epochs = 100\n    epochs = np.arange(max_epochs)\n    \n    schedules = {\n        'Constant': [initial_lr] * max_epochs,\n        'Step Decay': [LearningRateScheduler.step_decay(initial_lr, e) for e in epochs],\n        'Exponential': [LearningRateScheduler.exponential_decay(initial_lr, e) for e in epochs],\n        'Cosine Annealing': [LearningRateScheduler.cosine_annealing(initial_lr, e, max_epochs) for e in epochs],\n        'Warmup + Cosine': [LearningRateScheduler.warmup_cosine(initial_lr, e, max_epochs=max_epochs) for e in epochs]\n    }\n    \n    plt.figure(figsize=(12, 6))\n    for name, lr_values in schedules.items():\n        plt.plot(epochs, lr_values, label=name, linewidth=2)\n    \n    plt.xlabel('Epoch')\n    plt.ylabel('Learning Rate')\n    plt.title('Learning Rate Schedules Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüí° WHEN TO USE:\")\n    print(\"\\n1. CONSTANT:\")\n    print(\"   ‚úÖ Simple problems\")\n    print(\"   ‚ùå May not converge optimally\")\n    \n    print(\"\\n2. STEP DECAY:\")\n    print(\"   ‚úÖ Traditional approach\")\n    print(\"   ‚úÖ Easy to tune\")\n    \n    print(\"\\n3. EXPONENTIAL DECAY:\")\n    print(\"   ‚úÖ Smooth decrease\")\n    print(\"   ‚ùå Can decay too quickly\")\n    \n    print(\"\\n4. COSINE ANNEALING:\")\n    print(\"   ‚úÖ Smooth, gradual decay\")\n    print(\"   ‚úÖ Good final performance\")\n    print(\"   ‚úÖ Modern default choice\")\n    \n    print(\"\\n5. WARMUP + COSINE:\")\n    print(\"   ‚úÖ Best for large models/datasets\")\n    print(\"   ‚úÖ Stable start, optimal end\")\n    print(\"   ‚úÖ Used in BERT, GPT, etc.\")\n\nvisualize_lr_schedules()\n```\n\n---\n\n## 30. Deployment Strategies\n\n### Saving and Loading Models\n\n```python\nclass ModelSerializer:\n    \"\"\"\n    Save and load models\n    \"\"\"\n    \n    @staticmethod\n    def save_model(model, filepath):\n        \"\"\"\n        Save model parameters to file\n        \"\"\"\n        model_data = {\n            'architecture': {\n                'layer_sizes': model.layer_sizes,\n            },\n            'weights': model.weights,\n            'biases': model.biases,\n        }\n        \n        np.save(filepath, model_data)\n        print(f\"‚úÖ Model saved to {filepath}\")\n    \n    @staticmethod\n    def load_model(filepath):\n        \"\"\"\n        Load model from file\n        \"\"\"\n        model_data = np.load(filepath, allow_pickle=True).item()\n        \n        # Reconstruct model\n        model = NeuralNetworkScratch(model_data['architecture']['layer_sizes'])\n        model.weights = model_data['weights']\n        model.biases = model_data['biases']\n        \n        print(f\"‚úÖ Model loaded from {filepath}\")\n        return model\n\n\n# Model deployment API\nclass ModelAPI:\n    \"\"\"\n    Simple API for model inference\n    \"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n        self.request_count = 0\n    \n    def predict(self, input_data):\n        \"\"\"\n        Make prediction\n        \n        Parameters:\n        -----------\n        input_data : numpy array or dict\n            Input features\n        \n        Returns:\n        --------\n        result : dict\n            Prediction results with metadata\n        \"\"\"\n        self.request_count += 1\n        \n        # Preprocess input\n        if isinstance(input_data, dict):\n            X = np.array(list(input_data.values())).reshape(1, -1)\n        else:\n            X = input_data\n        \n        # Make prediction\n        predictions = self.model.predict(X)\n        probabilities = self.model.predict_proba(X)\n        \n        # Format response\n        result = {\n            'prediction': int(predictions[0]),\n            'probability': float(probabilities[0]),\n            'request_id': self.request_count,\n            'model_version': '1.0'\n        }\n        \n        return result\n    \n    def health_check(self):\n        \"\"\"Check if model is working\"\"\"\n        return {\n            'status': 'healthy',\n            'requests_served': self.request_count,\n            'model_loaded': self.model is not None\n        }\n\n\n# ============================================\n# DEMO: MODEL DEPLOYMENT\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL DEPLOYMENT DEMO\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nPRODUCTION DEPLOYMENT CHECKLIST:\n================================\n\n1. ‚úÖ Model Serialization\n   - Save model weights\n   - Save preprocessing params\n   - Version control\n\n2. ‚úÖ API Creation\n   - REST API (Flask/FastAPI)\n   - Input validation\n   - Error handling\n\n3. ‚úÖ Monitoring\n   - Track predictions\n   - Log errors\n   - Monitor latency\n\n4. ‚úÖ Scalability\n   - Load balancing\n   - Caching\n   - Batch processing\n\n5. ‚úÖ Model Updates\n   - A/B testing\n   - Gradual rollout\n   - Rollback capability\n\"\"\")\n\n# Simple API demo\nprint(f\"\\nüöÄ Creating Model API...\")\n\n# Create simple model\nsimple_model = NeuralNetworkScratch([10, 20, 2])\n\n# Create API\napi = ModelAPI(simple_model)\n\nprint(f\"‚úÖ API initialized\")\n\n# Make prediction\ntest_input = {\n    'feature_0': 0.5,\n    'feature_1': -0.3,\n    'feature_2': 1.2,\n    'feature_3': 0.8,\n    'feature_4': -0.5,\n    'feature_5': 0.2,\n    'feature_6': 0.9,\n    'feature_7': -0.7,\n    'feature_8': 0.4,\n    'feature_9': 0.1\n}\n\nprint(f\"\\nüìä Making prediction...\")\nresult = api.predict(test_input)\n\nprint(f\"\\nüîÆ Prediction Result:\")\nfor key, value in result.items():\n    print(f\"   {key}: {value}\")\n\n# Health check\nprint(f\"\\nüè• Health Check:\")\nhealth = api.health_check()\nfor key, value in health.items():\n    print(f\"   {key}: {value}\")\n\nprint(\"\\n‚úÖ Deployment demo complete!\")\n\nprint(\"\"\"\n\\nüì¶ NEXT STEPS FOR REAL DEPLOYMENT:\n\n1. Create Flask/FastAPI App:\n   ```python\n   from flask import Flask, request, jsonify\n   \n   app = Flask(__name__)\n   model_api = ModelAPI(load_model('model.npy'))\n   \n   @app.route('/predict', methods=['POST'])\n   def predict():\n       data = request.json\n       result = model_api.predict(data)\n       return jsonify(result)\n   ```\n\n2. Containerize with Docker:\n   ```dockerfile\n   FROM python:3.9-slim\n   COPY requirements.txt .\n   RUN pip install -r requirements.txt\n   COPY model.npy app.py ./\n   CMD [\"python\", \"app.py\"]\n   ```\n\n3. Deploy to Cloud:\n   - AWS Lambda + API Gateway (serverless)\n   - AWS EC2 + Docker (traditional)\n   - Google Cloud Run (containerized)\n   - Azure ML Services\n   - Heroku (simple deployment)\n\n4. Monitor in Production:\n   - CloudWatch / Stackdriver logs\n   - Prometheus + Grafana metrics\n   - Sentry for error tracking\n   - Custom dashboards\n\"\"\")\n```\n\n---\n\n## üéì Complete Learning Path Summary\n\n```python\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üéâ CONGRATULATIONS! COMPLETE GUIDE FINISHED!\")\nprint(\"=\" * 70)\n\nlearning_path = {\n    \"Part 1: Traditional ML\": [\n        \"‚úÖ Linear Regression from scratch\",\n        \"‚úÖ Logistic Regression from scratch\",\n        \"‚úÖ Decision Trees from scratch\",\n        \"‚úÖ Random Forests from scratch\",\n        \"‚úÖ K-Nearest Neighbors\",\n        \"‚úÖ K-Means Clustering\",\n        \"‚úÖ SVM\",\n        \"‚úÖ Naive Bayes\"\n    ],\n    \"Part 2: Neural Networks\": [\n        \"‚úÖ Single Neuron/Perceptron\",\n        \"‚úÖ Multi-layer Neural Network\",\n        \"‚úÖ 6 Activation Functions\",\n        \"‚úÖ 6 Loss Functions\",\n        \"‚úÖ 5 Optimizers (SGD, Adam, etc.)\",\n        \"‚úÖ Backpropagation algorithm\",\n        \"‚úÖ Gradient Descent variants\"\n    ],\n    \"Part 3: Deep Learning\": [\n        \"‚úÖ CNN - Convolution & Pooling\",\n        \"‚úÖ CNN - Complete Architecture\",\n        \"‚úÖ RNN - Vanilla RNN\",\n        \"‚úÖ LSTM - Gates & Cell State\",\n        \"‚úÖ Transformer - Self-Attention\",\n        \"‚úÖ Transformer - Complete Model\",\n        \"‚úÖ Positional Encoding\"\n    ],\n    \"Part 4: Advanced Techniques\": [\n        \"‚úÖ L1/L2 Regularization\",\n        \"‚úÖ Dropout\",\n        \"‚úÖ Batch Normalization\",\n        \"‚úÖ Layer Normalization\",\n        \"‚úÖ Early Stopping\",\n        \"‚úÖ Data Augmentation\"\n    ],\n    \"Part 5: Production\": [\n        \"‚úÖ Learning Rate Schedules\",\n        \"‚úÖ Model Serialization\",\n        \"‚úÖ API Creation\",\n        \"‚úÖ Deployment Strategies\",\n        \"‚úÖ Monitoring & Logging\",\n        \"‚úÖ A/B Testing\"\n    ]\n}\n\nprint(\"\\nüìö WHAT YOU'VE MASTERED:\")\nfor category, topics in learning_path.items():\n    print(f\"\\n{category}:\")\n    for topic in topics:\n        print(f\"  {topic}\")\n\nprint(\"\\n\\nüéØ YOUR SKILLS NOW:\")\nprint(\"=\"*70)\nskills = [\n    \"‚úÖ Build any ML model from first principles\",\n    \"‚úÖ Understand mathematics behind algorithms\",\n    \"‚úÖ Implement deep learning with just NumPy\",\n    \"‚úÖ Debug models effectively\",\n    \"‚úÖ Optimize model performance\",\n    \"‚úÖ Deploy models to production\",\n    \"‚úÖ Explain any algorithm in interviews\",\n    \"‚úÖ Create custom architectures\",\n    \"‚úÖ Read and implement research papers\"\n]\n\nfor skill in skills:\n    print(f\"  {skill}\")\n\nprint(\"\\n\\nüíº CAREER PATHS:\")\nprint(\"=\"*70)\nprint(\"\"\"\nWith this knowledge, you can pursue:\n  üöÄ Machine Learning Engineer\n  üß† Deep Learning Researcher\n  üìä Data Scientist (Senior/Lead)\n  üèóÔ∏è ML Infrastructure Engineer\n  üéì AI Researcher\n  üí° ML Startup Founder\n  üìö ML Educator/Course Creator\n\"\"\")\n\nprint(\"\\nüöÄ NEXT STEPS:\")\nprint(\"=\"*70)\nprint(\"\"\"\n1. PRACTICE:\n   - Implement models on real datasets\n   - Kaggle competitions\n   - Personal projects\n\n2. SPECIALIZE:\n   - Computer Vision (if interested in images)\n   - NLP (if interested in text)\n   - Reinforcement Learning (if interested in agents)\n   - Time Series (if interested in forecasting)\n\n3. READ RESEARCH:\n   - arXiv papers\n   - Distill.pub (visual explanations)\n   - Papers With Code\n\n4. BUILD PORTFOLIO:\n   - GitHub repositories\n   - Blog posts explaining concepts\n   - YouTube tutorials\n   - Kaggle notebooks\n\n5. CONTRIBUTE:\n   - Open source ML libraries\n   - Answer StackOverflow questions\n   - Write tutorials\n   - Mentor others\n\n6. STAY UPDATED:\n   - Follow ML researchers on Twitter\n   - Subscribe to ML newsletters\n   - Attend conferences (NeurIPS, ICML, CVPR)\n   - Join ML communities\n\"\"\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Remember: You now understand ML better than 99% of developers!\")\nprint(\"Keep building, keep learning, keep sharing! üåü\")\nprint(\"=\"*70)\n```\n\n---\n\n## üìñ Final Resources\n\n### Books\n- **Deep Learning** by Goodfellow, Bengio, Courville\n- **Pattern Recognition and Machine Learning** by Bishop\n- **Hands-On Machine Learning** by Aur√©lien G√©ron\n- **The Hundred-Page Machine Learning Book** by Andriy Burkov\n\n### Online Courses\n- **Fast.ai** - Practical Deep Learning\n- **Coursera** - Deep Learning Specialization (Andrew Ng)\n- **Stanford CS231n** - CNN for Visual Recognition\n- **Stanford CS224n** - NLP with Deep Learning\n\n### Websites & Tools\n- **Papers With Code** - Latest research with code\n- **Distill.pub** - Visual explanations\n- **Towards Data Science** - Articles & tutorials\n- **Kaggle** - Datasets & competitions\n- **Google Colab** - Free GPU for experiments\n\n### Communities\n- **Reddit**: r/MachineLearning, r/learnmachinelearning\n- **Discord**: Many ML community servers\n- **Twitter**: Follow researchers and practitioners\n- **LinkedIn**: Connect with ML professionals\n\n---\n\n## üèÜ Challenge Yourself\n\n### Beginner Projects\n1. Handwritten digit classifier (MNIST)\n2. Iris flower classification\n3. Housing price prediction\n4. Sentiment analysis on reviews\n\n### Intermediate Projects\n1. Object detection in images\n2. Text generation (character-level)\n3. Recommendation system\n4. Time series forecasting\n\n### Advanced Projects\n1. Style transfer (images)\n2. Machine translation\n3. Reinforcement learning game agent\n4. GANs for image generation\n5. Implement a research paper from scratch\n\n---\n\n**üéì You've completed the ULTIMATE guide to building ML models from scratch!**\n\n**Now go build something amazing! üöÄ**\n"}