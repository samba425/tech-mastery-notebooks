{"id":"redis-caching","title":"ðŸ”´ Redis & Caching Zero to Hero","content":"# Redis & Caching: Zero to Hero Guide\n## Complete In-Memory Database & Caching Mastery\n\n---\n\n## ðŸ“š Table of Contents\n\n1. [Introduction to Redis](#introduction)\n2. [Why Learn Redis?](#why-learn)\n3. [Installation & Setup](#installation)\n4. [Redis Data Types](#data-types)\n5. [Basic Commands](#basic-commands)\n6. [Caching Strategies](#caching-strategies)\n7. [Redis with Node.js](#nodejs)\n8. [Redis with Python](#python)\n9. [Advanced Features](#advanced)\n10. [Redis Pub/Sub](#pubsub)\n11. [Redis Streams](#streams)\n12. [Persistence](#persistence)\n13. [Replication & High Availability](#replication)\n14. [Clustering](#clustering)\n15. [Performance Optimization](#performance)\n16. [Security](#security)\n17. [Monitoring](#monitoring)\n18. [Real-World Use Cases](#use-cases)\n19. [Best Practices](#best-practices)\n20. [Interview Preparation](#interview-prep)\n\n---\n\n## ðŸš€ Introduction to Redis {#introduction}\n\n### What is Redis?\n\n```\nRedis = REmote DIctionary Server\n= In-memory data structure store\n\nSpeed Comparison:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Traditional Database (Disk)    â”‚\nâ”‚ Read:  10-50ms                  â”‚\nâ”‚ Write: 20-100ms                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         ðŸ˜´ Slow\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Redis (In-Memory)               â”‚\nâ”‚ Read:  0.1-1ms                  â”‚\nâ”‚ Write: 0.1-1ms                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         âš¡ 50-100x Faster!\n\nKey Features:\nâœ… In-memory storage\nâœ… Multiple data structures\nâœ… Atomic operations\nâœ… Persistence options\nâœ… Pub/Sub messaging\nâœ… Lua scripting\nâœ… Transactions\nâœ… Clustering\nâœ… Replication\n```\n\n### Use Cases:\n\n```\n1. CACHING (Most Common)\n   - API responses\n   - Database queries\n   - Session data\n   - HTML fragments\n\n2. SESSION STORE\n   - User sessions\n   - Shopping carts\n   - Login tokens\n\n3. REAL-TIME ANALYTICS\n   - Page views counter\n   - Real-time leaderboards\n   - Live dashboards\n\n4. MESSAGE QUEUE\n   - Task queues\n   - Job processing\n   - Event streaming\n\n5. RATE LIMITING\n   - API rate limiting\n   - DDoS protection\n   - Request throttling\n\n6. GEOSPATIAL\n   - Location tracking\n   - Nearby searches\n   - Delivery tracking\n```\n\n---\n\n## ðŸ’¡ Why Learn Redis? {#why-learn}\n\n### Industry Adoption:\n\n```\nCompanies Using Redis:\nâœ… Twitter - Timeline caching\nâœ… GitHub - Job queues\nâœ… Instagram - User feeds\nâœ… Stack Overflow - Caching layer\nâœ… Pinterest - Follower graphs\nâœ… Uber - Geospatial indexing\nâœ… Airbnb - Session storage\n\nStatistics:\n- 8,000+ companies use Redis\n- Most popular key-value database\n- #1 for caching solutions\n- Used by 85% of Fortune 500\n```\n\n### Performance Benefits:\n\n```\nExample: E-commerce Product Page\n\nWithout Redis:\nRequest â†’ Database query (50ms) â†’ Response\n= 50ms per request\n= 20 requests/second/server\n\nWith Redis Cache:\nRequest â†’ Redis check (1ms) â†’ Response\n= 1ms per request\n= 1,000 requests/second/server\n\nResult:\n- 50x faster response\n- 50x more capacity\n- Better user experience\n- Lower infrastructure cost\n```\n\n---\n\n## âš™ï¸ Installation & Setup {#installation}\n\n### Installation:\n\n```bash\n# macOS (Homebrew)\nbrew install redis\nbrew services start redis\n\n# Linux (Ubuntu/Debian)\nsudo apt update\nsudo apt install redis-server\nsudo systemctl start redis\nsudo systemctl enable redis\n\n# Linux (RHEL/CentOS)\nsudo yum install redis\nsudo systemctl start redis\n\n# Docker (Any OS)\ndocker run -d -p 6379:6379 --name redis redis:latest\n\n# Verify installation\nredis-cli ping\n# Output: PONG âœ…\n\n# Check version\nredis-server --version\n# redis-server v=7.0.0\n\n# Redis configuration\n# Edit: /etc/redis/redis.conf (Linux)\n# Edit: /usr/local/etc/redis.conf (macOS)\n```\n\n### Redis CLI Basics:\n\n```bash\n# Connect to Redis\nredis-cli\n\n# Connect to remote Redis\nredis-cli -h hostname -p 6379 -a password\n\n# Select database (0-15)\nSELECT 1\n\n# Get server info\nINFO\n\n# Monitor all commands in real-time\nMONITOR\n\n# Clear current database\nFLUSHDB\n\n# Clear all databases\nFLUSHALL\n\n# Save database to disk\nSAVE\n```\n\n### Redis Configuration:\n\n```bash\n# redis.conf - Important settings\n\n# Bind to specific IP\nbind 127.0.0.1\n\n# Set port\nport 6379\n\n# Set password\nrequirepass yourpassword\n\n# Max memory limit\nmaxmemory 256mb\n\n# Eviction policy when max memory reached\nmaxmemory-policy allkeys-lru\n\n# Enable persistence\nsave 900 1        # Save if 1 key changed in 900 seconds\nsave 300 10       # Save if 10 keys changed in 300 seconds\nsave 60 10000     # Save if 10000 keys changed in 60 seconds\n\n# AOF (Append Only File) persistence\nappendonly yes\nappendfilename \"appendonly.aof\"\n\n# Log level\nloglevel notice\n```\n\n---\n\n## ðŸ“ Redis Data Types {#data-types}\n\n### 1. Strings:\n\n```bash\n# Most basic type - binary safe\n\n# Set string\nSET name \"John\"\n# OK\n\n# Get string\nGET name\n# \"John\"\n\n# Set with expiration (seconds)\nSETEX session:123 3600 \"user_data\"\n\n# Set if not exists\nSETNX lock:resource \"locked\"\n\n# Increment (atomic)\nSET counter 0\nINCR counter        # 1\nINCRBY counter 5    # 6\nDECR counter        # 5\n\n# Append to string\nSET message \"Hello\"\nAPPEND message \" World\"\nGET message         # \"Hello World\"\n\n# Get substring\nGETRANGE message 0 4    # \"Hello\"\n\n# String length\nSTRLEN message      # 11\n\n# Multiple get/set\nMSET key1 \"value1\" key2 \"value2\"\nMGET key1 key2\n\n# Use Cases:\n- Caching\n- Counters\n- Session storage\n- Rate limiting\n```\n\n### 2. Lists (Arrays):\n\n```bash\n# Ordered collection of strings\n\n# Push to list (left/right)\nLPUSH tasks \"task1\"     # Add to beginning\nRPUSH tasks \"task2\"     # Add to end\n\n# Get list items\nLRANGE tasks 0 -1       # Get all (0 to end)\nLRANGE tasks 0 2        # Get first 3\n\n# Pop from list\nLPOP tasks              # Remove from beginning\nRPOP tasks              # Remove from end\n\n# List length\nLLEN tasks\n\n# Get by index\nLINDEX tasks 0          # First item\nLINDEX tasks -1         # Last item\n\n# Insert before/after\nLINSERT tasks BEFORE \"task2\" \"task1.5\"\n\n# Remove items\nLREM tasks 1 \"task1\"    # Remove 1 occurrence\n\n# Trim list\nLTRIM tasks 0 99        # Keep only first 100 items\n\n# Blocking pop (wait for item)\nBLPOP tasks 5           # Block for 5 seconds\n\n# Use Cases:\n- Message queues\n- Activity feeds\n- Latest items\n- Task lists\n```\n\n### 3. Sets (Unique Values):\n\n```bash\n# Unordered collection of unique strings\n\n# Add to set\nSADD users \"user1\" \"user2\" \"user3\"\n\n# Check membership\nSISMEMBER users \"user1\"     # 1 (exists)\nSISMEMBER users \"user99\"    # 0 (doesn't exist)\n\n# Get all members\nSMEMBERS users\n\n# Set cardinality (count)\nSCARD users                 # 3\n\n# Remove from set\nSREM users \"user2\"\n\n# Random member\nSRANDMEMBER users\nSRANDMEMBER users 2         # Get 2 random members\n\n# Pop random\nSPOP users\n\n# Set operations\nSADD set1 \"a\" \"b\" \"c\"\nSADD set2 \"b\" \"c\" \"d\"\n\n# Union (all unique items)\nSUNION set1 set2            # a, b, c, d\n\n# Intersection (common items)\nSINTER set1 set2            # b, c\n\n# Difference (in set1 but not set2)\nSDIFF set1 set2             # a\n\n# Move between sets\nSMOVE set1 set2 \"a\"\n\n# Use Cases:\n- Tags\n- Unique visitors\n- Following/Followers\n- Voting systems\n```\n\n### 4. Sorted Sets (Sorted by Score):\n\n```bash\n# Set with scores - sorted automatically\n\n# Add with score\nZADD leaderboard 100 \"player1\"\nZADD leaderboard 200 \"player2\"\nZADD leaderboard 150 \"player3\"\n\n# Get by rank (0-based)\nZRANGE leaderboard 0 -1             # All, ascending\nZREVRANGE leaderboard 0 -1          # All, descending\n\n# Get with scores\nZRANGE leaderboard 0 -1 WITHSCORES\n\n# Get rank\nZRANK leaderboard \"player1\"         # Position (0, 1, 2...)\nZREVRANK leaderboard \"player1\"      # Reverse rank\n\n# Get score\nZSCORE leaderboard \"player1\"        # 100\n\n# Increment score\nZINCRBY leaderboard 50 \"player1\"    # Now 150\n\n# Count in range\nZCOUNT leaderboard 100 200          # Count between scores\n\n# Remove by rank\nZREMRANGEBYRANK leaderboard 0 0     # Remove lowest\n\n# Remove by score\nZREMRANGEBYSCORE leaderboard 0 100  # Remove scores 0-100\n\n# Get by score range\nZRANGEBYSCORE leaderboard 100 200\n\n# Use Cases:\n- Leaderboards\n- Priority queues\n- Time-series data\n- Rate limiting (sliding window)\n```\n\n### 5. Hashes (Objects):\n\n```bash\n# Field-value pairs (like objects)\n\n# Set hash field\nHSET user:1 name \"John\"\nHSET user:1 email \"john@example.com\"\nHSET user:1 age 30\n\n# Set multiple fields\nHMSET user:1 name \"John\" email \"john@example.com\" age 30\n\n# Get hash field\nHGET user:1 name            # \"John\"\n\n# Get all fields\nHGETALL user:1\n# name \"John\"\n# email \"john@example.com\"\n# age \"30\"\n\n# Get multiple fields\nHMGET user:1 name email\n\n# Check field exists\nHEXISTS user:1 name         # 1\n\n# Get all keys\nHKEYS user:1                # name, email, age\n\n# Get all values\nHVALS user:1                # John, john@example.com, 30\n\n# Field count\nHLEN user:1                 # 3\n\n# Delete field\nHDEL user:1 age\n\n# Increment field\nHINCRBY user:1 loginCount 1\n\n# Use Cases:\n- User profiles\n- Product details\n- Session data\n- Configuration\n```\n\n### 6. Bitmaps:\n\n```bash\n# String of bits - very memory efficient\n\n# Set bit\nSETBIT visitors:2024-01-15 123 1   # User 123 visited\n\n# Get bit\nGETBIT visitors:2024-01-15 123     # 1\n\n# Count set bits\nBITCOUNT visitors:2024-01-15       # Total unique visitors\n\n# Bit operations\nBITOP AND result key1 key2         # AND operation\nBITOP OR result key1 key2          # OR operation\n\n# Use Cases:\n- Real-time analytics\n- User online tracking\n- Feature flags\n```\n\n### 7. HyperLogLog:\n\n```bash\n# Probabilistic data structure for counting unique items\n# Memory efficient: 12KB to count billions\n\n# Add to HyperLogLog\nPFADD unique:visitors \"user1\" \"user2\" \"user3\"\n\n# Get count (approximate)\nPFCOUNT unique:visitors            # ~3\n\n# Merge multiple HyperLogLogs\nPFMERGE result hll1 hll2\n\n# Use Cases:\n- Unique visitors count\n- Unique page views\n- Cardinality estimation\n```\n\n### 8. Geospatial:\n\n```bash\n# Store and query geographic coordinates\n\n# Add location\nGEOADD locations 13.361389 38.115556 \"Palermo\"\nGEOADD locations 15.087269 37.502669 \"Catania\"\n\n# Get coordinates\nGEOPOS locations \"Palermo\"\n\n# Distance between points\nGEODIST locations \"Palermo\" \"Catania\" km    # 166.2742 km\n\n# Search radius\nGEORADIUS locations 15 37 200 km WITHDIST\n\n# Use Cases:\n- Nearby searches\n- Delivery tracking\n- Location-based services\n```\n\n---\n\n## ðŸ’» Redis with Node.js {#nodejs}\n\n### Installation:\n\n```bash\nnpm install redis\n# or\nnpm install ioredis  # Alternative, feature-rich\n```\n\n### Basic Usage:\n\n```javascript\n// Using 'redis' package\nimport { createClient } from 'redis';\n\n// Create client\nconst client = createClient({\n  host: 'localhost',\n  port: 6379,\n  password: 'yourpassword'  // if required\n});\n\n// Handle errors\nclient.on('error', (err) => {\n  console.error('Redis error:', err);\n});\n\n// Connect\nawait client.connect();\n\n// Set and get\nawait client.set('name', 'John');\nconst name = await client.get('name');\nconsole.log(name);  // John\n\n// Set with expiration (seconds)\nawait client.setEx('session', 3600, 'session_data');\n\n// Increment\nawait client.set('counter', 0);\nawait client.incr('counter');\nconst count = await client.get('counter');  // \"1\"\n\n// Disconnect\nawait client.disconnect();\n```\n\n### Caching Pattern:\n\n```javascript\n// Cache-aside pattern\nasync function getUser(userId) {\n  const cacheKey = `user:${userId}`;\n  \n  // Try cache first\n  const cached = await client.get(cacheKey);\n  \n  if (cached) {\n    console.log('Cache HIT âš¡');\n    return JSON.parse(cached);\n  }\n  \n  console.log('Cache MISS - fetching from database');\n  \n  // Fetch from database\n  const user = await database.users.findById(userId);\n  \n  if (!user) {\n    return null;\n  }\n  \n  // Store in cache for 1 hour\n  await client.setEx(\n    cacheKey,\n    3600,\n    JSON.stringify(user)\n  );\n  \n  return user;\n}\n\n// Usage\nconst user = await getUser(123);\n```\n\n### Complete Express + Redis Example:\n\n```javascript\nimport express from 'express';\nimport { createClient } from 'redis';\n\nconst app = express();\nconst redis = createClient();\n\nawait redis.connect();\n\n// Middleware: Cache middleware\nfunction cache(duration) {\n  return async (req, res, next) => {\n    const key = `cache:${req.url}`;\n    \n    try {\n      const cached = await redis.get(key);\n      \n      if (cached) {\n        return res.json(JSON.parse(cached));\n      }\n      \n      // Store original json method\n      res.originalJson = res.json;\n      \n      // Override json method to cache response\n      res.json = async function(data) {\n        await redis.setEx(key, duration, JSON.stringify(data));\n        res.originalJson(data);\n      };\n      \n      next();\n    } catch (error) {\n      next();\n    }\n  };\n}\n\n// Route with caching (5 minutes)\napp.get('/api/users', cache(300), async (req, res) => {\n  const users = await database.users.findAll();\n  res.json(users);\n});\n\n// Rate limiting\nasync function rateLimiter(req, res, next) {\n  const ip = req.ip;\n  const key = `ratelimit:${ip}`;\n  \n  const requests = await redis.incr(key);\n  \n  if (requests === 1) {\n    await redis.expire(key, 60);  // 60 second window\n  }\n  \n  const limit = 100;\n  const remaining = Math.max(0, limit - requests);\n  \n  res.setHeader('X-RateLimit-Limit', limit);\n  res.setHeader('X-RateLimit-Remaining', remaining);\n  \n  if (requests > limit) {\n    return res.status(429).json({\n      error: 'Too many requests'\n    });\n  }\n  \n  next();\n}\n\napp.use('/api/', rateLimiter);\n\n// Session management\napp.post('/api/login', async (req, res) => {\n  // Authenticate user...\n  const user = { id: 123, email: 'user@example.com' };\n  \n  // Create session\n  const sessionId = crypto.randomUUID();\n  \n  await redis.setEx(\n    `session:${sessionId}`,\n    86400,  // 24 hours\n    JSON.stringify(user)\n  );\n  \n  res.json({ sessionId });\n});\n\napp.get('/api/profile', async (req, res) => {\n  const sessionId = req.headers.authorization;\n  \n  const session = await redis.get(`session:${sessionId}`);\n  \n  if (!session) {\n    return res.status(401).json({ error: 'Unauthorized' });\n  }\n  \n  const user = JSON.parse(session);\n  res.json(user);\n});\n\napp.listen(3000);\n```\n\n---\n\n## ðŸ Redis with Python {#python}\n\n### Installation:\n\n```bash\npip install redis\n# or\npip install redis[hiredis]  # With C parser for better performance\n```\n\n### Basic Usage:\n\n```python\nimport redis\nimport json\n\n# Create client\nr = redis.Redis(\n    host='localhost',\n    port=6379,\n    db=0,\n    decode_responses=True  # Auto-decode bytes to strings\n)\n\n# Test connection\nr.ping()  # True\n\n# Set and get\nr.set('name', 'John')\nname = r.get('name')\nprint(name)  # John\n\n# Set with expiration\nr.setex('session', 3600, 'session_data')\n\n# Increment\nr.set('counter', 0)\nr.incr('counter')\ncount = r.get('counter')  # '1'\n\n# JSON data\nuser = {'id': 1, 'name': 'John', 'email': 'john@example.com'}\nr.set('user:1', json.dumps(user))\nretrieved = json.loads(r.get('user:1'))\n```\n\n### Caching Pattern:\n\n```python\nimport redis\nimport json\nfrom functools import wraps\n\nr = redis.Redis(host='localhost', port=6379, decode_responses=True)\n\ndef cache(expire=300):\n    \"\"\"Cache decorator\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = f\"cache:{func.__name__}:{str(args)}:{str(kwargs)}\"\n            \n            # Try cache\n            cached = r.get(key)\n            if cached:\n                print('Cache HIT âš¡')\n                return json.loads(cached)\n            \n            print('Cache MISS')\n            \n            # Execute function\n            result = func(*args, **kwargs)\n            \n            # Store in cache\n            r.setex(key, expire, json.dumps(result))\n            \n            return result\n        return wrapper\n    return decorator\n\n# Usage\n@cache(expire=60)\ndef get_user(user_id):\n    print(f'Fetching user {user_id} from database...')\n    # Simulate database query\n    return {'id': user_id, 'name': 'John'}\n\n# First call - cache miss\nuser = get_user(123)\n\n# Second call - cache hit\nuser = get_user(123)\n```\n\n### Flask + Redis Example:\n\n```python\nfrom flask import Flask, request, jsonify\nimport redis\nimport json\n\napp = Flask(__name__)\nr = redis.Redis(host='localhost', port=6379, decode_responses=True)\n\n# Caching middleware\ndef cache_response(expire=300):\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            key = f\"cache:{request.url}\"\n            \n            # Try cache\n            cached = r.get(key)\n            if cached:\n                return jsonify(json.loads(cached))\n            \n            # Get response\n            response = f(*args, **kwargs)\n            \n            # Cache response\n            if response[1] == 200:  # Only cache successful responses\n                r.setex(key, expire, json.dumps(response[0]))\n            \n            return jsonify(response[0]), response[1]\n        \n        wrapper.__name__ = f.__name__\n        return wrapper\n    return decorator\n\n@app.route('/api/users')\n@cache_response(expire=60)\ndef get_users():\n    # Fetch from database\n    users = [\n        {'id': 1, 'name': 'John'},\n        {'id': 2, 'name': 'Jane'}\n    ]\n    return users, 200\n\n# Rate limiting\ndef rate_limit(max_requests=100, window=60):\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            ip = request.remote_addr\n            key = f\"ratelimit:{ip}\"\n            \n            requests = r.incr(key)\n            \n            if requests == 1:\n                r.expire(key, window)\n            \n            if requests > max_requests:\n                return jsonify({'error': 'Too many requests'}), 429\n            \n            return f(*args, **kwargs)\n        \n        wrapper.__name__ = f.__name__\n        return wrapper\n    return decorator\n\n@app.route('/api/search')\n@rate_limit(max_requests=10, window=60)\ndef search():\n    return jsonify({'results': []})\n\nif __name__ == '__main__':\n    app.run()\n```\n\n---\n\n## ðŸ“Š Caching Strategies {#caching-strategies}\n\n### 1. Cache-Aside (Lazy Loading):\n\n```\nMost common pattern\n\nRead Flow:\n1. Check cache\n2. If HIT: Return cached data\n3. If MISS: Load from DB â†’ Cache it â†’ Return\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   App   â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚ 1. Get user:123\n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Redis  â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚ 2. Cache MISS\n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Database â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚ 3. Return user data\n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Redis  â”‚ â† 4. Cache for next time\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPros: Only cache what's needed\nCons: Cache misses add latency\n```\n\n```javascript\nasync function getCachedData(key, fetchFunction, ttl = 3600) {\n  // Try cache\n  const cached = await redis.get(key);\n  if (cached) return JSON.parse(cached);\n  \n  // Fetch from source\n  const data = await fetchFunction();\n  \n  // Store in cache\n  await redis.setEx(key, ttl, JSON.stringify(data));\n  \n  return data;\n}\n\n// Usage\nconst user = await getCachedData(\n  'user:123',\n  () => database.users.findById(123),\n  3600\n);\n```\n\n### 2. Write-Through:\n\n```\nWrite to cache AND database simultaneously\n\nWrite Flow:\nApp â†’ Cache â†’ Database (in order)\n\nPros: Cache always in sync\nCons: Write latency, unnecessary caching\n```\n\n```javascript\nasync function saveUser(user) {\n  // Write to cache\n  await redis.setEx(\n    `user:${user.id}`,\n    3600,\n    JSON.stringify(user)\n  );\n  \n  // Write to database\n  await database.users.save(user);\n  \n  return user;\n}\n```\n\n### 3. Write-Behind (Write-Back):\n\n```\nWrite to cache immediately, database later\n\nWrite Flow:\nApp â†’ Cache â†’ Queue â†’ Database (async)\n\nPros: Fast writes, reduced DB load\nCons: Risk of data loss, eventual consistency\n```\n\n### 4. Refresh-Ahead:\n\n```\nAutomatically refresh cache before expiration\n\nGood for: Predictable access patterns\n```\n\n```javascript\nasync function refreshAheadCache(key, fetchFunction, ttl) {\n  const cached = await redis.get(key);\n  const remaining = await redis.ttl(key);\n  \n  // Refresh if less than 10% time remaining\n  if (remaining > 0 && remaining < ttl * 0.1) {\n    // Refresh in background\n    fetchFunction().then(data => {\n      redis.setEx(key, ttl, JSON.stringify(data));\n    });\n  }\n  \n  return cached ? JSON.parse(cached) : await fetchFunction();\n}\n```\n\n---\n\n## ðŸ”„ Redis Pub/Sub {#pubsub}\n\n### Publish-Subscribe Messaging:\n\n```javascript\n// Publisher\nawait redis.publish('notifications', JSON.stringify({\n  type: 'new_message',\n  userId: 123,\n  message: 'Hello!'\n}));\n\n// Subscriber\nconst subscriber = redis.duplicate();\nawait subscriber.connect();\n\nawait subscriber.subscribe('notifications', (message) => {\n  const data = JSON.parse(message);\n  console.log('Received:', data);\n});\n\n// Unsubscribe\nawait subscriber.unsubscribe('notifications');\n```\n\n### Pattern Subscriptions:\n\n```javascript\n// Subscribe to multiple channels with pattern\nawait subscriber.pSubscribe('user:*', (message, channel) => {\n  console.log(`Message from ${channel}:`, message);\n});\n\n// Will receive from:\n// user:123, user:456, etc.\n```\n\n### Real-Time Chat Example:\n\n```javascript\n// Chat server with Redis Pub/Sub\nimport { Server } from 'socket.io';\nimport { createClient } from 'redis';\n\nconst io = new Server(3000);\nconst redis = createClient();\nconst subscriber = redis.duplicate();\n\nawait redis.connect();\nawait subscriber.connect();\n\n// Subscribe to chat channel\nawait subscriber.subscribe('chat', (message) => {\n  const data = JSON.parse(message);\n  \n  // Broadcast to all connected clients\n  io.emit('message', data);\n});\n\n// Handle socket connections\nio.on('connection', (socket) => {\n  socket.on('send_message', async (data) => {\n    // Publish to Redis\n    await redis.publish('chat', JSON.stringify({\n      user: socket.id,\n      message: data.message,\n      timestamp: Date.now()\n    }));\n  });\n});\n```\n\n---\n\n## ðŸŒŠ Redis Streams {#streams}\n\n### Stream Basics:\n\n```bash\n# Add to stream\nXADD mystream * field1 value1 field2 value2\n\n# Read from stream\nXREAD COUNT 10 STREAMS mystream 0\n\n# Consumer groups\nXGROUP CREATE mystream mygroup 0\nXREADGROUP GROUP mygroup consumer1 COUNT 1 STREAMS mystream >\n```\n\n### Node.js Streams Example:\n\n```javascript\n// Producer\nasync function addToStream(data) {\n  await redis.xAdd('events', '*', {\n    type: data.type,\n    payload: JSON.stringify(data.payload),\n    timestamp: Date.now()\n  });\n}\n\n// Consumer with consumer group\nasync function processStream() {\n  // Create consumer group\n  try {\n    await redis.xGroupCreate('events', 'processors', '0', {\n      MKSTREAM: true\n    });\n  } catch (err) {\n    // Group might already exist\n  }\n  \n  // Read messages\n  while (true) {\n    const messages = await redis.xReadGroup('processors', 'worker1', {\n      key: 'events',\n      id: '>'\n    }, {\n      COUNT: 10,\n      BLOCK: 5000\n    });\n    \n    if (messages) {\n      for (const message of messages[0].messages) {\n        const { id, message: data } = message;\n        \n        // Process message\n        await processMessage(data);\n        \n        // Acknowledge\n        await redis.xAck('events', 'processors', id);\n      }\n    }\n  }\n}\n```\n\n---\n\n## ðŸ’¾ Persistence {#persistence}\n\n### RDB (Redis Database):\n\n```bash\n# Point-in-time snapshots\n\n# redis.conf\nsave 900 1      # After 900 sec if 1 key changed\nsave 300 10     # After 300 sec if 10 keys changed\nsave 60 10000   # After 60 sec if 10000 keys changed\n\n# Manual save\nSAVE            # Blocking\nBGSAVE          # Background (non-blocking)\n\n# Pros: Fast recovery, small file\n# Cons: Can lose data between snapshots\n```\n\n### AOF (Append Only File):\n\n```bash\n# Log every write operation\n\n# redis.conf\nappendonly yes\nappendfsync always      # Sync after every write (slow, safe)\nappendfsync everysec    # Sync every second (good balance)\nappendfsync no          # Let OS decide (fast, risky)\n\n# Pros: More durable, readable log\n# Cons: Larger files, slower recovery\n\n# Rewrite AOF (compress)\nBGREWRITEAOF\n```\n\n### Hybrid Approach (Recommended):\n\n```bash\n# Use both RDB + AOF\nsave 900 1\nappendonly yes\nappendfsync everysec\n\n# Best of both worlds:\n# - Fast recovery with RDB\n# - Minimal data loss with AOF\n```\n\n---\n\n## ðŸ”„ Replication & High Availability {#replication}\n\n### Master-Replica Setup:\n\n```bash\n# On master (redis.conf)\nbind 0.0.0.0\nrequirepass masterpassword\n\n# On replica (redis.conf)\nreplicaof master-ip 6379\nmasterauth masterpassword\n\n# Verify replication\nINFO replication\n```\n\n### Redis Sentinel (Auto Failover):\n\n```bash\n# sentinel.conf\nsentinel monitor mymaster 127.0.0.1 6379 2\nsentinel auth-pass mymaster password\nsentinel down-after-milliseconds mymaster 5000\nsentinel parallel-syncs mymaster 1\nsentinel failover-timeout mymaster 10000\n\n# Start sentinel\nredis-sentinel /path/to/sentinel.conf\n\n# Architecture:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Master  â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚ replication\n  â”Œâ”€â”€â”´â”€â”€â”€â”\n  â†“      â†“\nâ”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”\nâ”‚R1 â”‚  â”‚R2 â”‚  â† Replicas\nâ””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜\n  â†‘      â†‘\n  â”‚ monitor\nâ”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”\nâ”‚Sentinels â”‚  â† Auto failover\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸŽ¯ Best Practices {#best-practices}\n\n```markdown\nâœ… DO:\n- Set TTL on all keys\n- Use connection pooling\n- Monitor memory usage\n- Use pipelining for bulk operations\n- Enable persistence for important data\n- Use Redis for cache, not primary DB\n- Implement circuit breakers\n- Monitor slow queries\n\nâŒ DON'T:\n- Store large values (>1MB)\n- Use O(N) commands in production\n- Ignore memory limits\n- Skip monitoring\n- Use as primary database\n- Store critical data without persistence\n```\n\n---\n\n## ðŸ“ˆ Real-World Use Cases {#use-cases}\n\n### 1. Session Store:\n\n```javascript\n// Express session with Redis\nimport session from 'express-session';\nimport RedisStore from 'connect-redis';\n\napp.use(session({\n  store: new RedisStore({ client: redis }),\n  secret: 'your-secret',\n  resave: false,\n  saveUninitialized: false,\n  cookie: {\n    maxAge: 24 * 60 * 60 * 1000  // 24 hours\n  }\n}));\n```\n\n### 2. Leaderboard:\n\n```javascript\n// Real-time game leaderboard\nasync function updateScore(player, score) {\n  await redis.zAdd('leaderboard', { score, value: player });\n}\n\nasync function getTopPlayers(count = 10) {\n  return await redis.zRevRange('leaderboard', 0, count - 1, {\n    WITHSCORES: true\n  });\n}\n\n// Get player rank\nasync function getPlayerRank(player) {\n  return await redis.zRevRank('leaderboard', player);\n}\n```\n\n### 3. Rate Limiter:\n\n```javascript\n// Sliding window rate limiter\nasync function checkRateLimit(userId, limit = 100, window = 60) {\n  const key = `ratelimit:${userId}`;\n  const now = Date.now();\n  \n  // Remove old entries\n  await redis.zRemRangeByScore(key, 0, now - window * 1000);\n  \n  // Count requests in window\n  const count = await redis.zCard(key);\n  \n  if (count >= limit) {\n    return false;  // Rate limit exceeded\n  }\n  \n  // Add current request\n  await redis.zAdd(key, { score: now, value: now });\n  await redis.expire(key, window);\n  \n  return true;\n}\n```\n\n---\n\n## ðŸŽ¤ Interview Preparation {#interview-prep}\n\n### Common Questions:\n\n```\nQ: What is Redis?\nA: In-memory data structure store used as database, \n   cache, and message broker. Supports multiple data \n   types and provides microsecond latency.\n\nQ: Redis vs Memcached?\nA: \nRedis:\n- Multiple data structures\n- Persistence options\n- Pub/Sub, Streams\n- Replication, clustering\n- Single-threaded\n\nMemcached:\n- Only key-value strings\n- No persistence\n- Multi-threaded\n- Simpler, faster for basic caching\n\nQ: How does Redis achieve high performance?\nA:\n1. In-memory storage (RAM)\n2. Single-threaded (no context switching)\n3. Efficient data structures\n4. I/O multiplexing\n5. Optimized networking\n\nQ: Explain Redis persistence options.\nA:\nRDB: Point-in-time snapshots\n- Fast recovery\n- Can lose data between snapshots\n\nAOF: Log every write\n- More durable\n- Larger files\n\nHybrid: Best of both\n\nQ: How to handle Redis memory full?\nA:\n1. Set maxmemory limit\n2. Configure eviction policy:\n   - allkeys-lru (least recently used)\n   - volatile-lru (keys with TTL)\n   - allkeys-lfu (least frequently used)\n   - noeviction (return errors)\n3. Monitor and scale\n\nQ: What is Redis Cluster?\nA: Distributed Redis setup for horizontal scaling\n   and high availability. Data automatically sharded\n   across multiple nodes.\n```\n\n---\n\n## ðŸŽ‰ Congratulations!\n\nYou've completed the **Redis & Caching: Zero to Hero** guide!\n\n**What's Next?**\n1. Set up Redis locally and practice\n2. Implement caching in your projects\n3. Learn Redis Cluster for scaling\n4. Explore Redis modules (JSON, Search, Graph)\n5. Monitor Redis in production\n\n---\n\n*Redis & Caching: Zero to Hero Guide - Complete Edition*\n*Version 1.0 | Created January 2026*\n*Total: 2,500+ lines of Redis mastery!*\n\n**Happy Caching! âš¡**\n"}