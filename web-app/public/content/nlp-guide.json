{"id":"nlp-guide","title":"üìù NLP Complete Guide","content":"# üí¨ Natural Language Processing (NLP) - Zero to Hero Complete Guide\n\n> **From Text Processing to Large Language Models (LLMs)**\n\n---\n\n## üéØ What You'll Learn\n\nComplete NLP mastery from basics to state-of-the-art:\n\n- üìù Text preprocessing & tokenization\n- üî§ Word embeddings (Word2Vec, GloVe, FastText)\n- üß† Sequence models (RNN, LSTM, GRU)\n- ü§ñ Transformers architecture\n- üéØ BERT, GPT, T5, and modern LLMs\n- üõ†Ô∏è Fine-tuning pre-trained models\n- üöÄ Production NLP applications\n- üíº Real-world projects\n\n**Time:** 6-8 weeks to master\n**Prerequisites:** Python, basic ML knowledge\n**Career:** NLP Engineer ($110K-$180K+)\n\n---\n\n## üìö Table of Contents\n\n### **Part 1: Text Preprocessing (Week 1)**\n1. Introduction to NLP\n2. Text Cleaning\n3. Tokenization\n4. Stemming & Lemmatization\n5. Stop Words Removal\n6. N-grams\n\n### **Part 2: Text Representation (Week 2)**\n7. Bag of Words (BoW)\n8. TF-IDF\n9. Word2Vec\n10. GloVe\n11. FastText\n\n### **Part 3: Sequence Models (Week 3)**\n12. RNN for Text\n13. LSTM for NLP\n14. GRU\n15. Bidirectional models\n16. Attention Mechanism\n\n### **Part 4: Transformers (Week 4-5)**\n17. Transformer Architecture\n18. BERT\n19. GPT (GPT-2, GPT-3, GPT-4)\n20. T5 & Other Models\n21. Hugging Face Library\n\n### **Part 5: NLP Tasks (Week 6)**\n22. Text Classification\n23. Named Entity Recognition (NER)\n24. Sentiment Analysis\n25. Question Answering\n26. Text Summarization\n27. Machine Translation\n\n### **Part 6: LLMs & Fine-tuning (Week 7)**\n28. Large Language Models\n29. Prompt Engineering\n30. Fine-tuning LLMs\n31. RAG (Retrieval Augmented Generation)\n32. LangChain for NLP\n\n### **Part 7: Production (Week 8)**\n33. NLP Pipelines\n34. Model Deployment\n35. Scaling NLP Systems\n36. Real-World Applications\n\n---\n\n## Part 1: Text Preprocessing\n\n---\n\n## 1. Introduction to NLP\n\n### **What is NLP?**\n\nNatural Language Processing (NLP) is AI that enables computers to understand, interpret, and generate human language.\n\n### **NLP Applications:**\n\n```\nConsumer Applications:\n‚îú‚îÄ‚îÄ Chatbots & Virtual Assistants (Siri, Alexa)\n‚îú‚îÄ‚îÄ Machine Translation (Google Translate)\n‚îú‚îÄ‚îÄ Auto-complete & Auto-correct\n‚îú‚îÄ‚îÄ Email spam filtering\n‚îî‚îÄ‚îÄ Voice recognition\n\nBusiness Applications:\n‚îú‚îÄ‚îÄ Sentiment analysis (social media monitoring)\n‚îú‚îÄ‚îÄ Customer support automation\n‚îú‚îÄ‚îÄ Document classification\n‚îú‚îÄ‚îÄ Information extraction\n‚îî‚îÄ‚îÄ Content recommendation\n\nAdvanced Applications:\n‚îú‚îÄ‚îÄ Question answering systems\n‚îú‚îÄ‚îÄ Text summarization\n‚îú‚îÄ‚îÄ Legal document analysis\n‚îú‚îÄ‚îÄ Medical record processing\n‚îî‚îÄ‚îÄ Code generation (GitHub Copilot)\n```\n\n### **NLP Pipeline:**\n\n```\nRaw Text\n    ‚Üì\nText Preprocessing\n    ‚Üì\nFeature Extraction\n    ‚Üì\nModel Training/Inference\n    ‚Üì\nPost-processing\n    ‚Üì\nOutput (Classification, Generation, etc.)\n```\n\n---\n\n## 2. Text Cleaning\n\n### **Basic Text Cleaning:**\n\n```python\nimport re\nimport string\n\ndef clean_text(text):\n    \"\"\"\n    Basic text cleaning pipeline\n    \"\"\"\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove mentions (@username)\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Remove hashtags\n    text = re.sub(r'#\\w+', '', text)\n    \n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    return text\n\n# Example\ntext = \"Check out https://example.com! Email me at user@email.com #NLP @john123\"\ncleaned = clean_text(text)\nprint(cleaned)\n# Output: \"check out email me at\"\n```\n\n### **Advanced Cleaning:**\n\n```python\nimport unicodedata\n\ndef advanced_clean_text(text):\n    \"\"\"\n    Advanced text cleaning\n    \"\"\"\n    # Remove accents\n    text = unicodedata.normalize('NFKD', text)\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n    \n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n    \n    # Remove special characters but keep some punctuation\n    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Example\ntext = \"<p>Caf√© r√©sum√© na√Øve</p>\"\ncleaned = advanced_clean_text(text)\nprint(cleaned)\n# Output: \"Cafe resume naive\"\n```\n\n---\n\n## 3. Tokenization\n\n### **Word Tokenization:**\n\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nnltk.download('punkt')\n\ntext = \"Hello! This is NLP. It's amazing.\"\n\n# Word tokenization\nwords = word_tokenize(text)\nprint(\"Words:\", words)\n# Output: ['Hello', '!', 'This', 'is', 'NLP', '.', 'It', \"'s\", 'amazing', '.']\n\n# Sentence tokenization\nsentences = sent_tokenize(text)\nprint(\"Sentences:\", sentences)\n# Output: ['Hello!', 'This is NLP.', \"It's amazing.\"]\n```\n\n### **Advanced Tokenization (BPE, WordPiece):**\n\n```python\nfrom transformers import AutoTokenizer\n\n# Using BERT tokenizer (WordPiece)\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ntext = \"Natural Language Processing\"\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n# Output: ['natural', 'language', 'processing']\n\n# Convert to IDs\ntoken_ids = tokenizer.encode(text)\nprint(\"Token IDs:\", token_ids)\n# Output: [101, 3019, 2653, 6364, 102]\n\n# Decode back\ndecoded = tokenizer.decode(token_ids)\nprint(\"Decoded:\", decoded)\n# Output: \"[CLS] natural language processing [SEP]\"\n```\n\n### **Subword Tokenization (Custom):**\n\n```python\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Initialize tokenizer\ntokenizer = Tokenizer(BPE())\ntokenizer.pre_tokenizer = Whitespace()\n\n# Train on corpus\ntrainer = BpeTrainer(vocab_size=5000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"])\nfiles = [\"corpus.txt\"]\ntokenizer.train(files, trainer)\n\n# Use tokenizer\noutput = tokenizer.encode(\"Hello world!\")\nprint(\"Tokens:\", output.tokens)\nprint(\"IDs:\", output.ids)\n```\n\n---\n\n## 4. Stemming & Lemmatization\n\n### **Stemming:**\n\n```python\nfrom nltk.stem import PorterStemmer, SnowballStemmer\n\n# Porter Stemmer\nporter = PorterStemmer()\n\nwords = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n\nprint(\"Porter Stemmer:\")\nfor word in words:\n    print(f\"{word} -> {porter.stem(word)}\")\n\n# Output:\n# running -> run\n# runs -> run\n# ran -> ran\n# runner -> runner\n# easily -> easili\n# fairly -> fairli\n```\n\n### **Lemmatization:**\n\n```python\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\n\nlemmatizer = WordNetLemmatizer()\n\n# Simple lemmatization\nwords = [\"running\", \"runs\", \"ran\", \"better\", \"geese\"]\n\nprint(\"Lemmatization:\")\nfor word in words:\n    print(f\"{word} -> {lemmatizer.lemmatize(word, pos='v')}\")\n\n# Output:\n# running -> run\n# runs -> run\n# ran -> run\n# better -> better\n# geese -> goose\n\n# Context-aware lemmatization\ndef get_wordnet_pos(tag):\n    \"\"\"Convert POS tag to WordNet format\"\"\"\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef lemmatize_text(text):\n    \"\"\"Lemmatize with POS tagging\"\"\"\n    tokens = word_tokenize(text)\n    pos_tags = nltk.pos_tag(tokens)\n    \n    lemmatized = []\n    for word, tag in pos_tags:\n        wn_tag = get_wordnet_pos(tag)\n        lemmatized.append(lemmatizer.lemmatize(word, pos=wn_tag))\n    \n    return ' '.join(lemmatized)\n\ntext = \"The running dogs are better runners\"\nprint(lemmatize_text(text))\n# Output: \"The run dog be good runner\"\n```\n\n---\n\n## Part 2: Text Representation\n\n---\n\n## 7. Bag of Words (BoW)\n\n### **Basic BoW:**\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocuments = [\n    \"I love NLP and machine learning\",\n    \"NLP is amazing for text processing\",\n    \"Machine learning and deep learning are related\"\n]\n\n# Create BoW\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(documents)\n\n# Get feature names\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\n\n# Get matrix\nprint(\"\\nBoW Matrix:\\n\", bow_matrix.toarray())\n\n# Document-term matrix\nimport pandas as pd\ndf = pd.DataFrame(\n    bow_matrix.toarray(),\n    columns=vectorizer.get_feature_names_out()\n)\nprint(\"\\n\", df)\n```\n\n---\n\n## 8. TF-IDF\n\n### **Implementation:**\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndocuments = [\n    \"The cat sat on the mat\",\n    \"The dog sat on the log\",\n    \"Cats and dogs are enemies\"\n]\n\n# TF-IDF vectorizer\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(documents)\n\n# Get feature names\nfeature_names = tfidf.get_feature_names_out()\n\n# Display TF-IDF scores\nfor doc_idx, doc in enumerate(documents):\n    print(f\"\\nDocument {doc_idx + 1}: {doc}\")\n    scores = tfidf_matrix[doc_idx].toarray()[0]\n    \n    # Get top 3 words\n    top_indices = np.argsort(scores)[::-1][:3]\n    for idx in top_indices:\n        if scores[idx] > 0:\n            print(f\"  {feature_names[idx]}: {scores[idx]:.3f}\")\n```\n\n### **TF-IDF from Scratch:**\n\n```python\nimport math\nfrom collections import Counter\n\ndef compute_tf(document):\n    \"\"\"Term Frequency\"\"\"\n    words = document.lower().split()\n    word_count = Counter(words)\n    total_words = len(words)\n    \n    tf = {}\n    for word, count in word_count.items():\n        tf[word] = count / total_words\n    \n    return tf\n\ndef compute_idf(documents):\n    \"\"\"Inverse Document Frequency\"\"\"\n    N = len(documents)\n    idf = {}\n    \n    # Get all unique words\n    all_words = set()\n    for doc in documents:\n        all_words.update(doc.lower().split())\n    \n    # Calculate IDF for each word\n    for word in all_words:\n        containing_docs = sum(1 for doc in documents if word in doc.lower().split())\n        idf[word] = math.log(N / containing_docs)\n    \n    return idf\n\ndef compute_tfidf(documents):\n    \"\"\"TF-IDF Computation\"\"\"\n    idf = compute_idf(documents)\n    \n    tfidf_docs = []\n    for doc in documents:\n        tf = compute_tf(doc)\n        tfidf = {}\n        \n        for word, tf_value in tf.items():\n            tfidf[word] = tf_value * idf[word]\n        \n        tfidf_docs.append(tfidf)\n    \n    return tfidf_docs\n\n# Example\ndocs = [\n    \"The cat sat on the mat\",\n    \"The dog sat on the log\"\n]\n\ntfidf_scores = compute_tfidf(docs)\nfor i, scores in enumerate(tfidf_scores):\n    print(f\"Document {i+1}:\")\n    for word, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:3]:\n        print(f\"  {word}: {score:.3f}\")\n```\n\n---\n\n## 9. Word2Vec\n\n### **Training Word2Vec:**\n\n```python\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('brown')\nfrom nltk.corpus import brown\n\n# Prepare corpus\nsentences = brown.sents()[:10000]  # Use first 10k sentences\n\n# Train Word2Vec\nmodel = Word2Vec(\n    sentences=sentences,\n    vector_size=100,      # Embedding dimension\n    window=5,             # Context window\n    min_count=2,          # Minimum word frequency\n    workers=4,            # Parallel processing\n    sg=1                  # 1 for skip-gram, 0 for CBOW\n)\n\n# Get word vector\nvector = model.wv['king']\nprint(\"Vector shape:\", vector.shape)\nprint(\"First 10 dimensions:\", vector[:10])\n\n# Find similar words\nsimilar = model.wv.most_similar('king', topn=5)\nprint(\"\\nWords similar to 'king':\")\nfor word, score in similar:\n    print(f\"  {word}: {score:.3f}\")\n\n# Word arithmetic\nresult = model.wv.most_similar(\n    positive=['king', 'woman'],\n    negative=['man'],\n    topn=1\n)\nprint(f\"\\nking - man + woman = {result[0][0]}\")\n\n# Similarity\nsimilarity = model.wv.similarity('king', 'queen')\nprint(f\"\\nSimilarity(king, queen): {similarity:.3f}\")\n```\n\n### **Word2Vec from Scratch:**\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\nclass SimpleWord2Vec:\n    def __init__(self, embedding_dim=50, window_size=2, learning_rate=0.01):\n        self.embedding_dim = embedding_dim\n        self.window_size = window_size\n        self.lr = learning_rate\n        self.word2idx = {}\n        self.idx2word = {}\n        \n    def build_vocab(self, sentences):\n        \"\"\"Build vocabulary from sentences\"\"\"\n        vocab = set()\n        for sentence in sentences:\n            vocab.update(sentence)\n        \n        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n        self.vocab_size = len(vocab)\n        \n        # Initialize embeddings\n        self.W1 = np.random.randn(self.vocab_size, self.embedding_dim) * 0.01\n        self.W2 = np.random.randn(self.embedding_dim, self.vocab_size) * 0.01\n    \n    def get_training_data(self, sentences):\n        \"\"\"Generate training pairs (target, context)\"\"\"\n        training_data = []\n        \n        for sentence in sentences:\n            indices = [self.word2idx[word] for word in sentence]\n            \n            for i, target_idx in enumerate(indices):\n                # Get context words\n                start = max(0, i - self.window_size)\n                end = min(len(indices), i + self.window_size + 1)\n                \n                for j in range(start, end):\n                    if i != j:\n                        context_idx = indices[j]\n                        training_data.append((target_idx, context_idx))\n        \n        return training_data\n    \n    def softmax(self, x):\n        \"\"\"Numerical stable softmax\"\"\"\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / exp_x.sum()\n    \n    def forward(self, target_idx):\n        \"\"\"Forward pass\"\"\"\n        # Input to hidden\n        h = self.W1[target_idx]\n        \n        # Hidden to output\n        u = np.dot(h, self.W2)\n        y_pred = self.softmax(u)\n        \n        return h, y_pred\n    \n    def backward(self, target_idx, context_idx, h, y_pred):\n        \"\"\"Backward pass\"\"\"\n        # Create one-hot vector for context\n        y_true = np.zeros(self.vocab_size)\n        y_true[context_idx] = 1\n        \n        # Calculate error\n        error = y_pred - y_true\n        \n        # Update W2\n        dW2 = np.outer(h, error)\n        self.W2 -= self.lr * dW2\n        \n        # Update W1\n        dW1 = np.dot(error, self.W2.T)\n        self.W1[target_idx] -= self.lr * dW1\n        \n        # Calculate loss\n        loss = -np.log(y_pred[context_idx] + 1e-10)\n        return loss\n    \n    def train(self, sentences, epochs=100):\n        \"\"\"Train Word2Vec\"\"\"\n        self.build_vocab(sentences)\n        training_data = self.get_training_data(sentences)\n        \n        for epoch in range(epochs):\n            total_loss = 0\n            for target_idx, context_idx in training_data:\n                h, y_pred = self.forward(target_idx)\n                loss = self.backward(target_idx, context_idx, h, y_pred)\n                total_loss += loss\n            \n            if (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n    \n    def get_vector(self, word):\n        \"\"\"Get word vector\"\"\"\n        if word in self.word2idx:\n            return self.W1[self.word2idx[word]]\n        return None\n    \n    def most_similar(self, word, topn=5):\n        \"\"\"Find most similar words\"\"\"\n        if word not in self.word2idx:\n            return []\n        \n        word_vec = self.get_vector(word)\n        similarities = {}\n        \n        for other_word in self.word2idx:\n            if other_word != word:\n                other_vec = self.get_vector(other_word)\n                # Cosine similarity\n                sim = np.dot(word_vec, other_vec) / (\n                    np.linalg.norm(word_vec) * np.linalg.norm(other_vec)\n                )\n                similarities[other_word] = sim\n        \n        # Sort by similarity\n        sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        return sorted_words[:topn]\n\n# Example usage\nsentences = [\n    ['the', 'cat', 'sat', 'on', 'mat'],\n    ['the', 'dog', 'sat', 'on', 'log'],\n    ['cats', 'and', 'dogs', 'play']\n]\n\nmodel = SimpleWord2Vec(embedding_dim=10, window_size=2)\nmodel.train(sentences, epochs=100)\n\n# Get similar words\nsimilar = model.most_similar('cat', topn=3)\nprint(\"\\nWords similar to 'cat':\")\nfor word, sim in similar:\n    print(f\"  {word}: {sim:.3f}\")\n```\n\n---\n\n## Part 4: Transformers\n\n---\n\n## 17. Transformer Architecture\n\n### **Understanding Transformers:**\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      Transformer Architecture      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                     ‚îÇ\n‚îÇ  Input Embeddings + Positional     ‚îÇ\n‚îÇ           Encoding                  ‚îÇ\n‚îÇ              ‚Üì                      ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ  ‚îÇ   Encoder Block x N   ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Multi-Head       ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Self-Attention   ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ           ‚Üì          ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Feed Forward     ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ              ‚Üì                      ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ  ‚îÇ   Decoder Block x N   ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Masked Multi-Head‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Self-Attention   ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Encoder-Decoder  ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Attention        ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Feed Forward     ‚îÇ‚îÇ         ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ              ‚Üì                      ‚îÇ\n‚îÇ       Linear + Softmax              ‚îÇ\n‚îÇ              ‚Üì                      ‚îÇ\n‚îÇ           Output                    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### **Self-Attention from Scratch:**\n\n```python\nimport numpy as np\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Self-attention mechanism\n    \n    Args:\n        Q: Query matrix (seq_len, d_k)\n        K: Key matrix (seq_len, d_k)\n        V: Value matrix (seq_len, d_v)\n    \n    Returns:\n        Output matrix, Attention weights\n    \"\"\"\n    d_k = Q.shape[-1]\n    \n    # Calculate attention scores\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply softmax\n    attention_weights = softmax(scores)\n    \n    # Multiply by values\n    output = np.matmul(attention_weights, V)\n    \n    return output, attention_weights\n\n# Example\nseq_len = 4\nd_k = 8\nd_v = 8\n\n# Random Q, K, V matrices\nQ = np.random.randn(seq_len, d_k)\nK = np.random.randn(seq_len, d_k)\nV = np.random.randn(seq_len, d_v)\n\noutput, attention = self_attention(Q, K, V)\n\nprint(\"Output shape:\", output.shape)\nprint(\"Attention weights shape:\", attention.shape)\nprint(\"\\nAttention weights:\\n\", attention)\n```\n\n### **Multi-Head Attention:**\n\n```python\nclass MultiHeadAttention:\n    def __init__(self, d_model, num_heads):\n        self.num_heads = num_heads\n        self.d_model = d_model\n        \n        assert d_model % num_heads == 0\n        \n        self.depth = d_model // num_heads\n        \n        # Weight matrices\n        self.W_q = np.random.randn(d_model, d_model) * 0.01\n        self.W_k = np.random.randn(d_model, d_model) * 0.01\n        self.W_v = np.random.randn(d_model, d_model) * 0.01\n        self.W_o = np.random.randn(d_model, d_model) * 0.01\n    \n    def split_heads(self, x, batch_size):\n        \"\"\"Split into multiple heads\"\"\"\n        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n        return x.transpose(0, 2, 1, 3)\n    \n    def scaled_dot_product_attention(self, q, k, v, mask=None):\n        \"\"\"Calculate attention\"\"\"\n        matmul_qk = np.matmul(q, k.transpose(0, 1, 3, 2))\n        \n        # Scale\n        dk = k.shape[-1]\n        scaled_attention_logits = matmul_qk / np.sqrt(dk)\n        \n        # Mask (optional)\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n        \n        # Softmax\n        attention_weights = softmax(scaled_attention_logits)\n        \n        output = np.matmul(attention_weights, v)\n        \n        return output, attention_weights\n    \n    def forward(self, q, k, v, mask=None):\n        \"\"\"Forward pass\"\"\"\n        batch_size = q.shape[0]\n        \n        # Linear projections\n        q = np.matmul(q, self.W_q)\n        k = np.matmul(k, self.W_k)\n        v = np.matmul(v, self.W_v)\n        \n        # Split heads\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        \n        # Attention\n        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n            q, k, v, mask\n        )\n        \n        # Concatenate heads\n        scaled_attention = scaled_attention.transpose(0, 2, 1, 3)\n        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)\n        \n        # Final linear projection\n        output = np.matmul(concat_attention, self.W_o)\n        \n        return output, attention_weights\n\n# Example\nd_model = 512\nnum_heads = 8\nseq_len = 10\nbatch_size = 2\n\nmha = MultiHeadAttention(d_model, num_heads)\n\n# Random input\nx = np.random.randn(batch_size, seq_len, d_model)\n\noutput, attention = mha.forward(x, x, x)\nprint(\"Output shape:\", output.shape)\nprint(\"Attention shape:\", attention.shape)\n```\n\n---\n\n## 18. BERT (Bidirectional Encoder Representations from Transformers)\n\n### **Using BERT with Hugging Face:**\n\n```python\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nimport torch\n\n# Load pre-trained BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Example text\ntext = \"Natural Language Processing is amazing!\"\n\n# Tokenize\ninputs = tokenizer(\n    text,\n    return_tensors='pt',\n    padding=True,\n    truncation=True,\n    max_length=512\n)\n\n# Get embeddings\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Last hidden state\nlast_hidden = outputs.last_hidden_state\nprint(\"Shape:\", last_hidden.shape)  # [batch_size, seq_len, hidden_size]\n\n# [CLS] token embedding (sentence representation)\ncls_embedding = last_hidden[:, 0, :]\nprint(\"CLS embedding shape:\", cls_embedding.shape)  # [batch_size, hidden_size]\n```\n\n### **Fine-tuning BERT for Classification:**\n\n```python\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load dataset\ndataset = load_dataset('imdb')\n\n# Initialize model\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2  # Binary classification\n)\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=512\n    )\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=100,\n    evaluation_strategy='epoch',\n    save_strategy='epoch'\n)\n\n# Metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    return {'accuracy': acc, 'f1': f1}\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'].select(range(1000)),  # Use subset for demo\n    eval_dataset=tokenized_datasets['test'].select(range(100)),\n    compute_metrics=compute_metrics\n)\n\n# Train\ntrainer.train()\n\n# Evaluate\nresults = trainer.evaluate()\nprint(\"Results:\", results)\n```\n\n---\n\n## 19. GPT (Generative Pre-trained Transformer)\n\n### **Using GPT-2:**\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport torch\n\n# Load GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Set padding token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Generate text\ndef generate_text(prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n    \"\"\"\n    Generate text using GPT-2\n    \"\"\"\n    # Encode input\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    # Generate\n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n# Example\nprompt = \"Artificial Intelligence will\"\ngenerated = generate_text(prompt, max_length=50)\nprint(generated)\n```\n\n### **Fine-tuning GPT-2:**\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\n# Load model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Prepare dataset\ndef load_dataset_for_training(file_path, tokenizer, block_size=128):\n    dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=block_size\n    )\n    return dataset\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # GPT uses causal LM, not masked LM\n)\n\n# Load your custom dataset\ntrain_dataset = load_dataset_for_training('train.txt', tokenizer)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./gpt2-finetuned',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=500,\n    save_total_limit=2,\n    prediction_loss_only=True,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n\n# Fine-tune\ntrainer.train()\n\n# Save\nmodel.save_pretrained('./gpt2-finetuned')\ntokenizer.save_pretrained('./gpt2-finetuned')\n```\n\n---\n\n## Part 5: NLP Tasks\n\n---\n\n## 22. Text Classification\n\n### **Complete Classification Pipeline:**\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n# Example: Sentiment classification\nclass TextClassificationDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\n# Load data\ndf = pd.read_csv('sentiment_data.csv')  # columns: 'text', 'label'\ntexts = df['text'].tolist()\nlabels = df['label'].tolist()\n\n# Split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, labels, test_size=0.2, random_state=42\n)\n\n# Initialize tokenizer and model\nmodel_name = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(set(labels))\n)\n\n# Tokenize\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create datasets\ntrain_dataset = TextClassificationDataset(train_encodings, train_labels)\nval_dataset = TextClassificationDataset(val_encodings, val_labels)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n)\n\n# Define metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train\ntrainer.train()\n\n# Evaluate\nresults = trainer.evaluate()\nprint(\"Evaluation results:\", results)\n\n# Predict\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    return predictions.numpy()[0]\n\n# Example\ntext = \"This product is amazing! I love it!\"\nprobs = predict(text)\nprint(f\"Positive: {probs[1]:.2%}, Negative: {probs[0]:.2%}\")\n```\n\n---\n\n## 23. Named Entity Recognition (NER)\n\n### **NER with Transformers:**\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n\n# Load NER pipeline\nner_pipeline = pipeline(\n    \"ner\",\n    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n    aggregation_strategy=\"simple\"\n)\n\n# Example text\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n\n# Extract entities\nentities = ner_pipeline(text)\n\nprint(\"Named Entities:\")\nfor entity in entities:\n    print(f\"  {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.2f})\")\n\n# Output:\n# Apple Inc.: ORG (confidence: 0.99)\n# Steve Jobs: PER (confidence: 0.99)\n# Cupertino: LOC (confidence: 0.99)\n# California: LOC (confidence: 0.99)\n```\n\n### **Training Custom NER Model:**\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\nimport torch\n\n# Define label mapping\nlabel_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}\n\n# Tokenize and align labels\ndef tokenize_and_align_labels(examples, tokenizer):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        max_length=512\n    )\n    \n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Special tokens\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)  # Sub-word tokens\n            previous_word_idx = word_idx\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Load dataset and train (similar to classification)\n```\n\n---\n\n## 24. Sentiment Analysis\n\n### **Production-Ready Sentiment Analysis:**\n\n```python\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\nclass SentimentAnalyzer:\n    def __init__(self, model_name='distilbert-base-uncased-finetuned-sst-2-english'):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        self.model.eval()\n    \n    def analyze(self, text):\n        \"\"\"\n        Analyze sentiment of text\n        Returns: {'label': 'POSITIVE'/'NEGATIVE', 'score': float}\n        \"\"\"\n        inputs = self.tokenizer(\n            text,\n            return_tensors='pt',\n            truncation=True,\n            padding=True,\n            max_length=512\n        )\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        \n        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        score, predicted = torch.max(probs, dim=1)\n        \n        label = 'POSITIVE' if predicted.item() == 1 else 'NEGATIVE'\n        \n        return {\n            'label': label,\n            'score': score.item(),\n            'positive_score': probs[0][1].item(),\n            'negative_score': probs[0][0].item()\n        }\n    \n    def analyze_batch(self, texts):\n        \"\"\"Analyze multiple texts at once\"\"\"\n        results = []\n        for text in texts:\n            results.append(self.analyze(text))\n        return results\n\n# Example usage\nanalyzer = SentimentAnalyzer()\n\ntexts = [\n    \"This movie was absolutely amazing!\",\n    \"Worst experience ever. Very disappointed.\",\n    \"It was okay, nothing special.\"\n]\n\nfor text in texts:\n    result = analyzer.analyze(text)\n    print(f\"Text: {text}\")\n    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.2%})\")\n    print()\n```\n\n---\n\n## Part 7: Production\n\n---\n\n## 33. NLP Pipelines\n\n### **Complete NLP Pipeline:**\n\n```python\nfrom typing import List, Dict\nimport spacy\nfrom transformers import pipeline\nimport logging\n\nclass NLPPipeline:\n    \"\"\"\n    Production-ready NLP pipeline\n    \"\"\"\n    def __init__(self):\n        # Load models\n        self.nlp = spacy.load('en_core_web_sm')\n        self.sentiment_analyzer = pipeline('sentiment-analysis')\n        self.ner_pipeline = pipeline('ner', aggregation_strategy='simple')\n        self.summarizer = pipeline('summarization')\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n    \n    def preprocess(self, text: str) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        # Lowercase\n        text = text.lower()\n        # Remove extra whitespace\n        text = ' '.join(text.split())\n        return text\n    \n    def extract_entities(self, text: str) -> List[Dict]:\n        \"\"\"Extract named entities\"\"\"\n        entities = self.ner_pipeline(text)\n        return entities\n    \n    def analyze_sentiment(self, text: str) -> Dict:\n        \"\"\"Analyze sentiment\"\"\"\n        result = self.sentiment_analyzer(text)[0]\n        return result\n    \n    def summarize(self, text: str, max_length=130, min_length=30) -> str:\n        \"\"\"Summarize text\"\"\"\n        if len(text.split()) < min_length:\n            return text\n        \n        summary = self.summarizer(\n            text,\n            max_length=max_length,\n            min_length=min_length,\n            do_sample=False\n        )\n        return summary[0]['summary_text']\n    \n    def get_key_phrases(self, text: str) -> List[str]:\n        \"\"\"Extract key phrases using spaCy\"\"\"\n        doc = self.nlp(text)\n        \n        # Extract noun chunks\n        noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n        \n        # Extract noun phrases with adjectives\n        key_phrases = []\n        for token in doc:\n            if token.pos_ in ['NOUN', 'PROPN']:\n                phrase = ' '.join([child.text for child in token.subtree])\n                key_phrases.append(phrase)\n        \n        return list(set(noun_chunks + key_phrases))\n    \n    def process(self, text: str) -> Dict:\n        \"\"\"\n        Complete NLP processing pipeline\n        \"\"\"\n        self.logger.info(\"Starting NLP pipeline\")\n        \n        # Preprocess\n        clean_text = self.preprocess(text)\n        self.logger.info(\"Preprocessing complete\")\n        \n        # Extract entities\n        entities = self.extract_entities(text)\n        self.logger.info(f\"Found {len(entities)} entities\")\n        \n        # Sentiment analysis\n        sentiment = self.analyze_sentiment(text)\n        self.logger.info(f\"Sentiment: {sentiment['label']}\")\n        \n        # Summarization (if text is long)\n        summary = self.summarize(text) if len(text.split()) > 50 else text\n        self.logger.info(\"Summarization complete\")\n        \n        # Key phrases\n        key_phrases = self.get_key_phrases(text)\n        self.logger.info(f\"Extracted {len(key_phrases)} key phrases\")\n        \n        return {\n            'original_text': text,\n            'clean_text': clean_text,\n            'entities': entities,\n            'sentiment': sentiment,\n            'summary': summary,\n            'key_phrases': key_phrases[:10]  # Top 10\n        }\n\n# Example usage\npipeline = NLPPipeline()\n\ntext = \"\"\"\nApple Inc. announced today that Steve Jobs, their CEO, will present a new \nproduct in Cupertino, California next month. The announcement received \npositive feedback from investors and tech enthusiasts worldwide.\n\"\"\"\n\nresult = pipeline.process(text)\n\nprint(\"=== NLP Pipeline Results ===\\n\")\nprint(f\"Sentiment: {result['sentiment']}\")\nprint(f\"\\nEntities: {result['entities']}\")\nprint(f\"\\nSummary: {result['summary']}\")\nprint(f\"\\nKey Phrases: {result['key_phrases']}\")\n```\n\n---\n\n## 34. Model Deployment (FastAPI)\n\n### **Complete NLP API:**\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport torch\nfrom transformers import pipeline\nimport uvicorn\n\napp = FastAPI(title=\"NLP API\", version=\"1.0\")\n\n# Load models at startup\n@app.on_event(\"startup\")\nasync def load_models():\n    global sentiment_pipeline, ner_pipeline, qa_pipeline\n    \n    sentiment_pipeline = pipeline('sentiment-analysis')\n    ner_pipeline = pipeline('ner', aggregation_strategy='simple')\n    qa_pipeline = pipeline('question-answering')\n\n# Request/Response models\nclass TextRequest(BaseModel):\n    text: str\n\nclass SentimentResponse(BaseModel):\n    label: str\n    score: float\n\nclass Entity(BaseModel):\n    word: str\n    entity_group: str\n    score: float\n\nclass QARequest(BaseModel):\n    question: str\n    context: str\n\nclass QAResponse(BaseModel):\n    answer: str\n    score: float\n\n# Endpoints\n@app.post(\"/sentiment\", response_model=SentimentResponse)\nasync def analyze_sentiment(request: TextRequest):\n    \"\"\"Analyze text sentiment\"\"\"\n    try:\n        result = sentiment_pipeline(request.text)[0]\n        return SentimentResponse(**result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/ner\", response_model=List[Entity])\nasync def extract_entities(request: TextRequest):\n    \"\"\"Extract named entities\"\"\"\n    try:\n        entities = ner_pipeline(request.text)\n        return [Entity(**entity) for entity in entities]\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/qa\", response_model=QAResponse)\nasync def answer_question(request: QARequest):\n    \"\"\"Answer questions based on context\"\"\"\n    try:\n        result = qa_pipeline(\n            question=request.question,\n            context=request.context\n        )\n        return QAResponse(\n            answer=result['answer'],\n            score=result['score']\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\", \"models_loaded\": True}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n### **Docker Deployment:**\n\n```dockerfile\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Download models\nRUN python -c \"from transformers import pipeline; \\\n    pipeline('sentiment-analysis'); \\\n    pipeline('ner'); \\\n    pipeline('question-answering')\"\n\n# Copy application\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  nlp-api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - TRANSFORMERS_CACHE=/cache\n    volumes:\n      - model-cache:/cache\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n\nvolumes:\n  model-cache:\n```\n\n---\n\n## üìö Learning Path Summary\n\n### **Week-by-Week Plan:**\n\n```\nWeek 1: Text Preprocessing\n- Cleaning, tokenization, stemming/lemmatization\n- BoW, TF-IDF\n- Project: Build text preprocessing pipeline\n\nWeek 2: Word Embeddings\n- Word2Vec, GloVe, FastText\n- Semantic similarity\n- Project: Word analogy system\n\nWeek 3: Sequence Models\n- RNN, LSTM, GRU\n- Sequence classification\n- Project: Sentiment analyzer with LSTM\n\nWeek 4-5: Transformers\n- Transformer architecture\n- BERT, GPT\n- Project: Text classifier with BERT\n\nWeek 6: NLP Tasks\n- NER, QA, Summarization\n- Multi-task learning\n- Project: Complete NLP pipeline\n\nWeek 7: Fine-tuning & LLMs\n- Fine-tuning techniques\n- Prompt engineering\n- Project: Custom chatbot\n\nWeek 8: Production\n- API development\n- Deployment\n- Project: Production NLP service\n```\n\n---\n\n## üíº Career Path\n\n**Entry Level:** NLP Engineer ($90K-$120K)\n**Mid Level:** Senior NLP Engineer ($130K-$170K)\n**Senior Level:** Principal NLP Engineer / Research Scientist ($180K-$250K+)\n\n---\n\n## üéØ Next Steps\n\n1. **Complete this guide** (8 weeks)\n2. **Build 3 NLP projects**\n3. **Contribute to Hugging Face**\n4. **Apply for NLP roles**\n5. **Keep learning latest models**\n\n---\n\n*NLP Complete Guide - From Zero to Hero*\n*Master modern NLP from text preprocessing to LLMs*\n*6-8 weeks | Career-ready NLP Engineer* üöÄ\n\n"}