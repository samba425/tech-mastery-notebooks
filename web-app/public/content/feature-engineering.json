{"id":"feature-engineering","title":"‚öôÔ∏è Feature Engineering Complete Guide","content":"# Feature Engineering: Zero to Hero Complete Guide\n\n> **Master Feature Engineering from Basics to Advanced - A Complete Learning Path**\n\n---\n\n## üéØ What You'll Learn\n\nBy the end of this guide, you will:\n- ‚úÖ Understand what feature engineering is and why it's crucial\n- ‚úÖ Master all fundamental techniques (missing values, encoding, scaling)\n- ‚úÖ Learn advanced techniques (feature interactions, transformations)\n- ‚úÖ Apply domain-specific feature engineering (text, images, time series)\n- ‚úÖ Build complete end-to-end ML projects\n- ‚úÖ Prepare for data science interviews\n- ‚úÖ Use automated feature engineering tools\n- ‚úÖ Deploy feature engineering pipelines in production\n\n---\n\n## üìö Table of Contents\n\n### Part 1: Foundations (Beginner)\n1. [Introduction to Feature Engineering](#1-introduction-to-feature-engineering)\n2. [Understanding Your Data](#2-understanding-your-data)\n3. [Handling Missing Values](#3-handling-missing-values)\n4. [Handling Outliers](#4-handling-outliers)\n5. [Feature Scaling and Normalization](#5-feature-scaling-and-normalization)\n6. [Encoding Categorical Variables](#6-encoding-categorical-variables)\n\n### Part 2: Intermediate Techniques\n7. [Feature Creation and Extraction](#7-feature-creation-and-extraction)\n8. [Feature Selection Methods](#8-feature-selection-methods)\n9. [Handling Imbalanced Data](#9-handling-imbalanced-data)\n10. [Feature Transformation](#10-feature-transformation)\n11. [Handling Date and Time Features](#11-handling-date-and-time-features)\n12. [Text Feature Engineering](#12-text-feature-engineering)\n\n### Part 3: Advanced Techniques\n13. [Feature Interactions and Polynomial Features](#13-feature-interactions-and-polynomial-features)\n14. [Dimensionality Reduction](#14-dimensionality-reduction)\n15. [Time Series Feature Engineering](#15-time-series-feature-engineering)\n16. [Image Feature Engineering](#16-image-feature-engineering)\n17. [Automated Feature Engineering](#17-automated-feature-engineering)\n18. [Feature Engineering for Deep Learning](#18-feature-engineering-for-deep-learning)\n\n### Part 4: Real-World Applications\n19. [Complete End-to-End Projects](#19-complete-end-to-end-projects)\n20. [Domain-Specific Feature Engineering](#20-domain-specific-feature-engineering)\n21. [Production and Deployment](#21-production-and-deployment)\n22. [Performance Optimization](#22-performance-optimization)\n23. [Interview Preparation](#23-interview-preparation)\n24. [Best Practices and Common Mistakes](#24-best-practices-and-common-mistakes)\n\n---\n\n# Part 1: Foundations (Beginner)\n\n## 1. Introduction to Feature Engineering\n\n### What is Feature Engineering?\n\n**Simple Explanation:** Feature Engineering is the process of transforming raw data into meaningful features that help machine learning models understand patterns better.\n\n**Real-World Analogy:** \n- Think of it like cooking: Raw ingredients (data) need to be cleaned, cut, and prepared (feature engineering) before you can cook a great meal (train a model).\n- It's like translating a book - you're converting data from one form into another that your model can understand.\n\n### Why is Feature Engineering Important?\n\n```python\n# Example: Why feature engineering matters\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Create sample data\nnp.random.seed(42)\nn_samples = 1000\n\n# Original features (hard for model to learn)\ndata = {\n    'birth_year': np.random.randint(1950, 2005, n_samples),\n    'join_year': np.random.randint(2010, 2024, n_samples),\n    'purchases_text': np.random.choice(['low', 'medium', 'high'], n_samples)\n}\n\n# Create target: people born before 1980 and joined before 2015 tend to be loyal\ndata['loyal_customer'] = ((data['birth_year'] < 1980) & (data['join_year'] < 2015)).astype(int)\n\ndf = pd.DataFrame(data)\n\nprint(\"Sample of RAW data:\")\nprint(df.head())\n\n# Model 1: Without feature engineering (just label encoding)\nX_raw = df[['birth_year', 'join_year']].copy()\ny = df['loyal_customer']\n\nX_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=42)\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\nscore1 = accuracy_score(y_test, model1.predict(X_test))\nprint(f\"\\n‚ùå Without Feature Engineering - Accuracy: {score1:.3f}\")\n\n# Model 2: WITH feature engineering\ndf_engineered = df.copy()\ndf_engineered['age'] = 2024 - df_engineered['birth_year']\ndf_engineered['years_as_customer'] = 2024 - df_engineered['join_year']\ndf_engineered['age_group'] = pd.cut(df_engineered['age'], \n                                     bins=[0, 30, 50, 100], \n                                     labels=[0, 1, 2])\ndf_engineered['is_senior'] = (df_engineered['age'] > 60).astype(int)\n\n# Encode purchases\npurchase_map = {'low': 0, 'medium': 1, 'high': 2}\ndf_engineered['purchases_encoded'] = df_engineered['purchases_text'].map(purchase_map)\n\nX_engineered = df_engineered[['age', 'years_as_customer', 'age_group', 'is_senior', 'purchases_encoded']]\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train2, y_train2)\nscore2 = accuracy_score(y_test2, model2.predict(X_test2))\nprint(f\"‚úÖ WITH Feature Engineering - Accuracy: {score2:.3f}\")\nprint(f\"\\nüìà Improvement: {((score2 - score1) / score1 * 100):.1f}%\")\n```\n\n### The Feature Engineering Process\n\n```\nüìä Raw Data\n    ‚Üì\nüîç Understand Data (EDA)\n    ‚Üì\nüßπ Clean Data (Handle Missing, Outliers)\n    ‚Üì\nüî® Transform Features (Scale, Encode)\n    ‚Üì\n‚ú® Create New Features (Combine, Extract)\n    ‚Üì\nüéØ Select Best Features\n    ‚Üì\nü§ñ Train Model\n    ‚Üì\nüìà Evaluate & Iterate\n```\n\n---\n\n## 2. Understanding Your Data\n\nBefore engineering features, you must understand your data deeply.\n\n### Step 1: Load and Inspect Data\n\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = sns.load_dataset('titanic')\n\nprint(\"=\" * 50)\nprint(\"DATA INSPECTION CHECKLIST\")\nprint(\"=\" * 50)\n\n# 1. Shape\nprint(f\"\\n1. Dataset Shape: {df.shape}\")\nprint(f\"   - {df.shape[0]} rows (samples)\")\nprint(f\"   - {df.shape[1]} columns (features)\")\n\n# 2. Column types\nprint(\"\\n2. Column Data Types:\")\nprint(df.dtypes)\n\n# 3. First few rows\nprint(\"\\n3. First 5 Rows:\")\nprint(df.head())\n\n# 4. Statistical summary\nprint(\"\\n4. Statistical Summary:\")\nprint(df.describe())\n\n# 5. Info\nprint(\"\\n5. Dataset Info:\")\nprint(df.info())\n```\n\n### Step 2: Exploratory Data Analysis (EDA)\n\n```python\n# EDA Function\ndef perform_eda(df, target_column=None):\n    \"\"\"\n    Comprehensive EDA function\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"EXPLORATORY DATA ANALYSIS\")\n    print(\"=\" * 70)\n    \n    # 1. Missing values\n    print(\"\\n1. MISSING VALUES:\")\n    missing = df.isnull().sum()\n    missing_percent = (missing / len(df)) * 100\n    missing_df = pd.DataFrame({\n        'Missing_Count': missing,\n        'Percentage': missing_percent\n    }).sort_values('Percentage', ascending=False)\n    print(missing_df[missing_df['Missing_Count'] > 0])\n    \n    # Visualize missing values\n    if missing.sum() > 0:\n        plt.figure(figsize=(12, 4))\n        missing_df[missing_df['Missing_Count'] > 0]['Percentage'].plot(kind='bar')\n        plt.title('Missing Values by Feature')\n        plt.ylabel('Percentage Missing')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n    \n    # 2. Numeric features distribution\n    print(\"\\n2. NUMERIC FEATURES:\")\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    print(f\"Numeric columns: {numeric_cols}\")\n    \n    if len(numeric_cols) > 0:\n        fig, axes = plt.subplots(len(numeric_cols), 2, figsize=(12, 4*len(numeric_cols)))\n        if len(numeric_cols) == 1:\n            axes = axes.reshape(1, -1)\n        \n        for idx, col in enumerate(numeric_cols):\n            # Histogram\n            axes[idx, 0].hist(df[col].dropna(), bins=30, edgecolor='black')\n            axes[idx, 0].set_title(f'{col} - Distribution')\n            axes[idx, 0].set_xlabel(col)\n            axes[idx, 0].set_ylabel('Frequency')\n            \n            # Box plot\n            axes[idx, 1].boxplot(df[col].dropna())\n            axes[idx, 1].set_title(f'{col} - Box Plot')\n            axes[idx, 1].set_ylabel(col)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # 3. Categorical features\n    print(\"\\n3. CATEGORICAL FEATURES:\")\n    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    print(f\"Categorical columns: {categorical_cols}\")\n    \n    for col in categorical_cols[:5]:  # Show first 5\n        print(f\"\\n{col} - Value Counts:\")\n        print(df[col].value_counts())\n        \n        # Visualize\n        plt.figure(figsize=(10, 4))\n        df[col].value_counts().plot(kind='bar')\n        plt.title(f'{col} - Distribution')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n    \n    # 4. Correlations (if target provided)\n    if target_column and target_column in df.columns:\n        print(f\"\\n4. CORRELATION WITH TARGET ({target_column}):\")\n        numeric_df = df[numeric_cols + [target_column]].dropna()\n        correlation = numeric_df.corr()[target_column].sort_values(ascending=False)\n        print(correlation)\n        \n        # Correlation heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', center=0, \n                    fmt='.2f', square=True)\n        plt.title('Correlation Heatmap')\n        plt.tight_layout()\n        plt.show()\n    \n    # 5. Outlier detection summary\n    print(\"\\n5. OUTLIER DETECTION (IQR Method):\")\n    for col in numeric_cols:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.1f}%)\")\n\n# Use the function\nperform_eda(df, target_column='survived')\n```\n\n### Step 3: Understand Feature Types\n\n```python\ndef categorize_features(df):\n    \"\"\"\n    Categorize features into different types\n    \"\"\"\n    feature_types = {\n        'Numerical_Continuous': [],\n        'Numerical_Discrete': [],\n        'Categorical_Nominal': [],\n        'Categorical_Ordinal': [],\n        'DateTime': [],\n        'Text': [],\n        'Boolean': []\n    }\n    \n    for col in df.columns:\n        # DateTime\n        if pd.api.types.is_datetime64_any_dtype(df[col]):\n            feature_types['DateTime'].append(col)\n        \n        # Numerical\n        elif pd.api.types.is_numeric_dtype(df[col]):\n            unique_count = df[col].nunique()\n            if unique_count == 2:\n                feature_types['Boolean'].append(col)\n            elif unique_count < 20:\n                feature_types['Numerical_Discrete'].append(col)\n            else:\n                feature_types['Numerical_Continuous'].append(col)\n        \n        # Categorical\n        elif pd.api.types.is_object_dtype(df[col]):\n            unique_count = df[col].nunique()\n            if unique_count < 10:\n                feature_types['Categorical_Nominal'].append(col)\n            else:\n                feature_types['Text'].append(col)\n    \n    print(\"FEATURE CATEGORIZATION:\")\n    print(\"=\" * 50)\n    for feat_type, cols in feature_types.items():\n        if cols:\n            print(f\"\\n{feat_type}:\")\n            for col in cols:\n                print(f\"  - {col}\")\n    \n    return feature_types\n\n# Categorize features\nfeature_types = categorize_features(df)\n```\n\n---\n\n## 3. Handling Missing Values\n\nMissing values are one of the most common data quality issues you'll encounter.\n\n### Understanding Missing Data Mechanisms\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create examples of different missing mechanisms\n\n# 1. MCAR - Missing Completely at Random\nprint(\"=\" * 60)\nprint(\"1. MCAR - MISSING COMPLETELY AT RANDOM\")\nprint(\"=\" * 60)\nprint(\"The probability of missing is the same for all observations\\n\")\n\nnp.random.seed(42)\ndata_mcar = {\n    'patient_id': range(1, 21),\n    'age': np.random.randint(20, 80, 20),\n    'blood_pressure': np.random.randint(90, 180, 20)\n}\ndf_mcar = pd.DataFrame(data_mcar)\n\n# Randomly introduce 30% missing values (MCAR)\nmissing_mask = np.random.random(20) < 0.3\ndf_mcar.loc[missing_mask, 'blood_pressure'] = np.nan\n\nprint(\"Example: Blood pressure readings missing due to random equipment failures\")\nprint(df_mcar)\nprint(f\"\\nMissing values: {df_mcar['blood_pressure'].isnull().sum()}\")\n\n# 2. MAR - Missing at Random\nprint(\"\\n\" + \"=\" * 60)\nprint(\"2. MAR - MISSING AT RANDOM\")\nprint(\"=\" * 60)\nprint(\"Probability of missing depends on other observed variables\\n\")\n\ndata_mar = {\n    'patient_id': range(1, 21),\n    'age': np.random.randint(20, 80, 20),\n    'income': np.random.randint(20000, 100000, 20)\n}\ndf_mar = pd.DataFrame(data_mar)\n\n# Older people more likely to not report income (MAR - depends on age)\ndf_mar.loc[df_mar['age'] > 60, 'income'] = np.where(\n    np.random.random(sum(df_mar['age'] > 60)) < 0.7, \n    np.nan, \n    df_mar.loc[df_mar['age'] > 60, 'income']\n)\n\nprint(\"Example: Older people less likely to report income\")\nprint(df_mar)\nprint(f\"\\nYounger (<60) missing: {df_mar[df_mar['age'] <= 60]['income'].isnull().sum()}\")\nprint(f\"Older (>60) missing: {df_mar[df_mar['age'] > 60]['income'].isnull().sum()}\")\n\n# 3. MNAR - Missing Not at Random\nprint(\"\\n\" + \"=\" * 60)\nprint(\"3. MNAR - MISSING NOT AT RANDOM\")\nprint(\"=\" * 60)\nprint(\"Probability of missing depends on the value itself\\n\")\n\ndata_mnar = {\n    'patient_id': range(1, 21),\n    'age': np.random.randint(20, 80, 20),\n    'weight': np.random.randint(50, 120, 20)\n}\ndf_mnar = pd.DataFrame(data_mnar)\n\n# People with high weight less likely to report (MNAR - depends on weight itself)\ndf_mnar.loc[df_mnar['weight'] > 100, 'weight'] = np.where(\n    np.random.random(sum(df_mnar['weight'] > 100)) < 0.8,\n    np.nan,\n    df_mnar.loc[df_mnar['weight'] > 100, 'weight']\n)\n\nprint(\"Example: People with higher weight less likely to report it\")\nprint(df_mnar)\n```\n\n### Comprehensive Missing Value Handling\n\n```python\n# Load real dataset\ndf = sns.load_dataset('titanic')\n\nprint(\"MISSING VALUES ANALYSIS\")\nprint(\"=\" * 70)\nprint(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\nprint(\"\\nMissing by column:\")\nprint(df.isnull().sum().sort_values(ascending=False))\n\n# Visualize missing patterns\nimport missingno as msno  # pip install missingno\n\n# Matrix visualization\nmsno.matrix(df)\nplt.title('Missing Data Pattern')\nplt.show()\n\n# Heatmap of missing correlations\nmsno.heatmap(df)\nplt.title('Missing Data Correlation')\nplt.show()\n```\n\n### Method 1: Deletion Techniques\n\n```python\ndef demonstrate_deletion(df):\n    \"\"\"\n    Show different deletion strategies\n    \"\"\"\n    print(\"\\nDELETION STRATEGIES\")\n    print(\"=\" * 70)\n    \n    # Original shape\n    print(f\"Original shape: {df.shape}\")\n    \n    # 1. Drop rows with ANY missing value\n    df_drop_all = df.dropna()\n    print(f\"\\n1. Drop rows with ANY missing: {df_drop_all.shape}\")\n    print(f\"   Lost {len(df) - len(df_drop_all)} rows ({(len(df) - len(df_drop_all))/len(df)*100:.1f}%)\")\n    \n    # 2. Drop rows with missing in specific columns\n    important_cols = ['age', 'embarked']\n    df_drop_specific = df.dropna(subset=important_cols)\n    print(f\"\\n2. Drop rows with missing in {important_cols}: {df_drop_specific.shape}\")\n    print(f\"   Lost {len(df) - len(df_drop_specific)} rows ({(len(df) - len(df_drop_specific))/len(df)*100:.1f}%)\")\n    \n    # 3. Drop rows with more than X missing values\n    threshold = len(df.columns) - 3  # Allow max 3 missing values\n    df_drop_threshold = df.dropna(thresh=threshold)\n    print(f\"\\n3. Drop rows with more than 3 missing: {df_drop_threshold.shape}\")\n    print(f\"   Lost {len(df) - len(df_drop_threshold)} rows ({(len(df) - len(df_drop_threshold))/len(df)*100:.1f}%)\")\n    \n    # 4. Drop columns with too many missing values\n    missing_percent = (df.isnull().sum() / len(df)) * 100\n    cols_to_drop = missing_percent[missing_percent > 50].index.tolist()\n    df_drop_cols = df.drop(columns=cols_to_drop)\n    print(f\"\\n4. Drop columns with >50% missing: {df_drop_cols.shape}\")\n    print(f\"   Dropped columns: {cols_to_drop}\")\n    \n    # Recommendation\n    print(\"\\nüí° RECOMMENDATION:\")\n    print(\"   - Drop rows: When <5% data has missing values\")\n    print(\"   - Drop columns: When >60% values are missing\")\n    print(\"   - Otherwise: Use imputation methods\")\n\ndemonstrate_deletion(df)\n```\n\n### Method 2: Mean/Median/Mode Imputation\n\n```python\ndef demonstrate_simple_imputation(df):\n    \"\"\"\n    Demonstrate simple imputation strategies\n    \"\"\"\n    print(\"\\nSIMPLE IMPUTATION STRATEGIES\")\n    print(\"=\" * 70)\n    \n    df_imputed = df.copy()\n    \n    # 1. Mean imputation for normally distributed data\n    print(\"\\n1. MEAN IMPUTATION (for age):\")\n    print(f\"   Age distribution skewness: {df['age'].skew():.2f}\")\n    print(f\"   (Skewness close to 0 = normal distribution)\")\n    \n    age_mean = df['age'].mean()\n    df_imputed['age_mean'] = df['age'].fillna(age_mean)\n    print(f\"   Original mean: {df['age'].mean():.2f}\")\n    print(f\"   After imputation mean: {df_imputed['age_mean'].mean():.2f}\")\n    print(f\"   Missing filled: {df['age'].isnull().sum()}\")\n    \n    # 2. Median imputation for skewed data\n    print(\"\\n2. MEDIAN IMPUTATION (for fare):\")\n    print(f\"   Fare distribution skewness: {df['fare'].skew():.2f}\")\n    print(f\"   (Skewness far from 0 = skewed distribution)\")\n    \n    fare_median = df['fare'].median()\n    df_imputed['fare_median'] = df['fare'].fillna(fare_median)\n    print(f\"   Original median: {df['fare'].median():.2f}\")\n    print(f\"   Original mean: {df['fare'].mean():.2f}\")\n    print(f\"   After imputation median: {df_imputed['fare_median'].median():.2f}\")\n    \n    # Visualize the difference\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    axes[0].hist(df['fare'].dropna(), bins=50, alpha=0.7, label='Original')\n    axes[0].axvline(fare_median, color='r', linestyle='--', label=f'Median={fare_median:.2f}')\n    axes[0].set_title('Fare Distribution (Skewed)')\n    axes[0].set_xlabel('Fare')\n    axes[0].legend()\n    \n    axes[1].boxplot(df['fare'].dropna())\n    axes[1].set_title('Fare Boxplot (Shows Skewness)')\n    axes[1].set_ylabel('Fare')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 3. Mode imputation for categorical data\n    print(\"\\n3. MODE IMPUTATION (for embarked):\")\n    embarked_mode = df['embarked'].mode()[0]\n    df_imputed['embarked_mode'] = df['embarked'].fillna(embarked_mode)\n    print(f\"   Most common value: {embarked_mode}\")\n    print(f\"   Missing filled: {df['embarked'].isnull().sum()}\")\n    \n    print(\"\\n   Value distribution:\")\n    print(df_imputed['embarked_mode'].value_counts())\n    \n    return df_imputed\n\ndf_imputed = demonstrate_simple_imputation(df)\n```\n\n### Method 3: Advanced Imputation Techniques\n\n```python\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndef advanced_imputation(df):\n    \"\"\"\n    Advanced imputation methods\n    \"\"\"\n    print(\"\\nADVANCED IMPUTATION METHODS\")\n    print(\"=\" * 70)\n    \n    # Prepare data\n    numeric_cols = ['age', 'fare', 'sibsp', 'parch']\n    df_numeric = df[numeric_cols].copy()\n    \n    # 1. KNN Imputation\n    print(\"\\n1. KNN IMPUTATION:\")\n    print(\"   Uses K-Nearest Neighbors to impute missing values\")\n    print(\"   Logic: Find similar passengers and use their values\")\n    \n    knn_imputer = KNNImputer(n_neighbors=5)\n    df_knn = pd.DataFrame(\n        knn_imputer.fit_transform(df_numeric),\n        columns=numeric_cols\n    )\n    print(f\"   Missing values after KNN: {df_knn.isnull().sum().sum()}\")\n    \n    # 2. Iterative Imputation (MICE)\n    print(\"\\n2. ITERATIVE IMPUTATION (MICE):\")\n    print(\"   Multiple Imputation by Chained Equations\")\n    print(\"   Logic: Model each feature with missing values as a function of other features\")\n    \n    iterative_imputer = IterativeImputer(random_state=42, max_iter=10)\n    df_iterative = pd.DataFrame(\n        iterative_imputer.fit_transform(df_numeric),\n        columns=numeric_cols\n    )\n    print(f\"   Missing values after MICE: {df_iterative.isnull().sum().sum()}\")\n    \n    # Compare methods\n    print(\"\\n3. COMPARISON:\")\n    comparison = pd.DataFrame({\n        'Original': df_numeric['age'].iloc[:10],\n        'Mean': df_numeric['age'].fillna(df_numeric['age'].mean()).iloc[:10],\n        'KNN': df_knn['age'].iloc[:10],\n        'MICE': df_iterative['age'].iloc[:10]\n    })\n    print(comparison)\n    \n    # Visualize comparison\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    axes[0].hist(df_numeric['age'].dropna(), bins=30, alpha=0.7, label='Original')\n    axes[0].set_title('Original Age Distribution')\n    axes[0].set_xlabel('Age')\n    \n    axes[1].hist(df_knn['age'], bins=30, alpha=0.7, label='KNN', color='orange')\n    axes[1].set_title('After KNN Imputation')\n    axes[1].set_xlabel('Age')\n    \n    axes[2].hist(df_iterative['age'], bins=30, alpha=0.7, label='MICE', color='green')\n    axes[2].set_title('After MICE Imputation')\n    axes[2].set_xlabel('Age')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df_knn, df_iterative\n\ndf_knn, df_mice = advanced_imputation(df)\n```\n\n### Method 4: Domain-Specific Imputation\n\n```python\ndef domain_specific_imputation(df):\n    \"\"\"\n    Use domain knowledge for imputation\n    \"\"\"\n    print(\"\\nDOMAIN-SPECIFIC IMPUTATION\")\n    print(\"=\" * 70)\n    \n    df_domain = df.copy()\n    \n    # 1. Group-based imputation\n    print(\"\\n1. GROUP-BASED IMPUTATION:\")\n    print(\"   Logic: Fill age based on passenger class (rich people might be older)\")\n    \n    # Calculate median age by class\n    age_by_class = df.groupby('pclass')['age'].median()\n    print(\"\\n   Median age by class:\")\n    print(age_by_class)\n    \n    # Fill missing ages based on class\n    for pclass in df['pclass'].unique():\n        mask = (df['pclass'] == pclass) & (df['age'].isnull())\n        df_domain.loc[mask, 'age'] = age_by_class[pclass]\n    \n    print(f\"\\n   Missing values after group-based imputation: {df_domain['age'].isnull().sum()}\")\n    \n    # 2. Conditional imputation\n    print(\"\\n2. CONDITIONAL IMPUTATION:\")\n    print(\"   Logic: Embarkation port based on fare and class\")\n    \n    # Passengers in First class with high fare likely embarked from C (Cherbourg)\n    mask = (df['embarked'].isnull()) & (df['pclass'] == 1) & (df['fare'] > 75)\n    df_domain.loc[mask, 'embarked'] = 'C'\n    \n    # Fill remaining with mode\n    df_domain['embarked'].fillna(df_domain['embarked'].mode()[0], inplace=True)\n    \n    print(f\"   Missing embarked values: {df_domain['embarked'].isnull().sum()}\")\n    \n    # 3. Create \"missing\" indicator features\n    print(\"\\n3. MISSING INDICATOR FEATURES:\")\n    print(\"   Logic: Sometimes the fact that data is missing is informative!\")\n    \n    df_domain['age_was_missing'] = df['age'].isnull().astype(int)\n    df_domain['cabin_was_missing'] = df['cabin'].isnull().astype(int)\n    \n    print(\"\\n   New indicator features created:\")\n    print(f\"   - age_was_missing: {df_domain['age_was_missing'].sum()} passengers\")\n    print(f\"   - cabin_was_missing: {df_domain['cabin_was_missing'].sum()} passengers\")\n    \n    # Check if missing indicators are predictive\n    print(\"\\n   Survival rate by cabin information:\")\n    print(df_domain.groupby('cabin_was_missing')['survived'].mean())\n    \n    return df_domain\n\ndf_domain = domain_specific_imputation(df)\n```\n\n### Missing Value Handling Decision Tree\n\n```python\ndef missing_value_decision_helper(df, column):\n    \"\"\"\n    Help decide which imputation method to use\n    \"\"\"\n    print(f\"\\nDECISION HELPER FOR: {column}\")\n    print(\"=\" * 70)\n    \n    missing_percent = (df[column].isnull().sum() / len(df)) * 100\n    print(f\"Missing percentage: {missing_percent:.1f}%\")\n    \n    if missing_percent == 0:\n        print(\"‚úÖ No missing values!\")\n        return\n    \n    if missing_percent > 60:\n        print(\"‚ùå RECOMMENDATION: DROP this column (>60% missing)\")\n        return\n    \n    if missing_percent < 5:\n        print(\"üí° RECOMMENDATION: DROP rows with missing values (few missing)\")\n        return\n    \n    # Check data type\n    if pd.api.types.is_numeric_dtype(df[column]):\n        print(\"\\nüìä Numeric column detected\")\n        \n        # Check distribution\n        skewness = df[column].skew()\n        print(f\"Skewness: {skewness:.2f}\")\n        \n        if abs(skewness) < 0.5:\n            print(\"‚úÖ RECOMMENDATION: MEAN imputation (normal distribution)\")\n        else:\n            print(\"‚úÖ RECOMMENDATION: MEDIAN imputation (skewed distribution)\")\n        \n        print(\"üöÄ ADVANCED: Consider KNN or MICE imputation for better results\")\n        \n    else:\n        print(\"\\nüìù Categorical column detected\")\n        unique_values = df[column].nunique()\n        print(f\"Unique values: {unique_values}\")\n        \n        if unique_values < 10:\n            print(\"‚úÖ RECOMMENDATION: MODE imputation\")\n        else:\n            print(\"üí° RECOMMENDATION: Create 'Unknown' category or use predictive models\")\n\n# Test the helper\nfor col in ['age', 'fare', 'embarked', 'deck']:\n    if col in df.columns:\n        missing_value_decision_helper(df, col)\n```\n\n---\n\n## 4. Handling Outliers\n\nOutliers are extreme values that differ significantly from other observations.\n\n### Understanding Outliers\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create dataset with outliers\nnp.random.seed(42)\nnormal_data = np.random.normal(100, 15, 1000)\noutliers = np.array([200, 220, 250, 10, 5])  # Extreme values\ndata_with_outliers = np.concatenate([normal_data, outliers])\n\ndf_outlier = pd.DataFrame({\n    'value': data_with_outliers,\n    'is_outlier': [0]*1000 + [1]*5\n})\n\nprint(\"UNDERSTANDING OUTLIERS\")\nprint(\"=\" * 70)\nprint(f\"\\nDataset statistics:\")\nprint(df_outlier['value'].describe())\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Histogram\naxes[0].hist(df_outlier['value'], bins=50, edgecolor='black')\naxes[0].set_title('Distribution with Outliers')\naxes[0].set_xlabel('Value')\naxes[0].axvline(df_outlier['value'].mean(), color='r', linestyle='--', label='Mean')\naxes[0].axvline(df_outlier['value'].median(), color='g', linestyle='--', label='Median')\naxes[0].legend()\n\n# Box plot\naxes[1].boxplot(df_outlier['value'])\naxes[1].set_title('Box Plot (Shows Outliers as Points)')\naxes[1].set_ylabel('Value')\n\n# Scatter plot\naxes[2].scatter(range(len(df_outlier)), df_outlier['value'], \n                c=df_outlier['is_outlier'], cmap='coolwarm', alpha=0.6)\naxes[2].set_title('Scatter Plot (Red = Outliers)')\naxes[2].set_xlabel('Index')\naxes[2].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()\n```\n\n### Outlier Detection Methods\n\n```python\nfrom scipy import stats\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\n\ndef detect_outliers_multiple_methods(df, column):\n    \"\"\"\n    Detect outliers using multiple methods\n    \"\"\"\n    print(f\"\\nOUTLIER DETECTION FOR: {column}\")\n    print(\"=\" * 70)\n    \n    data = df[column].dropna()\n    \n    # Method 1: IQR Method\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound_iqr = Q1 - 1.5 * IQR\n    upper_bound_iqr = Q3 + 1.5 * IQR\n    \n    outliers_iqr = data[(data < lower_bound_iqr) | (data > upper_bound_iqr)]\n    print(f\"\\n1. IQR METHOD:\")\n    print(f\"   Lower bound: {lower_bound_iqr:.2f}\")\n    print(f\"   Upper bound: {upper_bound_iqr:.2f}\")\n    print(f\"   Outliers detected: {len(outliers_iqr)} ({len(outliers_iqr)/len(data)*100:.1f}%)\")\n    \n    # Method 2: Z-Score Method\n    z_scores = np.abs(stats.zscore(data))\n    outliers_zscore = data[z_scores > 3]\n    print(f\"\\n2. Z-SCORE METHOD:\")\n    print(f\"   Threshold: 3 standard deviations\")\n    print(f\"   Outliers detected: {len(outliers_zscore)} ({len(outliers_zscore)/len(data)*100:.1f}%)\")\n    \n    # Method 3: Modified Z-Score (using median)\n    median = np.median(data)\n    mad = np.median(np.abs(data - median))\n    modified_z_scores = 0.6745 * (data - median) / mad\n    outliers_modified_z = data[np.abs(modified_z_scores) > 3.5]\n    print(f\"\\n3. MODIFIED Z-SCORE METHOD:\")\n    print(f\"   Threshold: 3.5 (using median)\")\n    print(f\"   Outliers detected: {len(outliers_modified_z)} ({len(outliers_modified_z)/len(data)*100:.1f}%)\")\n    \n    # Method 4: Isolation Forest\n    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n    outliers_iso = iso_forest.fit_predict(data.values.reshape(-1, 1))\n    n_outliers_iso = (outliers_iso == -1).sum()\n    print(f\"\\n4. ISOLATION FOREST:\")\n    print(f\"   Contamination: 0.1 (expect 10% outliers)\")\n    print(f\"   Outliers detected: {n_outliers_iso} ({n_outliers_iso/len(data)*100:.1f}%)\")\n    \n    # Visualize all methods\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # IQR\n    axes[0, 0].hist(data, bins=50, alpha=0.7)\n    axes[0, 0].axvline(lower_bound_iqr, color='r', linestyle='--', label='Lower bound')\n    axes[0, 0].axvline(upper_bound_iqr, color='r', linestyle='--', label='Upper bound')\n    axes[0, 0].set_title(f'IQR Method ({len(outliers_iqr)} outliers)')\n    axes[0, 0].legend()\n    \n    # Z-Score\n    axes[0, 1].scatter(range(len(data)), data, c=(z_scores > 3), cmap='coolwarm', alpha=0.6)\n    axes[0, 1].set_title(f'Z-Score Method ({len(outliers_zscore)} outliers)')\n    axes[0, 1].set_ylabel(column)\n    \n    # Modified Z-Score\n    axes[1, 0].scatter(range(len(data)), data, \n                       c=(np.abs(modified_z_scores) > 3.5), \n                       cmap='coolwarm', alpha=0.6)\n    axes[1, 0].set_title(f'Modified Z-Score ({len(outliers_modified_z)} outliers)')\n    axes[1, 0].set_ylabel(column)\n    \n    # Isolation Forest\n    axes[1, 1].scatter(range(len(data)), data, \n                       c=(outliers_iso == -1), \n                       cmap='coolwarm', alpha=0.6)\n    axes[1, 1].set_title(f'Isolation Forest ({n_outliers_iso} outliers)')\n    axes[1, 1].set_ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'iqr': outliers_iqr,\n        'zscore': outliers_zscore,\n        'modified_z': outliers_modified_z,\n        'isolation_forest': outliers_iso\n    }\n\n# Load data and detect outliers\ndf = sns.load_dataset('titanic')\noutliers = detect_outliers_multiple_methods(df, 'fare')\n```\n\n### Outlier Handling Strategies\n\n```python\ndef handle_outliers(df, column, method='cap'):\n    \"\"\"\n    Handle outliers using different strategies\n    \"\"\"\n    print(f\"\\nOUTLIER HANDLING FOR: {column}\")\n    print(\"=\" * 70)\n    \n    df_handled = df.copy()\n    data = df[column].dropna()\n    \n    # Calculate bounds using IQR\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    print(f\"Original data:\")\n    print(f\"  Mean: {data.mean():.2f}\")\n    print(f\"  Median: {data.median():.2f}\")\n    print(f\"  Std: {data.std():.2f}\")\n    print(f\"  Min: {data.min():.2f}\")\n    print(f\"  Max: {data.max():.2f}\")\n    \n    if method == 'remove':\n        # Method 1: Remove outliers\n        print(f\"\\n1. REMOVE OUTLIERS:\")\n        mask = (df_handled[column] >= lower_bound) & (df_handled[column] <= upper_bound)\n        df_handled = df_handled[mask]\n        print(f\"   Rows removed: {len(df) - len(df_handled)}\")\n        print(f\"   New shape: {df_handled.shape}\")\n        \n    elif method == 'cap':\n        # Method 2: Cap outliers (Winsorization)\n        print(f\"\\n2. CAP OUTLIERS (Winsorization):\")\n        df_handled[f'{column}_capped'] = df_handled[column].copy()\n        df_handled.loc[df_handled[f'{column}_capped'] < lower_bound, f'{column}_capped'] = lower_bound\n        df_handled.loc[df_handled[f'{column}_capped'] > upper_bound, f'{column}_capped'] = upper_bound\n        print(f\"   Values capped at: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n        \n        # Show comparison\n        print(f\"\\n   After capping:\")\n        print(f\"   Mean: {df_handled[f'{column}_capped'].mean():.2f}\")\n        print(f\"   Median: {df_handled[f'{column}_capped'].median():.2f}\")\n        print(f\"   Min: {df_handled[f'{column}_capped'].min():.2f}\")\n        print(f\"   Max: {df_handled[f'{column}_capped'].max():.2f}\")\n        \n    elif method == 'log':\n        # Method 3: Log transformation\n        print(f\"\\n3. LOG TRANSFORMATION:\")\n        df_handled[f'{column}_log'] = np.log1p(df_handled[column])  # log1p = log(1+x)\n        print(f\"   Original skewness: {df_handled[column].skew():.2f}\")\n        print(f\"   After log skewness: {df_handled[f'{column}_log'].skew():.2f}\")\n        \n    elif method == 'percentile':\n        # Method 4: Percentile capping\n        print(f\"\\n4. PERCENTILE CAPPING:\")\n        lower_percentile = df_handled[column].quantile(0.01)\n        upper_percentile = df_handled[column].quantile(0.99)\n        df_handled[f'{column}_percentile'] = df_handled[column].clip(lower_percentile, upper_percentile)\n        print(f\"   Capped at: 1st and 99th percentiles\")\n        print(f\"   Range: [{lower_percentile:.2f}, {upper_percentile:.2f}]\")\n    \n    # Visualize comparison\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].boxplot([data, df_handled[f'{column}_{method}' if method in ['cap', 'log', 'percentile'] else column]])\n    axes[0].set_xticklabels(['Original', f'After {method}'])\n    axes[0].set_title(f'Box Plot Comparison')\n    axes[0].set_ylabel(column)\n    \n    axes[1].hist(data, bins=50, alpha=0.5, label='Original')\n    if method in ['cap', 'log', 'percentile']:\n        axes[1].hist(df_handled[f'{column}_{method}'], bins=50, alpha=0.5, label=f'After {method}')\n    axes[1].set_title('Distribution Comparison')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df_handled\n\n# Test different methods\ndf_capped = handle_outliers(df, 'fare', method='cap')\ndf_log = handle_outliers(df, 'fare', method='log')\ndf_percentile = handle_outliers(df, 'fare', method='percentile')\n```\n\n### When to Remove vs. Keep Outliers\n\n```python\ndef outlier_decision_guide(df, column, target=None):\n    \"\"\"\n    Guide to decide what to do with outliers\n    \"\"\"\n    print(f\"\\nOUTLIER DECISION GUIDE FOR: {column}\")\n    print(\"=\" * 70)\n    \n    data = df[column].dropna()\n    \n    # Detect outliers\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers_mask = (data < lower_bound) | (data > upper_bound)\n    n_outliers = outliers_mask.sum()\n    \n    print(f\"\\nOutliers found: {n_outliers} ({n_outliers/len(data)*100:.1f}%)\")\n    \n    # Decision logic\n    print(\"\\nü§î DECISION LOGIC:\")\n    \n    # 1. Check percentage\n    outlier_percent = n_outliers / len(data) * 100\n    if outlier_percent > 10:\n        print(\"‚ùå DON'T REMOVE: Too many outliers (>10%)\")\n        print(\"   ‚Üí Consider these might be valid extreme values\")\n        print(\"   ‚Üí RECOMMENDATION: Use robust methods (cap, log transform)\")\n    else:\n        print(\"‚úÖ Few outliers (<10%)\")\n    \n    # 2. Check if they're errors or valid\n    print(\"\\nüìä OUTLIER VALUES:\")\n    outlier_values = data[outliers_mask].sort_values()\n    print(f\"   Min outliers: {outlier_values.head(3).tolist()}\")\n    print(f\"   Max outliers: {outlier_values.tail(3).tolist()}\")\n    print(\"\\n   ‚ùì QUESTION: Are these values possible/valid in reality?\")\n    print(\"      - If NO (data errors): REMOVE them\")\n    print(\"      - If YES (rare but valid): KEEP or CAP them\")\n    \n    # 3. Check impact on target (if provided)\n    if target is not None and target in df.columns:\n        print(\"\\nüéØ IMPACT ON TARGET:\")\n        outlier_indices = data[outliers_mask].index\n        normal_indices = data[~outliers_mask].index\n        \n        outlier_target_mean = df.loc[outlier_indices, target].mean()\n        normal_target_mean = df.loc[normal_indices, target].mean()\n        \n        print(f\"   Target mean (normal): {normal_target_mean:.3f}\")\n        print(f\"   Target mean (outliers): {outlier_target_mean:.3f}\")\n        \n        if abs(outlier_target_mean - normal_target_mean) > 0.1:\n            print(\"   ‚úÖ Outliers are PREDICTIVE of target - KEEP them!\")\n        else:\n            print(\"   ‚ö†Ô∏è Outliers not very predictive - Safe to remove/cap\")\n    \n    # 4. Domain-specific considerations\n    print(\"\\nüí° DOMAIN CONSIDERATIONS:\")\n    print(\"   Fraud Detection: KEEP outliers (they might be fraud!)\")\n    print(\"   House Prices: CAP extreme values (billion-dollar homes rare)\")\n    print(\"   Medical Data: INVESTIGATE carefully (could be life-threatening)\")\n    print(\"   Sensor Data: REMOVE if physically impossible\")\n    \n    # Final recommendation\n    print(\"\\nüìã FINAL RECOMMENDATION:\")\n    if outlier_percent < 1:\n        print(\"   ‚Üí REMOVE outliers (very few, likely errors)\")\n    elif outlier_percent < 5:\n        print(\"   ‚Üí CAP or TRANSFORM outliers (few, but be careful)\")\n    else:\n        print(\"   ‚Üí KEEP outliers but use ROBUST models\")\n        print(\"      (Random Forest, Gradient Boosting handle outliers well)\")\n\n# Test the guide\noutlier_decision_guide(df, 'fare', target='survived')\noutlier_decision_guide(df, 'age', target='survived')\n```\n\n---\n\n## 5. Feature Scaling and Normalization\n\nFeature scaling is crucial for many machine learning algorithms.\n\n### Why Scaling Matters\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Create dataset with different scales\nnp.random.seed(42)\nn_samples = 1000\n\ndata = {\n    'age': np.random.randint(18, 80, n_samples),  # Range: 18-80\n    'salary': np.random.randint(20000, 200000, n_samples),  # Range: 20K-200K\n    'years_experience': np.random.randint(0, 40, n_samples),  # Range: 0-40\n}\n\n# Target: high salary and experience predict success\ndata['success'] = ((data['salary'] > 100000) & (data['years_experience'] > 10)).astype(int)\n\ndf_scale = pd.DataFrame(data)\n\nprint(\"WHY SCALING MATTERS\")\nprint(\"=\" * 70)\nprint(\"\\nOriginal Data Statistics:\")\nprint(df_scale.describe())\n\nprint(\"\\nüìä PROBLEM: Features have very different scales!\")\nprint(f\"   Age: {df_scale['age'].min()}-{df_scale['age'].max()}\")\nprint(f\"   Salary: {df_scale['salary'].min()}-{df_scale['salary'].max()}\")\nprint(f\"   Experience: {df_scale['years_experience'].min()}-{df_scale['years_experience'].max()}\")\n\n# Train model without scaling\nX = df_scale[['age', 'salary', 'years_experience']]\ny = df_scale['success']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel_no_scale = LogisticRegression(random_state=42, max_iter=1000)\nmodel_no_scale.fit(X_train, y_train)\nscore_no_scale = accuracy_score(y_test, model_no_scale.predict(X_test))\n\nprint(f\"\\n‚ùå Model WITHOUT scaling:\")\nprint(f\"   Accuracy: {score_no_scale:.3f}\")\nprint(f\"   Coefficients: {model_no_scale.coef_[0]}\")\nprint(f\"   Notice: Salary coefficient is tiny because salary values are huge!\")\n\n# Train model WITH scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel_scaled = LogisticRegression(random_state=42, max_iter=1000)\nmodel_scaled.fit(X_train_scaled, y_train)\nscore_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n\nprint(f\"\\n‚úÖ Model WITH scaling:\")\nprint(f\"   Accuracy: {score_scaled:.3f}\")\nprint(f\"   Coefficients: {model_scaled.coef_[0]}\")\nprint(f\"   Notice: All coefficients are comparable now!\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Before scaling\naxes[0].scatter(X_train['salary'], X_train['age'], alpha=0.5)\naxes[0].set_xlabel('Salary (20K-200K)')\naxes[0].set_ylabel('Age (18-80)')\naxes[0].set_title('Before Scaling\\n(Different scales)')\n\n# After scaling\naxes[1].scatter(X_train_scaled[:, 1], X_train_scaled[:, 0], alpha=0.5)\naxes[1].set_xlabel('Salary (scaled)')\naxes[1].set_ylabel('Age (scaled)')\naxes[1].set_title('After Scaling\\n(Same scale)')\n\nplt.tight_layout()\nplt.show()\n```\n\n### Scaling Methods Comprehensive Guide\n\n```python\ndef compare_scaling_methods(df, columns):\n    \"\"\"\n    Compare all scaling methods\n    \"\"\"\n    print(\"\\nCOMPARING SCALING METHODS\")\n    print(\"=\" * 70)\n    \n    df_compare = df[columns].copy()\n    \n    # 1. Standardization (Z-score)\n    print(\"\\n1. STANDARDIZATION (Z-score):\")\n    print(\"   Formula: (x - mean) / std\")\n    print(\"   Result: Mean=0, Std=1\")\n    print(\"   Use when: Normal distribution, distance-based algorithms\")\n    \n    scaler_standard = StandardScaler()\n    df_compare['age_standard'] = scaler_standard.fit_transform(df_compare[['age']])\n    \n    print(f\"\\n   Original age: mean={df_compare['age'].mean():.2f}, std={df_compare['age'].std():.2f}\")\n    print(f\"   Scaled age: mean={df_compare['age_standard'].mean():.2f}, std={df_compare['age_standard'].std():.2f}\")\n    \n    # 2. Min-Max Normalization\n    print(\"\\n2. MIN-MAX NORMALIZATION:\")\n    print(\"   Formula: (x - min) / (max - min)\")\n    print(\"   Result: Range [0, 1]\")\n    print(\"   Use when: Need bounded values, neural networks\")\n    \n    scaler_minmax = MinMaxScaler()\n    df_compare['age_minmax'] = scaler_minmax.fit_transform(df_compare[['age']])\n    \n    print(f\"\\n   Original age: min={df_compare['age'].min():.2f}, max={df_compare['age'].max():.2f}\")\n    print(f\"   Scaled age: min={df_compare['age_minmax'].min():.2f}, max={df_compare['age_minmax'].max():.2f}\")\n    \n    # 3. Robust Scaling\n    print(\"\\n3. ROBUST SCALING:\")\n    print(\"   Formula: (x - median) / IQR\")\n    print(\"   Result: Not affected by outliers\")\n    print(\"   Use when: Data has outliers\")\n    \n    scaler_robust = RobustScaler()\n    df_compare['age_robust'] = scaler_robust.fit_transform(df_compare[['age']])\n    \n    print(f\"\\n   Original age: median={df_compare['age'].median():.2f}\")\n    print(f\"   Scaled age: median={df_compare['age_robust'].median():.2f}\")\n    \n    # 4. Max Abs Scaling\n    from sklearn.preprocessing import MaxAbsScaler\n    print(\"\\n4. MAX ABS SCALING:\")\n    print(\"   Formula: x / max(abs(x))\")\n    print(\"   Result: Range [-1, 1]\")\n    print(\"   Use when: Sparse data, preserving zero entries\")\n    \n    scaler_maxabs = MaxAbsScaler()\n    df_compare['age_maxabs'] = scaler_maxabs.fit_transform(df_compare[['age']])\n    \n    print(f\"\\n   Scaled age: min={df_compare['age_maxabs'].min():.2f}, max={df_compare['age_maxabs'].max():.2f}\")\n    \n    # 5. Log Transformation\n    print(\"\\n5. LOG TRANSFORMATION:\")\n    print(\"   Formula: log(x + 1)\")\n    print(\"   Result: Reduces skewness\")\n    print(\"   Use when: Heavily skewed data\")\n    \n    df_compare['age_log'] = np.log1p(df_compare['age'])\n    \n    print(f\"\\n   Original skewness: {df_compare['age'].skew():.2f}\")\n    print(f\"   After log skewness: {df_compare['age_log'].skew():.2f}\")\n    \n    # Visualize all methods\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    methods = ['age', 'age_standard', 'age_minmax', 'age_robust', 'age_maxabs', 'age_log']\n    titles = ['Original', 'Standardization', 'Min-Max', 'Robust', 'MaxAbs', 'Log']\n    \n    for idx, (col, title) in enumerate(zip(methods, titles)):\n        axes[idx].hist(df_compare[col].dropna(), bins=30, edgecolor='black')\n        axes[idx].set_title(f'{title}\\nmean={df_compare[col].mean():.2f}, std={df_compare[col].std():.2f}')\n        axes[idx].set_xlabel(col)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df_compare\n\n# Compare methods\ndf = sns.load_dataset('titanic')\ndf_scaled = compare_scaling_methods(df, ['age', 'fare'])\n```\n\n### When to Use Which Scaling Method\n\n```python\ndef scaling_decision_tree(df, column, algorithm=None):\n    \"\"\"\n    Decision tree to choose scaling method\n    \"\"\"\n    print(f\"\\nSCALING DECISION FOR: {column}\")\n    print(\"=\" * 70)\n    \n    data = df[column].dropna()\n    \n    # Check distribution\n    skewness = data.skew()\n    print(f\"\\nData Analysis:\")\n    print(f\"  Range: [{data.min():.2f}, {data.max():.2f}]\")\n    print(f\"  Mean: {data.mean():.2f}\")\n    print(f\"  Median: {data.median():.2f}\")\n    print(f\"  Skewness: {skewness:.2f}\")\n    \n    # Check for outliers\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n    outlier_percent = len(outliers) / len(data) * 100\n    print(f\"  Outliers: {len(outliers)} ({outlier_percent:.1f}%)\")\n    \n    # Decision logic\n    print(\"\\nüéØ RECOMMENDATION:\")\n    \n    if abs(skewness) > 1:\n        print(f\"\\n1. HIGH SKEWNESS ({skewness:.2f})\")\n        print(\"   ‚úÖ PRIMARY: Log Transformation or Box-Cox\")\n        print(\"   ‚úÖ ALTERNATIVE: Robust Scaler\")\n        recommended = 'log'\n    elif outlier_percent > 5:\n        print(f\"\\n1. MANY OUTLIERS ({outlier_percent:.1f}%)\")\n        print(\"   ‚úÖ PRIMARY: Robust Scaler\")\n        print(\"   ‚ùå AVOID: Standardization (sensitive to outliers)\")\n        recommended = 'robust'\n    else:\n        print(\"\\n1. NORMAL DISTRIBUTION\")\n        if algorithm:\n            print(f\"\\n2. ALGORITHM: {algorithm}\")\n            if algorithm in ['Linear Regression', 'Logistic Regression', 'SVM', 'KNN']:\n                print(\"   ‚úÖ Standardization (StandardScaler)\")\n                recommended = 'standard'\n            elif algorithm in ['Neural Network', 'CNN']:\n                print(\"   ‚úÖ Min-Max Normalization\")\n                recommended = 'minmax'\n            elif algorithm in ['Random Forest', 'XGBoost', 'Decision Tree']:\n                print(\"   ‚úÖ NO SCALING NEEDED!\")\n                print(\"      Tree-based models are scale-invariant\")\n                recommended = 'none'\n        else:\n            print(\"   ‚úÖ DEFAULT: Standardization\")\n            recommended = 'standard'\n    \n    # Algorithm-specific guide\n    print(\"\\nüìö ALGORITHM GUIDE:\")\n    print(\"   Standardization: Linear/Logistic Regression, SVM, KNN, PCA\")\n    print(\"   Min-Max: Neural Networks, Image data\")\n    print(\"   Robust: Data with outliers\")\n    print(\"   None: Tree-based (RF, XGBoost, Decision Trees)\")\n    \n    return recommended\n\n# Test with different algorithms\nscaling_decision_tree(df, 'fare', algorithm='Linear Regression')\nscaling_decision_tree(df, 'fare', algorithm='Random Forest')\nscaling_decision_tree(df, 'age', algorithm='Neural Network')\n```\n\n### Advanced: Custom Scaling\n\n```python\ndef custom_scaling_examples(df):\n    \"\"\"\n    Advanced custom scaling techniques\n    \"\"\"\n    print(\"\\nADVANCED CUSTOM SCALING\")\n    print(\"=\" * 70)\n    \n    df_custom = df[['age', 'fare']].copy().dropna()\n    \n    # 1. Unit Vector Scaling (L2 Normalization)\n    print(\"\\n1. UNIT VECTOR SCALING (L2):\")\n    print(\"   Use: When direction matters more than magnitude\")\n    print(\"   Example: Text data, recommendation systems\")\n    \n    from sklearn.preprocessing import Normalizer\n    normalizer = Normalizer(norm='l2')\n    df_custom['age_l2'] = normalizer.fit_transform(df_custom[['age']])[:, 0]\n    \n    print(f\"   Original norm: {np.linalg.norm(df_custom['age']):.2f}\")\n    print(f\"   After L2 norm: {np.linalg.norm(df_custom['age_l2']):.2f}\")\n    \n    # 2. Quantile Transformation\n    print(\"\\n2. QUANTILE TRANSFORMATION:\")\n    print(\"   Use: Make data follow uniform or normal distribution\")\n    print(\"   Example: Heavily skewed data\")\n    \n    from sklearn.preprocessing import QuantileTransformer\n    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n    df_custom['fare_quantile'] = qt.fit_transform(df_custom[['fare']])\n    \n    print(f\"   Original skewness: {df_custom['fare'].skew():.2f}\")\n    print(f\"   After quantile skewness: {df_custom['fare_quantile'].skew():.2f}\")\n    \n    # 3. Power Transformation (Box-Cox, Yeo-Johnson)\n    print(\"\\n3. POWER TRANSFORMATION:\")\n    print(\"   Use: Make data more Gaussian-like\")\n    print(\"   Example: Positive skewed data\")\n    \n    from sklearn.preprocessing import PowerTransformer\n    pt = PowerTransformer(method='yeo-johnson')  # Works with negative values\n    df_custom['fare_power'] = pt.fit_transform(df_custom[['fare']])\n    \n    print(f\"   Original skewness: {df_custom['fare'].skew():.2f}\")\n    print(f\"   After power skewness: {df_custom['fare_power'].skew():.2f}\")\n    \n    # Visualize\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    # Row 1: Original and transformations\n    axes[0, 0].hist(df_custom['fare'], bins=50)\n    axes[0, 0].set_title(f'Original Fare\\nSkew={df_custom[\"fare\"].skew():.2f}')\n    \n    axes[0, 1].hist(df_custom['fare_quantile'], bins=50)\n    axes[0, 1].set_title(f'Quantile Transform\\nSkew={df_custom[\"fare_quantile\"].skew():.2f}')\n    \n    axes[0, 2].hist(df_custom['fare_power'], bins=50)\n    axes[0, 2].set_title(f'Power Transform\\nSkew={df_custom[\"fare_power\"].skew():.2f}')\n    \n    # Row 2: Q-Q plots\n    from scipy import stats\n    for idx, col in enumerate(['fare', 'fare_quantile', 'fare_power']):\n        stats.probplot(df_custom[col], dist=\"norm\", plot=axes[1, idx])\n        axes[1, idx].set_title(f'Q-Q Plot: {col}')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df_custom\n\ndf_custom = custom_scaling_examples(df)\n```\n\n---\n\n## 6. Encoding Categorical Variables\n\nConverting categorical data into numerical format is essential for ML models.\n\n### Understanding Categorical Data Types\n\n```python\nimport pandas as pd\nimport numpy as np\n\nprint(\"TYPES OF CATEGORICAL DATA\")\nprint(\"=\" * 70)\n\n# 1. Nominal: No order\nprint(\"\\n1. NOMINAL (No natural order):\")\nprint(\"   Examples:\")\nnominal_examples = {\n    'colors': ['Red', 'Blue', 'Green', 'Yellow'],\n    'cities': ['New York', 'London', 'Tokyo', 'Paris'],\n    'animals': ['Dog', 'Cat', 'Bird', 'Fish']\n}\nfor name, values in nominal_examples.items():\n    print(f\"   - {name}: {values}\")\n\nprint(\"\\n   ‚ùå BAD: Encoding as 1,2,3,4 (implies order!)\")\nprint(\"   ‚úÖ GOOD: One-Hot Encoding\")\n\n# 2. Ordinal: Has order\nprint(\"\\n2. ORDINAL (Natural order exists):\")\nprint(\"   Examples:\")\nordinal_examples = {\n    'education': ['High School', 'Bachelor', 'Master', 'PhD'],\n    'size': ['Small', 'Medium', 'Large', 'XL'],\n    'rating': ['Poor', 'Fair', 'Good', 'Excellent']\n}\nfor name, values in ordinal_examples.items():\n    print(f\"   - {name}: {values}\")\n\nprint(\"\\n   ‚úÖ GOOD: Ordinal Encoding (preserve order)\")\nprint(\"   Example: Poor=1, Fair=2, Good=3, Excellent=4\")\n\n# 3. Binary: Only two values\nprint(\"\\n3. BINARY (Two values only):\")\nprint(\"   Examples:\")\nbinary_examples = {\n    'gender': ['Male', 'Female'],\n    'subscribed': ['Yes', 'No'],\n    'passed': ['True', 'False']\n}\nfor name, values in binary_examples.items():\n    print(f\"   - {name}: {values}\")\n\nprint(\"\\n   ‚úÖ GOOD: Label Encoding (0, 1)\")\n\n# 4. High Cardinality: Many unique values\nprint(\"\\n4. HIGH CARDINALITY (Many unique values):\")\nprint(\"   Examples:\")\nprint(\"   - Country (195 countries)\")\nprint(\"   - ZIP code (40,000+ codes)\")\nprint(\"   - Product ID (millions)\")\n\nprint(\"\\n   ‚ùå BAD: One-Hot Encoding (too many columns!)\")\nprint(\"   ‚úÖ GOOD: Target Encoding, Frequency Encoding, or Embedding\")\n```\n\n### Encoding Methods Comprehensive Guide\n\n```python\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\nimport category_encoders as ce  # pip install category_encoders\n\ndef demonstrate_all_encoding_methods(df):\n    \"\"\"\n    Comprehensive demonstration of encoding methods\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ENCODING METHODS COMPREHENSIVE GUIDE\")\n    print(\"=\" * 70)\n    \n    df_encoded = df[['class', 'embarked', 'sex', 'survived']].copy().dropna()\n    \n    # METHOD 1: Label Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"1. LABEL ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Binary or ordinal categories\")\n    print(\"Pros: Simple, memory efficient\")\n    print(\"Cons: Implies order (bad for nominal data)\")\n    \n    le = LabelEncoder()\n    df_encoded['sex_label'] = le.fit_transform(df_encoded['sex'])\n    \n    print(\"\\nExample (sex):\")\n    print(df_encoded[['sex', 'sex_label']].drop_duplicates())\n    \n    # METHOD 2: One-Hot Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"2. ONE-HOT ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Nominal categories with few unique values (<15)\")\n    print(\"Pros: No false ordering, works with any ML algorithm\")\n    print(\"Cons: Many columns if high cardinality\")\n    \n    # Using pandas\n    df_onehot = pd.get_dummies(df_encoded[['embarked']], prefix='port', drop_first=False)\n    print(\"\\nExample (embarked) - Creates 3 binary columns:\")\n    print(df_onehot.head())\n    \n    # Using sklearn\n    ohe = OneHotEncoder(sparse=False, drop='first')  # drop_first avoids multicollinearity\n    embarked_ohe = ohe.fit_transform(df_encoded[['embarked']])\n    print(f\"\\nWith drop_first=True: Creates {embarked_ohe.shape[1]} columns instead of 3\")\n    print(\"(Avoids dummy variable trap in linear models)\")\n    \n    # METHOD 3: Ordinal Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"3. ORDINAL ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Categories have natural order\")\n    print(\"Pros: Preserves order information\")\n    print(\"Cons: Assumes equal spacing between categories\")\n    \n    # Define order\n    class_order = [['Third', 'Second', 'First']]\n    ordinal_enc = OrdinalEncoder(categories=class_order)\n    df_encoded['class_ordinal'] = ordinal_enc.fit_transform(df_encoded[['class']])\n    \n    print(\"\\nExample (class with custom order):\")\n    print(df_encoded[['class', 'class_ordinal']].drop_duplicates().sort_values('class_ordinal'))\n    \n    # METHOD 4: Frequency Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"4. FREQUENCY ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Frequency is meaningful\")\n    print(\"Pros: Single column, captures popularity\")\n    print(\"Cons: Different categories might have same frequency\")\n    \n    freq_encoding = df_encoded['embarked'].value_counts(normalize=True).to_dict()\n    df_encoded['embarked_freq'] = df_encoded['embarked'].map(freq_encoding)\n    \n    print(\"\\nExample (embarked):\")\n    print(df_encoded[['embarked', 'embarked_freq']].drop_duplicates().sort_values('embarked_freq', ascending=False))\n    \n    # METHOD 5: Target Encoding (Mean Encoding)\n    print(\"\\n\" + \"=\"*50)\n    print(\"5. TARGET ENCODING\")\n    print(\"=\"*50)\n    print(\"When: High cardinality, target is available\")\n    print(\"Pros: Powerful, captures target relationship\")\n    print(\"Cons: Risk of overfitting, needs regularization\")\n    \n    target_enc = ce.TargetEncoder(cols=['embarked'])\n    df_encoded['embarked_target'] = target_enc.fit_transform(\n        df_encoded[['embarked']], \n        df_encoded['survived']\n    )\n    \n    print(\"\\nExample (embarked vs survived):\")\n    result = df_encoded.groupby('embarked').agg({\n        'survived': 'mean',\n        'embarked_target': 'first'\n    }).round(3)\n    print(result)\n    print(\"\\nNote: Higher encoding = higher survival rate for that port\")\n    \n    # METHOD 6: Binary Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"6. BINARY ENCODING\")\n    print(\"=\"*50)\n    print(\"When: High cardinality (reduces dimensions)\")\n    print(\"Pros: Fewer columns than one-hot\")\n    print(\"Cons: Harder to interpret\")\n    \n    binary_enc = ce.BinaryEncoder(cols=['embarked'])\n    df_binary = binary_enc.fit_transform(df_encoded[['embarked']])\n    \n    print(\"\\nExample (embarked):\")\n    print(f\"Original unique values: {df_encoded['embarked'].nunique()}\")\n    print(f\"Binary encoding columns: {df_binary.shape[1]}\")\n    print(df_binary.head())\n    \n    # METHOD 7: Hash Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"7. HASH ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Very high cardinality, memory constraints\")\n    print(\"Pros: Fixed number of features, handles unseen categories\")\n    print(\"Cons: Hash collisions possible\")\n    \n    hash_enc = ce.HashingEncoder(cols=['embarked'], n_components=4)\n    df_hash = hash_enc.fit_transform(df_encoded[['embarked']])\n    \n    print(\"\\nExample (embarked into 4 dimensions):\")\n    print(df_hash.head())\n    \n    # METHOD 8: Leave-One-Out Encoding\n    print(\"\\n\" + \"=\"*50)\n    print(\"8. LEAVE-ONE-OUT ENCODING\")\n    print(\"=\"*50)\n    print(\"When: Similar to target encoding but less overfitting\")\n    print(\"Pros: Reduces overfitting vs target encoding\")\n    print(\"Cons: Computationally expensive\")\n    \n    loo_enc = ce.LeaveOneOutEncoder(cols=['embarked'])\n    df_encoded['embarked_loo'] = loo_enc.fit_transform(\n        df_encoded[['embarked']], \n        df_encoded['survived']\n    )\n    \n    print(\"\\nExample (embarked):\")\n    print(df_encoded[['embarked', 'embarked_target', 'embarked_loo']].groupby('embarked').mean())\n    \n    return df_encoded\n\n# Run comprehensive demonstration\ndf = sns.load_dataset('titanic')\ndf_encoded_full = demonstrate_all_encoding_methods(df)\n```\n\n### Encoding Decision Helper\n\n```python\ndef encoding_decision_helper(df, column, target=None, ml_algorithm=None):\n    \"\"\"\n    Help choose the best encoding method\n    \"\"\"\n    print(f\"\\nENCODING DECISION FOR: {column}\")\n    print(\"=\" * 70)\n    \n    # Analyze the column\n    n_unique = df[column].nunique()\n    n_samples = len(df[column].dropna())\n    cardinality_ratio = n_unique / n_samples\n    \n    print(f\"\\nColumn Analysis:\")\n    print(f\"  Unique values: {n_unique}\")\n    print(f\"  Total samples: {n_samples}\")\n    print(f\"  Cardinality ratio: {cardinality_ratio:.3f}\")\n    print(f\"\\n  Top 5 values:\")\n    print(df[column].value_counts().head())\n    \n    # Decision tree\n    print(\"\\nüéØ RECOMMENDATION:\")\n    \n    # Binary\n    if n_unique == 2:\n        print(\"\\n‚úÖ BINARY COLUMN\")\n        print(\"   ‚Üí Use LABEL ENCODING (0, 1)\")\n        return 'label'\n    \n    # Low cardinality\n    elif n_unique <= 10:\n        print(f\"\\n‚úÖ LOW CARDINALITY ({n_unique} categories)\")\n        \n        # Check if ordinal\n        print(\"\\n   ‚ùì QUESTION: Do these categories have a natural order?\")\n        print(\"      Examples of ordinal: [Small, Medium, Large], [Low, High]\")\n        print(\"\\n   If YES ‚Üí Use ORDINAL ENCODING\")\n        print(\"   If NO  ‚Üí Use ONE-HOT ENCODING\")\n        \n        if ml_algorithm in ['Linear Regression', 'Logistic Regression']:\n            print(\"\\n   ‚ö†Ô∏è  For linear models: Use drop_first=True to avoid multicollinearity\")\n        \n        return 'onehot or ordinal'\n    \n    # Medium cardinality\n    elif n_unique <= 50:\n        print(f\"\\n‚ö†Ô∏è  MEDIUM CARDINALITY ({n_unique} categories)\")\n        print(\"   ‚Üí One-hot will create many columns!\")\n        \n        if target is not None:\n            print(\"\\n   ‚úÖ BEST: TARGET ENCODING (since target is available)\")\n            print(\"      Or try: Frequency Encoding, Binary Encoding\")\n            return 'target'\n        else:\n            print(\"\\n   ‚úÖ BEST: FREQUENCY ENCODING or BINARY ENCODING\")\n            return 'frequency or binary'\n    \n    # High cardinality\n    else:\n        print(f\"\\n‚ùå HIGH CARDINALITY ({n_unique} categories)\")\n        print(\"   ‚Üí One-hot encoding will explode dimensions!\")\n        \n        if target is not None:\n            print(\"\\n   ‚úÖ BEST OPTIONS:\")\n            print(\"      1. TARGET ENCODING (captures target relationship)\")\n            print(\"      2. FREQUENCY ENCODING (simple, effective)\")\n            print(\"      3. Embeddings (for deep learning)\")\n            return 'target or frequency'\n        else:\n            print(\"\\n   ‚úÖ BEST OPTIONS:\")\n            print(\"      1. FREQUENCY ENCODING\")\n            print(\"      2. HASH ENCODING\")\n            print(\"      3. Drop column if not important\")\n            return 'frequency or hash'\n    \n    # Algorithm-specific advice\n    if ml_algorithm:\n        print(f\"\\nüí° ALGORITHM-SPECIFIC ADVICE ({ml_algorithm}):\")\n        if ml_algorithm in ['Random Forest', 'XGBoost', 'LightGBM']:\n            print(\"   ‚Üí Tree-based models handle label encoding well\")\n            print(\"   ‚Üí Can use label encoding even for nominal data\")\n        elif ml_algorithm in ['Linear Regression', 'Logistic Regression']:\n            print(\"   ‚Üí Use one-hot encoding for nominal data\")\n            print(\"   ‚Üí Must use drop_first=True\")\n        elif ml_algorithm in ['Neural Network']:\n            print(\"   ‚Üí Use embeddings for high cardinality\")\n            print(\"   ‚Üí One-hot for low cardinality\")\n\n# Test the helper\nencoding_decision_helper(df, 'embarked', target='survived', ml_algorithm='Logistic Regression')\nencoding_decision_helper(df, 'deck', target='survived', ml_algorithm='Random Forest')\n```\n\n### Handling Unseen Categories\n\n```python\ndef handle_unseen_categories():\n    \"\"\"\n    Handle categories in test set not seen in training\n    \"\"\"\n    print(\"\\nHANDLING UNSEEN CATEGORIES\")\n    print(\"=\" * 70)\n    \n    # Create train/test split with unseen categories\n    from sklearn.model_selection import train_test_split\n    \n    df = sns.load_dataset('titanic')\n    df_clean = df[['embarked', 'survived']].dropna()\n    \n    # Artificially create unseen category scenario\n    train_df = df_clean[df_clean['embarked'] != 'Q'].copy()\n    test_df = df_clean[df_clean['embarked'] == 'Q'].copy()\n    \n    print(f\"Train set embarked values: {train_df['embarked'].unique()}\")\n    print(f\"Test set embarked values: {test_df['embarked'].unique()}\")\n    print(\"\\n'Q' is UNSEEN in training!\")\n    \n    # METHOD 1: Use handle_unknown parameter\n    print(\"\\n1. SCIKIT-LEARN handle_unknown='ignore':\")\n    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n    ohe.fit(train_df[['embarked']])\n    \n    test_encoded = ohe.transform(test_df[['embarked']])\n    print(f\"   Test encoded (all zeros for unseen): {test_encoded[0]}\")\n    \n    # METHOD 2: Add 'Unknown' category during training\n    print(\"\\n2. ADD 'Unknown' CATEGORY:\")\n    train_df_mod = train_df.copy()\n    train_df_mod.loc[len(train_df_mod)] = ['Unknown', 0]  # Add unknown category\n    \n    ohe2 = OneHotEncoder(sparse=False)\n    ohe2.fit(train_df_mod[['embarked']])\n    \n    test_df_mod = test_df.copy()\n    test_df_mod['embarked'] = 'Unknown'  # Map unseen to unknown\n    test_encoded2 = ohe2.transform(test_df_mod[['embarked']])\n    print(f\"   Unseen category mapped to 'Unknown' column\")\n    \n    # METHOD 3: Use most frequent category\n    print(\"\\n3. MAP TO MOST FREQUENT:\")\n    most_frequent = train_df['embarked'].mode()[0]\n    test_df_mod2 = test_df.copy()\n    test_df_mod2['embarked'] = test_df_mod2['embarked'].apply(\n        lambda x: x if x in train_df['embarked'].values else most_frequent\n    )\n    print(f\"   Unseen 'Q' mapped to most frequent: '{most_frequent}'\")\n    \n    # Best practice\n    print(\"\\nüí° BEST PRACTICE:\")\n    print(\"   Always use handle_unknown='ignore' or add 'Unknown' category\")\n    print(\"   NEVER let your pipeline fail on unseen categories in production!\")\n\nhandle_unseen_categories()\n```\n\n---\n\n*Due to length constraints, I'll continue with the remaining sections in the next part. Would you like me to continue with Parts 2-4 (Intermediate and Advanced techniques)?*\n\n---\n\n## Quick Reference: Encoding Decision Matrix\n\n| Cardinality | Type | Best Method | Alternative |\n|-------------|------|-------------|-------------|\n| Binary (2) | Any | Label Encoding | - |\n| Low (3-10) | Nominal | One-Hot | Binary Encoding |\n| Low (3-10) | Ordinal | Ordinal Encoding | - |\n| Medium (11-50) | Any | Target Encoding | Frequency, Binary |\n| High (>50) | Any | Target/Frequency | Hash, Embeddings |\n\n---\n\n# Part 2: Intermediate Techniques\n\n## 7. Feature Creation and Extraction\n\nFeature creation is where you add domain knowledge and creativity to improve model performance.\n\n### Mathematical Operations\n\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndf = sns.load_dataset('titanic')\n\nprint(\"MATHEMATICAL FEATURE CREATION\")\nprint(\"=\" * 70)\n\n# 1. Arithmetic Operations\nprint(\"\\n1. ARITHMETIC OPERATIONS:\")\ndf['family_size'] = df['sibsp'] + df['parch'] + 1\ndf['fare_per_person'] = df['fare'] / df['family_size']\ndf['age_fare_interaction'] = df['age'] * df['fare']\n\nprint(\"Created features:\")\nprint(f\"  - family_size = siblings + parents + 1\")\nprint(f\"  - fare_per_person = fare / family_size\")\nprint(f\"  - age_fare_interaction = age √ó fare\")\n\nprint(df[['sibsp', 'parch', 'family_size', 'fare', 'fare_per_person']].head())\n\n# 2. Ratios and Proportions\nprint(\"\\n2. RATIOS AND PROPORTIONS:\")\ndf['age_class_ratio'] = df['age'] / df['pclass']\ndf['family_survival_rate'] = df.groupby('family_size')['survived'].transform('mean')\n\nprint(\"Examples of ratio features:\")\nprint(df[['age', 'pclass', 'age_class_ratio']].head())\n\n# 3. Aggregations\nprint(\"\\n3. AGGREGATION FEATURES:\")\n# Average fare by class\ndf['avg_fare_by_class'] = df.groupby('pclass')['fare'].transform('mean')\n# Difference from average\ndf['fare_diff_from_avg'] = df['fare'] - df['avg_fare_by_class']\n\nprint(\"Group-based aggregations:\")\nprint(df[['pclass', 'fare', 'avg_fare_by_class', 'fare_diff_from_avg']].head(10))\n```\n\n### Boolean and Indicator Features\n\n```python\ndef create_boolean_features(df):\n    \"\"\"\n    Create powerful boolean/indicator features\n    \"\"\"\n    print(\"\\nBOOLEAN AND INDICATOR FEATURES\")\n    print(\"=\" * 70)\n    \n    # 1. Threshold-based\n    print(\"\\n1. THRESHOLD-BASED:\")\n    df['is_child'] = (df['age'] < 18).astype(int)\n    df['is_senior'] = (df['age'] >= 60).astype(int)\n    df['is_expensive_ticket'] = (df['fare'] > df['fare'].median()).astype(int)\n    df['is_large_family'] = (df['family_size'] > 4).astype(int)\n    \n    print(\"Created indicators:\")\n    print(df[['age', 'is_child', 'is_senior', 'family_size', 'is_large_family']].head(10))\n    \n    # 2. Missing indicators (informative!)\n    print(\"\\n2. MISSING VALUE INDICATORS:\")\n    df['age_missing'] = df['age'].isnull().astype(int)\n    df['cabin_missing'] = df['cabin'].isnull().astype(int)\n    \n    # Check if missing info is predictive\n    print(\"\\nSurvival rate by cabin information:\")\n    print(df.groupby('cabin_missing')['survived'].agg(['mean', 'count']))\n    \n    # 3. Combination conditions\n    print(\"\\n3. COMBINATION CONDITIONS:\")\n    df['young_first_class'] = ((df['age'] < 30) & (df['pclass'] == 1)).astype(int)\n    df['female_child'] = ((df['sex'] == 'female') & (df['age'] < 18)).astype(int)\n    df['alone'] = (df['family_size'] == 1).astype(int)\n    \n    print(\"Complex conditions:\")\n    print(df[['age', 'pclass', 'young_first_class', 'sex', 'female_child']].head(10))\n    \n    return df\n\ndf = create_boolean_features(df)\n```\n\n### Binning and Discretization\n\n```python\ndef demonstrate_binning_strategies(df):\n    \"\"\"\n    Different binning strategies\n    \"\"\"\n    print(\"\\nBINNING STRATEGIES\")\n    print(\"=\" * 70)\n    \n    # 1. Equal-width binning\n    print(\"\\n1. EQUAL-WIDTH BINNING:\")\n    df['age_bins_equal'] = pd.cut(df['age'], bins=5)\n    print(\"Divides range into equal-sized intervals:\")\n    print(df['age_bins_equal'].value_counts().sort_index())\n    \n    # 2. Equal-frequency binning (quantiles)\n    print(\"\\n2. EQUAL-FREQUENCY BINNING (Quantiles):\")\n    df['age_quantiles'] = pd.qcut(df['age'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n    print(\"Each bin has roughly same number of samples:\")\n    print(df['age_quantiles'].value_counts().sort_index())\n    \n    # 3. Custom bins (domain knowledge)\n    print(\"\\n3. CUSTOM BINS (Domain Knowledge):\")\n    age_bins = [0, 12, 18, 35, 60, 100]\n    age_labels = ['Child', 'Teen', 'Young Adult', 'Adult', 'Senior']\n    df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n    print(\"Based on life stages:\")\n    print(df['age_group'].value_counts().sort_index())\n    \n    # 4. Survival rate by bins\n    print(\"\\n4. SURVIVAL RATE BY AGE GROUP:\")\n    survival_by_age = df.groupby('age_group')['survived'].agg(['mean', 'count'])\n    print(survival_by_age)\n    \n    # Visualize\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    df['age_bins_equal'].value_counts().sort_index().plot(kind='bar', ax=axes[0])\n    axes[0].set_title('Equal-Width Binning')\n    axes[0].set_xlabel('Age Bins')\n    \n    df['age_quantiles'].value_counts().sort_index().plot(kind='bar', ax=axes[1])\n    axes[1].set_title('Equal-Frequency Binning')\n    axes[1].set_xlabel('Quantiles')\n    \n    survival_by_age['mean'].plot(kind='bar', ax=axes[2])\n    axes[2].set_title('Survival Rate by Age Group')\n    axes[2].set_xlabel('Age Group')\n    axes[2].set_ylabel('Survival Rate')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df\n\ndf = demonstrate_binning_strategies(df)\n```\n\n### Date and Time Features (Detailed)\n\n## 11. Handling Date and Time Features\n\n```python\ndef comprehensive_datetime_features(df_date):\n    \"\"\"\n    Extract all possible datetime features\n    \"\"\"\n    print(\"\\nCOMPREHENSIVE DATETIME FEATURE EXTRACTION\")\n    print(\"=\" * 70)\n    \n    # Create sample data\n    dates = pd.date_range('2020-01-01', periods=365*2, freq='D')\n    df = pd.DataFrame({'date': dates})\n    df['sales'] = np.random.randint(100, 1000, len(df)) + np.sin(np.arange(len(df))/7) * 50\n    \n    # Basic components\n    print(\"\\n1. BASIC COMPONENTS:\")\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek  # 0=Monday\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['quarter'] = df['date'].dt.quarter\n    df['week'] = df['date'].dt.isocalendar().week\n    \n    print(\"Basic date components extracted:\")\n    print(df[['date', 'year', 'month', 'day', 'dayofweek']].head())\n    \n    # 2. Cyclical features\n    print(\"\\n2. CYCLICAL FEATURES (for periodicity):\")\n    print(\"   Why: Day 31 and Day 1 are close, but numerically far!\")\n    print(\"   Solution: Sine/Cosine encoding\")\n    \n    df['day_sin'] = np.sin(2 * np.pi * df['day']/31)\n    df['day_cos'] = np.cos(2 * np.pi * df['day']/31)\n    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n    \n    print(\"\\nCyclical encoding example:\")\n    print(df[['day', 'day_sin', 'day_cos']].head(10))\n    \n    # 3. Boolean features\n    print(\"\\n3. BOOLEAN DATE FEATURES:\")\n    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n    df['is_year_start'] = df['date'].dt.is_year_start.astype(int)\n    \n    # Holiday features (US holidays example)\n    us_holidays = pd.to_datetime(['2020-01-01', '2020-07-04', '2020-12-25',\n                                   '2021-01-01', '2021-07-04', '2021-12-25'])\n    df['is_holiday'] = df['date'].isin(us_holidays).astype(int)\n    \n    print(\"Boolean indicators:\")\n    print(df[['date', 'is_weekend', 'is_month_start', 'is_holiday']].head(10))\n    \n    # 4. Time-based features\n    print(\"\\n4. TIME-BASED FEATURES:\")\n    df['days_since_start'] = (df['date'] - df['date'].min()).dt.days\n    df['days_until_end'] = (df['date'].max() - df['date']).dt.days\n    df['days_in_month'] = df['date'].dt.days_in_month\n    \n    # 5. Lag features (for time series)\n    print(\"\\n5. LAG FEATURES:\")\n    df['sales_lag_1'] = df['sales'].shift(1)\n    df['sales_lag_7'] = df['sales'].shift(7)  # Same day last week\n    df['sales_lag_30'] = df['sales'].shift(30)  # Same day last month\n    \n    # 6. Rolling statistics\n    print(\"\\n6. ROLLING STATISTICS:\")\n    df['sales_rolling_7_mean'] = df['sales'].rolling(window=7).mean()\n    df['sales_rolling_7_std'] = df['sales'].rolling(window=7).std()\n    df['sales_rolling_30_mean'] = df['sales'].rolling(window=30).mean()\n    \n    print(\"Time series features:\")\n    print(df[['date', 'sales', 'sales_lag_7', 'sales_rolling_7_mean']].tail(10))\n    \n    # Visualize\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Sales over time\n    axes[0, 0].plot(df['date'], df['sales'])\n    axes[0, 0].set_title('Sales Over Time')\n    axes[0, 0].set_xlabel('Date')\n    axes[0, 0].set_ylabel('Sales')\n    \n    # Weekend effect\n    df.groupby('is_weekend')['sales'].mean().plot(kind='bar', ax=axes[0, 1])\n    axes[0, 1].set_title('Average Sales: Weekday vs Weekend')\n    axes[0, 1].set_xticklabels(['Weekday', 'Weekend'], rotation=0)\n    \n    # Monthly pattern\n    df.groupby('month')['sales'].mean().plot(kind='bar', ax=axes[1, 0])\n    axes[1, 0].set_title('Average Sales by Month')\n    axes[1, 0].set_xlabel('Month')\n    \n    # Day of week pattern\n    df.groupby('dayofweek')['sales'].mean().plot(kind='bar', ax=axes[1, 1])\n    axes[1, 1].set_title('Average Sales by Day of Week')\n    axes[1, 1].set_xlabel('Day (0=Monday)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df\n\ndf_time = comprehensive_datetime_features(None)\n```\n\n---\n\n## 12. Text Feature Engineering\n\n### Basic Text Features\n\n```python\ndef extract_text_features(df):\n    \"\"\"\n    Extract features from text data\n    \"\"\"\n    print(\"\\nTEXT FEATURE EXTRACTION\")\n    print(\"=\" * 70)\n    \n    df_text = df[['name']].copy()\n    \n    # 1. Length features\n    print(\"\\n1. LENGTH FEATURES:\")\n    df_text['name_length'] = df_text['name'].str.len()\n    df_text['name_word_count'] = df_text['name'].str.split().str.len()\n    df_text['avg_word_length'] = df_text['name_length'] / df_text['name_word_count']\n    \n    print(\"Length-based features:\")\n    print(df_text.head())\n    \n    # 2. Pattern extraction\n    print(\"\\n2. PATTERN EXTRACTION:\")\n    df_text['title'] = df_text['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df_text['has_parentheses'] = df_text['name'].str.contains('\\(').astype(int)\n    \n    print(\"\\nExtracted titles:\")\n    print(df_text['title'].value_counts())\n    \n    # 3. Character-based features\n    print(\"\\n3. CHARACTER-BASED FEATURES:\")\n    df_text['num_capitals'] = df_text['name'].str.count(r'[A-Z]')\n    df_text['num_digits'] = df_text['name'].str.count(r'\\d')\n    df_text['num_special'] = df_text['name'].str.count(r'[^A-Za-z0-9\\s]')\n    \n    print(\"Character counts:\")\n    print(df_text[['name', 'num_capitals', 'num_digits', 'num_special']].head())\n    \n    return df_text\n\ndf_text = extract_text_features(df)\n```\n\n### Advanced Text Features (TF-IDF, Word Embeddings)\n\n```python\ndef advanced_text_features():\n    \"\"\"\n    Advanced text feature extraction\n    \"\"\"\n    print(\"\\nADVANCED TEXT FEATURES\")\n    print(\"=\" * 70)\n    \n    # Sample text data\n    texts = [\n        \"I love machine learning\",\n        \"Deep learning is amazing\",\n        \"Natural language processing is fun\",\n        \"I love deep learning\",\n        \"Machine learning and NLP\"\n    ]\n    \n    # 1. Bag of Words (BoW)\n    print(\"\\n1. BAG OF WORDS:\")\n    from sklearn.feature_extraction.text import CountVectorizer\n    \n    bow = CountVectorizer()\n    bow_features = bow.fit_transform(texts)\n    \n    print(f\"Vocabulary size: {len(bow.vocabulary_)}\")\n    print(f\"Feature matrix shape: {bow_features.shape}\")\n    print(\"\\nFeature names (top 10):\")\n    print(bow.get_feature_names_out()[:10])\n    \n    # 2. TF-IDF\n    print(\"\\n2. TF-IDF (Term Frequency - Inverse Document Frequency):\")\n    print(\"   Gives more weight to rare words\")\n    \n    from sklearn.feature_extraction.text import TfidfVectorizer\n    \n    tfidf = TfidfVectorizer(max_features=10)\n    tfidf_features = tfidf.fit_transform(texts)\n    \n    print(f\"TF-IDF matrix shape: {tfidf_features.shape}\")\n    print(\"\\nTF-IDF scores for first document:\")\n    feature_names = tfidf.get_feature_names_out()\n    scores = tfidf_features[0].toarray()[0]\n    for name, score in zip(feature_names, scores):\n        if score > 0:\n            print(f\"  {name}: {score:.3f}\")\n    \n    # 3. N-grams\n    print(\"\\n3. N-GRAMS (word combinations):\")\n    ngram = CountVectorizer(ngram_range=(1, 2), max_features=15)\n    ngram_features = ngram.fit_transform(texts)\n    \n    print(\"Unigrams and Bigrams:\")\n    print(ngram.get_feature_names_out())\n    \n    # 4. Word Embeddings (using pre-trained)\n    print(\"\\n4. WORD EMBEDDINGS:\")\n    print(\"   Would use: Word2Vec, GloVe, FastText, or BERT\")\n    print(\"   These capture semantic meaning!\")\n    print(\"   Example: 'king' - 'man' + 'woman' ‚âà 'queen'\")\n    \n    return bow_features, tfidf_features\n\nbow, tfidf = advanced_text_features()\n```\n\n---\n\n## 8. Feature Selection Methods\n\n### Filter Methods\n\n```python\ndef filter_methods_comprehensive(df, target):\n    \"\"\"\n    All filter-based feature selection methods\n    \"\"\"\n    print(\"\\nFILTER METHODS FOR FEATURE SELECTION\")\n    print(\"=\" * 70)\n    \n    # Prepare data\n    numeric_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n    X = df[numeric_cols].dropna()\n    y = df.loc[X.index, target]\n    \n    from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\n    from scipy.stats import pearsonr\n    \n    # 1. Correlation-based\n    print(\"\\n1. CORRELATION-BASED:\")\n    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n    print(\"Absolute correlation with target:\")\n    print(correlations)\n    \n    # 2. Chi-Square Test\n    print(\"\\n2. CHI-SQUARE TEST (for non-negative features):\")\n    X_positive = X.abs()  # Chi2 requires non-negative values\n    selector_chi2 = SelectKBest(chi2, k=3)\n    selector_chi2.fit(X_positive, y)\n    \n    chi2_scores = pd.DataFrame({\n        'feature': numeric_cols,\n        'chi2_score': selector_chi2.scores_\n    }).sort_values('chi2_score', ascending=False)\n    print(chi2_scores)\n    \n    # 3. ANOVA F-test\n    print(\"\\n3. ANOVA F-TEST:\")\n    selector_f = SelectKBest(f_classif, k=3)\n    selector_f.fit(X, y)\n    \n    f_scores = pd.DataFrame({\n        'feature': numeric_cols,\n        'f_score': selector_f.scores_,\n        'p_value': selector_f.pvalues_\n    }).sort_values('f_score', ascending=False)\n    print(f_scores)\n    \n    # 4. Mutual Information\n    print(\"\\n4. MUTUAL INFORMATION:\")\n    print(\"   Captures non-linear relationships!\")\n    \n    mi_scores = mutual_info_classif(X, y, random_state=42)\n    mi_df = pd.DataFrame({\n        'feature': numeric_cols,\n        'mi_score': mi_scores\n    }).sort_values('mi_score', ascending=False)\n    print(mi_df)\n    \n    # Visualize all methods\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    correlations.plot(kind='barh', ax=axes[0, 0])\n    axes[0, 0].set_title('Correlation Scores')\n    \n    chi2_scores.plot(x='feature', y='chi2_score', kind='barh', ax=axes[0, 1], legend=False)\n    axes[0, 1].set_title('Chi-Square Scores')\n    \n    f_scores.plot(x='feature', y='f_score', kind='barh', ax=axes[1, 0], legend=False)\n    axes[1, 0].set_title('ANOVA F-Scores')\n    \n    mi_df.plot(x='feature', y='mi_score', kind='barh', ax=axes[1, 1], legend=False)\n    axes[1, 1].set_title('Mutual Information Scores')\n    \n    plt.tight_layout()\n    plt.show()\n\ndf = sns.load_dataset('titanic')\nfilter_methods_comprehensive(df, 'survived')\n```\n\n### Wrapper Methods\n\n```python\ndef wrapper_methods_comprehensive(df, target):\n    \"\"\"\n    Wrapper methods for feature selection\n    \"\"\"\n    print(\"\\nWRAPPER METHODS\")\n    print(\"=\" * 70)\n    print(\"These methods train models to evaluate feature subsets\")\n    \n    from sklearn.feature_selection import RFE, RFECV, SequentialFeatureSelector\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n    import matplotlib.pyplot as plt\n    \n    # Prepare data\n    numeric_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n    X = df[numeric_cols].dropna()\n    y = df.loc[X.index, target]\n    \n    model = LogisticRegression(max_iter=1000, random_state=42)\n    \n    # 1. Recursive Feature Elimination (RFE)\n    print(\"\\n1. RECURSIVE FEATURE ELIMINATION (RFE):\")\n    print(\"   Strategy: Remove least important feature iteratively\")\n    \n    rfe = RFE(estimator=model, n_features_to_select=3)\n    rfe.fit(X, y)\n    \n    rfe_results = pd.DataFrame({\n        'feature': numeric_cols,\n        'selected': rfe.support_,\n        'ranking': rfe.ranking_\n    }).sort_values('ranking')\n    \n    print(rfe_results)\n    \n    # 2. RFECV (with Cross-Validation)\n    print(\"\\n2. RFECV (Finds optimal number automatically):\")\n    \n    rfecv = RFECV(estimator=model, step=1, cv=5, scoring='accuracy')\n    rfecv.fit(X, y)\n    \n    print(f\"Optimal number of features: {rfecv.n_features_}\")\n    print(f\"Selected features: {[col for col, selected in zip(numeric_cols, rfecv.support_) if selected]}\")\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n             rfecv.cv_results_['mean_test_score'])\n    plt.xlabel('Number of Features')\n    plt.ylabel('Cross-Validation Score')\n    plt.title('RFECV: Feature Selection')\n    plt.axvline(rfecv.n_features_, color='r', linestyle='--', \n                label=f'Optimal = {rfecv.n_features_}')\n    plt.legend()\n    plt.show()\n    \n    # 3. Forward/Backward Selection\n    print(\"\\n3. SEQUENTIAL FEATURE SELECTION:\")\n    \n    # Forward\n    sfs_forward = SequentialFeatureSelector(model, n_features_to_select=3, \n                                           direction='forward', cv=5)\n    sfs_forward.fit(X, y)\n    \n    print(\"\\nForward Selection:\")\n    print([col for col, selected in zip(numeric_cols, sfs_forward.support_) if selected])\n    \n    # Backward\n    sfs_backward = SequentialFeatureSelector(model, n_features_to_select=3,\n                                            direction='backward', cv=5)\n    sfs_backward.fit(X, y)\n    \n    print(\"\\nBackward Selection:\")\n    print([col for col, selected in zip(numeric_cols, sfs_backward.support_) if selected])\n\nwrapper_methods_comprehensive(df, 'survived')\n```\n\n### Embedded Methods\n\n```python\ndef embedded_methods_comprehensive(df, target):\n    \"\"\"\n    Embedded feature selection methods\n    \"\"\"\n    print(\"\\nEMBEDDED METHODS\")\n    print(\"=\" * 70)\n    print(\"Feature selection happens during model training\")\n    \n    from sklearn.linear_model import Lasso, Ridge, ElasticNet\n    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n    from sklearn.feature_selection import SelectFromModel\n    import matplotlib.pyplot as plt\n    \n    # Prepare data\n    numeric_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n    X = df[numeric_cols].dropna()\n    y = df.loc[X.index, target]\n    \n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 1. L1 Regularization (Lasso)\n    print(\"\\n1. L1 REGULARIZATION (Lasso):\")\n    print(\"   Drives some coefficients to exactly zero\")\n    \n    lasso = Lasso(alpha=0.01, random_state=42)\n    lasso.fit(X_scaled, y)\n    \n    lasso_importance = pd.DataFrame({\n        'feature': numeric_cols,\n        'coefficient': np.abs(lasso.coef_)\n    }).sort_values('coefficient', ascending=False)\n    \n    print(lasso_importance)\n    print(f\"\\nFeatures with zero coefficient: {(lasso_importance['coefficient'] == 0).sum()}\")\n    \n    # 2. Tree-based Feature Importance\n    print(\"\\n2. RANDOM FOREST FEATURE IMPORTANCE:\")\n    \n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    \n    rf_importance = pd.DataFrame({\n        'feature': numeric_cols,\n        'importance': rf.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(rf_importance)\n    \n    # 3. Gradient Boosting Feature Importance\n    print(\"\\n3. GRADIENT BOOSTING FEATURE IMPORTANCE:\")\n    \n    gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n    gb.fit(X, y)\n    \n    gb_importance = pd.DataFrame({\n        'feature': numeric_cols,\n        'importance': gb.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(gb_importance)\n    \n    # 4. SelectFromModel\n    print(\"\\n4. SELECT FROM MODEL:\")\n    print(\"   Automatically select features above importance threshold\")\n    \n    selector = SelectFromModel(rf, threshold='median', prefit=True)\n    X_selected = selector.transform(X)\n    selected_features = [col for col, selected in zip(numeric_cols, selector.get_support()) if selected]\n    \n    print(f\"Selected features: {selected_features}\")\n    print(f\"Reduced from {X.shape[1]} to {X_selected.shape[1]} features\")\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    lasso_importance.plot(x='feature', y='coefficient', kind='barh', ax=axes[0], legend=False)\n    axes[0].set_title('Lasso Coefficients')\n    \n    rf_importance.plot(x='feature', y='importance', kind='barh', ax=axes[1], legend=False)\n    axes[1].set_title('Random Forest Importance')\n    \n    gb_importance.plot(x='feature', y='importance', kind='barh', ax=axes[2], legend=False)\n    axes[2].set_title('Gradient Boosting Importance')\n    \n    plt.tight_layout()\n    plt.show()\n\nembedded_methods_comprehensive(df, 'survived')\n```\n\n---\n\n## 13. Feature Interactions and Polynomial Features\n\n```python\ndef feature_interactions_comprehensive(df):\n    \"\"\"\n    Create feature interactions\n    \"\"\"\n    print(\"\\nFEATURE INTERACTIONS\")\n    print(\"=\" * 70)\n    \n    from sklearn.preprocessing import PolynomialFeatures\n    import itertools\n    \n    # Prepare data\n    numeric_cols = ['age', 'fare']\n    X = df[numeric_cols].dropna()\n    \n    # 1. Manual interactions\n    print(\"\\n1. MANUAL INTERACTIONS:\")\n    df_interact = X.copy()\n    \n    # Multiplication\n    df_interact['age_fare_mult'] = df_interact['age'] * df_interact['fare']\n    \n    # Division (with safety check)\n    df_interact['age_fare_div'] = df_interact['age'] / (df_interact['fare'] + 1)\n    \n    # Difference\n    df_interact['age_fare_diff'] = df_interact['age'] - df_interact['fare']\n    \n    print(\"Manual interaction features:\")\n    print(df_interact.head())\n    \n    # 2. Polynomial Features\n    print(\"\\n2. POLYNOMIAL FEATURES:\")\n    print(\"   Creates all combinations up to specified degree\")\n    \n    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n    poly_features = poly.fit_transform(X.head(5))\n    \n    feature_names = poly.get_feature_names_out(numeric_cols)\n    df_poly = pd.DataFrame(poly_features, columns=feature_names)\n    \n    print(f\"\\nOriginal features: {X.shape[1]}\")\n    print(f\"After polynomial (degree=2): {poly_features.shape[1]}\")\n    print(\"\\nGenerated features:\")\n    print(feature_names)\n    print(\"\\nExample:\")\n    print(df_poly.head())\n    \n    # 3. Interaction only (no squares)\n    print(\"\\n3. INTERACTION ONLY (no polynomial terms):\")\n    \n    poly_interact = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n    interact_features = poly_interact.fit_transform(X.head(5))\n    \n    interact_names = poly_interact.get_feature_names_out(numeric_cols)\n    print(f\"\\nFeatures: {interact_names}\")\n    print(\"Note: No age¬≤, fare¬≤ terms!\")\n    \n    # 4. Domain-specific interactions\n    print(\"\\n4. DOMAIN-SPECIFIC INTERACTIONS:\")\n    df_domain = df[['pclass', 'sex', 'age', 'fare', 'family_size']].copy()\n    \n    # Rich female vs poor male\n    df_domain['wealth_gender'] = df_domain['pclass'] * (df_domain['sex'] == 'female').astype(int)\n    \n    # Family fare per person\n    df_domain['family_fare'] = df_domain['fare'] * df_domain['family_size']\n    \n    print(\"Domain-specific interactions created!\")\n    \n    return df_interact\n\ndf_interactions = feature_interactions_comprehensive(df)\n```\n\n---\n\n# Part 3: Advanced Techniques\n\n## 14. Dimensionality Reduction\n\n### PCA (Principal Component Analysis)\n\n```python\ndef comprehensive_pca_tutorial(df):\n    \"\"\"\n    Complete PCA tutorial\n    \"\"\"\n    print(\"\\nPRINCIPAL COMPONENT ANALYSIS (PCA)\")\n    print(\"=\" * 70)\n    \n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import StandardScaler\n    import matplotlib.pyplot as plt\n    \n    # Prepare data\n    numeric_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n    X = df[numeric_cols].dropna()\n    \n    # Standardize (PCA requires standardization!)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 1. Fit PCA\n    print(\"\\n1. FITTING PCA:\")\n    pca = PCA()\n    X_pca = pca.fit_transform(X_scaled)\n    \n    # Explained variance\n    explained_var = pca.explained_variance_ratio_\n    cumulative_var = np.cumsum(explained_var)\n    \n    print(\"\\nExplained variance by component:\")\n    for i, (var, cum_var) in enumerate(zip(explained_var, cumulative_var)):\n        print(f\"  PC{i+1}: {var:.3f} (cumulative: {cum_var:.3f})\")\n    \n    # 2. Visualize explained variance\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].bar(range(1, len(explained_var)+1), explained_var)\n    axes[0].set_xlabel('Principal Component')\n    axes[0].set_ylabel('Explained Variance Ratio')\n    axes[0].set_title('Scree Plot')\n    \n    axes[1].plot(range(1, len(cumulative_var)+1), cumulative_var, marker='o')\n    axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n    axes[1].set_xlabel('Number of Components')\n    axes[1].set_ylabel('Cumulative Explained Variance')\n    axes[1].set_title('Cumulative Explained Variance')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 3. Choose optimal number of components\n    n_components_95 = np.argmax(cumulative_var >= 0.95) + 1\n    print(f\"\\n3. OPTIMAL COMPONENTS:\")\n    print(f\"   Components needed for 95% variance: {n_components_95}\")\n    print(f\"   Dimension reduction: {len(numeric_cols)} ‚Üí {n_components_95}\")\n    \n    # 4. Feature loadings\n    print(\"\\n4. FEATURE LOADINGS (How features contribute to PCs):\")\n    loadings = pd.DataFrame(\n        pca.components_[:3].T,\n        columns=['PC1', 'PC2', 'PC3'],\n        index=numeric_cols\n    )\n    print(loadings)\n    \n    # Visualize loadings\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(loadings, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n    plt.title('PCA Feature Loadings')\n    plt.show()\n    \n    # 5. Transform with optimal components\n    pca_final = PCA(n_components=n_components_95)\n    X_reduced = pca_final.fit_transform(X_scaled)\n    \n    print(f\"\\n5. FINAL TRANSFORMATION:\")\n    print(f\"   Original shape: {X_scaled.shape}\")\n    print(f\"   Reduced shape: {X_reduced.shape}\")\n    \n    return X_reduced, pca_final\n\nX_pca, pca_model = comprehensive_pca_tutorial(df)\n```\n\n### Other Dimensionality Reduction Methods\n\n```python\ndef other_dimensionality_reduction(df):\n    \"\"\"\n    Other dimensionality reduction techniques\n    \"\"\"\n    print(\"\\nOTHER DIMENSIONALITY REDUCTION METHODS\")\n    print(\"=\" * 70)\n    \n    from sklearn.manifold import TSNE\n    from sklearn.decomposition import TruncatedSVD, FastICA\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n    from sklearn.preprocessing import StandardScaler\n    import matplotlib.pyplot as plt\n    \n    # Prepare data\n    numeric_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n    X = df[numeric_cols].dropna()\n    y = df.loc[X.index, 'survived']\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 1. t-SNE\n    print(\"\\n1. t-SNE (t-Distributed Stochastic Neighbor Embedding):\")\n    print(\"   Best for: Visualization (non-linear)\")\n    print(\"   Warning: Slow for large datasets, only for visualization!\")\n    \n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X_scaled[:500])  # Use subset for speed\n    \n    # 2. LDA\n    print(\"\\n2. LDA (Linear Discriminant Analysis):\")\n    print(\"   Best for: Supervised dimensionality reduction\")\n    print(\"   Maximizes class separability!\")\n    \n    lda = LDA(n_components=1)\n    X_lda = lda.fit_transform(X_scaled, y)\n    \n    print(f\"   Explained variance ratio: {lda.explained_variance_ratio_}\")\n    \n    # 3. ICA\n    print(\"\\n3. ICA (Independent Component Analysis):\")\n    print(\"   Best for: Finding independent sources\")\n    print(\"   Use case: Signal processing, blind source separation\")\n    \n    ica = FastICA(n_components=3, random_state=42)\n    X_ica = ica.fit_transform(X_scaled)\n    \n    # 4. SVD\n    print(\"\\n4. TRUNCATED SVD:\")\n    print(\"   Best for: Sparse matrices, text data\")\n    print(\"   Similar to PCA but works on sparse data\")\n    \n    svd = TruncatedSVD(n_components=3, random_state=42)\n    X_svd = svd.fit_transform(X_scaled)\n    \n    print(f\"   Explained variance ratio: {svd.explained_variance_ratio_}\")\n    \n    # Visualize all methods\n    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n    \n    # t-SNE\n    scatter = axes[0, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], \n                                  c=y[:500], cmap='viridis', alpha=0.6)\n    axes[0, 0].set_title('t-SNE')\n    axes[0, 0].set_xlabel('Component 1')\n    axes[0, 0].set_ylabel('Component 2')\n    plt.colorbar(scatter, ax=axes[0, 0])\n    \n    # LDA\n    axes[0, 1].hist([X_lda[y==0], X_lda[y==1]], label=['Not Survived', 'Survived'], bins=30)\n    axes[0, 1].set_title('LDA (1D)')\n    axes[0, 1].set_xlabel('LDA Component')\n    axes[0, 1].legend()\n    \n    # ICA\n    axes[1, 0].scatter(X_ica[:, 0], X_ica[:, 1], c=y, cmap='viridis', alpha=0.6)\n    axes[1, 0].set_title('ICA')\n    axes[1, 0].set_xlabel('IC 1')\n    axes[1, 0].set_ylabel('IC 2')\n    \n    # SVD\n    axes[1, 1].scatter(X_svd[:, 0], X_svd[:, 1], c=y, cmap='viridis', alpha=0.6)\n    axes[1, 1].set_title('Truncated SVD')\n    axes[1, 1].set_xlabel('Component 1')\n    axes[1, 1].set_ylabel('Component 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Summary table\n    print(\"\\nüìã SUMMARY:\")\n    summary = pd.DataFrame({\n        'Method': ['PCA', 't-SNE', 'LDA', 'ICA', 'SVD'],\n        'Type': ['Linear', 'Non-linear', 'Supervised', 'Linear', 'Linear'],\n        'Best For': [\n            'General purpose',\n            'Visualization only',\n            'Classification',\n            'Signal separation',\n            'Sparse data'\n        ],\n        'Speed': ['Fast', 'Slow', 'Fast', 'Fast', 'Very Fast']\n    })\n    print(summary.to_string(index=False))\n\nother_dimensionality_reduction(df)\n```\n\n---\n\n## 17. Automated Feature Engineering\n\n```python\ndef automated_feature_engineering_demo():\n    \"\"\"\n    Automated feature engineering tools\n    \"\"\"\n    print(\"\\nAUTOMATED FEATURE ENGINEERING\")\n    print(\"=\" * 70)\n    \n    # Using Featuretools\n    try:\n        import featuretools as ft\n        \n        print(\"\\n1. FEATURETOOLS:\")\n        print(\"   Automatically generates features using Deep Feature Synthesis\")\n        \n        # Create sample data\n        import pandas as pd\n        customers = pd.DataFrame({\n            'customer_id': [1, 2, 3],\n            'age': [25, 35, 45],\n            'join_date': pd.to_datetime(['2020-01-01', '2020-02-01', '2020-03-01'])\n        })\n        \n        transactions = pd.DataFrame({\n            'transaction_id': range(1, 11),\n            'customer_id': [1, 1, 1, 2, 2, 2, 2, 3, 3, 3],\n            'amount': [100, 150, 200, 80, 90, 120, 200, 300, 250, 400],\n            'timestamp': pd.date_range('2020-01-15', periods=10, freq='D')\n        })\n        \n        # Create EntitySet\n        es = ft.EntitySet(id='transactions_data')\n        es = es.add_dataframe(dataframe_name='customers', \n                              dataframe=customers,\n                              index='customer_id',\n                              time_index='join_date')\n        es = es.add_dataframe(dataframe_name='transactions',\n                              dataframe=transactions,\n                              index='transaction_id',\n                              time_index='timestamp')\n        \n        # Add relationship\n        es = es.add_relationship('customers', 'customer_id', \n                                'transactions', 'customer_id')\n        \n        # Generate features\n        feature_matrix, feature_defs = ft.dfs(entityset=es,\n                                               target_dataframe_name='customers',\n                                               max_depth=2)\n        \n        print(f\"\\n   Original features: {len(customers.columns)}\")\n        print(f\"   Generated features: {len(feature_matrix.columns)}\")\n        print(f\"\\n   Sample generated features:\")\n        print(feature_matrix.columns.tolist()[:10])\n        \n    except ImportError:\n        print(\"\\n1. FEATURETOOLS: Not installed\")\n        print(\"   Install: pip install featuretools\")\n    \n    # Using tsfresh (for time series)\n    print(\"\\n2. TSFRESH (Time Series Feature Extraction):\")\n    print(\"   Automatically extracts 100+ features from time series\")\n    print(\"   Install: pip install tsfresh\")\n    \n    # Using AutoFeat\n    print(\"\\n3. AUTOFEAT:\")\n    print(\"   Creates polynomial and interaction features automatically\")\n    print(\"   Install: pip install autofeat\")\n    \n    print(\"\\nüí° RECOMMENDATION:\")\n    print(\"   Start with manual feature engineering to understand your data\")\n    print(\"   Then use automated tools to discover features you might have missed!\")\n\nautomated_feature_engineering_demo()\n```\n\n---\n\n# Part 4: Real-World Applications\n\n## 19. Complete End-to-End Projects\n\n### Project 1: Customer Churn Prediction\n\n```python\ndef customer_churn_complete_project():\n    \"\"\"\n    Complete customer churn prediction with feature engineering\n    \"\"\"\n    print(\"\\nPROJECT: CUSTOMER CHURN PREDICTION\")\n    print(\"=\" * 70)\n    \n    # Create synthetic dataset\n    np.random.seed(42)\n    n_customers = 5000\n    \n    data = {\n        'customer_id': range(1, n_customers + 1),\n        'age': np.random.randint(18, 70, n_customers),\n        'tenure_months': np.random.randint(1, 72, n_customers),\n        'monthly_charges': np.random.uniform(20, 100, n_customers),\n        'total_charges': None,  # Will calculate\n        'num_products': np.random.randint(1, 5, n_customers),\n        'num_support_calls': np.random.poisson(2, n_customers),\n        'contract_type': np.random.choice(['Month-to-Month', 'One Year', 'Two Year'], n_customers),\n        'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Electronic Check'], n_customers),\n        'last_interaction_days': np.random.randint(0, 180, n_customers)\n    }\n    \n    df_churn = pd.DataFrame(data)\n    \n    # Calculate total charges\n    df_churn['total_charges'] = df_churn['monthly_charges'] * df_churn['tenure_months']\n    \n    # Create target (churn) with some logic\n    churn_prob = (\n        0.5 * (df_churn['contract_type'] == 'Month-to-Month').astype(int) +\n        0.3 * (df_churn['num_support_calls'] > 3).astype(int) +\n        0.2 * (df_churn['tenure_months'] < 12).astype(int)\n    ) / 3\n    \n    df_churn['churn'] = (np.random.random(n_customers) < churn_prob).astype(int)\n    \n    print(\"\\n‚úÖ Step 1: Data Loaded\")\n    print(df_churn.head())\n    print(f\"\\nChurn rate: {df_churn['churn'].mean():.2%}\")\n    \n    # FEATURE ENGINEERING\n    print(\"\\n‚úÖ Step 2: Feature Engineering\")\n    \n    # 1. Numeric features\n    df_churn['avg_monthly_spend'] = df_churn['total_charges'] / df_churn['tenure_months']\n    df_churn['charges_per_product'] = df_churn['monthly_charges'] / df_churn['num_products']\n    df_churn['support_call_rate'] = df_churn['num_support_calls'] / df_churn['tenure_months']\n    \n    # 2. Categorical encoding\n    df_churn['is_month_to_month'] = (df_churn['contract_type'] == 'Month-to-Month').astype(int)\n    df_churn['is_electronic_check'] = (df_churn['payment_method'] == 'Electronic Check').astype(int)\n    \n    # 3. Binning\n    df_churn['tenure_group'] = pd.cut(df_churn['tenure_months'], \n                                       bins=[0, 12, 24, 48, 72],\n                                       labels=['<1yr', '1-2yr', '2-4yr', '4+yr'])\n    \n    # 4. Boolean features\n    df_churn['recent_interaction'] = (df_churn['last_interaction_days'] < 30).astype(int)\n    df_churn['high_support_calls'] = (df_churn['num_support_calls'] > 3).astype(int)\n    df_churn['is_new_customer'] = (df_churn['tenure_months'] < 6).astype(int)\n    \n    print(\"   Created features:\")\n    print(f\"   - avg_monthly_spend, charges_per_product, support_call_rate\")\n    print(f\"   - is_month_to_month, is_electronic_check\")\n    print(f\"   - tenure_group, recent_interaction, high_support_calls\")\n    \n    # MODEL TRAINING\n    print(\"\\n‚úÖ Step 3: Model Training\")\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n    \n    # Select features\n    feature_cols = [\n        'age', 'tenure_months', 'monthly_charges', 'num_products',\n        'avg_monthly_spend', 'charges_per_product', 'support_call_rate',\n        'is_month_to_month', 'is_electronic_check', 'recent_interaction',\n        'high_support_calls', 'is_new_customer'\n    ]\n    \n    X = df_churn[feature_cols]\n    y = df_churn['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Scale\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train_scaled, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    print(\"\\n‚úÖ Step 4: Results\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n    \n    # Feature importance\n    importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'importance': model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(\"\\nTop 5 Most Important Features:\")\n    print(importance_df.head())\n    \n    # Visualize\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Feature importance\n    importance_df.head(10).plot(x='feature', y='importance', kind='barh', ax=axes[0], legend=False)\n    axes[0].set_title('Feature Importance')\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n    axes[1].set_title('Confusion Matrix')\n    axes[1].set_ylabel('Actual')\n    axes[1].set_xlabel('Predicted')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n‚úÖ Project Complete!\")\n    print(\"\\nüí° KEY LEARNINGS:\")\n    print(\"   1. Feature engineering improved model performance\")\n    print(\"   2. Interaction features (support_call_rate) were very predictive\")\n    print(\"   3. Contract type and tenure are strong churn indicators\")\n\ncustomer_churn_complete_project()\n```\n\n---\n\n## 23. Interview Preparation\n\n### Common Interview Questions\n\n```python\nprint(\"\\nFEATURE ENGINEERING INTERVIEW QUESTIONS & ANSWERS\")\nprint(\"=\" * 70)\n\nquestions = [\n    {\n        \"Q\": \"What is feature engineering and why is it important?\",\n        \"A\": \"\"\"Feature engineering is the process of transforming raw data into meaningful \nfeatures that help ML models learn better patterns. It's important because:\n- Good features can make simple models outperform complex ones\n- Models can only learn from the features you provide\n- Can reduce training time and improve accuracy\n- Incorporates domain knowledge into the model\"\"\"\n    },\n    {\n        \"Q\": \"How do you handle missing values in a dataset?\",\n        \"A\": \"\"\"It depends on the type and amount of missing data:\n1. If <5% missing: Drop rows\n2. If 5-40% missing:\n   - Mean/Median imputation for numeric (median if outliers)\n   - Mode imputation for categorical\n   - KNN or MICE for advanced imputation\n3. If >60% missing: Drop column\n4. Always create a 'missing' indicator feature\n5. Consider the missing mechanism (MCAR, MAR, MNAR)\"\"\"\n    },\n    {\n        \"Q\": \"Explain the difference between Label Encoding and One-Hot Encoding.\",\n        \"A\": \"\"\"Label Encoding: Assigns integer to each category (Red=0, Blue=1, Green=2)\n- Use for: Ordinal data, tree-based models\n- Problem: Implies order even when there isn't one\n\nOne-Hot Encoding: Creates binary column for each category\n- Use for: Nominal data, linear models\n- Problem: Creates many columns for high cardinality\n- Tip: Use drop_first=True to avoid multicollinearity\"\"\"\n    },\n    {\n        \"Q\": \"How do you detect and handle outliers?\",\n        \"A\": \"\"\"Detection methods:\n1. IQR method: Q1 - 1.5*IQR and Q3 + 1.5*IQR\n2. Z-score: |z| > 3\n3. Isolation Forest\n4. Domain knowledge\n\nHandling strategies:\n1. Remove: If data errors or <1% of data\n2. Cap (Winsorize): If valid but extreme\n3. Transform: Log, sqrt to reduce impact\n4. Keep: If important for prediction (fraud detection)\n5. Use robust models: Tree-based algorithms\"\"\"\n    },\n    {\n        \"Q\": \"When should you use feature scaling and which method?\",\n        \"A\": \"\"\"Use scaling for:\n- Distance-based algorithms: KNN, SVM, K-Means\n- Gradient descent: Linear/Logistic Regression, Neural Networks\n- PCA\n\nDon't scale for:\n- Tree-based models: Decision Trees, Random Forest, XGBoost\n\nMethods:\n- StandardScaler: Normal distribution, most algorithms\n- MinMaxScaler: Neural networks, bounded values needed\n- RobustScaler: Data with many outliers\n- Always fit on train, transform on test!\"\"\"\n    },\n    {\n        \"Q\": \"How do you handle high cardinality categorical features?\",\n        \"A\": \"\"\"Options:\n1. Target Encoding: Replace with target mean (watch for overfitting)\n2. Frequency Encoding: Replace with frequency of category\n3. Binary Encoding: Reduces dimensions vs one-hot\n4. Hash Encoding: Fixed dimensions, handles unseen categories\n5. Embeddings: For deep learning\n6. Group rare categories: Combine infrequent ones into 'Other'\n\nNever use one-hot encoding for high cardinality (>50 categories)!\"\"\"\n    },\n    {\n        \"Q\": \"Explain feature selection methods and when to use each.\",\n        \"A\": \"\"\"1. Filter Methods (Fast, before modeling):\n   - Correlation: Quick first pass\n   - Chi-square, ANOVA: Statistical tests\n   - Mutual Information: Non-linear relationships\n\n2. Wrapper Methods (Accurate, slow):\n   - RFE: Iterative removal\n   - Forward/Backward selection: Greedy search\n\n3. Embedded Methods (During training):\n   - Lasso (L1): Drives coefficients to zero\n   - Tree importance: Random Forest, XGBoost\n   \nUse filter for quick exploration, embedded for final model.\"\"\"\n    },\n    {\n        \"Q\": \"How do you create features from datetime data?\",\n        \"A\": \"\"\"Extract components:\n- Basic: year, month, day, hour, minute, dayofweek\n- Derived: is_weekend, is_month_end, quarter\n- Cyclical: Use sin/cos encoding for periodicity\n- Time-based: days_since_event, age_in_days\n- Lag features: previous values (time series)\n- Rolling statistics: moving averages, std\n\nFor time series, always consider temporal ordering!\"\"\"\n    },\n    {\n        \"Q\": \"What is the curse of dimensionality and how do you handle it?\",\n        \"A\": \"\"\"Curse of dimensionality: As features increase, data becomes sparse, \ndistance metrics become meaningless, and models overfit.\n\nSolutions:\n1. Feature Selection: Remove irrelevant/redundant features\n2. Dimensionality Reduction: PCA, t-SNE, LDA\n3. Regularization: L1/L2 to penalize complexity\n4. More data: Exponentially more needed for each dimension\n5. Domain knowledge: Only include meaningful features\n6. Feature engineering: Create fewer, more meaningful features\"\"\"\n    },\n    {\n        \"Q\": \"How do you prevent data leakage in feature engineering?\",\n        \"A\": \"\"\"Data leakage = Using future information to predict the past\n\nPrevention strategies:\n1. Always split data BEFORE any transformation\n2. Fit scalers/encoders on training data only\n3. Be careful with target encoding (use cross-validation)\n4. Watch for features that won't be available at prediction time\n5. Time-based splits for time series\n6. Don't use target-derived features directly\n7. Validate your pipeline: train on past, predict future\n\nExample leakage: Using customer's lifetime value to predict first purchase!\"\"\"\n    }\n]\n\nfor i, qa in enumerate(questions, 1):\n    print(f\"\\n{i}. {qa['Q']}\")\n    print(\"-\" * 70)\n    print(qa['A'])\n```\n\n### Coding Challenges\n\n```python\nprint(\"\\n\\nCODING CHALLENGES FOR INTERVIEWS\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nCHALLENGE 1: Handle Missing Values\n-----------------------------------\nGiven a dataset with missing values, implement a function that:\n1. Identifies the best imputation strategy for each column\n2. Applies the imputation\n3. Creates missing indicators\n4. Returns the cleaned dataset\n\nCHALLENGE 2: Create Date Features\n-----------------------------------\nWrite a function that extracts all useful features from a datetime column:\n- Components (year, month, day, etc.)\n- Cyclical encoding (sin/cos)\n- Boolean indicators (weekend, month_end, etc.)\n- Time-based features (days_since_start)\n\nCHALLENGE 3: Encode High Cardinality Feature\n-----------------------------------\nGiven a categorical column with 1000+ unique values:\n1. Implement target encoding with cross-validation\n2. Handle unseen categories in test set\n3. Add smoothing to prevent overfitting\n\nCHALLENGE 4: Feature Selection Pipeline\n-----------------------------------\nBuild a complete feature selection pipeline:\n1. Remove low-variance features\n2. Remove highly correlated features (>0.95)\n3. Select top K features using mutual information\n4. Validate with cross-validation\n\nCHALLENGE 5: Handle Imbalanced Data\n-----------------------------------\nGiven a dataset with 95% class 0 and 5% class 1:\n1. Try different sampling strategies\n2. Implement proper cross-validation\n3. Choose appropriate evaluation metrics\n4. Compare results\n\nTIPS FOR CODING INTERVIEWS:\n- Write clean, readable code with comments\n- Handle edge cases (empty data, all missing, etc.)\n- Explain your thinking process\n- Discuss trade-offs of different approaches\n- Ask clarifying questions before coding\n- Test your code with sample data\n\"\"\")\n```\n\n---\n\n## 24. Best Practices and Common Mistakes\n\n### Best Practices Checklist\n\n```python\nprint(\"\\n‚úÖ FEATURE ENGINEERING BEST PRACTICES CHECKLIST\")\nprint(\"=\" * 70)\n\nbest_practices = {\n    \"Data Understanding\": [\n        \"Perform thorough EDA before feature engineering\",\n        \"Understand the business problem and domain\",\n        \"Check data types and distributions\",\n        \"Identify and understand missing patterns\",\n        \"Look for outliers and understand their cause\"\n    ],\n    \n    \"Feature Creation\": [\n        \"Start simple, then add complexity\",\n        \"Create features based on domain knowledge\",\n        \"Document why each feature was created\",\n        \"Test features individually before combining\",\n        \"Consider feature interactions\"\n    ],\n    \n    \"Data Splitting\": [\n        \"ALWAYS split data before any transformation\",\n        \"Use stratified splits for imbalanced data\",\n        \"Use time-based splits for time series\",\n        \"Keep a separate validation set\",\n        \"Never touch test set until final evaluation\"\n    ],\n    \n    \"Scaling and Encoding\": [\n        \"Fit transformers on training data only\",\n        \"Apply same transformation to test data\",\n        \"Save transformers for production\",\n        \"Choose appropriate encoding for each feature type\",\n        \"Handle unseen categories in test set\"\n    ],\n    \n    \"Feature Selection\": [\n        \"Remove highly correlated features\",\n        \"Use multiple selection methods\",\n        \"Validate with cross-validation\",\n        \"Consider computational cost vs performance\",\n        \"Keep features interpretable when possible\"\n    ],\n    \n    \"Validation\": [\n        \"Use cross-validation for robust estimates\",\n        \"Check for data leakage\",\n        \"Monitor for overfitting\",\n        \"Validate on out-of-time data for time series\",\n        \"Use appropriate metrics for the problem\"\n    ],\n    \n    \"Production\": [\n        \"Build reproducible pipelines\",\n        \"Version your feature engineering code\",\n        \"Monitor feature distributions in production\",\n        \"Handle missing values gracefully\",\n        \"Log and track feature importance\"\n    ]\n}\n\nfor category, practices in best_practices.items():\n    print(f\"\\n{category}:\")\n    for practice in practices:\n        print(f\"  ‚úì {practice}\")\n```\n\n### Common Mistakes to Avoid\n\n```python\nprint(\"\\n\\n‚ùå COMMON MISTAKES AND HOW TO AVOID THEM\")\nprint(\"=\" * 70)\n\nmistakes = [\n    {\n        \"mistake\": \"Fitting on entire dataset before splitting\",\n        \"why_bad\": \"Leaks information from test set into training\",\n        \"correct\": \"Always split first, then fit transformers on training only\"\n    },\n    {\n        \"mistake\": \"Using mean imputation for skewed data\",\n        \"why_bad\": \"Mean is sensitive to outliers and skews distribution\",\n        \"correct\": \"Use median for skewed data or distributions with outliers\"\n    },\n    {\n        \"mistake\": \"Label encoding nominal categorical features\",\n        \"why_bad\": \"Creates false ordering (Red=0, Blue=1 implies Blue>Red)\",\n        \"correct\": \"Use one-hot encoding for nominal features\"\n    },\n    {\n        \"mistake\": \"Not handling unseen categories in test set\",\n        \"why_bad\": \"Model fails in production on new categories\",\n        \"correct\": \"Use handle_unknown='ignore' or add 'Unknown' category\"\n    },\n    {\n        \"mistake\": \"Creating too many features without selection\",\n        \"why_bad\": \"Causes overfitting and slows training\",\n        \"correct\": \"Use feature selection to keep only useful features\"\n    },\n    {\n        \"mistake\": \"Not creating missing indicators\",\n        \"why_bad\": \"Loses information about missingness pattern\",\n        \"correct\": \"Create binary 'was_missing' features before imputation\"\n    },\n    {\n        \"mistake\": \"Using target encoding without cross-validation\",\n        \"why_bad\": \"Severe overfitting - model memorizes training data\",\n        \"correct\": \"Use cross-validation or leave-one-out encoding\"\n    },\n    {\n        \"mistake\": \"Removing all outliers without investigation\",\n        \"why_bad\": \"Might remove important patterns (fraud, rare events)\",\n        \"correct\": \"Understand why outliers exist, then decide to remove/cap/keep\"\n    },\n    {\n        \"mistake\": \"Not scaling features for distance-based algorithms\",\n        \"why_bad\": \"Features with larger ranges dominate the distance calculation\",\n        \"correct\": \"Always scale for KNN, SVM, K-Means, Neural Networks\"\n    },\n    {\n        \"mistake\": \"Using PCA before understanding features\",\n        \"why_bad\": \"Loses interpretability and might not improve performance\",\n        \"correct\": \"Try feature selection first, use PCA only if needed\"\n    }\n]\n\nfor i, mistake_dict in enumerate(mistakes, 1):\n    print(f\"\\n{i}. ‚ùå MISTAKE: {mistake_dict['mistake']}\")\n    print(f\"   ‚ö†Ô∏è  Why it's bad: {mistake_dict['why_bad']}\")\n    print(f\"   ‚úÖ Correct approach: {mistake_dict['correct']}\")\n```\n\n---\n\n## Final Summary: Your Feature Engineering Toolkit\n\n```python\nprint(\"\\n\\n\" + \"=\" * 70)\nprint(\"YOUR COMPLETE FEATURE ENGINEERING TOOLKIT\")\nprint(\"=\" * 70)\n\ntoolkit = {\n    \"Missing Values\": {\n        \"techniques\": [\"Mean/Median/Mode\", \"KNN\", \"MICE\", \"Forward/Backward Fill\"],\n        \"when\": \"Always check and handle before modeling\"\n    },\n    \"Outliers\": {\n        \"techniques\": [\"IQR\", \"Z-score\", \"Isolation Forest\", \"Cap/Remove/Transform\"],\n        \"when\": \"After understanding their cause\"\n    },\n    \"Scaling\": {\n        \"techniques\": [\"StandardScaler\", \"MinMaxScaler\", \"RobustScaler\"],\n        \"when\": \"For distance-based and gradient descent algorithms\"\n    },\n    \"Encoding\": {\n        \"techniques\": [\"Label\", \"One-Hot\", \"Target\", \"Frequency\", \"Binary\"],\n        \"when\": \"All categorical features must be encoded\"\n    },\n    \"Feature Creation\": {\n        \"techniques\": [\"Math operations\", \"Binning\", \"Interactions\", \"Domain features\"],\n        \"when\": \"After understanding data and domain\"\n    },\n    \"Date/Time\": {\n        \"techniques\": [\"Components\", \"Cyclical\", \"Lag\", \"Rolling stats\"],\n        \"when\": \"Any datetime column\"\n    },\n    \"Text\": {\n        \"techniques\": [\"TF-IDF\", \"Word embeddings\", \"Length features\"],\n        \"when\": \"Any text column\"\n    },\n    \"Feature Selection\": {\n        \"techniques\": [\"Filter\", \"Wrapper\", \"Embedded\", \"PCA\"],\n        \"when\": \"Too many features or overfitting\"\n    },\n    \"Imbalanced Data\": {\n        \"techniques\": [\"SMOTE\", \"Undersampling\", \"Class weights\"],\n        \"when\": \"Target class ratio >3:1\"\n    }\n}\n\nfor category, info in toolkit.items():\n    print(f\"\\n{category}:\")\n    print(f\"  Techniques: {', '.join(info['techniques'])}\")\n    print(f\"  When to use: {info['when']}\")\n\nprint(\"\\n\\n\" + \"=\" * 70)\nprint(\"üéì CONGRATULATIONS! YOU'VE COMPLETED THE ZERO TO HERO GUIDE!\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\nNext Steps:\n1. Practice on Kaggle competitions\n2. Apply techniques to your own projects\n3. Read research papers for advanced methods\n4. Build a portfolio showcasing your feature engineering skills\n5. Teach others - best way to solidify knowledge!\n\nRemember:\n- Feature engineering is both art and science\n- Domain knowledge is your superpower\n- Always validate your features with cross-validation\n- Simple features often outperform complex ones\n- Iterate and experiment - there's no perfect solution\n\nGood luck on your data science journey! üöÄ\n\"\"\")\n```\n\n---\n\n## üìö Additional Resources\n\n### Books\n- \"Feature Engineering for Machine Learning\" by Alice Zheng & Amanda Casari\n- \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n- \"Python Feature Engineering Cookbook\" by Soledad Galli\n\n### Online Courses\n- Kaggle Learn: Feature Engineering\n- Fast.ai: Practical Deep Learning\n- Coursera: Applied Machine Learning\n\n### Tools and Libraries\n- **pandas**: Data manipulation\n- **scikit-learn**: ML and preprocessing\n- **category_encoders**: Advanced encoding\n- **featuretools**: Automated feature engineering\n- **tsfresh**: Time series features\n- **missingno**: Missing data visualization\n\n### Kaggle Competitions for Practice\n- Titanic (beginner)\n- House Prices (intermediate)\n- Santander Customer Satisfaction (advanced)\n- IEEE-CIS Fraud Detection (expert)\n\n### Communities\n- Kaggle Forums\n- Reddit: r/MachineLearning, r/datascience\n- Stack Overflow\n- GitHub\n\n---\n\n## üéØ Final Project Exercise\n\n**Build a Complete ML Pipeline**\n\n1. Choose a dataset (Kaggle or your own)\n2. Perform comprehensive EDA\n3. Apply all relevant feature engineering techniques\n4. Build and evaluate multiple models\n5. Document your process\n6. Deploy your model\n\n**Checklist:**\n- [  ] Handle missing values appropriately\n- [  ] Detect and handle outliers\n- [  ] Encode categorical variables\n- [  ] Scale numerical features\n- [  ] Create new features\n- [  ] Select best features\n- [  ] Validate with cross-validation\n- [  ] Avoid data leakage\n- [  ] Document your decisions\n- [  ] Compare before/after feature engineering\n\n---\n\n**This is your complete Zero to Hero Feature Engineering Guide! Happy Learning! üéâ**\n"}